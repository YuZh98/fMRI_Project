{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "%matplotlib inline\n",
    "\n",
    "# Use a white background for matplotlib figures\n",
    "matplotlib.rcParams['figure.facecolor'] = '#ffffff'\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "random_seed = 2022\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5535519",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614f3ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_norm(lst):\n",
    "    '''Remove names starting with \"norm_fmri_img\"'''\n",
    "    lst.sort()\n",
    "    l = len(lst)\n",
    "    if l%2:\n",
    "        print('The list has an odd number of elements!')\n",
    "        return None\n",
    "    else:\n",
    "        take = l // 2\n",
    "        print(f'The length of the original list is {l}, and the lenth of the new list is {take}')\n",
    "        return lst[:take]\n",
    "    \n",
    "### CN ###\n",
    "dir_CN = '/blue/li.duan/zheng.yu/Research1/CN'\n",
    "CN_img_names = os.listdir(dir_CN)\n",
    "\n",
    "CN_img_names = remove_norm(CN_img_names) # Only keep main_images\n",
    "\n",
    "### AD ###\n",
    "dir_AD = '/blue/li.duan/zheng.yu/Research1/AD'\n",
    "AD_img_names = os.listdir(dir_AD)\n",
    "\n",
    "AD_img_names = remove_norm(AD_img_names) # Only keep main_images\n",
    "\n",
    "### LMCI ###\n",
    "# dir_LMCI = '/blue/li.duan/zheng.yu/Research1/LMCI'\n",
    "# LMCI_img_names = os.listdir(dir_LMCI)\n",
    "\n",
    "# LMCI_img_names = remove_norm(LMCI_img_names) # Only keep main_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf9458f",
   "metadata": {},
   "source": [
    "### CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558a076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data in CN group\n",
    "imgs_data = torch.tensor(nib.load(os.path.join(dir_CN, CN_img_names[0])).get_fdata()).unsqueeze(0).unsqueeze(0)\n",
    "for i in range(2,38,2):#range(3,39,2): (change to 1 above)\n",
    "    name = CN_img_names[i]\n",
    "    to_be_stacked = torch.tensor(nib.load(os.path.join(dir_CN, name)).get_fdata()).unsqueeze(0).unsqueeze(0)\n",
    "    to_be_stacked = to_be_stacked[...,:120] # All images must have the same times\n",
    "    imgs_data = torch.cat((imgs_data, to_be_stacked), dim=0)\n",
    "    print(imgs_data.shape)\n",
    "# imgs_data = []\n",
    "# for name in CN_img_names[:32]:\n",
    "#     imgs_data.append(nib.load(os.path.join(dir_CN, name)).get_fdata())\n",
    "# imgs_data = [nib.load(os.path.join(dir_CN, name)).get_fdata() for name in CN_img_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d333408",
   "metadata": {},
   "source": [
    "### AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecbdc91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data in AD group\n",
    "imgs_data = torch.tensor(nib.load(os.path.join(dir_AD, AD_img_names[0])).get_fdata()).unsqueeze(0).unsqueeze(0)\n",
    "for i in range(2,26,2):\n",
    "    name = AD_img_names[i]\n",
    "    to_be_stacked = torch.tensor(nib.load(os.path.join(dir_AD, name)).get_fdata()).unsqueeze(0).unsqueeze(0)\n",
    "    to_be_stacked = to_be_stacked[...,:120] # All images must have the same times\n",
    "    imgs_data = torch.cat((imgs_data, to_be_stacked), dim=0)\n",
    "    print(imgs_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902c1f54",
   "metadata": {},
   "source": [
    "### LMCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data in LMCI group\n",
    "imgs_data = torch.tensor(nib.load(os.path.join(dir_LMCI, LMCI_img_names[0])).get_fdata()).unsqueeze(0).unsqueeze(0)\n",
    "for name in LMCI_img_names[1:]:\n",
    "    to_be_stacked = torch.tensor(nib.load(os.path.join(dir_LMCI, name)).get_fdata()).unsqueeze(0).unsqueeze(0)\n",
    "    to_be_stacked = to_be_stacked[...,:120] # All images must have the same times\n",
    "    imgs_data = torch.cat((imgs_data, to_be_stacked), dim=0)\n",
    "    print(imgs_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a2ed16",
   "metadata": {},
   "source": [
    "### CN + AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f841a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data in CN group\n",
    "names = []\n",
    "for i in range(len(CN_img_names)//2):\n",
    "    if i==0:\n",
    "        names.append(CN_img_names[0])\n",
    "        imgs_data = torch.tensor(nib.load(os.path.join(dir_CN, CN_img_names[0])).get_fdata()).unsqueeze(0).unsqueeze(0)\n",
    "    else:\n",
    "        names.append(CN_img_names[2*i])\n",
    "        to_be_stacked = torch.tensor(nib.load(os.path.join(dir_CN, CN_img_names[2*i])).get_fdata()).unsqueeze(0).unsqueeze(0)\n",
    "        to_be_stacked = to_be_stacked[...,:120] # All images must have the same times\n",
    "        imgs_data = torch.cat((imgs_data, to_be_stacked), dim=0)\n",
    "        print(imgs_data.shape)\n",
    "print('CN finished!')\n",
    "for i in range(len(AD_img_names)//2):\n",
    "    names.append(AD_img_names[2*i])\n",
    "    to_be_stacked = torch.tensor(nib.load(os.path.join(dir_AD, AD_img_names[2*i])).get_fdata()).unsqueeze(0).unsqueeze(0)\n",
    "    to_be_stacked = to_be_stacked[...,:120] # All images must have the same times\n",
    "    imgs_data = torch.cat((imgs_data, to_be_stacked), dim=0)\n",
    "    print(imgs_data.shape)\n",
    "print('AD finished!')\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a798f573",
   "metadata": {},
   "source": [
    "# Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9af453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "def Data_Normalization(imgs_data):\n",
    "    '''imgs_data: train_size x 1 x . x . x . x time'''\n",
    "    '''mapping to [-1,1]'''\n",
    "    max_values = torch.amax(imgs_data, dim=(1,2,3,4))\n",
    "    min_values = torch.amin(imgs_data, dim=(1,2,3,4))\n",
    "    for i in range(imgs_data.shape[0]):\n",
    "        imgs_data[i] = 2 * ((imgs_data[i]-min_values[i])/(max_values[i]-min_values[i]) - 0.5)\n",
    "    return imgs_data, max_values, min_values\n",
    "\n",
    "\n",
    "# Data masking\n",
    "def mask_img2(img, mask):\n",
    "    '''masking an image; work even if there is nan'''\n",
    "    for i in range(img.size(-1)):\n",
    "        img[...,i][mask==0] = 0\n",
    "    return img\n",
    "\n",
    "def mask_img_list(lst, mask):\n",
    "    '''masking a tensor consisting of a bunch of images'''\n",
    "    for i in range(lst.shape[0]):\n",
    "        lst[i] = mask_img2(lst[i], mask)\n",
    "    return lst\n",
    "\n",
    "\n",
    "# # Replacing nan with 0\n",
    "# def replace_nan_with_0(img):\n",
    "#     '''replace nan entries of an image with 0'''\n",
    "#     img[torch.isnan(img)] = 0\n",
    "#     return img\n",
    "\n",
    "# def replace_nan_with_0_forlist(lst):\n",
    "#     '''apply replace_nan_with_0 to a list of images'''\n",
    "#     for i in range(len(lst)):\n",
    "#         lst[i] = replace_nan_with_0(lst[i])\n",
    "#     return lst\n",
    "\n",
    "\n",
    "# Truncate time\n",
    "def truncate_time(imgs_data, max_time=120):\n",
    "    for i in range(imgs_data.shape[0]):\n",
    "        imgs_data[i] = imgs_data[...,:max_time]\n",
    "    return imgs_data\n",
    "\n",
    "\n",
    "\n",
    "# Moving a list of tensors from GPU to CPU\n",
    "def to_cpu(lst):\n",
    "    return lst.cpu()\n",
    "\n",
    "\n",
    "# Helper functions for using GPU\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_default_device()\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True, dtype=torch.float)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e9ad57",
   "metadata": {},
   "source": [
    "# Some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a2306",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = imgs_data.shape[0]\n",
    "batch_size = 4\n",
    "latent_dim = 10\n",
    "z_dim = latent_dim\n",
    "print(f'train size:{train_size}, batch size:{batch_size}, latent dimension:{latent_dim}, z dimension:{z_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_z = 1.\n",
    "sig_h = 1.\n",
    "sig_x = 1.\n",
    "rho = 0.1\n",
    "lambda_z = 10.#e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98948194",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_time = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85f0168",
   "metadata": {},
   "source": [
    "# IDs & labels for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da064122",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs = torch.tensor(np.arange(train_size), dtype=torch.long)\n",
    "IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = torch.tensor(np.append(np.zeros(19), np.ones(13)), dtype=torch.long) # Only need this when using group-specific F but sharing other paramters\n",
    "Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b16dff",
   "metadata": {},
   "source": [
    "# Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_pipeline(imgs_data,\n",
    "                      data_normalization=True, \n",
    "                      include_IDs=True, \n",
    "                      masking=False,\n",
    "                      log_transform=False, epsilon=1e-6,\n",
    "                      replace_nan_with_0=False,\n",
    "                      move_to_cpu=False,\n",
    "                      trunc_time=False):\n",
    "    '''imgs_data: a tensor'''\n",
    "#     if trunc_time:\n",
    "#         imgs_data = truncate_time(imgs_data, 120)\n",
    "    if replace_nan_with_0:#******************\n",
    "        imgs_data = replace_nan_with_0_forlist(imgs_data)\n",
    "    if masking:\n",
    "        mask = torch.tensor(nib.load('mask.nii.gz').get_fdata()).unsqueeze(0)\n",
    "        imgs_data = mask_img_list(imgs_data, mask)\n",
    "    if log_transform:#******************\n",
    "        imgs_data = [torch.log(item+epsilon) for item in imgs_data]\n",
    "    if data_normalization:\n",
    "        imgs_data, max_values, min_values = Data_Normalization(imgs_data)\n",
    "    if move_to_cpu:\n",
    "        imgs_data = to_cpu(imgs_data)\n",
    "    if include_IDs:\n",
    "        imgs_data = [[imgs_data[i], IDs[i]] for i in range(imgs_data.shape[0])]\n",
    "    return imgs_data, max_values, min_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff2b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, max_value, min_value = training_pipeline(imgs_data)\n",
    "del imgs_data\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9864ca3d",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfff369",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device) # Make sure we are using GPU\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, generator=torch.Generator(device='cuda'))\n",
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "\n",
    "test_loader = DataLoader(train_ds, 1, shuffle=False)\n",
    "test_loader = DeviceDataLoader(test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b99f53",
   "metadata": {},
   "source": [
    "# Initializing $F$, $z^s$ and $h_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd9e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F^c (c=0: CN, c=1: AD)\n",
    "# num_group = 2\n",
    "# F_mat = torch.rand(num_group, latent_dim, latent_dim)\n",
    "F_mat = torch.rand(latent_dim, latent_dim)\n",
    "F_mat_original = F_mat.clone().detach()\n",
    "print(f'Shape of F:{F_mat.shape}, device:{F_mat.device}')\n",
    "\n",
    "# z^s\n",
    "z_vectors = torch.normal(mean=torch.zeros((train_size, z_dim)), std=sig_z).clone().detach().requires_grad_(True)\n",
    "z_vectors_original = z_vectors.clone().detach()\n",
    "#z_vectors = sig_z * torch.randn(train_size, z_dim, requires_grad=True)\n",
    "print(f'Shape of z:{z_vectors.shape}, device:{z_vectors.device}')\n",
    "\n",
    "# h_0\n",
    "h0 = torch.rand(1, latent_dim)\n",
    "#h0 = sig_h * torch.randn(1, latent_dim)\n",
    "print(f'Shape of h_0:{h0.shape}, device:{h0.device}')\n",
    "print(f'h0:{h0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c0727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in F from a saved file\n",
    "F_mat = torch.load(os.path.join('Recorded', 'F_mat_AD.pt'))\n",
    "\n",
    "# z^s\n",
    "z_vectors = torch.normal(mean=torch.zeros((train_size, z_dim)), std=sig_z).clone().detach().requires_grad_(True)\n",
    "z_vectors_original = z_vectors.clone().detach()\n",
    "#z_vectors = sig_z * torch.randn(train_size, z_dim, requires_grad=True)\n",
    "print(f'Shape of z:{z_vectors.shape}, device:{z_vectors.device}')\n",
    "\n",
    "# h_0\n",
    "h0 = torch.rand(1, latent_dim)\n",
    "#h0 = sig_h * torch.randn(1, latent_dim)\n",
    "print(f'Shape of h_0:{h0.shape}, device:{h0.device}')\n",
    "print(f'h0:{h0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "h0 = torch.tensor([[0.1719, 0.1631, 0.8127, 0.3485, 0.5798, 0.9957, 0.4949, 0.0442, 0.1098, 0.7062]])\n",
    "h0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dffaac",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59877908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RecVAEModel(nn.Module):\n",
    "#     def __init__(self, enc_out_dim=100, latent_dim=latent_dim, sqr_sig_x=sig_x, sqr_sig_h=sig_h, \n",
    "#                  lambda_z=lambda_z, F_mat=F_mat):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.sqr_sig_x = sqr_sig_x\n",
    "#         self.sqr_sig_h = sqr_sig_h\n",
    "#         self.latent_dim = latent_dim\n",
    "#         self.lambda_z = lambda_z\n",
    "#         self.F_mat = F_mat\n",
    "        \n",
    "#         # Encoder: from input(x) to one of the inputs of the hidden layer (enc_x)\n",
    "#         # input: 1 x 91 x 109 x 91\n",
    "#         self.encoder1 = nn.Sequential(\n",
    "#             nn.Conv3d(1, 4, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.BatchNorm3d(4),\n",
    "#             nn.LeakyReLU(0.2, inplace = True)) # output: 4 x 45 x 54 x 45\n",
    "        \n",
    "#         self.encoder2 = nn.Sequential(\n",
    "#             nn.Conv3d(4, 8, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.BatchNorm3d(8),\n",
    "#             nn.LeakyReLU(0.2, inplace = True)) # output: 8 x 22 x 27 x 22\n",
    "        \n",
    "#         self.encoder3 = nn.Sequential(\n",
    "#             nn.Conv3d(8, 16, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.BatchNorm3d(16),\n",
    "#             nn.LeakyReLU(0.2, inplace = True)) # output: 16 x 11 x 13 x 11\n",
    "        \n",
    "#         self.encoder4 = nn.Sequential(\n",
    "#             nn.Conv3d(16, 32, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.BatchNorm3d(32),\n",
    "#             nn.LeakyReLU(0.2, inplace = True)) # output: 32 x 5 x 6 x 5\n",
    "        \n",
    "#         self.encoder5 = nn.Sequential(\n",
    "#             nn.Flatten(), \n",
    "#             nn.Linear(32*5*6*5, enc_out_dim),\n",
    "#             nn.Tanh()) # output: enc_out_dim\n",
    "        \n",
    "        \n",
    "#         # Hidden: from (enc_x,h_{t-1}) to h_t\n",
    "#         self.hidden2mu = nn.Linear(enc_out_dim+latent_dim, latent_dim)\n",
    "#         self.hidden2log_var = nn.Linear(enc_out_dim+latent_dim, latent_dim)\n",
    "        \n",
    "#         # Decoder: from h_t to mu_t\n",
    "#         self.decoder1 = nn.Sequential(\n",
    "#             nn.Linear(latent_dim, 32*5*6*5),\n",
    "#             nn.Unflatten(1, (32, 5, 6, 5)),\n",
    "#             nn.BatchNorm3d(32),\n",
    "#             nn.LeakyReLU(0.2, inplace = True))\n",
    "          \n",
    "#         self.decoder2 = nn.Sequential(\n",
    "#             nn.ConvTranspose3d(32, 16, kernel_size=4, stride=2, padding=1, output_padding=1, bias=False), \n",
    "#             nn.BatchNorm3d(16),\n",
    "#             nn.LeakyReLU(0.2, inplace = True))\n",
    "        \n",
    "#         self.decoder3 = nn.Sequential(\n",
    "#             nn.ConvTranspose3d(16, 8, kernel_size=4, stride=2, padding=1, output_padding=(0,1,0), bias=False), \n",
    "#             nn.BatchNorm3d(8),\n",
    "#             nn.LeakyReLU(0.2, inplace = True))\n",
    "        \n",
    "#         self.decoder4 = nn.Sequential(\n",
    "#             nn.ConvTranspose3d(8, 4, kernel_size=4, stride=2, padding=1, output_padding=(1,0,1), bias=False), \n",
    "#             nn.BatchNorm3d(4),\n",
    "#             nn.LeakyReLU(0.2, inplace = True))\n",
    "        \n",
    "#         self.decoder5 = nn.Sequential(\n",
    "#             nn.ConvTranspose3d(4, 1, kernel_size=4, stride=2, padding=1, output_padding=1, bias=False), \n",
    "#             nn.Tanh())\n",
    "        \n",
    "#         # F\n",
    "#         # self.F = nn.Linear(latent_dim, latent_dim, bias=False)\n",
    "#         # self.F_AD = nn.Linear(latent_dim, latent_dim, bias=False)\n",
    "#         # self.F_CN = nn.Linear(latent_dim, latent_dim, bias=False)\n",
    "                   \n",
    "        \n",
    "       \n",
    "#     def g_transform(self, h_old, batch_labels):\n",
    "#         # h_old shape: batch_size(cunrrently) x latent_dim\n",
    "#         # batch_labels: 0 -> CN, 1 -> AD\n",
    "#         # h_new = torch.matmul(h_old, F_mat)\n",
    "#         h_old = h_old.unsqueeze(1)\n",
    "#         h_new = torch.zeros(len(batch_labels), latent_dim)\n",
    "#         for i in range(len(batch_labels)):\n",
    "#             h_new[i] = torch.matmul(self.F_mat[batch_labels[i]], h_old[i].transpose(0,1)).transpose(0,1) # 1 x latent_dim\n",
    "#         return h_new\n",
    "    \n",
    "    \n",
    "#     def encode(self, x):\n",
    "#         enc_x = self.encoder1(x)\n",
    "#         enc_x = self.encoder2(enc_x)\n",
    "#         enc_x = self.encoder3(enc_x)\n",
    "#         enc_x = self.encoder4(enc_x)\n",
    "#         enc_x = self.encoder5(enc_x)\n",
    "#         return enc_x\n",
    "    \n",
    "    \n",
    "#     def decode(self, h):\n",
    "#         dec_h = self.decoder1(h)\n",
    "#         dec_h = self.decoder2(dec_h)\n",
    "#         dec_h = self.decoder3(dec_h)\n",
    "#         dec_h = self.decoder4(dec_h)\n",
    "#         dec_h = self.decoder5(dec_h)\n",
    "#         return dec_h\n",
    "        \n",
    "    \n",
    "#     def reparametrize(self, mu_h,log_var_h):\n",
    "#         # Reparametrization Trick to allow gradients to backpropagate from the stochastic part of the model\n",
    "#         sigma_h = torch.exp(log_var_h / 2)\n",
    "#         z = torch.randn(size = (mu_h.size(0),mu_h.size(1)))\n",
    "#         # z = z.type_as(mu_h) # Setting z to be .cuda when using GPU training\n",
    "#         return mu_h + sigma_h*z\n",
    "   \n",
    "  \n",
    "#     def updating_F(self, h_history_history, h0, rho):\n",
    "#         # h_history_history: train_size x tol_time x latent_dim\n",
    "#         rhoI = 2*sig_h**2*rho * torch.eye(latent_dim)\n",
    "        \n",
    "#         for c in range(num_group):\n",
    "#             h_his_his_for_single_group = h_history_history[Labels==c]\n",
    "#             Y_tilde = h_his_his_for_single_group.view(-1, latent_dim) # output: train_size*tol_time x latent_dim\n",
    "#             for i in range(torch.sum(Labels==c)):\n",
    "#                 new_h_history = torch.cat((h0, h_his_his_for_single_group[i][0:(tol_time-1)]), 0)\n",
    "#                 h_his_his_for_single_group[i] = new_h_history\n",
    "#             X_tilde = h_his_his_for_single_group.view(-1, latent_dim) # output: train_size*tol_time x latent_dim\n",
    "            \n",
    "#             # updating F\n",
    "#             XX_tilde = torch.matmul(X_tilde.transpose(0,1), X_tilde)\n",
    "#             XY_tilde = torch.matmul(X_tilde.transpose(0,1), Y_tilde)\n",
    "#             self.F_mat[c] = torch.linalg.solve(XX_tilde+rhoI, XY_tilde).transpose(0,1)\n",
    "        \n",
    "    \n",
    "#     def VAE(self, x, h, which_ones):\n",
    "#         # encode x and h to get the mu and variance parameters for the latent space\n",
    "#         enc_x = self.encode(x)\n",
    "#         combined = torch.cat((enc_x, h), 1)\n",
    "#         mu_h, log_var_h = self.hidden2mu(combined), self.hidden2log_var(combined)\n",
    "        \n",
    "#         # sample h\n",
    "#         h = self.reparametrize(mu_h, log_var_h)\n",
    "        \n",
    "#         # add z^s\n",
    "#         h_tilde = h + z_vectors[which_ones,:]\n",
    "        \n",
    "#         # decode\n",
    "#         mu = self.decode(h_tilde)\n",
    "#         return mu, h\n",
    "    \n",
    "    \n",
    "#     def training_step(self, batch, h_0, which_ones, batch_labels):\n",
    "#         '''h_0: batch_size x latent_dim'''\n",
    "#         x_list, mu_history, h_history, gh_history = self(batch, h_0, which_ones, batch_labels)\n",
    "        \n",
    "#         temp = 2 * batch.size(0) * len(h_history)\n",
    "#         # calculate loss\n",
    "#         loss1 = sum([torch.sum(torch.pow(x-mu, 2)) for x, mu in zip(x_list, mu_history)])\n",
    "#         loss1 = loss1 / self.sqr_sig_x / temp\n",
    "        \n",
    "#         loss2 = sum([torch.sum(torch.pow(h-gh, 2)) for h, gh in zip(h_history, gh_history)])\n",
    "#         loss2 = loss2 / self.sqr_sig_h / temp\n",
    "        \n",
    "#         loss_F = rho * torch.sum(torch.pow(self.F_mat, 2))\n",
    "#         #loss_F = rho * loss_F / temp\n",
    "        \n",
    "#         #loss_z = torch.sum(torch.pow(z_vectors, 2)) ### L2->L1\n",
    "#         loss_z = torch.sum(torch.abs(z_vectors))\n",
    "#         loss_z = self.lambda_z * loss_z\n",
    "#         #loss3 = F.l1_loss(z_vectors, torch.zeros_like(z_vectors), reduction='sum')\n",
    "        \n",
    "#         loss = loss1 + loss2 + loss_z\n",
    "        \n",
    "#         return loss, {'loss1':loss1, 'loss2':loss2, 'loss_F':loss_F, 'loss_z':loss_z}, h_history\n",
    "    \n",
    "    \n",
    "#     def forward(self, x, h_0, which_ones, batch_labels):\n",
    "#         # which_ones is a list containing the IDs of all subjects in the current batch\n",
    "#         #tol_time = x.size(-1) # x is of size batch_size*channel*x1*x2*x3*tol_time\n",
    "#         x_list = [x[:,:,:,:,:,t] for t in range(tol_time)]\n",
    "#         del x\n",
    "#         h = h_0\n",
    "#         h_history = []\n",
    "#         gh_history = []\n",
    "#         mu_history = []\n",
    "#         for t in range(tol_time):\n",
    "#             gh_history.append(self.g_transform(h, batch_labels))\n",
    "#             #print(f't={t}:{torch.max(self.g_transform(h, batch_labels))}')\n",
    "#             mu, h = self.VAE(x_list[t], h, which_ones)\n",
    "#             #print(f't={t}:{torch.max(h)}')\n",
    "#             h_history.append(h)\n",
    "#             mu_history.append(mu)\n",
    "#         return x_list, mu_history, h_history, gh_history\n",
    "    \n",
    "\n",
    "# model = RecVAEModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17474aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecVAEModel(nn.Module):\n",
    "    def __init__(self, enc_out_dim=100, latent_dim=latent_dim, sqr_sig_x=sig_x, sqr_sig_h=sig_h, \n",
    "                 lambda_z=lambda_z, F_mat=F_mat):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sqr_sig_x = sqr_sig_x\n",
    "        self.sqr_sig_h = sqr_sig_h\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lambda_z = lambda_z\n",
    "        self.F_mat = F_mat\n",
    "        \n",
    "        # Encoder: from input(x) to one of the inputs of the hidden layer (enc_x)\n",
    "        # input: 1 x 91 x 109 x 91\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv3d(1, 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(4),\n",
    "            nn.LeakyReLU(0.2, inplace = True)) # output: 4 x 45 x 54 x 45\n",
    "        \n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv3d(4, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(8),\n",
    "            nn.LeakyReLU(0.2, inplace = True)) # output: 8 x 22 x 27 x 22\n",
    "        \n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv3d(8, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.LeakyReLU(0.2, inplace = True)) # output: 16 x 11 x 13 x 11\n",
    "        \n",
    "        self.encoder4 = nn.Sequential(\n",
    "            nn.Conv3d(16, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(0.2, inplace = True)) # output: 32 x 5 x 6 x 5\n",
    "        \n",
    "        self.encoder5 = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(32*5*6*5, enc_out_dim),\n",
    "            nn.Tanh()) # output: enc_out_dim\n",
    "        \n",
    "        \n",
    "        # Hidden: from (enc_x,h_{t-1}) to h_t\n",
    "        self.hidden2mu = nn.Linear(enc_out_dim+latent_dim, latent_dim)\n",
    "        self.hidden2log_var = nn.Linear(enc_out_dim+latent_dim, latent_dim)\n",
    "        \n",
    "        # Decoder: from h_t to mu_t\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32*5*6*5),\n",
    "            nn.Unflatten(1, (32, 5, 6, 5)),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(0.2, inplace = True))\n",
    "          \n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(32, 16, kernel_size=4, stride=2, padding=1, output_padding=1, bias=False), \n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.LeakyReLU(0.2, inplace = True))\n",
    "        \n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(16, 8, kernel_size=4, stride=2, padding=1, output_padding=(0,1,0), bias=False), \n",
    "            nn.BatchNorm3d(8),\n",
    "            nn.LeakyReLU(0.2, inplace = True))\n",
    "        \n",
    "        self.decoder4 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(8, 4, kernel_size=4, stride=2, padding=1, output_padding=(1,0,1), bias=False), \n",
    "            nn.BatchNorm3d(4),\n",
    "            nn.LeakyReLU(0.2, inplace = True))\n",
    "        \n",
    "        self.decoder5 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(4, 1, kernel_size=4, stride=2, padding=1, output_padding=1, bias=False), \n",
    "            nn.Tanh())\n",
    "        \n",
    "        # F\n",
    "        # self.F = nn.Linear(latent_dim, latent_dim, bias=False)\n",
    "        # self.F_AD = nn.Linear(latent_dim, latent_dim, bias=False)\n",
    "        # self.F_CN = nn.Linear(latent_dim, latent_dim, bias=False)\n",
    "                   \n",
    "        \n",
    "       \n",
    "    def g_transform(self, h_old):\n",
    "        # h_old shape: batch_size(cunrrently) x latent_dim\n",
    "        return torch.matmul(h_old, self.F_mat.transpose(0,1))\n",
    "    \n",
    "    \n",
    "    def encode(self, x):\n",
    "        enc_x = self.encoder1(x)\n",
    "        enc_x = self.encoder2(enc_x)\n",
    "        enc_x = self.encoder3(enc_x)\n",
    "        enc_x = self.encoder4(enc_x)\n",
    "        enc_x = self.encoder5(enc_x)\n",
    "        return enc_x\n",
    "    \n",
    "    \n",
    "    def decode(self, h):\n",
    "        dec_h = self.decoder1(h)\n",
    "        dec_h = self.decoder2(dec_h)\n",
    "        dec_h = self.decoder3(dec_h)\n",
    "        dec_h = self.decoder4(dec_h)\n",
    "        dec_h = self.decoder5(dec_h)\n",
    "        return dec_h\n",
    "        \n",
    "    \n",
    "    def reparametrize(self, mu_h,log_var_h):\n",
    "        # Reparametrization Trick to allow gradients to backpropagate from the stochastic part of the model\n",
    "        sigma_h = torch.exp(log_var_h / 2)\n",
    "        z = torch.randn(size = (mu_h.size(0),mu_h.size(1)))\n",
    "        # z = z.type_as(mu_h) # Setting z to be .cuda when using GPU training\n",
    "        return mu_h + sigma_h*z\n",
    "   \n",
    "  \n",
    "    def updating_F(self, h_history_history, h0, rho):\n",
    "        # h_history_history: train_size x tol_time x latent_dim\n",
    "        rhoI = 2*sig_h**2*rho * torch.eye(latent_dim)\n",
    "        \n",
    "        Y_tilde = h_history_history.view(-1, latent_dim) # output: train_size*tol_time x latent_dim\n",
    "        for i in range(train_size):\n",
    "            new_h_history = torch.cat((h0, h_history_history[i][0:(tol_time-1)]), 0)\n",
    "            h_history_history[i] = new_h_history\n",
    "        X_tilde = h_history_history.view(-1, latent_dim) # output: train_size*tol_time x latent_dim\n",
    "            \n",
    "        # updating F\n",
    "        XX_tilde = torch.matmul(X_tilde.transpose(0,1), X_tilde)\n",
    "        XY_tilde = torch.matmul(X_tilde.transpose(0,1), Y_tilde)\n",
    "        self.F_mat = torch.linalg.solve(XX_tilde+rhoI, XY_tilde).transpose(0,1)\n",
    "        \n",
    "    \n",
    "    def VAE(self, x, h, which_ones):\n",
    "        # encode x and h to get the mu and variance parameters for the latent space\n",
    "        enc_x = self.encode(x)\n",
    "        combined = torch.cat((enc_x, h), 1)\n",
    "        mu_h, log_var_h = self.hidden2mu(combined), self.hidden2log_var(combined)\n",
    "        \n",
    "        # sample h\n",
    "        h = self.reparametrize(mu_h, log_var_h)\n",
    "        \n",
    "        # add z^s\n",
    "        h_tilde = h + z_vectors[which_ones,:]\n",
    "        \n",
    "        # decode\n",
    "        mu = self.decode(h_tilde)\n",
    "        return mu, h\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, h_0, which_ones):\n",
    "        '''h_0: batch_size x latent_dim'''\n",
    "        x_list, mu_history, h_history, gh_history = self(batch, h_0, which_ones)\n",
    "        \n",
    "        temp = 2 * batch.size(0) * len(h_history)\n",
    "        # calculate loss\n",
    "        loss1 = sum([torch.sum(torch.pow(x-mu, 2)) for x, mu in zip(x_list, mu_history)])\n",
    "        loss1 = loss1 / self.sqr_sig_x / temp\n",
    "        \n",
    "        loss2 = sum([torch.sum(torch.pow(h-gh, 2)) for h, gh in zip(h_history, gh_history)])\n",
    "        loss2 = loss2 / self.sqr_sig_h / temp\n",
    "        \n",
    "        loss_F = rho * torch.sum(torch.pow(self.F_mat, 2))\n",
    "        #loss_F = rho * loss_F / temp\n",
    "        \n",
    "        #loss_z = torch.sum(torch.pow(z_vectors, 2)) ### L2->L1\n",
    "        loss_z = torch.sum(torch.abs(z_vectors))\n",
    "        loss_z = self.lambda_z * loss_z\n",
    "        #loss3 = F.l1_loss(z_vectors, torch.zeros_like(z_vectors), reduction='sum')\n",
    "        \n",
    "        loss = loss1 + loss2 + loss_z\n",
    "        \n",
    "        return loss, {'loss1':loss1, 'loss2':loss2, 'loss_F':loss_F, 'loss_z':loss_z}, h_history\n",
    "    \n",
    "    \n",
    "    def forward(self, x, h_0, which_ones):\n",
    "        # which_ones is a list containing the IDs of all subjects in the current batch\n",
    "        #tol_time = x.size(-1) # x is of size batch_size*channel*x1*x2*x3*tol_time\n",
    "        x_list = [x[:,:,:,:,:,t] for t in range(tol_time)]\n",
    "        del x\n",
    "        h = h_0\n",
    "        h_history = []\n",
    "        gh_history = []\n",
    "        mu_history = []\n",
    "        for t in range(tol_time):\n",
    "            gh_history.append(self.g_transform(h))\n",
    "            #print(f't={t}:{torch.max(self.g_transform(h, batch_labels))}')\n",
    "            mu, h = self.VAE(x_list[t], h, which_ones)\n",
    "            #print(f't={t}:{torch.max(h)}')\n",
    "            h_history.append(h)\n",
    "            mu_history.append(mu)\n",
    "        return x_list, mu_history, h_history, gh_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e7a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CN = RecVAEModel(F_mat=F_mat)\n",
    "#model_AD = RecVAEModel(F_mat=F_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71acd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit(epochs, lr, h0, model, rho=rho, train_loader=train_loader, opt_func=torch.optim.SGD):\n",
    "#     train_loss_history = []\n",
    "#     #optimizer = opt_func(model.parameters(), lr)\n",
    "#     optimizer = opt_func([{'params': model.parameters()}, {'params': z_vectors}], lr)\n",
    "#     h_history_history = torch.zeros(train_size, tol_time, latent_dim)\n",
    "#     for epoch in range(epochs):\n",
    "#         # Training Phase \n",
    "#         model.train()\n",
    "#         for batch, batch_index in train_loader:\n",
    "#             # print(f'z: {z_vectors}')\n",
    "#             h_0 = h0.expand(batch.size(0), -1) # h0: 1 x latent_dim, h_0: batch_size x latent_dim\n",
    "#             which_ones = IDs[batch_index.to(int)]\n",
    "#             batch_labels = Labels[batch_index.to(int)]\n",
    "        \n",
    "#             loss, loss_dic, h_history = model.training_step(batch, h_0, which_ones, batch_labels)\n",
    "        \n",
    "#             # Updating h_history_history\n",
    "#             h_history_history[which_ones] = torch.stack(h_history).transpose(0,1) # output: batch_size x tol_time x latent_dim\n",
    "            \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#         if True:#epoch%10==0:\n",
    "#             train_loss_history.append(loss)\n",
    "#             print(\"Epoch [{}]: train loss: {:.2f} with loss1: {:.2f}, loss2: {:.2f}, loss_z: {:.4f}, loss_F: {:.4f}\"\n",
    "#                   .format(epoch, loss, loss_dic['loss1'], loss_dic['loss2'], loss_dic['loss_z'], loss_dic['loss_F']))\n",
    "        \n",
    "#         # Updating F\n",
    "#         with torch.no_grad():\n",
    "#             model.updating_F(h_history_history, h0, rho)\n",
    "#     return {'train_loss_history': train_loss_history, 'all_h_history': h_history_history, 'last_index': which_ones}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, h0, model, rho=rho, train_loader=train_loader, opt_func=torch.optim.SGD):\n",
    "    train_loss_history = []\n",
    "    #optimizer = opt_func(model.parameters(), lr)\n",
    "    optimizer = opt_func([{'params': model.parameters()}, {'params': z_vectors}], lr)\n",
    "    h_history_history = torch.zeros(train_size, tol_time, latent_dim)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        for batch, batch_index in train_loader:\n",
    "            # print(f'z: {z_vectors}')\n",
    "            h_0 = h0.expand(batch.size(0), -1) # h0: 1 x latent_dim, h_0: batch_size x latent_dim\n",
    "            which_ones = IDs[batch_index.to(int)]\n",
    "        \n",
    "            loss, loss_dic, h_history = model.training_step(batch, h_0, which_ones)\n",
    "        \n",
    "            # Updating h_history_history\n",
    "            h_history_history[which_ones] = torch.stack(h_history).transpose(0,1) # output: batch_size x tol_time x latent_dim\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        if True:#epoch%10==0:\n",
    "            train_loss_history.append(loss)\n",
    "            print(\"Epoch [{}]: train loss: {:.2f} with loss1: {:.2f}, loss2: {:.2f}, loss_z: {:.4f}, loss_F: {:.4f}\"\n",
    "                  .format(epoch, loss, loss_dic['loss1'], loss_dic['loss2'], loss_dic['loss_z'], loss_dic['loss_F']))\n",
    "        \n",
    "        # Updating F\n",
    "        with torch.no_grad():\n",
    "            model.updating_F(h_history_history, h0, rho)\n",
    "    return {'train_loss_history': train_loss_history, 'all_h_history': h_history_history, 'last_index': which_ones}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ffe203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = fit(epochs=500, lr=1e-6, h0=h0, model=model_CN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a643e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = fit(epochs=500, lr=1e-6, h0=h0, model=model_AD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd15f3",
   "metadata": {},
   "source": [
    "# Save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec208e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_CN, os.path.join('Recorded', 'model_CN_half.pt'))\n",
    "torch.save(model_CN.F_mat, os.path.join('Recorded', 'F_mat_CN.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a3a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_AD = torch.load(os.path.join('Recorded', 'model_AD_half.pt'))\n",
    "# model_CN.eval() \n",
    "'''Remember that you must call model.eval() to set dropout and batch normalization layers \n",
    "to evaluation mode before running inference. Failing to do this will yield inconsistent inference results.\n",
    "** We will call .eval() in the function named evaluate.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc638e",
   "metadata": {},
   "source": [
    "# Save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af647b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tocompare = 60 # the timestamp on which we are to do comparison\n",
    "\n",
    "def save_image(mu_history, name, which_one, t=t_tocompare):\n",
    "    maxim = torch.Tensor.cpu(max_value[which_one,t]).detach().numpy()\n",
    "    minim = torch.Tensor.cpu(min_value[which_one,t]).detach().numpy()\n",
    "    one_mu = torch.Tensor.cpu(mu_history[t]).detach().numpy()\n",
    "    one_mu = one_mu[0,:,:,:,:]\n",
    "    one_mu = (one_mu * .5 + .5) * (maxim - minim) + minim\n",
    "    img_new = nib.Nifti1Image(one_mu, np.eye(4))\n",
    "    nib.save(img_new, os.path.join('Generated', name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd514e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(file_position, data_dir = './Generated'):\n",
    "    return os.listdir(data_dir)[file_position]\n",
    "\n",
    "\n",
    "def show_slices(slices):\n",
    "   \"\"\" Function to display row of image slices \"\"\"\n",
    "   fig, axes = plt.subplots(1, len(slices))\n",
    "   for i, slice in enumerate(slices):\n",
    "       axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")\n",
    "\n",
    "\n",
    "def get_plot(file_name, data_dir = './Generated', plot_name = 'Slices', x1=40, x2=40, x3=40, t=t_tocompare):\n",
    "    img = nib.load(os.path.join(data_dir, file_name))\n",
    "    img_data = img.get_fdata()\n",
    "    if data_dir == './Dataset':\n",
    "        show_slices([img_data[x1,:,:,t], img_data[:,x2,:,t], img_data[:,:,x3,t]])\n",
    "        plt.suptitle(plot_name)\n",
    "    else:\n",
    "        show_slices([img_data[0,x1,:,:], img_data[0,:,x2,:], img_data[0,:,:,x3]])\n",
    "        plt.suptitle(plot_name) \n",
    "        \n",
    "def get_plot_from_blue(img_name, file_dir = 'CN', plot_name = 'Slices', \n",
    "             x1=40, x2=40, x3=40, t=60):\n",
    "    '''Plot images from blue'''\n",
    "    data_dir = os.path.join('/blue/li.duan/zheng.yu/Research1/', file_dir, img_name)\n",
    "    img = nib.load(data_dir)\n",
    "    img_data = img.get_fdata()\n",
    "    print(f'Shape:{img_data.shape}')\n",
    "    show_slices([img_data[x1,:,:,t], img_data[:,x2,:,t], img_data[:,:,x3,t]])\n",
    "    plt.suptitle(plot_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e45090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, x, h0, which_ones):\n",
    "    model.eval()\n",
    "    # x = x.unsqueeze(0)\n",
    "    # x = to_device(x, device)\n",
    "    x_list, mu_history, h_history, gh_history = model(x, h0, which_ones)\n",
    "    return x_list, mu_history, h_history, gh_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b76d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_dict_to_json(dict, name):\n",
    "    # create json object from dictionary\n",
    "    j_dict = json.dumps(dict)\n",
    "\n",
    "    # open file for writing, \"w\" \n",
    "    f = open(os.path.join('Recorded', name+\".json\"),\"w\")\n",
    "\n",
    "    # write json object to file\n",
    "    f.write(j_dict)\n",
    "\n",
    "    # close file\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c851c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plot('Reconstructedmainimage_002_S_4225_2012-10-11.nii.gz.nii.gz', plot_name = 'Reconstructed')\n",
    "get_plot_from_blue('mainimage_002_S_4225_2012-10-11.nii.gz', file_dir = 'CN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec927c9",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e55b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting z to be 0\n",
    "z_vectors = torch.zeros(38, z_vectors.size(1)) # CN\n",
    "#z_vectors = torch.zeros(26, z_vectors.size(1)) # AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0eceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_F_CN_for_CN = {'loss1':[],'loss2':[], 'tol_loss':[]}\n",
    "losses_F_CN_for_CN['loss_F'] = rho * torch.sum(torch.pow(model_CN.F_mat, 2)).cpu().detach().numpy()\n",
    "#losses_F_CN_for_CN['loss_z'] = lambda_z * torch.sum(torch.abs(z_vectors)).cpu().detach().numpy()\n",
    "for test_batch, test_batch_index in test_loader:\n",
    "    test_batch = test_batch.to(torch.float32)\n",
    "    h_0 = h0.expand(test_batch.size(0), -1) # h0: 1 x latent_dim, h_0: batch_size x latent_dim\n",
    "    which_ones = IDs[test_batch_index.to(int)]\n",
    "    test_x_list, test_mu_history, test_h_history, test_gh_history = evaluate(model_CN, test_batch, h_0, which_ones)\n",
    "    \n",
    "    # calculate loss\n",
    "    temp = 2 * test_batch.size(0) * tol_time\n",
    "    loss1 = sum([torch.sum(torch.pow(x-mu, 2)) for x, mu in zip(test_x_list, test_mu_history)])\n",
    "    loss1 = loss1 / (sig_x**2) / temp\n",
    "        \n",
    "    loss2 = sum([torch.sum(torch.pow(h-gh, 2)) for h, gh in zip(test_h_history, test_gh_history)])\n",
    "    loss2 = loss2 / (sig_h**2) / temp\n",
    "    \n",
    "    losses_F_CN_for_CN['loss1'].append(loss1.item())\n",
    "    losses_F_CN_for_CN['loss2'].append(loss2.item())\n",
    "    losses_F_CN_for_CN['tol_loss'].append((loss1+loss2+losses_F_CN_for_CN['loss_F']).item())\n",
    "    \n",
    "    # Saving images\n",
    "    save_image(mu_history=test_mu_history, \n",
    "               name='CN_test_Reconstructed_by_model_CN_half' + CN_img_names[which_ones], #Pay attention to the name when un-commenting this line\n",
    "               which_one=which_ones)\n",
    "    \n",
    "save_dict_to_json(losses_F_CN_for_CN, 'losses_F_CN_for_CN_test')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_F_CN_for_AD = {'loss1':[],'loss2':[], 'tol_loss':[]}\n",
    "losses_F_CN_for_AD['loss_F'] = rho * torch.sum(torch.pow(model_CN.F_mat, 2)).cpu().detach().numpy()\n",
    "for test_batch, test_batch_index in test_loader:\n",
    "    test_batch = test_batch.to(torch.float32)\n",
    "    h_0 = h0.expand(test_batch.size(0), -1) # h0: 1 x latent_dim, h_0: batch_size x latent_dim\n",
    "    which_ones = IDs[test_batch_index.to(int)]\n",
    "    test_x_list, test_mu_history, test_h_history, test_gh_history = evaluate(model_CN, test_batch, h_0, which_ones)\n",
    "    \n",
    "    # calculate loss\n",
    "    temp = 2 * test_batch.size(0) * tol_time\n",
    "    loss1 = sum([torch.sum(torch.pow(x-mu, 2)) for x, mu in zip(test_x_list, test_mu_history)])\n",
    "    loss1 = loss1 / (sig_x**2) / temp\n",
    "        \n",
    "    loss2 = sum([torch.sum(torch.pow(h-gh, 2)) for h, gh in zip(test_h_history, test_gh_history)])\n",
    "    loss2 = loss2 / (sig_h**2) / temp\n",
    "    \n",
    "    losses_F_CN_for_AD['loss1'].append(loss1.item())\n",
    "    losses_F_CN_for_AD['loss2'].append(loss2.item())\n",
    "    losses_F_CN_for_AD['tol_loss'].append((loss1+loss2+losses_F_CN_for_AD['loss_F']).item())\n",
    "    \n",
    "    # Saving images\n",
    "    save_image(mu_history=test_mu_history, \n",
    "               name='AD_test_Reconstructed_by_model_CN_half' + AD_img_names[which_ones], #Pay attention to the name when un-commenting this line\n",
    "               which_one=which_ones)\n",
    "    \n",
    "save_dict_to_json(losses_F_CN_for_AD, 'losses_F_CN_for_AD_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff7be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_F_AD_for_AD = {'loss1':[],'loss2':[], 'tol_loss':[]}\n",
    "losses_F_AD_for_AD['loss_F'] = rho * torch.sum(torch.pow(model_AD.F_mat, 2)).cpu().detach().numpy()\n",
    "for test_batch, test_batch_index in test_loader:\n",
    "    test_batch = test_batch.to(torch.float32)\n",
    "    h_0 = h0.expand(test_batch.size(0), -1) # h0: 1 x latent_dim, h_0: batch_size x latent_dim\n",
    "    which_ones = IDs[test_batch_index.to(int)]\n",
    "    test_x_list, test_mu_history, test_h_history, test_gh_history = evaluate(model_AD, test_batch, h_0, which_ones)\n",
    "    \n",
    "    # calculate loss\n",
    "    temp = 2 * test_batch.size(0) * tol_time\n",
    "    loss1 = sum([torch.sum(torch.pow(x-mu, 2)) for x, mu in zip(test_x_list, test_mu_history)])\n",
    "    loss1 = loss1 / (sig_x**2) / temp\n",
    "        \n",
    "    loss2 = sum([torch.sum(torch.pow(h-gh, 2)) for h, gh in zip(test_h_history, test_gh_history)])\n",
    "    loss2 = loss2 / (sig_h**2) / temp\n",
    "    \n",
    "    losses_F_AD_for_AD['loss1'].append(loss1.item())\n",
    "    losses_F_AD_for_AD['loss2'].append(loss2.item())\n",
    "    losses_F_AD_for_AD['tol_loss'].append((loss1+loss2+losses_F_AD_for_AD['loss_F']).item())\n",
    "    \n",
    "    # Saving images\n",
    "    save_image(mu_history=test_mu_history, \n",
    "               name='AD_test_Reconstructed_by_model_AD_half' + AD_img_names[which_ones], #Pay attention to the name when un-commenting this line\n",
    "               which_one=which_ones)\n",
    "    \n",
    "save_dict_to_json(losses_F_AD_for_AD, 'losses_F_AD_for_AD_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb74109",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_F_AD_for_CN = {'loss1':[],'loss2':[], 'tol_loss':[]}\n",
    "losses_F_AD_for_CN['loss_F'] = rho * torch.sum(torch.pow(model_AD.F_mat, 2)).cpu().detach().numpy()\n",
    "for test_batch, test_batch_index in test_loader:\n",
    "    test_batch = test_batch.to(torch.float32)\n",
    "    h_0 = h0.expand(test_batch.size(0), -1) # h0: 1 x latent_dim, h_0: batch_size x latent_dim\n",
    "    which_ones = IDs[test_batch_index.to(int)]\n",
    "    test_x_list, test_mu_history, test_h_history, test_gh_history = evaluate(model_AD, test_batch, h_0, which_ones)\n",
    "    \n",
    "    # calculate loss\n",
    "    temp = 2 * test_batch.size(0) * tol_time\n",
    "    loss1 = sum([torch.sum(torch.pow(x-mu, 2)) for x, mu in zip(test_x_list, test_mu_history)])\n",
    "    loss1 = loss1 / (sig_x**2) / temp\n",
    "        \n",
    "    loss2 = sum([torch.sum(torch.pow(h-gh, 2)) for h, gh in zip(test_h_history, test_gh_history)])\n",
    "    loss2 = loss2 / (sig_h**2) / temp\n",
    "    \n",
    "    losses_F_AD_for_CN['loss1'].append(loss1.item())\n",
    "    losses_F_AD_for_CN['loss2'].append(loss2.item())\n",
    "    losses_F_AD_for_CN['tol_loss'].append((loss1+loss2+losses_F_AD_for_CN['loss_F']).item())\n",
    "    \n",
    "    # Saving images\n",
    "    save_image(mu_history=test_mu_history, \n",
    "               name='CN_test_Reconstructed_by_model_AD_half' + CN_img_names[which_ones], #Pay attention to the name when un-commenting this line\n",
    "               which_one=which_ones)\n",
    "    \n",
    "save_dict_to_json(losses_F_AD_for_CN, 'losses_F_AD_for_CN_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0cda5b",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000be9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_z = F.l1_loss(z_vectors, z_vectors_original, reduction='sum')\n",
    "z_norm = F.l1_loss(z_vectors, torch.zeros_like(z_vectors), reduction='sum')\n",
    "z_0_norm = F.l1_loss(z_vectors_original, torch.zeros_like(z_vectors_original), reduction='sum')\n",
    "d_F = F.l1_loss(F_mat, F_mat_original, reduction='sum')\n",
    "F_norm = F.l1_loss(F_mat, torch.zeros_like(F_mat), reduction='sum')\n",
    "F_0_norm = F.l1_loss(F_mat_original, torch.zeros_like(F_mat_original), reduction='sum')\n",
    "print(f'The l1 difference between z and z_0 is: {d_z}')\n",
    "print(f'The l1 norm of z is: {z_norm}')\n",
    "print(f'The l1 norm of z_0 is: {z_0_norm}')\n",
    "print(f'The l1 difference between F and F_0 is: {d_F}')\n",
    "print(f'The l1 norm of F is: {F_norm}')\n",
    "print(f'The l1 norm of F_0 is: {F_0_norm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d516b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_mat = model.F.weight\n",
    "num_ = 10\n",
    "diff_F = torch.zeros(num_, num_)\n",
    "for i in range(diff_F.shape[0]):\n",
    "    for j in range(diff_F.shape[1]):\n",
    "        diff_F[i,j]=F.l1_loss(F_mat[i], F_mat[j], reduction='sum')\n",
    "print(diff_F)\n",
    "diff_F_same_subject = []\n",
    "for i in range(train_size//2):\n",
    "    diff_F_same_subject.append(F.l1_loss(F_mat[2*i], F_mat[2*i+1], reduction='sum'))\n",
    "print(diff_F_same_subject)\n",
    "diff_F_diff_subject = []\n",
    "for i in range(train_size//2-1):\n",
    "    diff_F_diff_subject.append(F.l1_loss(F_mat[2*i+1], F_mat[2*i+2], reduction='sum'))\n",
    "print(diff_F_diff_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14649d52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_for_plot = 0\n",
    "h_history = history['h_history']\n",
    "print(h_history[0].shape)\n",
    "print(h_history[1].shape)\n",
    "h1_all = torch.stack(h_history).transpose(0,1) # output: batch_size x tol_time x latent_dim\n",
    "h1 = h1_all[index_for_plot]\n",
    "h_history.pop(-1)\n",
    "h_history.insert(0, h0.expand(2, -1)) # The size of expanded h0 needs to be adjusted (num_train mod batch_size)\n",
    "#h_history.insert(0, h0.expand(batch_size, -1))\n",
    "print(h_history[0].shape)\n",
    "h2_all = torch.stack(h_history).transpose(0,1) # output: batch_size x tol_time x latent_dim\n",
    "h2 = h2_all[index_for_plot]\n",
    "\n",
    "# index_for_plot = 1\n",
    "# h1 = h1_all[index_for_plot]\n",
    "# h2 = h2_all[index_for_plot]\n",
    "\n",
    "h2 = model.F(h2)\n",
    "#h2 = torch.mm(h2, F_mat)\n",
    "\n",
    "print(torch.dist(h1, h2))\n",
    "for i in range(120):\n",
    "    plt.plot([-5,5],[-5,5])\n",
    "    plt.scatter(h1[i].cpu().detach().numpy(),h2[i].cpu().detach().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, batch_index in train_loader:\n",
    "    h_0 = h0.expand(batch.size(0), -1) # h0: 1 x latent_dim, h_0: batch_size x latent_dim\n",
    "    which_ones = IDs[batch_index.to(int)]\n",
    "    x, mu_his, h_his, gh_his = model(batch, h0.expand(batch.size(0), -1), which_ones)\n",
    "\n",
    "    \n",
    "\n",
    "index_for_plot = 0\n",
    "h_all = torch.stack(h_his).transpose(0,1) # output: batch_size x tol_time x latent_dim\n",
    "h = h_all[index_for_plot]\n",
    "\n",
    "gh_all = torch.stack(gh_his).transpose(0,1) # output: batch_size x tol_time x latent_dim\n",
    "gh = gh_all[index_for_plot]\n",
    "\n",
    "\n",
    "print(torch.dist(h, gh))\n",
    "for i in range(120):\n",
    "    plt.plot([-5,5],[-5,5])\n",
    "    plt.scatter(h[i].cpu().detach().numpy(),gh[i].cpu().detach().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c45ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_for_plot = 0\n",
    "h_history = history['h_history']\n",
    "print(h_history[0].shape)\n",
    "print(h_history[1].shape)\n",
    "h1_all = torch.stack(h_history).transpose(0,1) # output: batch_size x tol_time x latent_dim\n",
    "h1 = h1_all[index_for_plot]\n",
    "h_history.pop(-1)\n",
    "h_history.insert(0, h0.expand(2, -1)) # The size of expanded h0 needs to be adjusted (num_train mod batch_size)\n",
    "#h_history.insert(0, h0.expand(batch_size, -1))\n",
    "print(h_history[0].shape)\n",
    "h2_all = torch.stack(h_history).transpose(0,1) # output: batch_size x tol_time x latent_dim\n",
    "h2 = h2_all[index_for_plot]\n",
    "\n",
    "# index_for_plot = 1\n",
    "# h1 = h1_all[index_for_plot]\n",
    "# h2 = h2_all[index_for_plot]\n",
    "\n",
    "h2 = model.F(h2)\n",
    "#h2 = torch.mm(h2, F_mat)\n",
    "\n",
    "print(torch.dist(h1, h2))\n",
    "for i in range(120):\n",
    "    plt.plot([-5,5],[-5,5])\n",
    "    plt.scatter(h1[i].cpu().detach().numpy(),h2[i].cpu().detach().numpy())\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
