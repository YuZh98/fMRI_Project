{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec79178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "%matplotlib inline\n",
    "\n",
    "# Use a white background for matplotlib figures\n",
    "matplotlib.rcParams['figure.facecolor'] = '#ffffff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2fbb5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I269254_I989324imagedata.nii.gz', 'I269254_I1304069imagedata.nii.gz', 'I269254_I1501115imagedata.nii.gz', 'I269254_I1241097imagedataLMCI.nii.gz', 'I269254_I235238imagedataLMCI.nii.gz', 'I269254_I1132801imagedata.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "data_dir = './Dataset'\n",
    "print(os.listdir(data_dir))\n",
    "\n",
    "img_dir = [os.path.join(data_dir, x) for x in os.listdir(data_dir)]\n",
    "imgs = [nib.load(img_dir[i]) for i in [0,1,2,5]]\n",
    "imgs_data = [torch.tensor(i.get_fdata()).unsqueeze(0) for i in imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e487d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 20220509\n",
    "torch.manual_seed(random_seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "869ee905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Normalization(imgs_data):\n",
    "    max_value = []\n",
    "    min_value = []\n",
    "    for img in imgs_data:\n",
    "        max_value.append(torch.max(img))\n",
    "        min_value.append(torch.min(img))\n",
    "    imgs_data = [2*((x-mi)/(ma-mi)-0.5) for x, ma, mi in zip(imgs_data, max_value, min_value)]\n",
    "    return imgs_data, max_value, min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4979435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_data, max_value, min_value = Data_Normalization(imgs_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fc1f3a",
   "metadata": {},
   "source": [
    "Create my own dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca4fe08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 91, 109, 91, 177])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_data[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82731570",
   "metadata": {},
   "source": [
    "Helper functions for using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "664acf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True, dtype=torch.float)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735cd47",
   "metadata": {},
   "source": [
    "We can now create PyTorch data loaders for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1665a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "val_size = 0\n",
    "train_size = len(imgs) - val_size\n",
    "\n",
    "# move to GPU\n",
    "device = get_default_device()\n",
    "print(device)\n",
    "train_ds = to_device(imgs_data, device)\n",
    "\n",
    "batch_size = train_size\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size)\n",
    "# # val_loader = DataLoader(val_ds, batch_size*2, num_workers=0, pin_memory = True)\n",
    "\n",
    "# # move to GPU\n",
    "# device = get_default_device()\n",
    "# print(device)\n",
    "# train_loader = DeviceDataLoader(train_loader, device)\n",
    "# # val_loader = DeviceDataLoader(val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db9a73",
   "metadata": {},
   "source": [
    "Next is the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a5ec9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 50, 50])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dim = 50\n",
    "F_matrices = torch.rand(batch_size, latent_dim, latent_dim, device = device, requires_grad = True)\n",
    "F_matrices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef67f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_mapping(x, lamb):\n",
    "    return torch.sign(x)*(torch.abs(x)-lamb)*(torch.abs(x)>lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ee6d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrobeniusNormalization(nn.Module):\n",
    "    def forward(self, X):\n",
    "        F_norm = torch.linalg.matrix_norm(X).item()\n",
    "        return(torch.div(X, F_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "89ff2770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecVAEModel(nn.Module):\n",
    "    def __init__(self, enc_out_dim=100, latent_dim=latent_dim, sqr_sig_x=1., sqr_sig_h=1e-1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sqr_sig_x = sqr_sig_x\n",
    "        self.sqr_sig_h = sqr_sig_h\n",
    "        # Encoder: from input(x) to one of the inputs of the hidden layer (enc_x)\n",
    "        # input: 1 x 91 x 109 x 91\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv3d(1, 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(4),\n",
    "            nn.LeakyReLU(0.2, inplace = True)) # output: 4 x 45 x 54 x 45\n",
    "        \n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv3d(4, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(8),\n",
    "            nn.LeakyReLU(0.2, inplace = True)) # output: 8 x 22 x 27 x 22\n",
    "        \n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv3d(8, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.LeakyReLU(0.2, inplace = True)) # output: 16 x 11 x 13 x 11\n",
    "        \n",
    "        self.encoder4 = nn.Sequential(\n",
    "            nn.Conv3d(16, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(0.2, inplace = True)) # output: 32 x 5 x 6 x 5\n",
    "        \n",
    "        self.encoder5 = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(32*5*6*5, enc_out_dim),\n",
    "            nn.Tanh()) # output: enc_out_dim\n",
    "        \n",
    "        \n",
    "        # Hidden: from (enc_x,h_{t-1}) to h_t\n",
    "        self.hidden2mu = nn.Linear(enc_out_dim+latent_dim, latent_dim)\n",
    "        self.hidden2log_var = nn.Linear(enc_out_dim+latent_dim, latent_dim)\n",
    "        \n",
    "        # Decoder: from h_t to mu_t\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32*5*6*5),\n",
    "            nn.Unflatten(1, (32, 5, 6, 5)),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(0.2, inplace = True))\n",
    "          \n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(32, 16, kernel_size=4, stride=2, padding=1, output_padding=1, bias=False), \n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.LeakyReLU(0.2, inplace = True))\n",
    "        \n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(16, 8, kernel_size=4, stride=2, padding=1, output_padding=(0,1,0), bias=False), \n",
    "            nn.BatchNorm3d(8),\n",
    "            nn.LeakyReLU(0.2, inplace = True))\n",
    "        \n",
    "        self.decoder4 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(8, 4, kernel_size=4, stride=2, padding=1, output_padding=(1,0,1), bias=False), \n",
    "            nn.BatchNorm3d(4),\n",
    "            nn.LeakyReLU(0.2, inplace = True))\n",
    "        \n",
    "        self.decoder5 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(4, 1, kernel_size=4, stride=2, padding=1, output_padding=1, bias=False), \n",
    "            nn.Tanh())\n",
    "                   \n",
    "        \n",
    "       \n",
    "    def g_transform(self, h_old, which_ones):\n",
    "        # h_old shape: batch_size x latent_dim\n",
    "        #print(h_old.shape)\n",
    "        h_old = h_old.unsqueeze(1)\n",
    "        if len(which_ones)==1:\n",
    "            h_new = torch.bmm(h_old, F_matrices[which_ones[0]:(which_ones[0]+1),:,:])\n",
    "        else:\n",
    "            h_new = torch.bmm(h_old, F_matrices[which_ones,:,:])\n",
    "        #print(h_new.shape)\n",
    "        #print(torch.max(h_new))\n",
    "        return h_new.squeeze(1)\n",
    "     \n",
    "    \n",
    "    def encode(self, x):\n",
    "        enc_x = self.encoder1(x)\n",
    "        enc_x = self.encoder2(enc_x)\n",
    "        enc_x = self.encoder3(enc_x)\n",
    "        enc_x = self.encoder4(enc_x)\n",
    "        enc_x = self.encoder5(enc_x)\n",
    "        return enc_x\n",
    "    \n",
    "    \n",
    "    def decode(self, h):\n",
    "        dec_h = self.decoder1(h)\n",
    "        dec_h = self.decoder2(dec_h)\n",
    "        dec_h = self.decoder3(dec_h)\n",
    "        dec_h = self.decoder4(dec_h)\n",
    "        dec_h = self.decoder5(dec_h)\n",
    "        return dec_h\n",
    "        \n",
    "    \n",
    "    def reparametrize(self, mu_h,log_var_h):\n",
    "        # Reparametrization Trick to allow gradients to backpropagate from the stochastic part of the model\n",
    "        sigma_h = torch.exp(log_var_h / 2)\n",
    "        z = torch.randn(size = (mu_h.size(0),mu_h.size(1)))\n",
    "        z = z.type_as(mu_h) # Setting z to be .cuda when using GPU training\n",
    "        return mu_h + sigma_h*z\n",
    "      \n",
    "    \n",
    "    def VAE(self, x, h):   \n",
    "        # encode x and h to get the mu and variance parameters for the latent space\n",
    "        enc_x = self.encode(x)\n",
    "        combined = torch.cat((enc_x, h), 1)\n",
    "        mu_h, log_var_h = self.hidden2mu(combined), self.hidden2log_var(combined)\n",
    "        \n",
    "        # sample h\n",
    "        h = self.reparametrize(mu_h, log_var_h)\n",
    "        \n",
    "        # decode  \n",
    "        mu = self.decode(h)\n",
    "        return mu, h\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, h, which_ones):\n",
    "        '''h is h_0. It is of size batch_size*latent_dim'''\n",
    "        batch = batch.to(torch.float32)\n",
    "        x_list, mu_history, h_history, gh_history = self(batch, h, which_ones)\n",
    "        \n",
    "        temp = 2 * batch_size * len(h_history)\n",
    "        # calculate loss\n",
    "        loss1 = sum([torch.sum(torch.pow(x-mu, 2)) for x, mu in zip(x_list, mu_history)])\n",
    "        loss1 = loss1 / self.sqr_sig_x / temp\n",
    "        \n",
    "        loss2 = sum([torch.sum(torch.pow(h-gh, 2)) for h, gh in zip(h_history, gh_history)])\n",
    "        loss2 = loss2 / self.sqr_sig_h / temp\n",
    "        \n",
    "        loss3 = F.l1_loss(F_matrices, torch.zeros_like(F_matrices), reduction='sum')\n",
    "        \n",
    "        loss = loss1 + loss2\n",
    "        \n",
    "        return loss1, loss2, loss3, loss\n",
    "    \n",
    "    \n",
    "    def forward(self, x, h, which_ones):\n",
    "        # which_ones must be a list\n",
    "        tol_time = x.size(-1) # x is of size batch_size*channel*x1*x2*x3*tol_time\n",
    "        x_list = [x[:,:,:,:,:,t] for t in range(tol_time)]\n",
    "        h_history = []\n",
    "        gh_history = []\n",
    "        mu_history = []\n",
    "        for t in range(tol_time):\n",
    "            #print(t)\n",
    "            gh_history.append(self.g_transform(h, which_ones))\n",
    "            mu, h = self.VAE(x_list[t], h)\n",
    "            h_history.append(h)\n",
    "            mu_history.append(mu)\n",
    "        # print(\"h: \", h_history[50], \"\\n\", \"gh: \", gh_history[50])\n",
    "        return x_list, mu_history, h_history, gh_history\n",
    "    \n",
    "\n",
    "model = RecVAEModel()\n",
    "model = to_device(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f484fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, h0, model, lamb, train_loader=None, val_loader=None, opt_func=torch.optim.SGD):\n",
    "    train_loss_history = []\n",
    "    optimizer = opt_func([{'params': model.parameters()}, {'params': F_matrices}], lr)\n",
    "    batch = torch.stack(train_ds)\n",
    "    which_ones = [i for i in range(batch_size)]\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        loss1, loss2, loss3, loss = model.training_step(batch, h0.expand(batch.size(0), -1), which_ones)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            F_matrices_new = prox_mapping(F_matrices, lamb)\n",
    "            F_matrices.copy_(F_matrices_new)\n",
    "        optimizer.zero_grad()\n",
    "        # train_loss_history.append(loss)\n",
    "        # print(\"Epoch [{}], train_loss: {:.2f}\".format(epoch, loss))\n",
    "        print(\"Epoch [{}], train_loss: {:.2f} with loss1: {:.2f}, loss2: {:.2f} and loss3: {:.2f}\".format(epoch, loss, loss1, loss2, loss3))\n",
    "    return train_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a947495a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3242, 0.8223, 0.4404, 0.2839, 0.7096, 0.8749, 0.6743, 0.8482, 0.2078,\n",
       "         0.9575, 0.7758, 0.6036, 0.4181, 0.8889, 0.8722, 0.6919, 0.7953, 0.5428,\n",
       "         0.1517, 0.7272, 0.5854, 0.5523, 0.6572, 0.7832, 0.1770, 0.1144, 0.7519,\n",
       "         0.3479, 0.5873, 0.3989, 0.6967, 0.3291, 0.8412, 0.8612, 0.5244, 0.6021,\n",
       "         0.6012, 0.0470, 0.2567, 0.7362, 0.8824, 0.0203, 0.3594, 0.2966, 0.9863,\n",
       "         0.7287, 0.4492, 0.4691, 0.3128, 0.4787]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0 = torch.rand(1, 50)\n",
    "h0 = to_device(h0, device)\n",
    "h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ebe5438d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 344228.66 with loss1: 343650.00, loss2: 578.67 and loss3: 5042.70\n",
      "Epoch [1], train_loss: 134539.42 with loss1: 134006.14, loss2: 533.28 and loss3: 5032.70\n",
      "Epoch [2], train_loss: 135185.92 with loss1: 134593.86, loss2: 592.06 and loss3: 5022.71\n",
      "Epoch [3], train_loss: 204719.59 with loss1: 204080.86, loss2: 638.74 and loss3: 5012.73\n",
      "Epoch [4], train_loss: 78048.16 with loss1: 77246.23, loss2: 801.94 and loss3: 5002.76\n",
      "Epoch [5], train_loss: 57784.69 with loss1: 57177.11, loss2: 607.57 and loss3: 4992.79\n",
      "Epoch [6], train_loss: 38913.80 with loss1: 38319.82, loss2: 593.98 and loss3: 4982.84\n",
      "Epoch [7], train_loss: 29597.63 with loss1: 28892.67, loss2: 704.96 and loss3: 4972.90\n",
      "Epoch [8], train_loss: 23919.14 with loss1: 23196.64, loss2: 722.50 and loss3: 4962.96\n",
      "Epoch [9], train_loss: 20390.68 with loss1: 19692.22, loss2: 698.46 and loss3: 4953.04\n",
      "Epoch [10], train_loss: 17585.33 with loss1: 16902.52, loss2: 682.81 and loss3: 4943.12\n",
      "Epoch [11], train_loss: 15527.97 with loss1: 14830.52, loss2: 697.45 and loss3: 4933.22\n",
      "Epoch [12], train_loss: 13592.08 with loss1: 12833.36, loss2: 758.72 and loss3: 4923.33\n",
      "Epoch [13], train_loss: 12296.80 with loss1: 11510.51, loss2: 786.29 and loss3: 4913.45\n",
      "Epoch [14], train_loss: 11284.12 with loss1: 10470.51, loss2: 813.61 and loss3: 4903.58\n",
      "Epoch [15], train_loss: 10573.41 with loss1: 9742.61, loss2: 830.80 and loss3: 4893.72\n",
      "Epoch [16], train_loss: 10044.01 with loss1: 9199.42, loss2: 844.58 and loss3: 4883.86\n",
      "Epoch [17], train_loss: 9570.35 with loss1: 8711.62, loss2: 858.74 and loss3: 4874.02\n",
      "Epoch [18], train_loss: 9179.34 with loss1: 8316.60, loss2: 862.74 and loss3: 4864.19\n",
      "Epoch [19], train_loss: 8964.68 with loss1: 8098.58, loss2: 866.10 and loss3: 4854.36\n",
      "Epoch [20], train_loss: 8666.02 with loss1: 7806.49, loss2: 859.54 and loss3: 4844.54\n",
      "Epoch [21], train_loss: 8444.08 with loss1: 7587.45, loss2: 856.63 and loss3: 4834.74\n",
      "Epoch [22], train_loss: 8229.33 with loss1: 7358.56, loss2: 870.77 and loss3: 4824.94\n",
      "Epoch [23], train_loss: 8025.94 with loss1: 7174.04, loss2: 851.90 and loss3: 4815.15\n",
      "Epoch [24], train_loss: 7851.00 with loss1: 6996.86, loss2: 854.14 and loss3: 4805.37\n",
      "Epoch [25], train_loss: 7733.71 with loss1: 6899.32, loss2: 834.39 and loss3: 4795.59\n",
      "Epoch [26], train_loss: 7649.18 with loss1: 6813.14, loss2: 836.04 and loss3: 4785.83\n",
      "Epoch [27], train_loss: 7546.57 with loss1: 6729.53, loss2: 817.04 and loss3: 4776.08\n",
      "Epoch [28], train_loss: 7494.55 with loss1: 6680.85, loss2: 813.70 and loss3: 4766.34\n",
      "Epoch [29], train_loss: 7437.39 with loss1: 6625.32, loss2: 812.07 and loss3: 4756.61\n",
      "Epoch [30], train_loss: 7566.94 with loss1: 6750.43, loss2: 816.51 and loss3: 4746.88\n",
      "Epoch [31], train_loss: 8082.11 with loss1: 7307.37, loss2: 774.74 and loss3: 4737.17\n",
      "Epoch [32], train_loss: 8382.56 with loss1: 7547.77, loss2: 834.79 and loss3: 4727.47\n",
      "Epoch [33], train_loss: 9370.09 with loss1: 8596.56, loss2: 773.53 and loss3: 4717.77\n",
      "Epoch [34], train_loss: 8052.51 with loss1: 7204.31, loss2: 848.20 and loss3: 4708.10\n",
      "Epoch [35], train_loss: 7861.40 with loss1: 7093.16, loss2: 768.24 and loss3: 4698.43\n",
      "Epoch [36], train_loss: 7213.17 with loss1: 6427.28, loss2: 785.89 and loss3: 4688.77\n",
      "Epoch [37], train_loss: 7041.43 with loss1: 6303.24, loss2: 738.20 and loss3: 4679.12\n",
      "Epoch [38], train_loss: 6844.65 with loss1: 6097.08, loss2: 747.57 and loss3: 4669.48\n",
      "Epoch [39], train_loss: 6772.60 with loss1: 6062.47, loss2: 710.14 and loss3: 4659.86\n",
      "Epoch [40], train_loss: 6694.91 with loss1: 5992.24, loss2: 702.67 and loss3: 4650.24\n",
      "Epoch [41], train_loss: 6650.31 with loss1: 5952.59, loss2: 697.72 and loss3: 4640.64\n",
      "Epoch [42], train_loss: 6603.14 with loss1: 5910.10, loss2: 693.04 and loss3: 4631.04\n",
      "Epoch [43], train_loss: 6580.08 with loss1: 5893.09, loss2: 686.99 and loss3: 4621.46\n",
      "Epoch [44], train_loss: 6560.15 with loss1: 5894.79, loss2: 665.36 and loss3: 4611.88\n",
      "Epoch [45], train_loss: 6637.58 with loss1: 5959.45, loss2: 678.14 and loss3: 4602.30\n",
      "Epoch [46], train_loss: 6623.78 with loss1: 5967.50, loss2: 656.28 and loss3: 4592.74\n",
      "Epoch [47], train_loss: 6632.29 with loss1: 5968.76, loss2: 663.53 and loss3: 4583.18\n",
      "Epoch [48], train_loss: 6516.92 with loss1: 5873.79, loss2: 643.13 and loss3: 4573.64\n",
      "Epoch [49], train_loss: 6515.10 with loss1: 5859.22, loss2: 655.88 and loss3: 4564.11\n",
      "Epoch [50], train_loss: 6406.72 with loss1: 5778.15, loss2: 628.57 and loss3: 4554.59\n",
      "Epoch [51], train_loss: 6391.20 with loss1: 5759.60, loss2: 631.59 and loss3: 4545.08\n",
      "Epoch [52], train_loss: 6302.72 with loss1: 5679.33, loss2: 623.38 and loss3: 4535.59\n",
      "Epoch [53], train_loss: 6255.46 with loss1: 5626.00, loss2: 629.45 and loss3: 4526.11\n",
      "Epoch [54], train_loss: 6140.08 with loss1: 5524.84, loss2: 615.24 and loss3: 4516.63\n",
      "Epoch [55], train_loss: 6104.52 with loss1: 5493.57, loss2: 610.96 and loss3: 4507.17\n",
      "Epoch [56], train_loss: 6033.19 with loss1: 5438.82, loss2: 594.37 and loss3: 4497.72\n",
      "Epoch [57], train_loss: 6018.51 with loss1: 5424.11, loss2: 594.40 and loss3: 4488.27\n",
      "Epoch [58], train_loss: 5938.92 with loss1: 5350.36, loss2: 588.56 and loss3: 4478.83\n",
      "Epoch [59], train_loss: 5937.41 with loss1: 5345.01, loss2: 592.40 and loss3: 4469.41\n",
      "Epoch [60], train_loss: 5867.38 with loss1: 5290.99, loss2: 576.40 and loss3: 4459.99\n",
      "Epoch [61], train_loss: 5843.57 with loss1: 5272.78, loss2: 570.79 and loss3: 4450.59\n",
      "Epoch [62], train_loss: 5770.28 with loss1: 5201.44, loss2: 568.84 and loss3: 4441.20\n",
      "Epoch [63], train_loss: 5799.78 with loss1: 5228.99, loss2: 570.79 and loss3: 4431.81\n",
      "Epoch [64], train_loss: 5724.73 with loss1: 5160.65, loss2: 564.08 and loss3: 4422.43\n",
      "Epoch [65], train_loss: 5737.23 with loss1: 5178.67, loss2: 558.56 and loss3: 4413.06\n",
      "Epoch [66], train_loss: 5656.38 with loss1: 5100.53, loss2: 555.85 and loss3: 4403.70\n",
      "Epoch [67], train_loss: 5721.69 with loss1: 5174.80, loss2: 546.89 and loss3: 4394.36\n",
      "Epoch [68], train_loss: 5640.00 with loss1: 5099.02, loss2: 540.98 and loss3: 4385.03\n",
      "Epoch [69], train_loss: 5724.24 with loss1: 5183.55, loss2: 540.69 and loss3: 4375.71\n",
      "Epoch [70], train_loss: 5627.67 with loss1: 5094.26, loss2: 533.41 and loss3: 4366.40\n",
      "Epoch [71], train_loss: 5696.01 with loss1: 5164.17, loss2: 531.83 and loss3: 4357.11\n",
      "Epoch [72], train_loss: 5573.96 with loss1: 5050.65, loss2: 523.31 and loss3: 4347.83\n",
      "Epoch [73], train_loss: 5610.43 with loss1: 5089.04, loss2: 521.39 and loss3: 4338.56\n",
      "Epoch [74], train_loss: 5495.79 with loss1: 4979.96, loss2: 515.83 and loss3: 4329.30\n",
      "Epoch [75], train_loss: 5526.69 with loss1: 5012.99, loss2: 513.70 and loss3: 4320.05\n",
      "Epoch [76], train_loss: 5387.17 with loss1: 4877.49, loss2: 509.68 and loss3: 4310.81\n",
      "Epoch [77], train_loss: 5445.12 with loss1: 4938.12, loss2: 506.99 and loss3: 4301.58\n",
      "Epoch [78], train_loss: 5309.00 with loss1: 4802.01, loss2: 506.99 and loss3: 4292.36\n",
      "Epoch [79], train_loss: 5326.34 with loss1: 4833.24, loss2: 493.10 and loss3: 4283.15\n",
      "Epoch [80], train_loss: 5225.57 with loss1: 4725.72, loss2: 499.85 and loss3: 4273.94\n",
      "Epoch [81], train_loss: 5226.62 with loss1: 4740.18, loss2: 486.44 and loss3: 4264.75\n",
      "Epoch [82], train_loss: 5147.11 with loss1: 4653.05, loss2: 494.06 and loss3: 4255.57\n",
      "Epoch [83], train_loss: 5176.73 with loss1: 4691.81, loss2: 484.92 and loss3: 4246.40\n",
      "Epoch [84], train_loss: 5069.20 with loss1: 4587.98, loss2: 481.22 and loss3: 4237.25\n",
      "Epoch [85], train_loss: 5068.13 with loss1: 4588.08, loss2: 480.05 and loss3: 4228.11\n",
      "Epoch [86], train_loss: 4999.18 with loss1: 4520.39, loss2: 478.78 and loss3: 4218.98\n",
      "Epoch [87], train_loss: 5047.61 with loss1: 4579.39, loss2: 468.22 and loss3: 4209.87\n",
      "Epoch [88], train_loss: 4938.40 with loss1: 4467.48, loss2: 470.92 and loss3: 4200.75\n",
      "Epoch [89], train_loss: 4960.97 with loss1: 4496.80, loss2: 464.17 and loss3: 4191.65\n",
      "Epoch [90], train_loss: 4852.24 with loss1: 4389.73, loss2: 462.51 and loss3: 4182.57\n",
      "Epoch [91], train_loss: 4916.85 with loss1: 4456.51, loss2: 460.34 and loss3: 4173.50\n",
      "Epoch [92], train_loss: 4830.55 with loss1: 4374.96, loss2: 455.59 and loss3: 4164.44\n",
      "Epoch [93], train_loss: 4872.30 with loss1: 4420.36, loss2: 451.94 and loss3: 4155.39\n",
      "Epoch [94], train_loss: 4837.35 with loss1: 4380.53, loss2: 456.82 and loss3: 4146.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [95], train_loss: 4895.50 with loss1: 4438.97, loss2: 456.54 and loss3: 4137.31\n",
      "Epoch [96], train_loss: 4830.57 with loss1: 4386.97, loss2: 443.60 and loss3: 4128.29\n",
      "Epoch [97], train_loss: 4889.43 with loss1: 4433.36, loss2: 456.07 and loss3: 4119.27\n",
      "Epoch [98], train_loss: 4771.53 with loss1: 4328.00, loss2: 443.53 and loss3: 4110.27\n",
      "Epoch [99], train_loss: 4825.77 with loss1: 4378.34, loss2: 447.43 and loss3: 4101.27\n",
      "Epoch [100], train_loss: 4688.27 with loss1: 4247.96, loss2: 440.31 and loss3: 4092.28\n",
      "Epoch [101], train_loss: 4745.25 with loss1: 4300.99, loss2: 444.26 and loss3: 4083.30\n",
      "Epoch [102], train_loss: 4647.89 with loss1: 4212.78, loss2: 435.11 and loss3: 4074.33\n",
      "Epoch [103], train_loss: 4655.03 with loss1: 4211.62, loss2: 443.41 and loss3: 4065.37\n",
      "Epoch [104], train_loss: 4521.30 with loss1: 4090.04, loss2: 431.26 and loss3: 4056.41\n",
      "Epoch [105], train_loss: 4507.59 with loss1: 4072.79, loss2: 434.79 and loss3: 4047.47\n",
      "Epoch [106], train_loss: 4431.93 with loss1: 4001.65, loss2: 430.27 and loss3: 4038.54\n",
      "Epoch [107], train_loss: 4461.39 with loss1: 4028.44, loss2: 432.96 and loss3: 4029.62\n",
      "Epoch [108], train_loss: 4397.36 with loss1: 3970.06, loss2: 427.30 and loss3: 4020.70\n",
      "Epoch [109], train_loss: 4379.56 with loss1: 3955.50, loss2: 424.05 and loss3: 4011.80\n",
      "Epoch [110], train_loss: 4334.32 with loss1: 3911.08, loss2: 423.24 and loss3: 4002.91\n",
      "Epoch [111], train_loss: 4346.28 with loss1: 3920.41, loss2: 425.87 and loss3: 3994.03\n",
      "Epoch [112], train_loss: 4298.03 with loss1: 3873.85, loss2: 424.18 and loss3: 3985.15\n",
      "Epoch [113], train_loss: 4292.97 with loss1: 3874.00, loss2: 418.97 and loss3: 3976.29\n",
      "Epoch [114], train_loss: 4264.76 with loss1: 3842.38, loss2: 422.39 and loss3: 3967.45\n",
      "Epoch [115], train_loss: 4300.28 with loss1: 3875.62, loss2: 424.66 and loss3: 3958.61\n",
      "Epoch [116], train_loss: 4253.44 with loss1: 3836.18, loss2: 417.26 and loss3: 3949.78\n",
      "Epoch [117], train_loss: 4280.60 with loss1: 3867.54, loss2: 413.06 and loss3: 3940.95\n",
      "Epoch [118], train_loss: 4207.80 with loss1: 3787.54, loss2: 420.25 and loss3: 3932.14\n",
      "Epoch [119], train_loss: 4251.75 with loss1: 3839.29, loss2: 412.45 and loss3: 3923.34\n",
      "Epoch [120], train_loss: 4195.15 with loss1: 3782.63, loss2: 412.52 and loss3: 3914.54\n",
      "Epoch [121], train_loss: 4214.61 with loss1: 3800.61, loss2: 414.01 and loss3: 3905.76\n",
      "Epoch [122], train_loss: 4164.95 with loss1: 3754.32, loss2: 410.63 and loss3: 3896.98\n",
      "Epoch [123], train_loss: 4205.34 with loss1: 3796.34, loss2: 409.00 and loss3: 3888.20\n",
      "Epoch [124], train_loss: 4148.85 with loss1: 3734.03, loss2: 414.81 and loss3: 3879.44\n",
      "Epoch [125], train_loss: 4174.42 with loss1: 3767.10, loss2: 407.32 and loss3: 3870.68\n",
      "Epoch [126], train_loss: 4073.49 with loss1: 3664.59, loss2: 408.90 and loss3: 3861.92\n",
      "Epoch [127], train_loss: 4111.05 with loss1: 3708.84, loss2: 402.21 and loss3: 3853.17\n",
      "Epoch [128], train_loss: 4027.26 with loss1: 3612.87, loss2: 414.38 and loss3: 3844.43\n",
      "Epoch [129], train_loss: 4022.89 with loss1: 3622.60, loss2: 400.29 and loss3: 3835.69\n",
      "Epoch [130], train_loss: 3964.54 with loss1: 3557.89, loss2: 406.65 and loss3: 3826.97\n",
      "Epoch [131], train_loss: 3938.47 with loss1: 3538.51, loss2: 399.96 and loss3: 3818.25\n",
      "Epoch [132], train_loss: 3884.67 with loss1: 3480.39, loss2: 404.28 and loss3: 3809.55\n",
      "Epoch [133], train_loss: 3875.01 with loss1: 3477.84, loss2: 397.17 and loss3: 3800.85\n",
      "Epoch [134], train_loss: 3838.67 with loss1: 3437.85, loss2: 400.82 and loss3: 3792.17\n",
      "Epoch [135], train_loss: 3836.71 with loss1: 3442.63, loss2: 394.08 and loss3: 3783.49\n",
      "Epoch [136], train_loss: 3798.98 with loss1: 3400.32, loss2: 398.66 and loss3: 3774.82\n",
      "Epoch [137], train_loss: 3806.46 with loss1: 3412.92, loss2: 393.54 and loss3: 3766.16\n",
      "Epoch [138], train_loss: 3747.01 with loss1: 3351.89, loss2: 395.13 and loss3: 3757.50\n",
      "Epoch [139], train_loss: 3754.67 with loss1: 3362.30, loss2: 392.37 and loss3: 3748.85\n",
      "Epoch [140], train_loss: 3714.33 with loss1: 3319.09, loss2: 395.25 and loss3: 3740.21\n",
      "Epoch [141], train_loss: 3736.24 with loss1: 3344.22, loss2: 392.03 and loss3: 3731.58\n",
      "Epoch [142], train_loss: 3697.23 with loss1: 3308.00, loss2: 389.23 and loss3: 3722.97\n",
      "Epoch [143], train_loss: 3717.68 with loss1: 3329.30, loss2: 388.38 and loss3: 3714.36\n",
      "Epoch [144], train_loss: 3672.67 with loss1: 3284.73, loss2: 387.94 and loss3: 3705.76\n",
      "Epoch [145], train_loss: 3691.21 with loss1: 3305.95, loss2: 385.26 and loss3: 3697.17\n",
      "Epoch [146], train_loss: 3669.18 with loss1: 3282.52, loss2: 386.66 and loss3: 3688.58\n",
      "Epoch [147], train_loss: 3691.19 with loss1: 3308.96, loss2: 382.23 and loss3: 3680.01\n",
      "Epoch [148], train_loss: 3646.37 with loss1: 3258.61, loss2: 387.76 and loss3: 3671.45\n",
      "Epoch [149], train_loss: 3665.69 with loss1: 3285.27, loss2: 380.41 and loss3: 3662.89\n",
      "Epoch [150], train_loss: 3623.85 with loss1: 3240.64, loss2: 383.21 and loss3: 3654.35\n",
      "Epoch [151], train_loss: 3646.00 with loss1: 3266.97, loss2: 379.03 and loss3: 3645.81\n",
      "Epoch [152], train_loss: 3591.21 with loss1: 3210.41, loss2: 380.80 and loss3: 3637.29\n",
      "Epoch [153], train_loss: 3619.42 with loss1: 3237.71, loss2: 381.72 and loss3: 3628.78\n",
      "Epoch [154], train_loss: 3562.90 with loss1: 3186.04, loss2: 376.86 and loss3: 3620.28\n",
      "Epoch [155], train_loss: 3574.89 with loss1: 3199.53, loss2: 375.36 and loss3: 3611.79\n",
      "Epoch [156], train_loss: 3540.68 with loss1: 3168.27, loss2: 372.41 and loss3: 3603.31\n",
      "Epoch [157], train_loss: 3560.09 with loss1: 3184.85, loss2: 375.24 and loss3: 3594.84\n",
      "Epoch [158], train_loss: 3527.95 with loss1: 3153.53, loss2: 374.42 and loss3: 3586.38\n",
      "Epoch [159], train_loss: 3542.69 with loss1: 3167.20, loss2: 375.49 and loss3: 3577.93\n",
      "Epoch [160], train_loss: 3491.94 with loss1: 3120.81, loss2: 371.13 and loss3: 3569.49\n",
      "Epoch [161], train_loss: 3498.07 with loss1: 3128.63, loss2: 369.44 and loss3: 3561.06\n",
      "Epoch [162], train_loss: 3438.22 with loss1: 3067.18, loss2: 371.04 and loss3: 3552.64\n",
      "Epoch [163], train_loss: 3486.10 with loss1: 3117.32, loss2: 368.78 and loss3: 3544.23\n",
      "Epoch [164], train_loss: 3416.91 with loss1: 3049.82, loss2: 367.09 and loss3: 3535.83\n",
      "Epoch [165], train_loss: 3431.44 with loss1: 3067.94, loss2: 363.50 and loss3: 3527.44\n",
      "Epoch [166], train_loss: 3378.22 with loss1: 3014.34, loss2: 363.88 and loss3: 3519.06\n",
      "Epoch [167], train_loss: 3388.28 with loss1: 3024.36, loss2: 363.92 and loss3: 3510.69\n",
      "Epoch [168], train_loss: 3338.24 with loss1: 2976.61, loss2: 361.63 and loss3: 3502.34\n",
      "Epoch [169], train_loss: 3381.87 with loss1: 3018.63, loss2: 363.24 and loss3: 3494.00\n",
      "Epoch [170], train_loss: 3311.31 with loss1: 2950.23, loss2: 361.08 and loss3: 3485.66\n",
      "Epoch [171], train_loss: 3343.02 with loss1: 2982.35, loss2: 360.67 and loss3: 3477.34\n",
      "Epoch [172], train_loss: 3295.47 with loss1: 2936.87, loss2: 358.59 and loss3: 3469.04\n",
      "Epoch [173], train_loss: 3300.94 with loss1: 2944.73, loss2: 356.21 and loss3: 3460.74\n",
      "Epoch [174], train_loss: 3275.62 with loss1: 2918.46, loss2: 357.16 and loss3: 3452.44\n",
      "Epoch [175], train_loss: 3313.82 with loss1: 2960.85, loss2: 352.97 and loss3: 3444.15\n",
      "Epoch [176], train_loss: 3260.43 with loss1: 2902.63, loss2: 357.79 and loss3: 3435.88\n",
      "Epoch [177], train_loss: 3286.40 with loss1: 2934.93, loss2: 351.47 and loss3: 3427.61\n",
      "Epoch [178], train_loss: 3248.15 with loss1: 2894.72, loss2: 353.42 and loss3: 3419.35\n",
      "Epoch [179], train_loss: 3276.77 with loss1: 2925.35, loss2: 351.42 and loss3: 3411.10\n",
      "Epoch [180], train_loss: 3227.24 with loss1: 2876.23, loss2: 351.01 and loss3: 3402.86\n",
      "Epoch [181], train_loss: 3248.28 with loss1: 2899.71, loss2: 348.57 and loss3: 3394.62\n",
      "Epoch [182], train_loss: 3215.16 with loss1: 2867.29, loss2: 347.87 and loss3: 3386.40\n",
      "Epoch [183], train_loss: 3249.61 with loss1: 2902.99, loss2: 346.62 and loss3: 3378.18\n",
      "Epoch [184], train_loss: 3213.41 with loss1: 2863.42, loss2: 349.99 and loss3: 3369.98\n",
      "Epoch [185], train_loss: 3255.06 with loss1: 2911.49, loss2: 343.57 and loss3: 3361.79\n",
      "Epoch [186], train_loss: 3198.49 with loss1: 2853.14, loss2: 345.36 and loss3: 3353.61\n",
      "Epoch [187], train_loss: 3234.80 with loss1: 2887.31, loss2: 347.49 and loss3: 3345.45\n",
      "Epoch [188], train_loss: 3194.61 with loss1: 2849.66, loss2: 344.95 and loss3: 3337.29\n",
      "Epoch [189], train_loss: 3212.76 with loss1: 2872.38, loss2: 340.38 and loss3: 3329.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190], train_loss: 3157.13 with loss1: 2815.62, loss2: 341.51 and loss3: 3321.01\n",
      "Epoch [191], train_loss: 3179.88 with loss1: 2842.83, loss2: 337.05 and loss3: 3312.88\n",
      "Epoch [192], train_loss: 3131.89 with loss1: 2793.48, loss2: 338.41 and loss3: 3304.77\n",
      "Epoch [193], train_loss: 3133.42 with loss1: 2795.81, loss2: 337.61 and loss3: 3296.67\n",
      "Epoch [194], train_loss: 3095.86 with loss1: 2758.73, loss2: 337.13 and loss3: 3288.58\n",
      "Epoch [195], train_loss: 3108.88 with loss1: 2774.14, loss2: 334.73 and loss3: 3280.50\n",
      "Epoch [196], train_loss: 3071.56 with loss1: 2735.53, loss2: 336.03 and loss3: 3272.43\n",
      "Epoch [197], train_loss: 3093.66 with loss1: 2761.03, loss2: 332.63 and loss3: 3264.36\n",
      "Epoch [198], train_loss: 3048.90 with loss1: 2712.58, loss2: 336.32 and loss3: 3256.31\n",
      "Epoch [199], train_loss: 3063.81 with loss1: 2732.98, loss2: 330.83 and loss3: 3248.26\n",
      "Epoch [200], train_loss: 3034.39 with loss1: 2703.28, loss2: 331.11 and loss3: 3240.23\n",
      "Epoch [201], train_loss: 3039.87 with loss1: 2709.36, loss2: 330.50 and loss3: 3232.21\n",
      "Epoch [202], train_loss: 3005.45 with loss1: 2675.75, loss2: 329.70 and loss3: 3224.21\n",
      "Epoch [203], train_loss: 3013.30 with loss1: 2684.21, loss2: 329.09 and loss3: 3216.21\n",
      "Epoch [204], train_loss: 2984.42 with loss1: 2654.64, loss2: 329.78 and loss3: 3208.22\n",
      "Epoch [205], train_loss: 3017.76 with loss1: 2693.45, loss2: 324.31 and loss3: 3200.25\n",
      "Epoch [206], train_loss: 2982.42 with loss1: 2655.46, loss2: 326.96 and loss3: 3192.28\n",
      "Epoch [207], train_loss: 2998.07 with loss1: 2674.14, loss2: 323.93 and loss3: 3184.32\n",
      "Epoch [208], train_loss: 2970.62 with loss1: 2644.52, loss2: 326.09 and loss3: 3176.38\n",
      "Epoch [209], train_loss: 2990.35 with loss1: 2669.62, loss2: 320.73 and loss3: 3168.43\n",
      "Epoch [210], train_loss: 2956.73 with loss1: 2634.66, loss2: 322.07 and loss3: 3160.50\n",
      "Epoch [211], train_loss: 2972.11 with loss1: 2652.30, loss2: 319.81 and loss3: 3152.58\n",
      "Epoch [212], train_loss: 2933.80 with loss1: 2614.81, loss2: 318.98 and loss3: 3144.67\n",
      "Epoch [213], train_loss: 2943.73 with loss1: 2626.22, loss2: 317.51 and loss3: 3136.77\n",
      "Epoch [214], train_loss: 2929.48 with loss1: 2608.92, loss2: 320.56 and loss3: 3128.87\n",
      "Epoch [215], train_loss: 2920.41 with loss1: 2603.07, loss2: 317.35 and loss3: 3120.98\n",
      "Epoch [216], train_loss: 2881.94 with loss1: 2566.29, loss2: 315.65 and loss3: 3113.11\n",
      "Epoch [217], train_loss: 2887.45 with loss1: 2573.77, loss2: 313.69 and loss3: 3105.25\n",
      "Epoch [218], train_loss: 2859.58 with loss1: 2545.54, loss2: 314.05 and loss3: 3097.41\n",
      "Epoch [219], train_loss: 2879.63 with loss1: 2567.10, loss2: 312.53 and loss3: 3089.57\n",
      "Epoch [220], train_loss: 2850.81 with loss1: 2537.37, loss2: 313.44 and loss3: 3081.74\n",
      "Epoch [221], train_loss: 2872.05 with loss1: 2560.73, loss2: 311.32 and loss3: 3073.91\n",
      "Epoch [222], train_loss: 2833.82 with loss1: 2524.48, loss2: 309.34 and loss3: 3066.10\n",
      "Epoch [223], train_loss: 2860.78 with loss1: 2551.63, loss2: 309.15 and loss3: 3058.30\n",
      "Epoch [224], train_loss: 2839.50 with loss1: 2531.35, loss2: 308.16 and loss3: 3050.51\n",
      "Epoch [225], train_loss: 2869.58 with loss1: 2562.28, loss2: 307.30 and loss3: 3042.72\n",
      "Epoch [226], train_loss: 2851.33 with loss1: 2544.35, loss2: 306.98 and loss3: 3034.95\n",
      "Epoch [227], train_loss: 2869.78 with loss1: 2565.14, loss2: 304.63 and loss3: 3027.19\n",
      "Epoch [228], train_loss: 2840.16 with loss1: 2536.08, loss2: 304.08 and loss3: 3019.43\n",
      "Epoch [229], train_loss: 2856.82 with loss1: 2553.01, loss2: 303.81 and loss3: 3011.69\n",
      "Epoch [230], train_loss: 2815.07 with loss1: 2511.59, loss2: 303.48 and loss3: 3003.96\n",
      "Epoch [231], train_loss: 2854.75 with loss1: 2552.99, loss2: 301.75 and loss3: 2996.24\n",
      "Epoch [232], train_loss: 2808.92 with loss1: 2508.58, loss2: 300.34 and loss3: 2988.53\n",
      "Epoch [233], train_loss: 2842.43 with loss1: 2541.52, loss2: 300.91 and loss3: 2980.83\n",
      "Epoch [234], train_loss: 2802.24 with loss1: 2502.39, loss2: 299.84 and loss3: 2973.14\n",
      "Epoch [235], train_loss: 2822.78 with loss1: 2525.17, loss2: 297.61 and loss3: 2965.46\n",
      "Epoch [236], train_loss: 2785.69 with loss1: 2488.55, loss2: 297.14 and loss3: 2957.79\n",
      "Epoch [237], train_loss: 2802.10 with loss1: 2504.64, loss2: 297.47 and loss3: 2950.13\n",
      "Epoch [238], train_loss: 2754.46 with loss1: 2458.77, loss2: 295.69 and loss3: 2942.48\n",
      "Epoch [239], train_loss: 2791.59 with loss1: 2496.55, loss2: 295.05 and loss3: 2934.83\n",
      "Epoch [240], train_loss: 2739.02 with loss1: 2445.67, loss2: 293.35 and loss3: 2927.19\n",
      "Epoch [241], train_loss: 2753.97 with loss1: 2459.75, loss2: 294.22 and loss3: 2919.56\n",
      "Epoch [242], train_loss: 2711.42 with loss1: 2418.09, loss2: 293.33 and loss3: 2911.94\n",
      "Epoch [243], train_loss: 2729.24 with loss1: 2436.17, loss2: 293.07 and loss3: 2904.32\n",
      "Epoch [244], train_loss: 2702.19 with loss1: 2413.17, loss2: 289.02 and loss3: 2896.72\n",
      "Epoch [245], train_loss: 2706.66 with loss1: 2414.62, loss2: 292.03 and loss3: 2889.12\n",
      "Epoch [246], train_loss: 2671.46 with loss1: 2384.13, loss2: 287.33 and loss3: 2881.54\n",
      "Epoch [247], train_loss: 2682.68 with loss1: 2394.41, loss2: 288.27 and loss3: 2873.96\n",
      "Epoch [248], train_loss: 2664.29 with loss1: 2377.30, loss2: 286.98 and loss3: 2866.40\n",
      "Epoch [249], train_loss: 2675.44 with loss1: 2388.14, loss2: 287.30 and loss3: 2858.84\n",
      "Epoch [250], train_loss: 2643.56 with loss1: 2358.63, loss2: 284.93 and loss3: 2851.30\n",
      "Epoch [251], train_loss: 2667.55 with loss1: 2381.65, loss2: 285.89 and loss3: 2843.77\n",
      "Epoch [252], train_loss: 2627.12 with loss1: 2343.07, loss2: 284.06 and loss3: 2836.25\n",
      "Epoch [253], train_loss: 2631.65 with loss1: 2346.82, loss2: 284.83 and loss3: 2828.74\n",
      "Epoch [254], train_loss: 2608.09 with loss1: 2326.50, loss2: 281.60 and loss3: 2821.23\n",
      "Epoch [255], train_loss: 2613.04 with loss1: 2330.57, loss2: 282.47 and loss3: 2813.73\n",
      "Epoch [256], train_loss: 2589.89 with loss1: 2309.06, loss2: 280.83 and loss3: 2806.25\n",
      "Epoch [257], train_loss: 2597.82 with loss1: 2316.27, loss2: 281.55 and loss3: 2798.77\n",
      "Epoch [258], train_loss: 2571.97 with loss1: 2293.61, loss2: 278.36 and loss3: 2791.30\n",
      "Epoch [259], train_loss: 2585.85 with loss1: 2306.78, loss2: 279.07 and loss3: 2783.84\n",
      "Epoch [260], train_loss: 2557.17 with loss1: 2279.59, loss2: 277.57 and loss3: 2776.39\n",
      "Epoch [261], train_loss: 2564.03 with loss1: 2285.90, loss2: 278.13 and loss3: 2768.96\n",
      "Epoch [262], train_loss: 2548.82 with loss1: 2273.26, loss2: 275.55 and loss3: 2761.54\n",
      "Epoch [263], train_loss: 2565.13 with loss1: 2288.71, loss2: 276.41 and loss3: 2754.12\n",
      "Epoch [264], train_loss: 2546.10 with loss1: 2272.82, loss2: 273.28 and loss3: 2746.72\n",
      "Epoch [265], train_loss: 2561.76 with loss1: 2286.72, loss2: 275.04 and loss3: 2739.32\n",
      "Epoch [266], train_loss: 2531.63 with loss1: 2259.06, loss2: 272.57 and loss3: 2731.94\n",
      "Epoch [267], train_loss: 2553.53 with loss1: 2280.67, loss2: 272.86 and loss3: 2724.56\n",
      "Epoch [268], train_loss: 2534.11 with loss1: 2264.34, loss2: 269.77 and loss3: 2717.20\n",
      "Epoch [269], train_loss: 2539.82 with loss1: 2267.97, loss2: 271.85 and loss3: 2709.85\n",
      "Epoch [270], train_loss: 2518.10 with loss1: 2248.78, loss2: 269.32 and loss3: 2702.50\n",
      "Epoch [271], train_loss: 2527.54 with loss1: 2258.48, loss2: 269.06 and loss3: 2695.16\n",
      "Epoch [272], train_loss: 2508.84 with loss1: 2241.63, loss2: 267.21 and loss3: 2687.83\n",
      "Epoch [273], train_loss: 2540.48 with loss1: 2272.81, loss2: 267.67 and loss3: 2680.51\n",
      "Epoch [274], train_loss: 2520.54 with loss1: 2253.69, loss2: 266.86 and loss3: 2673.20\n",
      "Epoch [275], train_loss: 2542.68 with loss1: 2276.67, loss2: 266.01 and loss3: 2665.90\n",
      "Epoch [276], train_loss: 2523.09 with loss1: 2258.12, loss2: 264.97 and loss3: 2658.60\n",
      "Epoch [277], train_loss: 2553.96 with loss1: 2288.75, loss2: 265.21 and loss3: 2651.32\n",
      "Epoch [278], train_loss: 2523.67 with loss1: 2260.57, loss2: 263.10 and loss3: 2644.05\n",
      "Epoch [279], train_loss: 2568.88 with loss1: 2306.24, loss2: 262.65 and loss3: 2636.79\n",
      "Epoch [280], train_loss: 2532.80 with loss1: 2269.85, loss2: 262.95 and loss3: 2629.53\n",
      "Epoch [281], train_loss: 2566.85 with loss1: 2304.42, loss2: 262.43 and loss3: 2622.30\n",
      "Epoch [282], train_loss: 2526.52 with loss1: 2265.07, loss2: 261.45 and loss3: 2615.07\n",
      "Epoch [283], train_loss: 2566.06 with loss1: 2304.71, loss2: 261.35 and loss3: 2607.85\n",
      "Epoch [284], train_loss: 2527.67 with loss1: 2267.99, loss2: 259.69 and loss3: 2600.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [285], train_loss: 2561.94 with loss1: 2304.61, loss2: 257.33 and loss3: 2593.44\n",
      "Epoch [286], train_loss: 2508.55 with loss1: 2250.04, loss2: 258.51 and loss3: 2586.26\n",
      "Epoch [287], train_loss: 2536.97 with loss1: 2279.39, loss2: 257.58 and loss3: 2579.08\n",
      "Epoch [288], train_loss: 2508.45 with loss1: 2250.96, loss2: 257.49 and loss3: 2571.92\n",
      "Epoch [289], train_loss: 2519.64 with loss1: 2264.80, loss2: 254.84 and loss3: 2564.77\n",
      "Epoch [290], train_loss: 2491.67 with loss1: 2234.63, loss2: 257.04 and loss3: 2557.63\n",
      "Epoch [291], train_loss: 2518.16 with loss1: 2264.44, loss2: 253.73 and loss3: 2550.50\n",
      "Epoch [292], train_loss: 2479.56 with loss1: 2224.80, loss2: 254.77 and loss3: 2543.38\n",
      "Epoch [293], train_loss: 2494.59 with loss1: 2242.24, loss2: 252.35 and loss3: 2536.26\n",
      "Epoch [294], train_loss: 2461.62 with loss1: 2209.03, loss2: 252.59 and loss3: 2529.16\n",
      "Epoch [295], train_loss: 2466.22 with loss1: 2214.64, loss2: 251.58 and loss3: 2522.06\n",
      "Epoch [296], train_loss: 2437.23 with loss1: 2184.53, loss2: 252.70 and loss3: 2514.97\n",
      "Epoch [297], train_loss: 2452.96 with loss1: 2204.79, loss2: 248.17 and loss3: 2507.89\n",
      "Epoch [298], train_loss: 2430.61 with loss1: 2180.43, loss2: 250.18 and loss3: 2500.83\n",
      "Epoch [299], train_loss: 2437.40 with loss1: 2190.11, loss2: 247.29 and loss3: 2493.77\n",
      "Epoch [300], train_loss: 2409.32 with loss1: 2161.98, loss2: 247.34 and loss3: 2486.73\n",
      "Epoch [301], train_loss: 2444.08 with loss1: 2198.25, loss2: 245.83 and loss3: 2479.68\n",
      "Epoch [302], train_loss: 2424.43 with loss1: 2177.41, loss2: 247.02 and loss3: 2472.65\n",
      "Epoch [303], train_loss: 2432.85 with loss1: 2188.71, loss2: 244.14 and loss3: 2465.63\n",
      "Epoch [304], train_loss: 2409.68 with loss1: 2165.90, loss2: 243.79 and loss3: 2458.62\n",
      "Epoch [305], train_loss: 2423.84 with loss1: 2181.44, loss2: 242.40 and loss3: 2451.62\n",
      "Epoch [306], train_loss: 2402.84 with loss1: 2159.91, loss2: 242.92 and loss3: 2444.62\n",
      "Epoch [307], train_loss: 2423.95 with loss1: 2181.83, loss2: 242.12 and loss3: 2437.64\n",
      "Epoch [308], train_loss: 2402.34 with loss1: 2159.42, loss2: 242.92 and loss3: 2430.66\n",
      "Epoch [309], train_loss: 2438.09 with loss1: 2197.36, loss2: 240.72 and loss3: 2423.69\n",
      "Epoch [310], train_loss: 2401.51 with loss1: 2160.12, loss2: 241.39 and loss3: 2416.73\n",
      "Epoch [311], train_loss: 2416.25 with loss1: 2177.38, loss2: 238.87 and loss3: 2409.79\n",
      "Epoch [312], train_loss: 2377.37 with loss1: 2137.98, loss2: 239.39 and loss3: 2402.85\n",
      "Epoch [313], train_loss: 2390.55 with loss1: 2154.96, loss2: 235.59 and loss3: 2395.93\n",
      "Epoch [314], train_loss: 2357.42 with loss1: 2119.37, loss2: 238.05 and loss3: 2389.02\n",
      "Epoch [315], train_loss: 2372.62 with loss1: 2136.04, loss2: 236.58 and loss3: 2382.12\n",
      "Epoch [316], train_loss: 2327.21 with loss1: 2090.68, loss2: 236.52 and loss3: 2375.24\n",
      "Epoch [317], train_loss: 2330.05 with loss1: 2094.73, loss2: 235.32 and loss3: 2368.36\n",
      "Epoch [318], train_loss: 2306.80 with loss1: 2071.33, loss2: 235.47 and loss3: 2361.48\n",
      "Epoch [319], train_loss: 2300.62 with loss1: 2066.55, loss2: 234.07 and loss3: 2354.62\n",
      "Epoch [320], train_loss: 2264.63 with loss1: 2030.96, loss2: 233.67 and loss3: 2347.77\n",
      "Epoch [321], train_loss: 2263.66 with loss1: 2031.61, loss2: 232.05 and loss3: 2340.93\n",
      "Epoch [322], train_loss: 2249.62 with loss1: 2017.57, loss2: 232.05 and loss3: 2334.09\n",
      "Epoch [323], train_loss: 2240.99 with loss1: 2010.28, loss2: 230.71 and loss3: 2327.26\n",
      "Epoch [324], train_loss: 2215.87 with loss1: 1985.07, loss2: 230.80 and loss3: 2320.45\n",
      "Epoch [325], train_loss: 2220.78 with loss1: 1991.16, loss2: 229.63 and loss3: 2313.65\n",
      "Epoch [326], train_loss: 2212.02 with loss1: 1983.75, loss2: 228.28 and loss3: 2306.85\n",
      "Epoch [327], train_loss: 2205.91 with loss1: 1977.42, loss2: 228.49 and loss3: 2300.07\n",
      "Epoch [328], train_loss: 2194.88 with loss1: 1967.86, loss2: 227.02 and loss3: 2293.29\n",
      "Epoch [329], train_loss: 2195.16 with loss1: 1967.47, loss2: 227.69 and loss3: 2286.53\n",
      "Epoch [330], train_loss: 2192.44 with loss1: 1965.47, loss2: 226.97 and loss3: 2279.78\n",
      "Epoch [331], train_loss: 2189.44 with loss1: 1963.02, loss2: 226.42 and loss3: 2273.03\n",
      "Epoch [332], train_loss: 2177.84 with loss1: 1953.67, loss2: 224.17 and loss3: 2266.29\n",
      "Epoch [333], train_loss: 2179.88 with loss1: 1955.47, loss2: 224.41 and loss3: 2259.56\n",
      "Epoch [334], train_loss: 2162.28 with loss1: 1937.16, loss2: 225.12 and loss3: 2252.84\n",
      "Epoch [335], train_loss: 2174.01 with loss1: 1950.54, loss2: 223.48 and loss3: 2246.13\n",
      "Epoch [336], train_loss: 2172.02 with loss1: 1949.29, loss2: 222.74 and loss3: 2239.43\n",
      "Epoch [337], train_loss: 2178.33 with loss1: 1955.99, loss2: 222.35 and loss3: 2232.74\n",
      "Epoch [338], train_loss: 2166.38 with loss1: 1943.73, loss2: 222.65 and loss3: 2226.06\n",
      "Epoch [339], train_loss: 2173.69 with loss1: 1952.48, loss2: 221.21 and loss3: 2219.39\n",
      "Epoch [340], train_loss: 2166.81 with loss1: 1946.00, loss2: 220.81 and loss3: 2212.73\n",
      "Epoch [341], train_loss: 2186.12 with loss1: 1965.01, loss2: 221.11 and loss3: 2206.08\n",
      "Epoch [342], train_loss: 2182.67 with loss1: 1962.89, loss2: 219.78 and loss3: 2199.44\n",
      "Epoch [343], train_loss: 2203.33 with loss1: 1983.90, loss2: 219.43 and loss3: 2192.81\n",
      "Epoch [344], train_loss: 2197.21 with loss1: 1978.04, loss2: 219.18 and loss3: 2186.20\n",
      "Epoch [345], train_loss: 2219.57 with loss1: 2000.52, loss2: 219.06 and loss3: 2179.59\n",
      "Epoch [346], train_loss: 2207.23 with loss1: 1989.92, loss2: 217.31 and loss3: 2173.00\n",
      "Epoch [347], train_loss: 2243.06 with loss1: 2024.61, loss2: 218.45 and loss3: 2166.41\n",
      "Epoch [348], train_loss: 2237.30 with loss1: 2021.11, loss2: 216.19 and loss3: 2159.83\n",
      "Epoch [349], train_loss: 2273.12 with loss1: 2056.31, loss2: 216.81 and loss3: 2153.25\n",
      "Epoch [350], train_loss: 2256.61 with loss1: 2042.01, loss2: 214.60 and loss3: 2146.70\n",
      "Epoch [351], train_loss: 2304.42 with loss1: 2087.99, loss2: 216.43 and loss3: 2140.15\n",
      "Epoch [352], train_loss: 2263.99 with loss1: 2049.40, loss2: 214.58 and loss3: 2133.60\n",
      "Epoch [353], train_loss: 2313.71 with loss1: 2099.73, loss2: 213.98 and loss3: 2127.06\n",
      "Epoch [354], train_loss: 2267.13 with loss1: 2054.74, loss2: 212.39 and loss3: 2120.54\n",
      "Epoch [355], train_loss: 2307.40 with loss1: 2093.70, loss2: 213.70 and loss3: 2114.02\n",
      "Epoch [356], train_loss: 2264.77 with loss1: 2053.25, loss2: 211.51 and loss3: 2107.51\n",
      "Epoch [357], train_loss: 2302.35 with loss1: 2089.50, loss2: 212.85 and loss3: 2101.00\n",
      "Epoch [358], train_loss: 2238.37 with loss1: 2028.07, loss2: 210.31 and loss3: 2094.51\n",
      "Epoch [359], train_loss: 2265.09 with loss1: 2054.47, loss2: 210.63 and loss3: 2088.02\n",
      "Epoch [360], train_loss: 2210.95 with loss1: 2001.62, loss2: 209.33 and loss3: 2081.53\n",
      "Epoch [361], train_loss: 2231.35 with loss1: 2021.40, loss2: 209.95 and loss3: 2075.06\n",
      "Epoch [362], train_loss: 2173.59 with loss1: 1964.58, loss2: 209.02 and loss3: 2068.59\n",
      "Epoch [363], train_loss: 2179.72 with loss1: 1971.37, loss2: 208.34 and loss3: 2062.13\n",
      "Epoch [364], train_loss: 2141.22 with loss1: 1934.60, loss2: 206.61 and loss3: 2055.68\n",
      "Epoch [365], train_loss: 2142.97 with loss1: 1935.61, loss2: 207.36 and loss3: 2049.24\n",
      "Epoch [366], train_loss: 2102.79 with loss1: 1896.95, loss2: 205.84 and loss3: 2042.81\n",
      "Epoch [367], train_loss: 2106.43 with loss1: 1899.75, loss2: 206.68 and loss3: 2036.39\n",
      "Epoch [368], train_loss: 2079.01 with loss1: 1874.14, loss2: 204.87 and loss3: 2029.97\n",
      "Epoch [369], train_loss: 2083.36 with loss1: 1878.29, loss2: 205.07 and loss3: 2023.56\n",
      "Epoch [370], train_loss: 2053.49 with loss1: 1849.46, loss2: 204.03 and loss3: 2017.16\n",
      "Epoch [371], train_loss: 2057.37 with loss1: 1852.75, loss2: 204.62 and loss3: 2010.77\n",
      "Epoch [372], train_loss: 2039.99 with loss1: 1837.21, loss2: 202.78 and loss3: 2004.40\n",
      "Epoch [373], train_loss: 2044.39 with loss1: 1841.24, loss2: 203.15 and loss3: 1998.03\n",
      "Epoch [374], train_loss: 2036.79 with loss1: 1835.40, loss2: 201.38 and loss3: 1991.67\n",
      "Epoch [375], train_loss: 2047.27 with loss1: 1845.41, loss2: 201.86 and loss3: 1985.32\n",
      "Epoch [376], train_loss: 2030.82 with loss1: 1829.47, loss2: 201.35 and loss3: 1978.98\n",
      "Epoch [377], train_loss: 2027.01 with loss1: 1826.09, loss2: 200.92 and loss3: 1972.65\n",
      "Epoch [378], train_loss: 2014.80 with loss1: 1814.90, loss2: 199.90 and loss3: 1966.34\n",
      "Epoch [379], train_loss: 2026.56 with loss1: 1825.90, loss2: 200.66 and loss3: 1960.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [380], train_loss: 2014.26 with loss1: 1815.02, loss2: 199.24 and loss3: 1953.76\n",
      "Epoch [381], train_loss: 2020.35 with loss1: 1820.60, loss2: 199.75 and loss3: 1947.48\n",
      "Epoch [382], train_loss: 2007.35 with loss1: 1808.95, loss2: 198.40 and loss3: 1941.21\n",
      "Epoch [383], train_loss: 2020.84 with loss1: 1821.99, loss2: 198.86 and loss3: 1934.95\n",
      "Epoch [384], train_loss: 2007.53 with loss1: 1811.11, loss2: 196.42 and loss3: 1928.70\n",
      "Epoch [385], train_loss: 2018.96 with loss1: 1821.70, loss2: 197.26 and loss3: 1922.46\n",
      "Epoch [386], train_loss: 2000.19 with loss1: 1804.08, loss2: 196.10 and loss3: 1916.22\n",
      "Epoch [387], train_loss: 2013.32 with loss1: 1817.18, loss2: 196.14 and loss3: 1910.00\n",
      "Epoch [388], train_loss: 2006.98 with loss1: 1812.24, loss2: 194.74 and loss3: 1903.79\n",
      "Epoch [389], train_loss: 2017.18 with loss1: 1821.70, loss2: 195.48 and loss3: 1897.59\n",
      "Epoch [390], train_loss: 2001.45 with loss1: 1807.11, loss2: 194.34 and loss3: 1891.40\n",
      "Epoch [391], train_loss: 2007.90 with loss1: 1813.10, loss2: 194.80 and loss3: 1885.22\n",
      "Epoch [392], train_loss: 1997.02 with loss1: 1803.81, loss2: 193.22 and loss3: 1879.04\n",
      "Epoch [393], train_loss: 2015.84 with loss1: 1822.09, loss2: 193.75 and loss3: 1872.88\n",
      "Epoch [394], train_loss: 1996.73 with loss1: 1804.13, loss2: 192.60 and loss3: 1866.72\n",
      "Epoch [395], train_loss: 2022.76 with loss1: 1830.67, loss2: 192.08 and loss3: 1860.57\n",
      "Epoch [396], train_loss: 1996.44 with loss1: 1805.25, loss2: 191.18 and loss3: 1854.43\n",
      "Epoch [397], train_loss: 2027.49 with loss1: 1836.25, loss2: 191.24 and loss3: 1848.31\n",
      "Epoch [398], train_loss: 2006.32 with loss1: 1815.44, loss2: 190.89 and loss3: 1842.19\n",
      "Epoch [399], train_loss: 2011.73 with loss1: 1820.58, loss2: 191.15 and loss3: 1836.08\n",
      "Epoch [400], train_loss: 1992.27 with loss1: 1801.84, loss2: 190.43 and loss3: 1829.99\n",
      "Epoch [401], train_loss: 2007.26 with loss1: 1817.05, loss2: 190.21 and loss3: 1823.90\n",
      "Epoch [402], train_loss: 1984.02 with loss1: 1794.95, loss2: 189.06 and loss3: 1817.82\n",
      "Epoch [403], train_loss: 2001.37 with loss1: 1812.97, loss2: 188.40 and loss3: 1811.76\n",
      "Epoch [404], train_loss: 1987.03 with loss1: 1798.83, loss2: 188.21 and loss3: 1805.70\n",
      "Epoch [405], train_loss: 2002.99 with loss1: 1815.24, loss2: 187.75 and loss3: 1799.66\n",
      "Epoch [406], train_loss: 1989.86 with loss1: 1802.72, loss2: 187.15 and loss3: 1793.63\n",
      "Epoch [407], train_loss: 2000.15 with loss1: 1813.55, loss2: 186.60 and loss3: 1787.60\n",
      "Epoch [408], train_loss: 1993.73 with loss1: 1806.72, loss2: 187.02 and loss3: 1781.58\n",
      "Epoch [409], train_loss: 2000.20 with loss1: 1814.02, loss2: 186.17 and loss3: 1775.57\n",
      "Epoch [410], train_loss: 1982.17 with loss1: 1796.83, loss2: 185.34 and loss3: 1769.57\n",
      "Epoch [411], train_loss: 1998.19 with loss1: 1813.05, loss2: 185.13 and loss3: 1763.58\n",
      "Epoch [412], train_loss: 1979.70 with loss1: 1795.30, loss2: 184.40 and loss3: 1757.60\n",
      "Epoch [413], train_loss: 1990.63 with loss1: 1807.01, loss2: 183.62 and loss3: 1751.63\n",
      "Epoch [414], train_loss: 1967.86 with loss1: 1783.73, loss2: 184.14 and loss3: 1745.66\n",
      "Epoch [415], train_loss: 1981.69 with loss1: 1798.44, loss2: 183.25 and loss3: 1739.71\n",
      "Epoch [416], train_loss: 1963.61 with loss1: 1780.89, loss2: 182.72 and loss3: 1733.76\n",
      "Epoch [417], train_loss: 1974.28 with loss1: 1791.69, loss2: 182.60 and loss3: 1727.83\n",
      "Epoch [418], train_loss: 1957.65 with loss1: 1775.57, loss2: 182.08 and loss3: 1721.91\n",
      "Epoch [419], train_loss: 1971.31 with loss1: 1789.58, loss2: 181.73 and loss3: 1716.00\n",
      "Epoch [420], train_loss: 1958.51 with loss1: 1777.60, loss2: 180.91 and loss3: 1710.09\n",
      "Epoch [421], train_loss: 1967.30 with loss1: 1786.53, loss2: 180.76 and loss3: 1704.20\n",
      "Epoch [422], train_loss: 1949.80 with loss1: 1769.82, loss2: 179.98 and loss3: 1698.31\n",
      "Epoch [423], train_loss: 1960.87 with loss1: 1781.02, loss2: 179.84 and loss3: 1692.44\n",
      "Epoch [424], train_loss: 1936.56 with loss1: 1757.32, loss2: 179.24 and loss3: 1686.57\n",
      "Epoch [425], train_loss: 1940.53 with loss1: 1762.16, loss2: 178.37 and loss3: 1680.72\n",
      "Epoch [426], train_loss: 1926.10 with loss1: 1748.02, loss2: 178.08 and loss3: 1674.88\n",
      "Epoch [427], train_loss: 1932.30 with loss1: 1753.96, loss2: 178.34 and loss3: 1669.05\n",
      "Epoch [428], train_loss: 1917.46 with loss1: 1739.50, loss2: 177.95 and loss3: 1663.23\n",
      "Epoch [429], train_loss: 1931.94 with loss1: 1755.03, loss2: 176.91 and loss3: 1657.42\n",
      "Epoch [430], train_loss: 1912.38 with loss1: 1735.31, loss2: 177.07 and loss3: 1651.63\n",
      "Epoch [431], train_loss: 1910.32 with loss1: 1734.03, loss2: 176.29 and loss3: 1645.85\n",
      "Epoch [432], train_loss: 1895.46 with loss1: 1718.86, loss2: 176.60 and loss3: 1640.07\n",
      "Epoch [433], train_loss: 1905.95 with loss1: 1730.25, loss2: 175.70 and loss3: 1634.30\n",
      "Epoch [434], train_loss: 1884.69 with loss1: 1709.15, loss2: 175.54 and loss3: 1628.55\n",
      "Epoch [435], train_loss: 1895.25 with loss1: 1721.19, loss2: 174.06 and loss3: 1622.81\n",
      "Epoch [436], train_loss: 1883.91 with loss1: 1709.05, loss2: 174.87 and loss3: 1617.07\n",
      "Epoch [437], train_loss: 1880.58 with loss1: 1706.25, loss2: 174.33 and loss3: 1611.35\n",
      "Epoch [438], train_loss: 1869.52 with loss1: 1696.00, loss2: 173.51 and loss3: 1605.65\n",
      "Epoch [439], train_loss: 1878.09 with loss1: 1705.18, loss2: 172.91 and loss3: 1599.95\n",
      "Epoch [440], train_loss: 1874.40 with loss1: 1701.40, loss2: 173.00 and loss3: 1594.25\n",
      "Epoch [441], train_loss: 1863.05 with loss1: 1690.94, loss2: 172.11 and loss3: 1588.57\n",
      "Epoch [442], train_loss: 1856.49 with loss1: 1684.65, loss2: 171.84 and loss3: 1582.90\n",
      "Epoch [443], train_loss: 1861.77 with loss1: 1691.00, loss2: 170.76 and loss3: 1577.24\n",
      "Epoch [444], train_loss: 1847.90 with loss1: 1677.43, loss2: 170.47 and loss3: 1571.59\n",
      "Epoch [445], train_loss: 1854.52 with loss1: 1684.82, loss2: 169.70 and loss3: 1565.95\n",
      "Epoch [446], train_loss: 1842.71 with loss1: 1672.83, loss2: 169.88 and loss3: 1560.33\n",
      "Epoch [447], train_loss: 1856.05 with loss1: 1687.65, loss2: 168.40 and loss3: 1554.71\n",
      "Epoch [448], train_loss: 1844.29 with loss1: 1675.36, loss2: 168.94 and loss3: 1549.10\n",
      "Epoch [449], train_loss: 1851.42 with loss1: 1683.55, loss2: 167.86 and loss3: 1543.50\n",
      "Epoch [450], train_loss: 1838.71 with loss1: 1670.42, loss2: 168.28 and loss3: 1537.90\n",
      "Epoch [451], train_loss: 1843.63 with loss1: 1675.87, loss2: 167.75 and loss3: 1532.32\n",
      "Epoch [452], train_loss: 1838.39 with loss1: 1670.40, loss2: 167.99 and loss3: 1526.75\n",
      "Epoch [453], train_loss: 1835.98 with loss1: 1669.03, loss2: 166.95 and loss3: 1521.19\n",
      "Epoch [454], train_loss: 1825.80 with loss1: 1658.96, loss2: 166.84 and loss3: 1515.63\n",
      "Epoch [455], train_loss: 1822.05 with loss1: 1655.42, loss2: 166.63 and loss3: 1510.08\n",
      "Epoch [456], train_loss: 1811.98 with loss1: 1645.73, loss2: 166.25 and loss3: 1504.54\n",
      "Epoch [457], train_loss: 1809.94 with loss1: 1644.51, loss2: 165.43 and loss3: 1499.01\n",
      "Epoch [458], train_loss: 1795.83 with loss1: 1630.84, loss2: 164.99 and loss3: 1493.49\n",
      "Epoch [459], train_loss: 1792.31 with loss1: 1627.77, loss2: 164.54 and loss3: 1487.98\n",
      "Epoch [460], train_loss: 1787.52 with loss1: 1623.51, loss2: 164.01 and loss3: 1482.49\n",
      "Epoch [461], train_loss: 1782.19 with loss1: 1618.60, loss2: 163.59 and loss3: 1477.00\n",
      "Epoch [462], train_loss: 1773.38 with loss1: 1609.85, loss2: 163.53 and loss3: 1471.52\n",
      "Epoch [463], train_loss: 1775.33 with loss1: 1613.41, loss2: 161.92 and loss3: 1466.06\n",
      "Epoch [464], train_loss: 1764.84 with loss1: 1602.32, loss2: 162.53 and loss3: 1460.61\n",
      "Epoch [465], train_loss: 1757.41 with loss1: 1595.91, loss2: 161.50 and loss3: 1455.17\n",
      "Epoch [466], train_loss: 1755.51 with loss1: 1593.46, loss2: 162.05 and loss3: 1449.74\n",
      "Epoch [467], train_loss: 1750.39 with loss1: 1589.78, loss2: 160.61 and loss3: 1444.31\n",
      "Epoch [468], train_loss: 1750.29 with loss1: 1589.41, loss2: 160.88 and loss3: 1438.90\n",
      "Epoch [469], train_loss: 1746.63 with loss1: 1586.60, loss2: 160.04 and loss3: 1433.50\n",
      "Epoch [470], train_loss: 1742.50 with loss1: 1582.01, loss2: 160.49 and loss3: 1428.10\n",
      "Epoch [471], train_loss: 1739.73 with loss1: 1580.95, loss2: 158.78 and loss3: 1422.71\n",
      "Epoch [472], train_loss: 1738.54 with loss1: 1579.01, loss2: 159.53 and loss3: 1417.33\n",
      "Epoch [473], train_loss: 1728.46 with loss1: 1569.87, loss2: 158.59 and loss3: 1411.96\n",
      "Epoch [474], train_loss: 1721.72 with loss1: 1563.57, loss2: 158.15 and loss3: 1406.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [475], train_loss: 1715.95 with loss1: 1558.24, loss2: 157.71 and loss3: 1401.24\n",
      "Epoch [476], train_loss: 1718.65 with loss1: 1561.24, loss2: 157.41 and loss3: 1395.90\n",
      "Epoch [477], train_loss: 1706.22 with loss1: 1549.17, loss2: 157.05 and loss3: 1390.57\n",
      "Epoch [478], train_loss: 1705.93 with loss1: 1549.34, loss2: 156.59 and loss3: 1385.25\n",
      "Epoch [479], train_loss: 1704.88 with loss1: 1548.85, loss2: 156.03 and loss3: 1379.94\n",
      "Epoch [480], train_loss: 1701.05 with loss1: 1544.61, loss2: 156.45 and loss3: 1374.64\n",
      "Epoch [481], train_loss: 1700.48 with loss1: 1545.27, loss2: 155.22 and loss3: 1369.36\n",
      "Epoch [482], train_loss: 1702.06 with loss1: 1546.48, loss2: 155.57 and loss3: 1364.08\n",
      "Epoch [483], train_loss: 1690.88 with loss1: 1536.22, loss2: 154.67 and loss3: 1358.81\n",
      "Epoch [484], train_loss: 1689.41 with loss1: 1534.44, loss2: 154.96 and loss3: 1353.55\n",
      "Epoch [485], train_loss: 1692.98 with loss1: 1539.03, loss2: 153.95 and loss3: 1348.31\n",
      "Epoch [486], train_loss: 1684.56 with loss1: 1529.90, loss2: 154.66 and loss3: 1343.07\n",
      "Epoch [487], train_loss: 1690.42 with loss1: 1537.24, loss2: 153.18 and loss3: 1337.84\n",
      "Epoch [488], train_loss: 1687.85 with loss1: 1534.27, loss2: 153.58 and loss3: 1332.62\n",
      "Epoch [489], train_loss: 1692.27 with loss1: 1539.87, loss2: 152.40 and loss3: 1327.41\n",
      "Epoch [490], train_loss: 1699.84 with loss1: 1546.98, loss2: 152.86 and loss3: 1322.21\n",
      "Epoch [491], train_loss: 1712.79 with loss1: 1560.68, loss2: 152.10 and loss3: 1317.03\n",
      "Epoch [492], train_loss: 1715.58 with loss1: 1563.31, loss2: 152.27 and loss3: 1311.85\n",
      "Epoch [493], train_loss: 1731.32 with loss1: 1579.62, loss2: 151.70 and loss3: 1306.68\n",
      "Epoch [494], train_loss: 1732.50 with loss1: 1581.05, loss2: 151.46 and loss3: 1301.53\n",
      "Epoch [495], train_loss: 1752.26 with loss1: 1601.84, loss2: 150.42 and loss3: 1296.38\n",
      "Epoch [496], train_loss: 1765.82 with loss1: 1615.47, loss2: 150.35 and loss3: 1291.24\n",
      "Epoch [497], train_loss: 1804.66 with loss1: 1654.62, loss2: 150.03 and loss3: 1286.12\n",
      "Epoch [498], train_loss: 1803.60 with loss1: 1653.79, loss2: 149.81 and loss3: 1281.00\n",
      "Epoch [499], train_loss: 1834.27 with loss1: 1684.72, loss2: 149.56 and loss3: 1275.90\n",
      "Epoch [500], train_loss: 1837.14 with loss1: 1687.58, loss2: 149.55 and loss3: 1270.82\n",
      "Epoch [501], train_loss: 1861.80 with loss1: 1712.67, loss2: 149.13 and loss3: 1265.75\n",
      "Epoch [502], train_loss: 1842.99 with loss1: 1694.45, loss2: 148.53 and loss3: 1260.68\n",
      "Epoch [503], train_loss: 1878.57 with loss1: 1731.09, loss2: 147.49 and loss3: 1255.62\n",
      "Epoch [504], train_loss: 1849.19 with loss1: 1701.70, loss2: 147.49 and loss3: 1250.57\n",
      "Epoch [505], train_loss: 1869.21 with loss1: 1721.78, loss2: 147.43 and loss3: 1245.54\n",
      "Epoch [506], train_loss: 1834.61 with loss1: 1687.74, loss2: 146.88 and loss3: 1240.51\n",
      "Epoch [507], train_loss: 1845.37 with loss1: 1699.07, loss2: 146.30 and loss3: 1235.49\n",
      "Epoch [508], train_loss: 1815.35 with loss1: 1669.41, loss2: 145.94 and loss3: 1230.48\n",
      "Epoch [509], train_loss: 1827.04 with loss1: 1681.40, loss2: 145.64 and loss3: 1225.49\n",
      "Epoch [510], train_loss: 1785.95 with loss1: 1640.90, loss2: 145.05 and loss3: 1220.50\n",
      "Epoch [511], train_loss: 1793.91 with loss1: 1648.77, loss2: 145.14 and loss3: 1215.53\n",
      "Epoch [512], train_loss: 1760.71 with loss1: 1616.15, loss2: 144.56 and loss3: 1210.56\n",
      "Epoch [513], train_loss: 1766.25 with loss1: 1622.04, loss2: 144.20 and loss3: 1205.61\n",
      "Epoch [514], train_loss: 1737.06 with loss1: 1593.32, loss2: 143.73 and loss3: 1200.66\n",
      "Epoch [515], train_loss: 1757.51 with loss1: 1614.25, loss2: 143.27 and loss3: 1195.72\n",
      "Epoch [516], train_loss: 1718.48 with loss1: 1575.17, loss2: 143.31 and loss3: 1190.79\n",
      "Epoch [517], train_loss: 1727.07 with loss1: 1584.60, loss2: 142.46 and loss3: 1185.87\n",
      "Epoch [518], train_loss: 1696.92 with loss1: 1554.71, loss2: 142.21 and loss3: 1180.95\n",
      "Epoch [519], train_loss: 1704.40 with loss1: 1562.37, loss2: 142.03 and loss3: 1176.05\n",
      "Epoch [520], train_loss: 1679.50 with loss1: 1538.16, loss2: 141.35 and loss3: 1171.15\n",
      "Epoch [521], train_loss: 1680.38 with loss1: 1539.34, loss2: 141.04 and loss3: 1166.27\n",
      "Epoch [522], train_loss: 1661.77 with loss1: 1521.09, loss2: 140.68 and loss3: 1161.39\n",
      "Epoch [523], train_loss: 1664.61 with loss1: 1524.12, loss2: 140.49 and loss3: 1156.53\n",
      "Epoch [524], train_loss: 1652.69 with loss1: 1512.80, loss2: 139.89 and loss3: 1151.67\n",
      "Epoch [525], train_loss: 1654.06 with loss1: 1514.20, loss2: 139.86 and loss3: 1146.83\n",
      "Epoch [526], train_loss: 1636.23 with loss1: 1496.54, loss2: 139.70 and loss3: 1142.00\n",
      "Epoch [527], train_loss: 1642.34 with loss1: 1503.30, loss2: 139.04 and loss3: 1137.19\n",
      "Epoch [528], train_loss: 1633.56 with loss1: 1494.78, loss2: 138.78 and loss3: 1132.38\n",
      "Epoch [529], train_loss: 1636.64 with loss1: 1498.35, loss2: 138.29 and loss3: 1127.59\n",
      "Epoch [530], train_loss: 1626.39 with loss1: 1488.28, loss2: 138.11 and loss3: 1122.81\n",
      "Epoch [531], train_loss: 1631.26 with loss1: 1493.00, loss2: 138.26 and loss3: 1118.04\n",
      "Epoch [532], train_loss: 1616.78 with loss1: 1479.71, loss2: 137.07 and loss3: 1113.28\n",
      "Epoch [533], train_loss: 1629.98 with loss1: 1492.40, loss2: 137.58 and loss3: 1108.54\n",
      "Epoch [534], train_loss: 1611.44 with loss1: 1474.70, loss2: 136.74 and loss3: 1103.80\n",
      "Epoch [535], train_loss: 1622.43 with loss1: 1485.85, loss2: 136.58 and loss3: 1099.07\n",
      "Epoch [536], train_loss: 1613.91 with loss1: 1477.45, loss2: 136.46 and loss3: 1094.34\n",
      "Epoch [537], train_loss: 1623.55 with loss1: 1487.26, loss2: 136.29 and loss3: 1089.63\n",
      "Epoch [538], train_loss: 1608.94 with loss1: 1472.92, loss2: 136.02 and loss3: 1084.93\n",
      "Epoch [539], train_loss: 1626.98 with loss1: 1491.21, loss2: 135.77 and loss3: 1080.24\n",
      "Epoch [540], train_loss: 1611.22 with loss1: 1475.91, loss2: 135.31 and loss3: 1075.56\n",
      "Epoch [541], train_loss: 1615.34 with loss1: 1480.28, loss2: 135.06 and loss3: 1070.89\n",
      "Epoch [542], train_loss: 1602.90 with loss1: 1468.50, loss2: 134.40 and loss3: 1066.23\n",
      "Epoch [543], train_loss: 1616.11 with loss1: 1481.82, loss2: 134.29 and loss3: 1061.59\n",
      "Epoch [544], train_loss: 1595.66 with loss1: 1461.68, loss2: 133.98 and loss3: 1056.94\n",
      "Epoch [545], train_loss: 1610.64 with loss1: 1476.92, loss2: 133.71 and loss3: 1052.31\n",
      "Epoch [546], train_loss: 1596.90 with loss1: 1463.62, loss2: 133.28 and loss3: 1047.68\n",
      "Epoch [547], train_loss: 1613.08 with loss1: 1479.86, loss2: 133.22 and loss3: 1043.06\n",
      "Epoch [548], train_loss: 1592.53 with loss1: 1459.82, loss2: 132.71 and loss3: 1038.45\n",
      "Epoch [549], train_loss: 1602.36 with loss1: 1469.51, loss2: 132.86 and loss3: 1033.85\n",
      "Epoch [550], train_loss: 1592.12 with loss1: 1459.85, loss2: 132.27 and loss3: 1029.26\n",
      "Epoch [551], train_loss: 1603.18 with loss1: 1471.00, loss2: 132.18 and loss3: 1024.68\n",
      "Epoch [552], train_loss: 1586.68 with loss1: 1454.60, loss2: 132.09 and loss3: 1020.12\n",
      "Epoch [553], train_loss: 1594.82 with loss1: 1463.22, loss2: 131.59 and loss3: 1015.56\n",
      "Epoch [554], train_loss: 1588.44 with loss1: 1457.29, loss2: 131.14 and loss3: 1011.02\n",
      "Epoch [555], train_loss: 1588.20 with loss1: 1457.16, loss2: 131.04 and loss3: 1006.49\n",
      "Epoch [556], train_loss: 1577.09 with loss1: 1446.21, loss2: 130.87 and loss3: 1001.97\n",
      "Epoch [557], train_loss: 1585.27 with loss1: 1455.03, loss2: 130.24 and loss3: 997.46\n",
      "Epoch [558], train_loss: 1567.77 with loss1: 1437.59, loss2: 130.18 and loss3: 992.95\n",
      "Epoch [559], train_loss: 1574.45 with loss1: 1444.48, loss2: 129.98 and loss3: 988.45\n",
      "Epoch [560], train_loss: 1558.93 with loss1: 1429.40, loss2: 129.52 and loss3: 983.97\n",
      "Epoch [561], train_loss: 1571.90 with loss1: 1442.34, loss2: 129.55 and loss3: 979.49\n",
      "Epoch [562], train_loss: 1557.19 with loss1: 1428.47, loss2: 128.71 and loss3: 975.02\n",
      "Epoch [563], train_loss: 1562.84 with loss1: 1433.75, loss2: 129.08 and loss3: 970.57\n",
      "Epoch [564], train_loss: 1548.70 with loss1: 1420.62, loss2: 128.08 and loss3: 966.12\n",
      "Epoch [565], train_loss: 1553.87 with loss1: 1425.95, loss2: 127.92 and loss3: 961.68\n",
      "Epoch [566], train_loss: 1545.62 with loss1: 1418.17, loss2: 127.46 and loss3: 957.26\n",
      "Epoch [567], train_loss: 1552.81 with loss1: 1425.16, loss2: 127.65 and loss3: 952.84\n",
      "Epoch [568], train_loss: 1542.76 with loss1: 1415.57, loss2: 127.19 and loss3: 948.44\n",
      "Epoch [569], train_loss: 1555.00 with loss1: 1427.96, loss2: 127.04 and loss3: 944.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [570], train_loss: 1538.88 with loss1: 1412.14, loss2: 126.73 and loss3: 939.66\n",
      "Epoch [571], train_loss: 1547.53 with loss1: 1420.78, loss2: 126.74 and loss3: 935.28\n",
      "Epoch [572], train_loss: 1538.55 with loss1: 1412.26, loss2: 126.29 and loss3: 930.91\n",
      "Epoch [573], train_loss: 1549.97 with loss1: 1423.93, loss2: 126.05 and loss3: 926.56\n",
      "Epoch [574], train_loss: 1535.63 with loss1: 1410.09, loss2: 125.53 and loss3: 922.21\n",
      "Epoch [575], train_loss: 1543.92 with loss1: 1418.32, loss2: 125.60 and loss3: 917.88\n",
      "Epoch [576], train_loss: 1528.65 with loss1: 1404.22, loss2: 124.43 and loss3: 913.56\n",
      "Epoch [577], train_loss: 1539.88 with loss1: 1415.10, loss2: 124.78 and loss3: 909.24\n",
      "Epoch [578], train_loss: 1521.57 with loss1: 1397.23, loss2: 124.34 and loss3: 904.94\n",
      "Epoch [579], train_loss: 1537.37 with loss1: 1413.25, loss2: 124.12 and loss3: 900.65\n",
      "Epoch [580], train_loss: 1531.43 with loss1: 1407.16, loss2: 124.27 and loss3: 896.36\n",
      "Epoch [581], train_loss: 1530.35 with loss1: 1406.61, loss2: 123.74 and loss3: 892.09\n",
      "Epoch [582], train_loss: 1525.35 with loss1: 1401.95, loss2: 123.40 and loss3: 887.82\n",
      "Epoch [583], train_loss: 1538.88 with loss1: 1415.55, loss2: 123.33 and loss3: 883.57\n",
      "Epoch [584], train_loss: 1524.18 with loss1: 1401.37, loss2: 122.81 and loss3: 879.32\n",
      "Epoch [585], train_loss: 1531.58 with loss1: 1408.92, loss2: 122.66 and loss3: 875.08\n",
      "Epoch [586], train_loss: 1517.85 with loss1: 1395.22, loss2: 122.62 and loss3: 870.84\n",
      "Epoch [587], train_loss: 1529.25 with loss1: 1406.83, loss2: 122.41 and loss3: 866.62\n",
      "Epoch [588], train_loss: 1516.88 with loss1: 1394.66, loss2: 122.22 and loss3: 862.41\n",
      "Epoch [589], train_loss: 1521.87 with loss1: 1400.14, loss2: 121.72 and loss3: 858.20\n",
      "Epoch [590], train_loss: 1511.05 with loss1: 1389.26, loss2: 121.78 and loss3: 854.01\n",
      "Epoch [591], train_loss: 1518.74 with loss1: 1397.67, loss2: 121.07 and loss3: 849.82\n",
      "Epoch [592], train_loss: 1507.01 with loss1: 1386.20, loss2: 120.81 and loss3: 845.65\n",
      "Epoch [593], train_loss: 1511.84 with loss1: 1390.74, loss2: 121.11 and loss3: 841.48\n",
      "Epoch [594], train_loss: 1502.40 with loss1: 1381.89, loss2: 120.50 and loss3: 837.32\n",
      "Epoch [595], train_loss: 1515.83 with loss1: 1395.51, loss2: 120.33 and loss3: 833.16\n",
      "Epoch [596], train_loss: 1493.23 with loss1: 1372.95, loss2: 120.28 and loss3: 829.01\n",
      "Epoch [597], train_loss: 1499.41 with loss1: 1379.51, loss2: 119.89 and loss3: 824.87\n",
      "Epoch [598], train_loss: 1491.10 with loss1: 1371.77, loss2: 119.33 and loss3: 820.75\n",
      "Epoch [599], train_loss: 1498.07 with loss1: 1378.83, loss2: 119.24 and loss3: 816.64\n",
      "Epoch [600], train_loss: 1485.62 with loss1: 1366.30, loss2: 119.33 and loss3: 812.53\n",
      "Epoch [601], train_loss: 1487.43 with loss1: 1368.40, loss2: 119.03 and loss3: 808.44\n",
      "Epoch [602], train_loss: 1479.89 with loss1: 1361.53, loss2: 118.36 and loss3: 804.37\n",
      "Epoch [603], train_loss: 1490.47 with loss1: 1372.27, loss2: 118.20 and loss3: 800.30\n",
      "Epoch [604], train_loss: 1473.82 with loss1: 1355.91, loss2: 117.91 and loss3: 796.24\n",
      "Epoch [605], train_loss: 1486.30 with loss1: 1368.29, loss2: 118.01 and loss3: 792.20\n",
      "Epoch [606], train_loss: 1470.72 with loss1: 1353.07, loss2: 117.65 and loss3: 788.16\n",
      "Epoch [607], train_loss: 1481.34 with loss1: 1363.63, loss2: 117.71 and loss3: 784.14\n",
      "Epoch [608], train_loss: 1476.82 with loss1: 1359.36, loss2: 117.47 and loss3: 780.13\n",
      "Epoch [609], train_loss: 1474.54 with loss1: 1357.13, loss2: 117.41 and loss3: 776.12\n",
      "Epoch [610], train_loss: 1467.99 with loss1: 1351.23, loss2: 116.76 and loss3: 772.12\n",
      "Epoch [611], train_loss: 1474.33 with loss1: 1357.52, loss2: 116.80 and loss3: 768.14\n",
      "Epoch [612], train_loss: 1456.82 with loss1: 1340.42, loss2: 116.41 and loss3: 764.16\n",
      "Epoch [613], train_loss: 1467.47 with loss1: 1351.22, loss2: 116.25 and loss3: 760.20\n",
      "Epoch [614], train_loss: 1455.25 with loss1: 1339.40, loss2: 115.85 and loss3: 756.24\n",
      "Epoch [615], train_loss: 1457.96 with loss1: 1342.26, loss2: 115.70 and loss3: 752.29\n",
      "Epoch [616], train_loss: 1448.18 with loss1: 1333.06, loss2: 115.12 and loss3: 748.35\n",
      "Epoch [617], train_loss: 1457.11 with loss1: 1341.91, loss2: 115.20 and loss3: 744.43\n",
      "Epoch [618], train_loss: 1450.21 with loss1: 1334.95, loss2: 115.26 and loss3: 740.51\n",
      "Epoch [619], train_loss: 1453.58 with loss1: 1338.81, loss2: 114.77 and loss3: 736.60\n",
      "Epoch [620], train_loss: 1440.81 with loss1: 1326.35, loss2: 114.47 and loss3: 732.69\n",
      "Epoch [621], train_loss: 1455.90 with loss1: 1341.23, loss2: 114.67 and loss3: 728.80\n",
      "Epoch [622], train_loss: 1442.49 with loss1: 1328.46, loss2: 114.03 and loss3: 724.92\n",
      "Epoch [623], train_loss: 1455.07 with loss1: 1340.82, loss2: 114.25 and loss3: 721.05\n",
      "Epoch [624], train_loss: 1440.40 with loss1: 1326.71, loss2: 113.69 and loss3: 717.19\n",
      "Epoch [625], train_loss: 1448.60 with loss1: 1334.88, loss2: 113.72 and loss3: 713.33\n",
      "Epoch [626], train_loss: 1440.44 with loss1: 1327.39, loss2: 113.04 and loss3: 709.49\n",
      "Epoch [627], train_loss: 1448.92 with loss1: 1335.42, loss2: 113.49 and loss3: 705.66\n",
      "Epoch [628], train_loss: 1437.36 with loss1: 1324.39, loss2: 112.97 and loss3: 701.84\n",
      "Epoch [629], train_loss: 1445.06 with loss1: 1332.19, loss2: 112.87 and loss3: 698.03\n",
      "Epoch [630], train_loss: 1436.54 with loss1: 1323.41, loss2: 113.14 and loss3: 694.23\n",
      "Epoch [631], train_loss: 1442.35 with loss1: 1329.97, loss2: 112.37 and loss3: 690.44\n",
      "Epoch [632], train_loss: 1430.50 with loss1: 1318.09, loss2: 112.41 and loss3: 686.66\n",
      "Epoch [633], train_loss: 1442.25 with loss1: 1329.99, loss2: 112.25 and loss3: 682.90\n",
      "Epoch [634], train_loss: 1425.92 with loss1: 1314.27, loss2: 111.64 and loss3: 679.14\n",
      "Epoch [635], train_loss: 1432.81 with loss1: 1320.93, loss2: 111.88 and loss3: 675.40\n",
      "Epoch [636], train_loss: 1422.01 with loss1: 1310.37, loss2: 111.64 and loss3: 671.67\n",
      "Epoch [637], train_loss: 1434.52 with loss1: 1322.96, loss2: 111.56 and loss3: 667.96\n",
      "Epoch [638], train_loss: 1418.45 with loss1: 1307.34, loss2: 111.11 and loss3: 664.25\n",
      "Epoch [639], train_loss: 1426.95 with loss1: 1315.61, loss2: 111.34 and loss3: 660.55\n",
      "Epoch [640], train_loss: 1416.68 with loss1: 1305.93, loss2: 110.75 and loss3: 656.86\n",
      "Epoch [641], train_loss: 1421.77 with loss1: 1310.91, loss2: 110.86 and loss3: 653.18\n",
      "Epoch [642], train_loss: 1410.58 with loss1: 1299.91, loss2: 110.68 and loss3: 649.50\n",
      "Epoch [643], train_loss: 1417.29 with loss1: 1306.90, loss2: 110.39 and loss3: 645.84\n",
      "Epoch [644], train_loss: 1399.51 with loss1: 1289.26, loss2: 110.25 and loss3: 642.18\n",
      "Epoch [645], train_loss: 1406.55 with loss1: 1296.26, loss2: 110.29 and loss3: 638.53\n",
      "Epoch [646], train_loss: 1388.44 with loss1: 1278.48, loss2: 109.96 and loss3: 634.89\n",
      "Epoch [647], train_loss: 1390.63 with loss1: 1280.90, loss2: 109.73 and loss3: 631.26\n",
      "Epoch [648], train_loss: 1382.89 with loss1: 1273.64, loss2: 109.25 and loss3: 627.63\n",
      "Epoch [649], train_loss: 1386.39 with loss1: 1276.89, loss2: 109.50 and loss3: 624.01\n",
      "Epoch [650], train_loss: 1377.17 with loss1: 1268.11, loss2: 109.06 and loss3: 620.41\n",
      "Epoch [651], train_loss: 1384.88 with loss1: 1275.97, loss2: 108.91 and loss3: 616.82\n",
      "Epoch [652], train_loss: 1372.64 with loss1: 1263.96, loss2: 108.68 and loss3: 613.24\n",
      "Epoch [653], train_loss: 1377.78 with loss1: 1269.04, loss2: 108.74 and loss3: 609.66\n",
      "Epoch [654], train_loss: 1376.06 with loss1: 1267.68, loss2: 108.38 and loss3: 606.10\n",
      "Epoch [655], train_loss: 1386.46 with loss1: 1278.04, loss2: 108.42 and loss3: 602.55\n",
      "Epoch [656], train_loss: 1378.20 with loss1: 1269.95, loss2: 108.25 and loss3: 599.01\n",
      "Epoch [657], train_loss: 1376.87 with loss1: 1268.85, loss2: 108.02 and loss3: 595.47\n",
      "Epoch [658], train_loss: 1369.21 with loss1: 1261.53, loss2: 107.68 and loss3: 591.95\n",
      "Epoch [659], train_loss: 1370.96 with loss1: 1263.14, loss2: 107.83 and loss3: 588.44\n",
      "Epoch [660], train_loss: 1364.07 with loss1: 1256.78, loss2: 107.29 and loss3: 584.93\n",
      "Epoch [661], train_loss: 1371.59 with loss1: 1264.39, loss2: 107.19 and loss3: 581.44\n",
      "Epoch [662], train_loss: 1367.61 with loss1: 1260.44, loss2: 107.17 and loss3: 577.96\n",
      "Epoch [663], train_loss: 1370.08 with loss1: 1262.99, loss2: 107.09 and loss3: 574.49\n",
      "Epoch [664], train_loss: 1367.29 with loss1: 1260.44, loss2: 106.85 and loss3: 571.02\n",
      "Epoch [665], train_loss: 1375.45 with loss1: 1268.76, loss2: 106.69 and loss3: 567.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [666], train_loss: 1363.12 with loss1: 1256.46, loss2: 106.67 and loss3: 564.13\n",
      "Epoch [667], train_loss: 1368.27 with loss1: 1262.00, loss2: 106.26 and loss3: 560.70\n",
      "Epoch [668], train_loss: 1360.36 with loss1: 1254.01, loss2: 106.35 and loss3: 557.28\n",
      "Epoch [669], train_loss: 1369.86 with loss1: 1263.58, loss2: 106.28 and loss3: 553.87\n",
      "Epoch [670], train_loss: 1361.52 with loss1: 1255.49, loss2: 106.03 and loss3: 550.47\n",
      "Epoch [671], train_loss: 1370.17 with loss1: 1264.68, loss2: 105.49 and loss3: 547.08\n",
      "Epoch [672], train_loss: 1362.62 with loss1: 1257.07, loss2: 105.54 and loss3: 543.70\n",
      "Epoch [673], train_loss: 1378.95 with loss1: 1273.53, loss2: 105.41 and loss3: 540.35\n",
      "Epoch [674], train_loss: 1373.57 with loss1: 1268.21, loss2: 105.35 and loss3: 537.01\n",
      "Epoch [675], train_loss: 1382.60 with loss1: 1277.58, loss2: 105.02 and loss3: 533.69\n",
      "Epoch [676], train_loss: 1372.12 with loss1: 1267.24, loss2: 104.88 and loss3: 530.37\n",
      "Epoch [677], train_loss: 1380.68 with loss1: 1275.63, loss2: 105.05 and loss3: 527.06\n",
      "Epoch [678], train_loss: 1373.21 with loss1: 1268.70, loss2: 104.51 and loss3: 523.77\n",
      "Epoch [679], train_loss: 1388.25 with loss1: 1283.92, loss2: 104.33 and loss3: 520.48\n",
      "Epoch [680], train_loss: 1379.19 with loss1: 1274.64, loss2: 104.55 and loss3: 517.20\n",
      "Epoch [681], train_loss: 1393.72 with loss1: 1289.80, loss2: 103.92 and loss3: 513.93\n",
      "Epoch [682], train_loss: 1381.89 with loss1: 1277.76, loss2: 104.13 and loss3: 510.68\n",
      "Epoch [683], train_loss: 1392.41 with loss1: 1288.65, loss2: 103.77 and loss3: 507.44\n",
      "Epoch [684], train_loss: 1384.24 with loss1: 1280.37, loss2: 103.87 and loss3: 504.21\n",
      "Epoch [685], train_loss: 1397.39 with loss1: 1293.85, loss2: 103.54 and loss3: 500.99\n",
      "Epoch [686], train_loss: 1391.59 with loss1: 1287.92, loss2: 103.67 and loss3: 497.78\n",
      "Epoch [687], train_loss: 1396.51 with loss1: 1293.61, loss2: 102.91 and loss3: 494.59\n",
      "Epoch [688], train_loss: 1384.61 with loss1: 1281.27, loss2: 103.34 and loss3: 491.41\n",
      "Epoch [689], train_loss: 1390.26 with loss1: 1287.62, loss2: 102.64 and loss3: 488.23\n",
      "Epoch [690], train_loss: 1382.84 with loss1: 1280.05, loss2: 102.79 and loss3: 485.07\n",
      "Epoch [691], train_loss: 1387.20 with loss1: 1284.93, loss2: 102.26 and loss3: 481.92\n",
      "Epoch [692], train_loss: 1391.96 with loss1: 1289.26, loss2: 102.70 and loss3: 478.78\n",
      "Epoch [693], train_loss: 1387.79 with loss1: 1286.16, loss2: 101.63 and loss3: 475.65\n",
      "Epoch [694], train_loss: 1382.76 with loss1: 1280.40, loss2: 102.36 and loss3: 472.54\n",
      "Epoch [695], train_loss: 1382.49 with loss1: 1280.95, loss2: 101.54 and loss3: 469.43\n",
      "Epoch [696], train_loss: 1371.72 with loss1: 1269.56, loss2: 102.16 and loss3: 466.33\n",
      "Epoch [697], train_loss: 1383.15 with loss1: 1281.76, loss2: 101.39 and loss3: 463.24\n",
      "Epoch [698], train_loss: 1374.52 with loss1: 1272.95, loss2: 101.57 and loss3: 460.16\n",
      "Epoch [699], train_loss: 1373.03 with loss1: 1271.93, loss2: 101.10 and loss3: 457.10\n",
      "Epoch [700], train_loss: 1371.42 with loss1: 1269.94, loss2: 101.48 and loss3: 454.05\n",
      "Epoch [701], train_loss: 1376.48 with loss1: 1275.70, loss2: 100.77 and loss3: 451.02\n",
      "Epoch [702], train_loss: 1369.08 with loss1: 1267.69, loss2: 101.39 and loss3: 447.99\n",
      "Epoch [703], train_loss: 1374.50 with loss1: 1273.78, loss2: 100.72 and loss3: 444.97\n",
      "Epoch [704], train_loss: 1373.05 with loss1: 1272.04, loss2: 101.02 and loss3: 441.96\n",
      "Epoch [705], train_loss: 1373.83 with loss1: 1273.54, loss2: 100.29 and loss3: 438.96\n",
      "Epoch [706], train_loss: 1376.32 with loss1: 1275.61, loss2: 100.71 and loss3: 435.98\n",
      "Epoch [707], train_loss: 1376.93 with loss1: 1276.82, loss2: 100.11 and loss3: 433.00\n",
      "Epoch [708], train_loss: 1370.68 with loss1: 1270.40, loss2: 100.28 and loss3: 430.04\n",
      "Epoch [709], train_loss: 1371.93 with loss1: 1272.15, loss2: 99.78 and loss3: 427.10\n",
      "Epoch [710], train_loss: 1362.41 with loss1: 1262.61, loss2: 99.80 and loss3: 424.16\n",
      "Epoch [711], train_loss: 1371.06 with loss1: 1271.63, loss2: 99.43 and loss3: 421.23\n",
      "Epoch [712], train_loss: 1359.27 with loss1: 1259.56, loss2: 99.70 and loss3: 418.31\n",
      "Epoch [713], train_loss: 1363.40 with loss1: 1264.06, loss2: 99.34 and loss3: 415.40\n",
      "Epoch [714], train_loss: 1353.46 with loss1: 1254.03, loss2: 99.42 and loss3: 412.50\n",
      "Epoch [715], train_loss: 1359.84 with loss1: 1260.79, loss2: 99.05 and loss3: 409.61\n",
      "Epoch [716], train_loss: 1348.97 with loss1: 1249.86, loss2: 99.10 and loss3: 406.72\n",
      "Epoch [717], train_loss: 1353.10 with loss1: 1254.37, loss2: 98.72 and loss3: 403.85\n",
      "Epoch [718], train_loss: 1341.57 with loss1: 1242.67, loss2: 98.89 and loss3: 400.98\n",
      "Epoch [719], train_loss: 1341.62 with loss1: 1243.12, loss2: 98.50 and loss3: 398.12\n",
      "Epoch [720], train_loss: 1334.95 with loss1: 1236.45, loss2: 98.50 and loss3: 395.28\n",
      "Epoch [721], train_loss: 1332.89 with loss1: 1234.37, loss2: 98.52 and loss3: 392.45\n",
      "Epoch [722], train_loss: 1320.13 with loss1: 1221.73, loss2: 98.40 and loss3: 389.63\n",
      "Epoch [723], train_loss: 1322.81 with loss1: 1224.62, loss2: 98.18 and loss3: 386.82\n",
      "Epoch [724], train_loss: 1312.83 with loss1: 1214.66, loss2: 98.17 and loss3: 384.02\n",
      "Epoch [725], train_loss: 1318.08 with loss1: 1220.17, loss2: 97.90 and loss3: 381.23\n",
      "Epoch [726], train_loss: 1301.56 with loss1: 1203.72, loss2: 97.84 and loss3: 378.45\n",
      "Epoch [727], train_loss: 1314.29 with loss1: 1216.71, loss2: 97.58 and loss3: 375.69\n",
      "Epoch [728], train_loss: 1295.96 with loss1: 1198.38, loss2: 97.58 and loss3: 372.94\n",
      "Epoch [729], train_loss: 1296.40 with loss1: 1198.93, loss2: 97.47 and loss3: 370.20\n",
      "Epoch [730], train_loss: 1286.64 with loss1: 1189.38, loss2: 97.26 and loss3: 367.47\n",
      "Epoch [731], train_loss: 1287.64 with loss1: 1190.38, loss2: 97.26 and loss3: 364.75\n",
      "Epoch [732], train_loss: 1278.74 with loss1: 1181.75, loss2: 96.99 and loss3: 362.04\n",
      "Epoch [733], train_loss: 1279.78 with loss1: 1182.81, loss2: 96.96 and loss3: 359.33\n",
      "Epoch [734], train_loss: 1270.55 with loss1: 1173.65, loss2: 96.90 and loss3: 356.64\n",
      "Epoch [735], train_loss: 1278.29 with loss1: 1181.54, loss2: 96.75 and loss3: 353.96\n",
      "Epoch [736], train_loss: 1271.19 with loss1: 1174.23, loss2: 96.96 and loss3: 351.29\n",
      "Epoch [737], train_loss: 1272.65 with loss1: 1176.12, loss2: 96.53 and loss3: 348.62\n",
      "Epoch [738], train_loss: 1265.69 with loss1: 1169.11, loss2: 96.58 and loss3: 345.97\n",
      "Epoch [739], train_loss: 1268.95 with loss1: 1172.51, loss2: 96.44 and loss3: 343.33\n",
      "Epoch [740], train_loss: 1266.02 with loss1: 1169.96, loss2: 96.06 and loss3: 340.69\n",
      "Epoch [741], train_loss: 1273.13 with loss1: 1176.82, loss2: 96.31 and loss3: 338.06\n",
      "Epoch [742], train_loss: 1264.17 with loss1: 1168.13, loss2: 96.04 and loss3: 335.44\n",
      "Epoch [743], train_loss: 1275.40 with loss1: 1179.12, loss2: 96.27 and loss3: 332.83\n",
      "Epoch [744], train_loss: 1260.98 with loss1: 1165.09, loss2: 95.90 and loss3: 330.24\n",
      "Epoch [745], train_loss: 1269.87 with loss1: 1173.69, loss2: 96.18 and loss3: 327.65\n",
      "Epoch [746], train_loss: 1268.21 with loss1: 1172.62, loss2: 95.59 and loss3: 325.07\n",
      "Epoch [747], train_loss: 1273.18 with loss1: 1177.38, loss2: 95.81 and loss3: 322.51\n",
      "Epoch [748], train_loss: 1265.66 with loss1: 1170.13, loss2: 95.53 and loss3: 319.96\n",
      "Epoch [749], train_loss: 1276.56 with loss1: 1180.59, loss2: 95.97 and loss3: 317.42\n",
      "Epoch [750], train_loss: 1267.25 with loss1: 1171.97, loss2: 95.28 and loss3: 314.89\n",
      "Epoch [751], train_loss: 1274.36 with loss1: 1178.85, loss2: 95.51 and loss3: 312.37\n",
      "Epoch [752], train_loss: 1267.47 with loss1: 1172.42, loss2: 95.05 and loss3: 309.86\n",
      "Epoch [753], train_loss: 1275.14 with loss1: 1180.09, loss2: 95.06 and loss3: 307.36\n",
      "Epoch [754], train_loss: 1265.35 with loss1: 1170.48, loss2: 94.88 and loss3: 304.87\n",
      "Epoch [755], train_loss: 1272.09 with loss1: 1176.90, loss2: 95.19 and loss3: 302.38\n",
      "Epoch [756], train_loss: 1256.56 with loss1: 1161.74, loss2: 94.82 and loss3: 299.91\n",
      "Epoch [757], train_loss: 1268.30 with loss1: 1173.38, loss2: 94.92 and loss3: 297.44\n",
      "Epoch [758], train_loss: 1249.81 with loss1: 1155.58, loss2: 94.23 and loss3: 294.98\n",
      "Epoch [759], train_loss: 1256.76 with loss1: 1162.04, loss2: 94.72 and loss3: 292.54\n",
      "Epoch [760], train_loss: 1248.36 with loss1: 1153.95, loss2: 94.40 and loss3: 290.12\n",
      "Epoch [761], train_loss: 1252.02 with loss1: 1157.32, loss2: 94.69 and loss3: 287.70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [762], train_loss: 1233.98 with loss1: 1139.77, loss2: 94.22 and loss3: 285.29\n",
      "Epoch [763], train_loss: 1236.10 with loss1: 1141.64, loss2: 94.45 and loss3: 282.90\n",
      "Epoch [764], train_loss: 1225.04 with loss1: 1131.11, loss2: 93.93 and loss3: 280.51\n",
      "Epoch [765], train_loss: 1229.68 with loss1: 1135.44, loss2: 94.24 and loss3: 278.14\n",
      "Epoch [766], train_loss: 1227.24 with loss1: 1133.36, loss2: 93.88 and loss3: 275.77\n",
      "Epoch [767], train_loss: 1218.09 with loss1: 1123.99, loss2: 94.09 and loss3: 273.43\n",
      "Epoch [768], train_loss: 1211.44 with loss1: 1117.74, loss2: 93.71 and loss3: 271.09\n",
      "Epoch [769], train_loss: 1214.24 with loss1: 1120.41, loss2: 93.83 and loss3: 268.76\n",
      "Epoch [770], train_loss: 1201.74 with loss1: 1108.17, loss2: 93.58 and loss3: 266.44\n",
      "Epoch [771], train_loss: 1203.80 with loss1: 1110.01, loss2: 93.79 and loss3: 264.12\n",
      "Epoch [772], train_loss: 1196.09 with loss1: 1102.68, loss2: 93.41 and loss3: 261.82\n",
      "Epoch [773], train_loss: 1200.72 with loss1: 1107.17, loss2: 93.55 and loss3: 259.54\n",
      "Epoch [774], train_loss: 1193.60 with loss1: 1100.30, loss2: 93.30 and loss3: 257.26\n",
      "Epoch [775], train_loss: 1198.32 with loss1: 1104.72, loss2: 93.60 and loss3: 254.99\n",
      "Epoch [776], train_loss: 1188.32 with loss1: 1095.18, loss2: 93.15 and loss3: 252.74\n",
      "Epoch [777], train_loss: 1194.89 with loss1: 1101.70, loss2: 93.19 and loss3: 250.49\n",
      "Epoch [778], train_loss: 1191.73 with loss1: 1098.89, loss2: 92.84 and loss3: 248.26\n",
      "Epoch [779], train_loss: 1189.30 with loss1: 1096.07, loss2: 93.22 and loss3: 246.03\n",
      "Epoch [780], train_loss: 1186.58 with loss1: 1093.56, loss2: 93.02 and loss3: 243.81\n",
      "Epoch [781], train_loss: 1190.35 with loss1: 1097.18, loss2: 93.18 and loss3: 241.60\n",
      "Epoch [782], train_loss: 1184.43 with loss1: 1091.62, loss2: 92.80 and loss3: 239.41\n",
      "Epoch [783], train_loss: 1192.07 with loss1: 1098.79, loss2: 93.27 and loss3: 237.22\n",
      "Epoch [784], train_loss: 1187.43 with loss1: 1094.99, loss2: 92.44 and loss3: 235.04\n",
      "Epoch [785], train_loss: 1193.98 with loss1: 1101.15, loss2: 92.83 and loss3: 232.87\n",
      "Epoch [786], train_loss: 1187.22 with loss1: 1094.49, loss2: 92.73 and loss3: 230.70\n",
      "Epoch [787], train_loss: 1191.37 with loss1: 1098.55, loss2: 92.82 and loss3: 228.55\n",
      "Epoch [788], train_loss: 1194.39 with loss1: 1101.96, loss2: 92.44 and loss3: 226.40\n",
      "Epoch [789], train_loss: 1190.22 with loss1: 1097.59, loss2: 92.63 and loss3: 224.25\n",
      "Epoch [790], train_loss: 1189.07 with loss1: 1096.44, loss2: 92.63 and loss3: 222.11\n",
      "Epoch [791], train_loss: 1190.58 with loss1: 1098.15, loss2: 92.44 and loss3: 219.98\n",
      "Epoch [792], train_loss: 1190.05 with loss1: 1097.60, loss2: 92.45 and loss3: 217.86\n",
      "Epoch [793], train_loss: 1191.76 with loss1: 1099.27, loss2: 92.49 and loss3: 215.74\n",
      "Epoch [794], train_loss: 1186.64 with loss1: 1094.39, loss2: 92.25 and loss3: 213.63\n",
      "Epoch [795], train_loss: 1196.03 with loss1: 1103.72, loss2: 92.32 and loss3: 211.53\n",
      "Epoch [796], train_loss: 1188.24 with loss1: 1096.16, loss2: 92.08 and loss3: 209.44\n",
      "Epoch [797], train_loss: 1198.60 with loss1: 1106.19, loss2: 92.41 and loss3: 207.36\n",
      "Epoch [798], train_loss: 1193.51 with loss1: 1101.54, loss2: 91.97 and loss3: 205.29\n",
      "Epoch [799], train_loss: 1200.54 with loss1: 1108.49, loss2: 92.05 and loss3: 203.23\n",
      "Epoch [800], train_loss: 1198.73 with loss1: 1106.83, loss2: 91.90 and loss3: 201.19\n",
      "Epoch [801], train_loss: 1211.19 with loss1: 1119.11, loss2: 92.08 and loss3: 199.15\n",
      "Epoch [802], train_loss: 1199.73 with loss1: 1107.81, loss2: 91.92 and loss3: 197.13\n",
      "Epoch [803], train_loss: 1207.69 with loss1: 1115.89, loss2: 91.80 and loss3: 195.11\n",
      "Epoch [804], train_loss: 1203.27 with loss1: 1111.51, loss2: 91.75 and loss3: 193.11\n",
      "Epoch [805], train_loss: 1210.64 with loss1: 1118.83, loss2: 91.81 and loss3: 191.12\n",
      "Epoch [806], train_loss: 1206.84 with loss1: 1115.18, loss2: 91.66 and loss3: 189.15\n",
      "Epoch [807], train_loss: 1215.68 with loss1: 1124.20, loss2: 91.49 and loss3: 187.18\n",
      "Epoch [808], train_loss: 1205.26 with loss1: 1113.61, loss2: 91.66 and loss3: 185.21\n",
      "Epoch [809], train_loss: 1212.86 with loss1: 1121.21, loss2: 91.65 and loss3: 183.26\n",
      "Epoch [810], train_loss: 1207.22 with loss1: 1115.88, loss2: 91.33 and loss3: 181.31\n",
      "Epoch [811], train_loss: 1214.32 with loss1: 1122.88, loss2: 91.44 and loss3: 179.37\n",
      "Epoch [812], train_loss: 1209.36 with loss1: 1117.98, loss2: 91.38 and loss3: 177.45\n",
      "Epoch [813], train_loss: 1214.30 with loss1: 1122.97, loss2: 91.33 and loss3: 175.54\n",
      "Epoch [814], train_loss: 1213.23 with loss1: 1121.88, loss2: 91.35 and loss3: 173.64\n",
      "Epoch [815], train_loss: 1217.99 with loss1: 1126.85, loss2: 91.13 and loss3: 171.76\n",
      "Epoch [816], train_loss: 1214.56 with loss1: 1123.23, loss2: 91.33 and loss3: 169.89\n",
      "Epoch [817], train_loss: 1220.08 with loss1: 1129.03, loss2: 91.05 and loss3: 168.03\n",
      "Epoch [818], train_loss: 1211.12 with loss1: 1120.14, loss2: 90.99 and loss3: 166.18\n",
      "Epoch [819], train_loss: 1215.76 with loss1: 1124.86, loss2: 90.90 and loss3: 164.34\n",
      "Epoch [820], train_loss: 1213.86 with loss1: 1122.88, loss2: 90.98 and loss3: 162.51\n",
      "Epoch [821], train_loss: 1216.11 with loss1: 1125.28, loss2: 90.84 and loss3: 160.69\n",
      "Epoch [822], train_loss: 1208.20 with loss1: 1117.44, loss2: 90.76 and loss3: 158.88\n",
      "Epoch [823], train_loss: 1214.42 with loss1: 1123.73, loss2: 90.69 and loss3: 157.09\n",
      "Epoch [824], train_loss: 1206.24 with loss1: 1115.65, loss2: 90.59 and loss3: 155.30\n",
      "Epoch [825], train_loss: 1207.72 with loss1: 1117.09, loss2: 90.63 and loss3: 153.54\n",
      "Epoch [826], train_loss: 1202.16 with loss1: 1111.61, loss2: 90.55 and loss3: 151.77\n",
      "Epoch [827], train_loss: 1204.81 with loss1: 1114.13, loss2: 90.68 and loss3: 150.02\n",
      "Epoch [828], train_loss: 1195.69 with loss1: 1105.39, loss2: 90.31 and loss3: 148.28\n",
      "Epoch [829], train_loss: 1198.15 with loss1: 1107.76, loss2: 90.39 and loss3: 146.55\n",
      "Epoch [830], train_loss: 1193.28 with loss1: 1102.82, loss2: 90.46 and loss3: 144.83\n",
      "Epoch [831], train_loss: 1196.45 with loss1: 1106.00, loss2: 90.45 and loss3: 143.12\n",
      "Epoch [832], train_loss: 1191.98 with loss1: 1101.47, loss2: 90.51 and loss3: 141.42\n",
      "Epoch [833], train_loss: 1201.75 with loss1: 1111.36, loss2: 90.40 and loss3: 139.72\n",
      "Epoch [834], train_loss: 1188.29 with loss1: 1098.02, loss2: 90.27 and loss3: 138.03\n",
      "Epoch [835], train_loss: 1192.08 with loss1: 1101.75, loss2: 90.33 and loss3: 136.35\n",
      "Epoch [836], train_loss: 1188.11 with loss1: 1097.89, loss2: 90.22 and loss3: 134.69\n",
      "Epoch [837], train_loss: 1192.18 with loss1: 1102.05, loss2: 90.13 and loss3: 133.04\n",
      "Epoch [838], train_loss: 1186.45 with loss1: 1096.36, loss2: 90.08 and loss3: 131.40\n",
      "Epoch [839], train_loss: 1190.76 with loss1: 1100.71, loss2: 90.06 and loss3: 129.77\n",
      "Epoch [840], train_loss: 1183.42 with loss1: 1093.10, loss2: 90.32 and loss3: 128.16\n",
      "Epoch [841], train_loss: 1188.50 with loss1: 1098.45, loss2: 90.05 and loss3: 126.55\n",
      "Epoch [842], train_loss: 1182.58 with loss1: 1092.35, loss2: 90.23 and loss3: 124.95\n",
      "Epoch [843], train_loss: 1189.94 with loss1: 1099.99, loss2: 89.95 and loss3: 123.36\n",
      "Epoch [844], train_loss: 1180.53 with loss1: 1090.43, loss2: 90.11 and loss3: 121.78\n",
      "Epoch [845], train_loss: 1186.74 with loss1: 1096.76, loss2: 89.98 and loss3: 120.21\n",
      "Epoch [846], train_loss: 1184.02 with loss1: 1093.96, loss2: 90.06 and loss3: 118.64\n",
      "Epoch [847], train_loss: 1186.79 with loss1: 1097.05, loss2: 89.74 and loss3: 117.09\n",
      "Epoch [848], train_loss: 1182.13 with loss1: 1091.86, loss2: 90.27 and loss3: 115.55\n",
      "Epoch [849], train_loss: 1190.13 with loss1: 1100.24, loss2: 89.89 and loss3: 114.01\n",
      "Epoch [850], train_loss: 1182.81 with loss1: 1092.65, loss2: 90.16 and loss3: 112.49\n",
      "Epoch [851], train_loss: 1188.57 with loss1: 1098.72, loss2: 89.85 and loss3: 110.97\n",
      "Epoch [852], train_loss: 1189.09 with loss1: 1098.90, loss2: 90.19 and loss3: 109.47\n",
      "Epoch [853], train_loss: 1185.64 with loss1: 1095.68, loss2: 89.96 and loss3: 107.98\n",
      "Epoch [854], train_loss: 1173.82 with loss1: 1083.55, loss2: 90.27 and loss3: 106.50\n",
      "Epoch [855], train_loss: 1173.98 with loss1: 1084.05, loss2: 89.93 and loss3: 105.03\n",
      "Epoch [856], train_loss: 1166.04 with loss1: 1076.06, loss2: 89.99 and loss3: 103.56\n",
      "Epoch [857], train_loss: 1165.92 with loss1: 1076.15, loss2: 89.76 and loss3: 102.11\n",
      "Epoch [858], train_loss: 1158.31 with loss1: 1067.98, loss2: 90.33 and loss3: 100.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [859], train_loss: 1154.66 with loss1: 1064.63, loss2: 90.03 and loss3: 99.23\n",
      "Epoch [860], train_loss: 1151.39 with loss1: 1061.34, loss2: 90.05 and loss3: 97.80\n",
      "Epoch [861], train_loss: 1147.42 with loss1: 1057.41, loss2: 90.01 and loss3: 96.39\n",
      "Epoch [862], train_loss: 1139.48 with loss1: 1049.25, loss2: 90.23 and loss3: 94.98\n",
      "Epoch [863], train_loss: 1133.32 with loss1: 1043.47, loss2: 89.84 and loss3: 93.58\n",
      "Epoch [864], train_loss: 1127.81 with loss1: 1037.64, loss2: 90.17 and loss3: 92.20\n",
      "Epoch [865], train_loss: 1125.76 with loss1: 1035.75, loss2: 90.01 and loss3: 90.82\n",
      "Epoch [866], train_loss: 1121.58 with loss1: 1031.49, loss2: 90.08 and loss3: 89.45\n",
      "Epoch [867], train_loss: 1115.51 with loss1: 1025.40, loss2: 90.11 and loss3: 88.09\n",
      "Epoch [868], train_loss: 1110.06 with loss1: 1020.03, loss2: 90.04 and loss3: 86.74\n",
      "Epoch [869], train_loss: 1105.88 with loss1: 1015.98, loss2: 89.90 and loss3: 85.40\n",
      "Epoch [870], train_loss: 1105.59 with loss1: 1015.42, loss2: 90.17 and loss3: 84.06\n",
      "Epoch [871], train_loss: 1097.42 with loss1: 1007.57, loss2: 89.85 and loss3: 82.73\n",
      "Epoch [872], train_loss: 1095.84 with loss1: 1005.82, loss2: 90.02 and loss3: 81.41\n",
      "Epoch [873], train_loss: 1094.18 with loss1: 1004.19, loss2: 89.99 and loss3: 80.10\n",
      "Epoch [874], train_loss: 1088.45 with loss1: 998.19, loss2: 90.26 and loss3: 78.80\n",
      "Epoch [875], train_loss: 1087.93 with loss1: 998.07, loss2: 89.85 and loss3: 77.52\n",
      "Epoch [876], train_loss: 1084.73 with loss1: 994.64, loss2: 90.09 and loss3: 76.24\n",
      "Epoch [877], train_loss: 1088.34 with loss1: 998.34, loss2: 90.00 and loss3: 74.98\n",
      "Epoch [878], train_loss: 1082.23 with loss1: 991.97, loss2: 90.26 and loss3: 73.72\n",
      "Epoch [879], train_loss: 1083.47 with loss1: 993.47, loss2: 90.00 and loss3: 72.48\n",
      "Epoch [880], train_loss: 1081.51 with loss1: 991.16, loss2: 90.35 and loss3: 71.25\n",
      "Epoch [881], train_loss: 1083.49 with loss1: 993.32, loss2: 90.18 and loss3: 70.03\n",
      "Epoch [882], train_loss: 1082.75 with loss1: 992.48, loss2: 90.27 and loss3: 68.82\n",
      "Epoch [883], train_loss: 1082.14 with loss1: 991.88, loss2: 90.26 and loss3: 67.62\n",
      "Epoch [884], train_loss: 1082.91 with loss1: 992.26, loss2: 90.64 and loss3: 66.43\n",
      "Epoch [885], train_loss: 1084.11 with loss1: 993.85, loss2: 90.26 and loss3: 65.25\n",
      "Epoch [886], train_loss: 1082.86 with loss1: 992.42, loss2: 90.45 and loss3: 64.07\n",
      "Epoch [887], train_loss: 1083.45 with loss1: 993.06, loss2: 90.39 and loss3: 62.91\n",
      "Epoch [888], train_loss: 1082.93 with loss1: 992.47, loss2: 90.46 and loss3: 61.76\n",
      "Epoch [889], train_loss: 1081.65 with loss1: 991.17, loss2: 90.48 and loss3: 60.61\n",
      "Epoch [890], train_loss: 1083.13 with loss1: 992.43, loss2: 90.70 and loss3: 59.48\n",
      "Epoch [891], train_loss: 1082.13 with loss1: 991.51, loss2: 90.62 and loss3: 58.36\n",
      "Epoch [892], train_loss: 1081.05 with loss1: 990.33, loss2: 90.72 and loss3: 57.25\n",
      "Epoch [893], train_loss: 1079.70 with loss1: 988.91, loss2: 90.79 and loss3: 56.15\n",
      "Epoch [894], train_loss: 1082.07 with loss1: 991.15, loss2: 90.92 and loss3: 55.07\n",
      "Epoch [895], train_loss: 1086.24 with loss1: 995.42, loss2: 90.82 and loss3: 54.00\n",
      "Epoch [896], train_loss: 1078.38 with loss1: 987.38, loss2: 91.00 and loss3: 52.95\n",
      "Epoch [897], train_loss: 1080.05 with loss1: 989.03, loss2: 91.02 and loss3: 51.90\n",
      "Epoch [898], train_loss: 1082.56 with loss1: 991.49, loss2: 91.08 and loss3: 50.87\n",
      "Epoch [899], train_loss: 1085.29 with loss1: 994.30, loss2: 90.99 and loss3: 49.86\n",
      "Epoch [900], train_loss: 1082.11 with loss1: 990.74, loss2: 91.37 and loss3: 48.86\n",
      "Epoch [901], train_loss: 1085.99 with loss1: 994.72, loss2: 91.27 and loss3: 47.86\n",
      "Epoch [902], train_loss: 1082.47 with loss1: 991.17, loss2: 91.30 and loss3: 46.88\n",
      "Epoch [903], train_loss: 1083.30 with loss1: 992.04, loss2: 91.26 and loss3: 45.91\n",
      "Epoch [904], train_loss: 1076.24 with loss1: 984.90, loss2: 91.34 and loss3: 44.95\n",
      "Epoch [905], train_loss: 1080.35 with loss1: 988.92, loss2: 91.42 and loss3: 44.00\n",
      "Epoch [906], train_loss: 1076.62 with loss1: 985.10, loss2: 91.52 and loss3: 43.07\n",
      "Epoch [907], train_loss: 1076.14 with loss1: 984.63, loss2: 91.51 and loss3: 42.15\n",
      "Epoch [908], train_loss: 1071.05 with loss1: 979.62, loss2: 91.43 and loss3: 41.24\n",
      "Epoch [909], train_loss: 1076.07 with loss1: 984.43, loss2: 91.65 and loss3: 40.34\n",
      "Epoch [910], train_loss: 1073.06 with loss1: 981.46, loss2: 91.60 and loss3: 39.45\n",
      "Epoch [911], train_loss: 1066.99 with loss1: 975.54, loss2: 91.45 and loss3: 38.57\n",
      "Epoch [912], train_loss: 1063.34 with loss1: 971.61, loss2: 91.73 and loss3: 37.71\n",
      "Epoch [913], train_loss: 1066.18 with loss1: 974.37, loss2: 91.82 and loss3: 36.85\n",
      "Epoch [914], train_loss: 1064.31 with loss1: 972.53, loss2: 91.78 and loss3: 36.01\n",
      "Epoch [915], train_loss: 1066.42 with loss1: 974.77, loss2: 91.65 and loss3: 35.18\n",
      "Epoch [916], train_loss: 1063.90 with loss1: 971.82, loss2: 92.08 and loss3: 34.36\n",
      "Epoch [917], train_loss: 1072.66 with loss1: 980.85, loss2: 91.81 and loss3: 33.54\n",
      "Epoch [918], train_loss: 1062.98 with loss1: 970.88, loss2: 92.10 and loss3: 32.74\n",
      "Epoch [919], train_loss: 1071.11 with loss1: 979.06, loss2: 92.05 and loss3: 31.95\n",
      "Epoch [920], train_loss: 1069.95 with loss1: 977.86, loss2: 92.09 and loss3: 31.16\n",
      "Epoch [921], train_loss: 1075.04 with loss1: 982.98, loss2: 92.06 and loss3: 30.39\n",
      "Epoch [922], train_loss: 1070.71 with loss1: 978.60, loss2: 92.11 and loss3: 29.62\n",
      "Epoch [923], train_loss: 1075.91 with loss1: 983.91, loss2: 92.00 and loss3: 28.87\n",
      "Epoch [924], train_loss: 1075.78 with loss1: 983.44, loss2: 92.35 and loss3: 28.14\n",
      "Epoch [925], train_loss: 1080.62 with loss1: 988.38, loss2: 92.24 and loss3: 27.40\n",
      "Epoch [926], train_loss: 1079.69 with loss1: 987.39, loss2: 92.30 and loss3: 26.68\n",
      "Epoch [927], train_loss: 1090.62 with loss1: 998.38, loss2: 92.25 and loss3: 25.96\n",
      "Epoch [928], train_loss: 1093.68 with loss1: 1001.12, loss2: 92.57 and loss3: 25.25\n",
      "Epoch [929], train_loss: 1103.04 with loss1: 1010.69, loss2: 92.35 and loss3: 24.55\n",
      "Epoch [930], train_loss: 1105.00 with loss1: 1012.52, loss2: 92.48 and loss3: 23.86\n",
      "Epoch [931], train_loss: 1122.96 with loss1: 1030.52, loss2: 92.44 and loss3: 23.18\n",
      "Epoch [932], train_loss: 1126.39 with loss1: 1033.82, loss2: 92.57 and loss3: 22.51\n",
      "Epoch [933], train_loss: 1133.12 with loss1: 1040.40, loss2: 92.71 and loss3: 21.85\n",
      "Epoch [934], train_loss: 1131.40 with loss1: 1039.05, loss2: 92.35 and loss3: 21.20\n",
      "Epoch [935], train_loss: 1148.85 with loss1: 1056.18, loss2: 92.67 and loss3: 20.56\n",
      "Epoch [936], train_loss: 1152.80 with loss1: 1060.06, loss2: 92.74 and loss3: 19.93\n",
      "Epoch [937], train_loss: 1168.16 with loss1: 1075.29, loss2: 92.87 and loss3: 19.31\n",
      "Epoch [938], train_loss: 1168.32 with loss1: 1075.64, loss2: 92.68 and loss3: 18.70\n",
      "Epoch [939], train_loss: 1195.34 with loss1: 1102.46, loss2: 92.89 and loss3: 18.10\n",
      "Epoch [940], train_loss: 1185.11 with loss1: 1092.42, loss2: 92.70 and loss3: 17.51\n",
      "Epoch [941], train_loss: 1206.58 with loss1: 1113.47, loss2: 93.11 and loss3: 16.94\n",
      "Epoch [942], train_loss: 1194.12 with loss1: 1101.43, loss2: 92.69 and loss3: 16.37\n",
      "Epoch [943], train_loss: 1209.99 with loss1: 1116.85, loss2: 93.14 and loss3: 15.81\n",
      "Epoch [944], train_loss: 1191.82 with loss1: 1099.20, loss2: 92.62 and loss3: 15.25\n",
      "Epoch [945], train_loss: 1203.38 with loss1: 1110.33, loss2: 93.05 and loss3: 14.71\n",
      "Epoch [946], train_loss: 1179.45 with loss1: 1086.58, loss2: 92.86 and loss3: 14.18\n",
      "Epoch [947], train_loss: 1187.26 with loss1: 1094.07, loss2: 93.19 and loss3: 13.66\n",
      "Epoch [948], train_loss: 1163.41 with loss1: 1070.65, loss2: 92.76 and loss3: 13.15\n",
      "Epoch [949], train_loss: 1162.80 with loss1: 1069.72, loss2: 93.08 and loss3: 12.65\n",
      "Epoch [950], train_loss: 1139.64 with loss1: 1046.87, loss2: 92.77 and loss3: 12.17\n",
      "Epoch [951], train_loss: 1143.93 with loss1: 1050.87, loss2: 93.06 and loss3: 11.70\n",
      "Epoch [952], train_loss: 1122.72 with loss1: 1029.85, loss2: 92.86 and loss3: 11.24\n",
      "Epoch [953], train_loss: 1117.89 with loss1: 1024.70, loss2: 93.19 and loss3: 10.79\n",
      "Epoch [954], train_loss: 1097.56 with loss1: 1004.59, loss2: 92.97 and loss3: 10.35\n",
      "Epoch [955], train_loss: 1098.68 with loss1: 1005.77, loss2: 92.91 and loss3: 9.92\n",
      "Epoch [956], train_loss: 1079.06 with loss1: 986.14, loss2: 92.92 and loss3: 9.50\n",
      "Epoch [957], train_loss: 1077.05 with loss1: 984.05, loss2: 93.01 and loss3: 9.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [958], train_loss: 1065.41 with loss1: 972.48, loss2: 92.92 and loss3: 8.67\n",
      "Epoch [959], train_loss: 1064.95 with loss1: 971.85, loss2: 93.10 and loss3: 8.26\n",
      "Epoch [960], train_loss: 1053.56 with loss1: 960.62, loss2: 92.93 and loss3: 7.87\n",
      "Epoch [961], train_loss: 1049.75 with loss1: 956.67, loss2: 93.09 and loss3: 7.49\n",
      "Epoch [962], train_loss: 1044.36 with loss1: 951.35, loss2: 93.01 and loss3: 7.11\n",
      "Epoch [963], train_loss: 1039.82 with loss1: 946.44, loss2: 93.38 and loss3: 6.74\n",
      "Epoch [964], train_loss: 1037.92 with loss1: 944.60, loss2: 93.31 and loss3: 6.39\n",
      "Epoch [965], train_loss: 1035.66 with loss1: 942.22, loss2: 93.45 and loss3: 6.04\n",
      "Epoch [966], train_loss: 1028.91 with loss1: 935.84, loss2: 93.07 and loss3: 5.70\n",
      "Epoch [967], train_loss: 1032.43 with loss1: 939.09, loss2: 93.34 and loss3: 5.37\n",
      "Epoch [968], train_loss: 1025.01 with loss1: 931.66, loss2: 93.35 and loss3: 5.05\n",
      "Epoch [969], train_loss: 1027.34 with loss1: 933.92, loss2: 93.43 and loss3: 4.73\n",
      "Epoch [970], train_loss: 1023.04 with loss1: 929.94, loss2: 93.09 and loss3: 4.43\n",
      "Epoch [971], train_loss: 1023.23 with loss1: 929.70, loss2: 93.53 and loss3: 4.14\n",
      "Epoch [972], train_loss: 1022.47 with loss1: 928.85, loss2: 93.62 and loss3: 3.86\n",
      "Epoch [973], train_loss: 1025.58 with loss1: 931.95, loss2: 93.63 and loss3: 3.58\n",
      "Epoch [974], train_loss: 1021.17 with loss1: 927.53, loss2: 93.64 and loss3: 3.32\n",
      "Epoch [975], train_loss: 1020.11 with loss1: 926.25, loss2: 93.86 and loss3: 3.06\n",
      "Epoch [976], train_loss: 1018.01 with loss1: 924.14, loss2: 93.87 and loss3: 2.82\n",
      "Epoch [977], train_loss: 1021.34 with loss1: 927.49, loss2: 93.85 and loss3: 2.58\n",
      "Epoch [978], train_loss: 1018.66 with loss1: 924.80, loss2: 93.86 and loss3: 2.36\n",
      "Epoch [979], train_loss: 1021.47 with loss1: 927.51, loss2: 93.95 and loss3: 2.14\n",
      "Epoch [980], train_loss: 1016.40 with loss1: 922.70, loss2: 93.70 and loss3: 1.94\n",
      "Epoch [981], train_loss: 1021.24 with loss1: 927.11, loss2: 94.12 and loss3: 1.75\n",
      "Epoch [982], train_loss: 1018.54 with loss1: 924.65, loss2: 93.89 and loss3: 1.56\n",
      "Epoch [983], train_loss: 1024.82 with loss1: 930.79, loss2: 94.03 and loss3: 1.39\n",
      "Epoch [984], train_loss: 1017.30 with loss1: 923.36, loss2: 93.94 and loss3: 1.23\n",
      "Epoch [985], train_loss: 1025.36 with loss1: 931.17, loss2: 94.19 and loss3: 1.08\n",
      "Epoch [986], train_loss: 1023.04 with loss1: 929.04, loss2: 94.00 and loss3: 0.94\n",
      "Epoch [987], train_loss: 1025.61 with loss1: 931.39, loss2: 94.22 and loss3: 0.82\n",
      "Epoch [988], train_loss: 1021.46 with loss1: 927.47, loss2: 93.98 and loss3: 0.70\n",
      "Epoch [989], train_loss: 1027.82 with loss1: 933.63, loss2: 94.19 and loss3: 0.60\n",
      "Epoch [990], train_loss: 1024.79 with loss1: 930.66, loss2: 94.14 and loss3: 0.51\n",
      "Epoch [991], train_loss: 1030.22 with loss1: 936.06, loss2: 94.16 and loss3: 0.43\n",
      "Epoch [992], train_loss: 1028.68 with loss1: 934.43, loss2: 94.25 and loss3: 0.36\n",
      "Epoch [993], train_loss: 1034.50 with loss1: 940.20, loss2: 94.30 and loss3: 0.29\n",
      "Epoch [994], train_loss: 1031.44 with loss1: 937.23, loss2: 94.21 and loss3: 0.24\n",
      "Epoch [995], train_loss: 1039.48 with loss1: 945.13, loss2: 94.36 and loss3: 0.20\n",
      "Epoch [996], train_loss: 1037.43 with loss1: 943.25, loss2: 94.18 and loss3: 0.16\n",
      "Epoch [997], train_loss: 1043.64 with loss1: 949.24, loss2: 94.39 and loss3: 0.13\n",
      "Epoch [998], train_loss: 1037.35 with loss1: 943.04, loss2: 94.31 and loss3: 0.10\n",
      "Epoch [999], train_loss: 1044.46 with loss1: 950.03, loss2: 94.42 and loss3: 0.09\n",
      "Epoch [1000], train_loss: 1038.81 with loss1: 944.55, loss2: 94.26 and loss3: 0.07\n",
      "Epoch [1001], train_loss: 1045.65 with loss1: 951.22, loss2: 94.43 and loss3: 0.06\n",
      "Epoch [1002], train_loss: 1042.29 with loss1: 948.00, loss2: 94.30 and loss3: 0.06\n",
      "Epoch [1003], train_loss: 1049.28 with loss1: 954.81, loss2: 94.47 and loss3: 0.05\n",
      "Epoch [1004], train_loss: 1042.28 with loss1: 947.91, loss2: 94.37 and loss3: 0.04\n",
      "Epoch [1005], train_loss: 1047.68 with loss1: 953.23, loss2: 94.46 and loss3: 0.04\n",
      "Epoch [1006], train_loss: 1035.26 with loss1: 941.00, loss2: 94.26 and loss3: 0.03\n",
      "Epoch [1007], train_loss: 1041.28 with loss1: 946.86, loss2: 94.41 and loss3: 0.03\n",
      "Epoch [1008], train_loss: 1037.47 with loss1: 943.17, loss2: 94.30 and loss3: 0.02\n",
      "Epoch [1009], train_loss: 1037.59 with loss1: 943.01, loss2: 94.58 and loss3: 0.02\n",
      "Epoch [1010], train_loss: 1030.74 with loss1: 936.70, loss2: 94.04 and loss3: 0.02\n",
      "Epoch [1011], train_loss: 1038.93 with loss1: 944.69, loss2: 94.24 and loss3: 0.01\n",
      "Epoch [1012], train_loss: 1034.25 with loss1: 940.11, loss2: 94.14 and loss3: 0.01\n",
      "Epoch [1013], train_loss: 1030.32 with loss1: 935.93, loss2: 94.39 and loss3: 0.01\n",
      "Epoch [1014], train_loss: 1022.51 with loss1: 928.29, loss2: 94.21 and loss3: 0.01\n",
      "Epoch [1015], train_loss: 1027.10 with loss1: 932.76, loss2: 94.34 and loss3: 0.01\n",
      "Epoch [1016], train_loss: 1017.92 with loss1: 923.73, loss2: 94.20 and loss3: 0.00\n",
      "Epoch [1017], train_loss: 1022.34 with loss1: 928.02, loss2: 94.32 and loss3: 0.00\n",
      "Epoch [1018], train_loss: 1017.69 with loss1: 923.57, loss2: 94.13 and loss3: 0.00\n",
      "Epoch [1019], train_loss: 1022.53 with loss1: 928.32, loss2: 94.21 and loss3: 0.00\n",
      "Epoch [1020], train_loss: 1015.86 with loss1: 921.98, loss2: 93.87 and loss3: 0.00\n",
      "Epoch [1021], train_loss: 1019.91 with loss1: 925.89, loss2: 94.02 and loss3: 0.00\n",
      "Epoch [1022], train_loss: 1013.10 with loss1: 919.03, loss2: 94.07 and loss3: 0.00\n",
      "Epoch [1023], train_loss: 1017.92 with loss1: 923.83, loss2: 94.08 and loss3: 0.00\n",
      "Epoch [1024], train_loss: 1011.23 with loss1: 917.31, loss2: 93.92 and loss3: 0.00\n",
      "Epoch [1025], train_loss: 1016.49 with loss1: 922.66, loss2: 93.83 and loss3: 0.00\n",
      "Epoch [1026], train_loss: 1011.59 with loss1: 917.56, loss2: 94.03 and loss3: 0.00\n",
      "Epoch [1027], train_loss: 1013.37 with loss1: 919.28, loss2: 94.09 and loss3: 0.00\n",
      "Epoch [1028], train_loss: 1009.59 with loss1: 915.69, loss2: 93.90 and loss3: 0.00\n",
      "Epoch [1029], train_loss: 1015.17 with loss1: 921.29, loss2: 93.88 and loss3: 0.00\n",
      "Epoch [1030], train_loss: 1006.04 with loss1: 912.12, loss2: 93.92 and loss3: 0.00\n",
      "Epoch [1031], train_loss: 1010.35 with loss1: 916.39, loss2: 93.97 and loss3: 0.00\n",
      "Epoch [1032], train_loss: 1006.30 with loss1: 912.49, loss2: 93.81 and loss3: 0.00\n",
      "Epoch [1033], train_loss: 1012.12 with loss1: 918.20, loss2: 93.92 and loss3: 0.00\n",
      "Epoch [1034], train_loss: 1007.62 with loss1: 913.86, loss2: 93.77 and loss3: 0.00\n",
      "Epoch [1035], train_loss: 1010.24 with loss1: 916.38, loss2: 93.87 and loss3: 0.00\n",
      "Epoch [1036], train_loss: 1005.70 with loss1: 912.03, loss2: 93.67 and loss3: 0.00\n",
      "Epoch [1037], train_loss: 1007.60 with loss1: 913.86, loss2: 93.74 and loss3: 0.00\n",
      "Epoch [1038], train_loss: 1005.30 with loss1: 911.66, loss2: 93.64 and loss3: 0.00\n",
      "Epoch [1039], train_loss: 1006.50 with loss1: 912.90, loss2: 93.60 and loss3: 0.00\n",
      "Epoch [1040], train_loss: 1003.51 with loss1: 910.01, loss2: 93.50 and loss3: 0.00\n",
      "Epoch [1041], train_loss: 1005.70 with loss1: 911.98, loss2: 93.71 and loss3: 0.00\n",
      "Epoch [1042], train_loss: 1002.34 with loss1: 908.79, loss2: 93.55 and loss3: 0.00\n",
      "Epoch [1043], train_loss: 1001.94 with loss1: 908.24, loss2: 93.70 and loss3: 0.00\n",
      "Epoch [1044], train_loss: 998.80 with loss1: 905.03, loss2: 93.76 and loss3: 0.00\n",
      "Epoch [1045], train_loss: 1005.50 with loss1: 911.89, loss2: 93.61 and loss3: 0.00\n",
      "Epoch [1046], train_loss: 997.30 with loss1: 903.76, loss2: 93.54 and loss3: 0.00\n",
      "Epoch [1047], train_loss: 999.66 with loss1: 906.02, loss2: 93.64 and loss3: 0.00\n",
      "Epoch [1048], train_loss: 999.24 with loss1: 905.88, loss2: 93.36 and loss3: 0.00\n",
      "Epoch [1049], train_loss: 999.04 with loss1: 905.51, loss2: 93.53 and loss3: 0.00\n",
      "Epoch [1050], train_loss: 995.99 with loss1: 902.59, loss2: 93.40 and loss3: 0.00\n",
      "Epoch [1051], train_loss: 1008.49 with loss1: 915.10, loss2: 93.39 and loss3: 0.00\n",
      "Epoch [1052], train_loss: 998.48 with loss1: 905.03, loss2: 93.44 and loss3: 0.00\n",
      "Epoch [1053], train_loss: 998.60 with loss1: 905.25, loss2: 93.35 and loss3: 0.00\n",
      "Epoch [1054], train_loss: 995.23 with loss1: 901.79, loss2: 93.44 and loss3: 0.00\n",
      "Epoch [1055], train_loss: 1002.02 with loss1: 908.53, loss2: 93.50 and loss3: 0.00\n",
      "Epoch [1056], train_loss: 992.06 with loss1: 898.90, loss2: 93.16 and loss3: 0.00\n",
      "Epoch [1057], train_loss: 999.24 with loss1: 905.96, loss2: 93.28 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1058], train_loss: 991.49 with loss1: 898.29, loss2: 93.20 and loss3: 0.00\n",
      "Epoch [1059], train_loss: 995.40 with loss1: 901.92, loss2: 93.48 and loss3: 0.00\n",
      "Epoch [1060], train_loss: 991.73 with loss1: 898.55, loss2: 93.18 and loss3: 0.00\n",
      "Epoch [1061], train_loss: 995.01 with loss1: 901.84, loss2: 93.17 and loss3: 0.00\n",
      "Epoch [1062], train_loss: 992.47 with loss1: 899.32, loss2: 93.16 and loss3: 0.00\n",
      "Epoch [1063], train_loss: 988.97 with loss1: 895.80, loss2: 93.16 and loss3: 0.00\n",
      "Epoch [1064], train_loss: 983.28 with loss1: 890.25, loss2: 93.03 and loss3: 0.00\n",
      "Epoch [1065], train_loss: 988.46 with loss1: 895.28, loss2: 93.18 and loss3: 0.00\n",
      "Epoch [1066], train_loss: 981.20 with loss1: 888.32, loss2: 92.89 and loss3: 0.00\n",
      "Epoch [1067], train_loss: 983.11 with loss1: 889.99, loss2: 93.12 and loss3: 0.00\n",
      "Epoch [1068], train_loss: 981.04 with loss1: 888.00, loss2: 93.05 and loss3: 0.00\n",
      "Epoch [1069], train_loss: 986.59 with loss1: 893.58, loss2: 93.01 and loss3: 0.00\n",
      "Epoch [1070], train_loss: 978.09 with loss1: 885.07, loss2: 93.02 and loss3: 0.00\n",
      "Epoch [1071], train_loss: 977.87 with loss1: 884.92, loss2: 92.96 and loss3: 0.00\n",
      "Epoch [1072], train_loss: 985.03 with loss1: 892.27, loss2: 92.76 and loss3: 0.00\n",
      "Epoch [1073], train_loss: 978.38 with loss1: 885.54, loss2: 92.85 and loss3: 0.00\n",
      "Epoch [1074], train_loss: 975.17 with loss1: 882.27, loss2: 92.90 and loss3: 0.00\n",
      "Epoch [1075], train_loss: 974.59 with loss1: 881.70, loss2: 92.89 and loss3: 0.00\n",
      "Epoch [1076], train_loss: 968.71 with loss1: 875.98, loss2: 92.73 and loss3: 0.00\n",
      "Epoch [1077], train_loss: 977.03 with loss1: 884.20, loss2: 92.83 and loss3: 0.00\n",
      "Epoch [1078], train_loss: 969.63 with loss1: 876.63, loss2: 93.00 and loss3: 0.00\n",
      "Epoch [1079], train_loss: 971.61 with loss1: 878.65, loss2: 92.96 and loss3: 0.00\n",
      "Epoch [1080], train_loss: 967.92 with loss1: 875.24, loss2: 92.69 and loss3: 0.00\n",
      "Epoch [1081], train_loss: 972.16 with loss1: 879.27, loss2: 92.90 and loss3: 0.00\n",
      "Epoch [1082], train_loss: 967.11 with loss1: 874.23, loss2: 92.88 and loss3: 0.00\n",
      "Epoch [1083], train_loss: 970.05 with loss1: 877.29, loss2: 92.76 and loss3: 0.00\n",
      "Epoch [1084], train_loss: 965.43 with loss1: 872.63, loss2: 92.80 and loss3: 0.00\n",
      "Epoch [1085], train_loss: 970.82 with loss1: 878.15, loss2: 92.67 and loss3: 0.00\n",
      "Epoch [1086], train_loss: 965.77 with loss1: 873.05, loss2: 92.72 and loss3: 0.00\n",
      "Epoch [1087], train_loss: 968.47 with loss1: 875.74, loss2: 92.73 and loss3: 0.00\n",
      "Epoch [1088], train_loss: 969.34 with loss1: 876.87, loss2: 92.47 and loss3: 0.00\n",
      "Epoch [1089], train_loss: 971.37 with loss1: 878.65, loss2: 92.73 and loss3: 0.00\n",
      "Epoch [1090], train_loss: 970.14 with loss1: 877.69, loss2: 92.45 and loss3: 0.00\n",
      "Epoch [1091], train_loss: 976.52 with loss1: 883.82, loss2: 92.70 and loss3: 0.00\n",
      "Epoch [1092], train_loss: 975.08 with loss1: 882.52, loss2: 92.56 and loss3: 0.00\n",
      "Epoch [1093], train_loss: 980.48 with loss1: 887.83, loss2: 92.65 and loss3: 0.00\n",
      "Epoch [1094], train_loss: 979.64 with loss1: 887.21, loss2: 92.44 and loss3: 0.00\n",
      "Epoch [1095], train_loss: 987.25 with loss1: 894.64, loss2: 92.60 and loss3: 0.00\n",
      "Epoch [1096], train_loss: 985.42 with loss1: 893.03, loss2: 92.39 and loss3: 0.00\n",
      "Epoch [1097], train_loss: 997.08 with loss1: 904.58, loss2: 92.50 and loss3: 0.00\n",
      "Epoch [1098], train_loss: 991.89 with loss1: 899.70, loss2: 92.19 and loss3: 0.00\n",
      "Epoch [1099], train_loss: 1003.98 with loss1: 911.31, loss2: 92.67 and loss3: 0.00\n",
      "Epoch [1100], train_loss: 998.38 with loss1: 906.31, loss2: 92.07 and loss3: 0.00\n",
      "Epoch [1101], train_loss: 1007.59 with loss1: 915.22, loss2: 92.37 and loss3: 0.00\n",
      "Epoch [1102], train_loss: 1006.69 with loss1: 914.63, loss2: 92.06 and loss3: 0.00\n",
      "Epoch [1103], train_loss: 1022.03 with loss1: 929.60, loss2: 92.43 and loss3: 0.00\n",
      "Epoch [1104], train_loss: 1018.89 with loss1: 926.64, loss2: 92.24 and loss3: 0.00\n",
      "Epoch [1105], train_loss: 1029.93 with loss1: 937.64, loss2: 92.30 and loss3: 0.00\n",
      "Epoch [1106], train_loss: 1023.78 with loss1: 931.74, loss2: 92.04 and loss3: 0.00\n",
      "Epoch [1107], train_loss: 1036.03 with loss1: 943.80, loss2: 92.23 and loss3: 0.00\n",
      "Epoch [1108], train_loss: 1022.46 with loss1: 930.40, loss2: 92.05 and loss3: 0.00\n",
      "Epoch [1109], train_loss: 1030.88 with loss1: 938.87, loss2: 92.00 and loss3: 0.00\n",
      "Epoch [1110], train_loss: 1019.71 with loss1: 927.64, loss2: 92.07 and loss3: 0.00\n",
      "Epoch [1111], train_loss: 1024.72 with loss1: 932.71, loss2: 92.01 and loss3: 0.00\n",
      "Epoch [1112], train_loss: 1010.68 with loss1: 918.76, loss2: 91.92 and loss3: 0.00\n",
      "Epoch [1113], train_loss: 1015.51 with loss1: 923.59, loss2: 91.91 and loss3: 0.00\n",
      "Epoch [1114], train_loss: 1005.04 with loss1: 913.12, loss2: 91.92 and loss3: 0.00\n",
      "Epoch [1115], train_loss: 1007.00 with loss1: 915.06, loss2: 91.94 and loss3: 0.00\n",
      "Epoch [1116], train_loss: 998.70 with loss1: 906.80, loss2: 91.90 and loss3: 0.00\n",
      "Epoch [1117], train_loss: 999.65 with loss1: 907.81, loss2: 91.85 and loss3: 0.00\n",
      "Epoch [1118], train_loss: 990.42 with loss1: 898.62, loss2: 91.80 and loss3: 0.00\n",
      "Epoch [1119], train_loss: 996.60 with loss1: 904.86, loss2: 91.74 and loss3: 0.00\n",
      "Epoch [1120], train_loss: 983.02 with loss1: 891.32, loss2: 91.70 and loss3: 0.00\n",
      "Epoch [1121], train_loss: 986.73 with loss1: 894.83, loss2: 91.91 and loss3: 0.00\n",
      "Epoch [1122], train_loss: 983.21 with loss1: 891.55, loss2: 91.66 and loss3: 0.00\n",
      "Epoch [1123], train_loss: 981.69 with loss1: 889.92, loss2: 91.77 and loss3: 0.00\n",
      "Epoch [1124], train_loss: 979.95 with loss1: 888.22, loss2: 91.73 and loss3: 0.00\n",
      "Epoch [1125], train_loss: 980.68 with loss1: 889.02, loss2: 91.67 and loss3: 0.00\n",
      "Epoch [1126], train_loss: 976.01 with loss1: 884.30, loss2: 91.72 and loss3: 0.00\n",
      "Epoch [1127], train_loss: 984.37 with loss1: 892.77, loss2: 91.60 and loss3: 0.00\n",
      "Epoch [1128], train_loss: 982.10 with loss1: 890.50, loss2: 91.60 and loss3: 0.00\n",
      "Epoch [1129], train_loss: 984.64 with loss1: 893.05, loss2: 91.59 and loss3: 0.00\n",
      "Epoch [1130], train_loss: 983.55 with loss1: 892.07, loss2: 91.48 and loss3: 0.00\n",
      "Epoch [1131], train_loss: 988.04 with loss1: 896.58, loss2: 91.45 and loss3: 0.00\n",
      "Epoch [1132], train_loss: 987.63 with loss1: 896.06, loss2: 91.57 and loss3: 0.00\n",
      "Epoch [1133], train_loss: 994.51 with loss1: 903.03, loss2: 91.48 and loss3: 0.00\n",
      "Epoch [1134], train_loss: 992.75 with loss1: 901.08, loss2: 91.67 and loss3: 0.00\n",
      "Epoch [1135], train_loss: 991.75 with loss1: 900.23, loss2: 91.52 and loss3: 0.00\n",
      "Epoch [1136], train_loss: 991.21 with loss1: 899.55, loss2: 91.65 and loss3: 0.00\n",
      "Epoch [1137], train_loss: 993.57 with loss1: 902.08, loss2: 91.49 and loss3: 0.00\n",
      "Epoch [1138], train_loss: 989.12 with loss1: 897.69, loss2: 91.43 and loss3: 0.00\n",
      "Epoch [1139], train_loss: 988.00 with loss1: 896.58, loss2: 91.42 and loss3: 0.00\n",
      "Epoch [1140], train_loss: 981.98 with loss1: 890.48, loss2: 91.50 and loss3: 0.00\n",
      "Epoch [1141], train_loss: 988.27 with loss1: 897.08, loss2: 91.19 and loss3: 0.00\n",
      "Epoch [1142], train_loss: 978.52 with loss1: 887.00, loss2: 91.51 and loss3: 0.00\n",
      "Epoch [1143], train_loss: 975.34 with loss1: 884.05, loss2: 91.28 and loss3: 0.00\n",
      "Epoch [1144], train_loss: 968.67 with loss1: 877.41, loss2: 91.26 and loss3: 0.00\n",
      "Epoch [1145], train_loss: 974.36 with loss1: 883.19, loss2: 91.17 and loss3: 0.00\n",
      "Epoch [1146], train_loss: 963.66 with loss1: 872.41, loss2: 91.25 and loss3: 0.00\n",
      "Epoch [1147], train_loss: 966.97 with loss1: 875.89, loss2: 91.08 and loss3: 0.00\n",
      "Epoch [1148], train_loss: 957.29 with loss1: 866.06, loss2: 91.23 and loss3: 0.00\n",
      "Epoch [1149], train_loss: 954.64 with loss1: 863.48, loss2: 91.16 and loss3: 0.00\n",
      "Epoch [1150], train_loss: 947.22 with loss1: 856.15, loss2: 91.07 and loss3: 0.00\n",
      "Epoch [1151], train_loss: 949.03 with loss1: 858.03, loss2: 91.00 and loss3: 0.00\n",
      "Epoch [1152], train_loss: 943.80 with loss1: 852.83, loss2: 90.97 and loss3: 0.00\n",
      "Epoch [1153], train_loss: 940.20 with loss1: 849.28, loss2: 90.92 and loss3: 0.00\n",
      "Epoch [1154], train_loss: 936.03 with loss1: 845.14, loss2: 90.88 and loss3: 0.00\n",
      "Epoch [1155], train_loss: 937.17 with loss1: 846.26, loss2: 90.92 and loss3: 0.00\n",
      "Epoch [1156], train_loss: 935.25 with loss1: 844.49, loss2: 90.76 and loss3: 0.00\n",
      "Epoch [1157], train_loss: 934.52 with loss1: 843.85, loss2: 90.67 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1158], train_loss: 929.60 with loss1: 838.75, loss2: 90.85 and loss3: 0.00\n",
      "Epoch [1159], train_loss: 928.25 with loss1: 837.45, loss2: 90.80 and loss3: 0.00\n",
      "Epoch [1160], train_loss: 927.53 with loss1: 836.68, loss2: 90.85 and loss3: 0.00\n",
      "Epoch [1161], train_loss: 925.28 with loss1: 834.60, loss2: 90.69 and loss3: 0.00\n",
      "Epoch [1162], train_loss: 921.74 with loss1: 830.99, loss2: 90.75 and loss3: 0.00\n",
      "Epoch [1163], train_loss: 923.08 with loss1: 832.40, loss2: 90.68 and loss3: 0.00\n",
      "Epoch [1164], train_loss: 923.16 with loss1: 832.38, loss2: 90.78 and loss3: 0.00\n",
      "Epoch [1165], train_loss: 921.84 with loss1: 831.23, loss2: 90.60 and loss3: 0.00\n",
      "Epoch [1166], train_loss: 922.70 with loss1: 832.01, loss2: 90.68 and loss3: 0.00\n",
      "Epoch [1167], train_loss: 918.34 with loss1: 827.68, loss2: 90.66 and loss3: 0.00\n",
      "Epoch [1168], train_loss: 919.06 with loss1: 828.32, loss2: 90.74 and loss3: 0.00\n",
      "Epoch [1169], train_loss: 917.10 with loss1: 826.39, loss2: 90.71 and loss3: 0.00\n",
      "Epoch [1170], train_loss: 914.40 with loss1: 823.60, loss2: 90.80 and loss3: 0.00\n",
      "Epoch [1171], train_loss: 917.09 with loss1: 826.64, loss2: 90.45 and loss3: 0.00\n",
      "Epoch [1172], train_loss: 913.16 with loss1: 822.59, loss2: 90.57 and loss3: 0.00\n",
      "Epoch [1173], train_loss: 910.05 with loss1: 819.60, loss2: 90.45 and loss3: 0.00\n",
      "Epoch [1174], train_loss: 910.82 with loss1: 820.25, loss2: 90.58 and loss3: 0.00\n",
      "Epoch [1175], train_loss: 910.94 with loss1: 820.50, loss2: 90.45 and loss3: 0.00\n",
      "Epoch [1176], train_loss: 905.69 with loss1: 815.26, loss2: 90.43 and loss3: 0.00\n",
      "Epoch [1177], train_loss: 909.77 with loss1: 819.39, loss2: 90.39 and loss3: 0.00\n",
      "Epoch [1178], train_loss: 906.88 with loss1: 816.42, loss2: 90.45 and loss3: 0.00\n",
      "Epoch [1179], train_loss: 911.02 with loss1: 820.67, loss2: 90.36 and loss3: 0.00\n",
      "Epoch [1180], train_loss: 912.62 with loss1: 822.21, loss2: 90.42 and loss3: 0.00\n",
      "Epoch [1181], train_loss: 908.48 with loss1: 818.04, loss2: 90.44 and loss3: 0.00\n",
      "Epoch [1182], train_loss: 907.60 with loss1: 817.22, loss2: 90.39 and loss3: 0.00\n",
      "Epoch [1183], train_loss: 912.34 with loss1: 821.96, loss2: 90.38 and loss3: 0.00\n",
      "Epoch [1184], train_loss: 909.07 with loss1: 818.66, loss2: 90.41 and loss3: 0.00\n",
      "Epoch [1185], train_loss: 911.87 with loss1: 821.53, loss2: 90.34 and loss3: 0.00\n",
      "Epoch [1186], train_loss: 907.12 with loss1: 816.84, loss2: 90.28 and loss3: 0.00\n",
      "Epoch [1187], train_loss: 913.42 with loss1: 823.24, loss2: 90.18 and loss3: 0.00\n",
      "Epoch [1188], train_loss: 911.82 with loss1: 821.46, loss2: 90.36 and loss3: 0.00\n",
      "Epoch [1189], train_loss: 911.57 with loss1: 821.23, loss2: 90.34 and loss3: 0.00\n",
      "Epoch [1190], train_loss: 910.46 with loss1: 820.18, loss2: 90.28 and loss3: 0.00\n",
      "Epoch [1191], train_loss: 910.95 with loss1: 820.72, loss2: 90.22 and loss3: 0.00\n",
      "Epoch [1192], train_loss: 910.06 with loss1: 819.92, loss2: 90.13 and loss3: 0.00\n",
      "Epoch [1193], train_loss: 913.80 with loss1: 823.51, loss2: 90.29 and loss3: 0.00\n",
      "Epoch [1194], train_loss: 914.78 with loss1: 824.57, loss2: 90.21 and loss3: 0.00\n",
      "Epoch [1195], train_loss: 913.25 with loss1: 823.10, loss2: 90.15 and loss3: 0.00\n",
      "Epoch [1196], train_loss: 911.11 with loss1: 820.87, loss2: 90.24 and loss3: 0.00\n",
      "Epoch [1197], train_loss: 913.66 with loss1: 823.58, loss2: 90.07 and loss3: 0.00\n",
      "Epoch [1198], train_loss: 914.69 with loss1: 824.67, loss2: 90.02 and loss3: 0.00\n",
      "Epoch [1199], train_loss: 917.33 with loss1: 827.40, loss2: 89.93 and loss3: 0.00\n",
      "Epoch [1200], train_loss: 916.89 with loss1: 826.71, loss2: 90.18 and loss3: 0.00\n",
      "Epoch [1201], train_loss: 921.57 with loss1: 831.55, loss2: 90.02 and loss3: 0.00\n",
      "Epoch [1202], train_loss: 916.55 with loss1: 826.35, loss2: 90.20 and loss3: 0.00\n",
      "Epoch [1203], train_loss: 920.19 with loss1: 830.10, loss2: 90.09 and loss3: 0.00\n",
      "Epoch [1204], train_loss: 919.93 with loss1: 829.81, loss2: 90.11 and loss3: 0.00\n",
      "Epoch [1205], train_loss: 920.15 with loss1: 830.18, loss2: 89.98 and loss3: 0.00\n",
      "Epoch [1206], train_loss: 922.98 with loss1: 833.05, loss2: 89.93 and loss3: 0.00\n",
      "Epoch [1207], train_loss: 923.28 with loss1: 833.20, loss2: 90.07 and loss3: 0.00\n",
      "Epoch [1208], train_loss: 921.04 with loss1: 830.96, loss2: 90.08 and loss3: 0.00\n",
      "Epoch [1209], train_loss: 922.96 with loss1: 832.97, loss2: 89.98 and loss3: 0.00\n",
      "Epoch [1210], train_loss: 919.03 with loss1: 829.15, loss2: 89.89 and loss3: 0.00\n",
      "Epoch [1211], train_loss: 926.49 with loss1: 836.60, loss2: 89.89 and loss3: 0.00\n",
      "Epoch [1212], train_loss: 924.34 with loss1: 834.51, loss2: 89.83 and loss3: 0.00\n",
      "Epoch [1213], train_loss: 929.25 with loss1: 839.57, loss2: 89.68 and loss3: 0.00\n",
      "Epoch [1214], train_loss: 922.38 with loss1: 832.51, loss2: 89.86 and loss3: 0.00\n",
      "Epoch [1215], train_loss: 926.61 with loss1: 836.80, loss2: 89.81 and loss3: 0.00\n",
      "Epoch [1216], train_loss: 922.81 with loss1: 833.05, loss2: 89.77 and loss3: 0.00\n",
      "Epoch [1217], train_loss: 922.41 with loss1: 832.70, loss2: 89.71 and loss3: 0.00\n",
      "Epoch [1218], train_loss: 921.47 with loss1: 831.75, loss2: 89.72 and loss3: 0.00\n",
      "Epoch [1219], train_loss: 923.36 with loss1: 833.81, loss2: 89.54 and loss3: 0.00\n",
      "Epoch [1220], train_loss: 920.21 with loss1: 830.60, loss2: 89.62 and loss3: 0.00\n",
      "Epoch [1221], train_loss: 924.55 with loss1: 834.91, loss2: 89.64 and loss3: 0.00\n",
      "Epoch [1222], train_loss: 919.62 with loss1: 830.07, loss2: 89.55 and loss3: 0.00\n",
      "Epoch [1223], train_loss: 926.55 with loss1: 836.93, loss2: 89.62 and loss3: 0.00\n",
      "Epoch [1224], train_loss: 922.43 with loss1: 832.77, loss2: 89.67 and loss3: 0.00\n",
      "Epoch [1225], train_loss: 927.35 with loss1: 837.71, loss2: 89.65 and loss3: 0.00\n",
      "Epoch [1226], train_loss: 920.97 with loss1: 831.48, loss2: 89.49 and loss3: 0.00\n",
      "Epoch [1227], train_loss: 926.85 with loss1: 837.25, loss2: 89.60 and loss3: 0.00\n",
      "Epoch [1228], train_loss: 921.15 with loss1: 831.66, loss2: 89.49 and loss3: 0.00\n",
      "Epoch [1229], train_loss: 930.24 with loss1: 840.73, loss2: 89.51 and loss3: 0.00\n",
      "Epoch [1230], train_loss: 925.28 with loss1: 835.84, loss2: 89.44 and loss3: 0.00\n",
      "Epoch [1231], train_loss: 932.80 with loss1: 843.28, loss2: 89.51 and loss3: 0.00\n",
      "Epoch [1232], train_loss: 928.52 with loss1: 839.19, loss2: 89.34 and loss3: 0.00\n",
      "Epoch [1233], train_loss: 931.62 with loss1: 842.23, loss2: 89.38 and loss3: 0.00\n",
      "Epoch [1234], train_loss: 926.51 with loss1: 837.22, loss2: 89.29 and loss3: 0.00\n",
      "Epoch [1235], train_loss: 928.49 with loss1: 839.17, loss2: 89.32 and loss3: 0.00\n",
      "Epoch [1236], train_loss: 927.89 with loss1: 838.67, loss2: 89.22 and loss3: 0.00\n",
      "Epoch [1237], train_loss: 928.49 with loss1: 839.18, loss2: 89.31 and loss3: 0.00\n",
      "Epoch [1238], train_loss: 926.68 with loss1: 837.59, loss2: 89.09 and loss3: 0.00\n",
      "Epoch [1239], train_loss: 933.15 with loss1: 843.93, loss2: 89.22 and loss3: 0.00\n",
      "Epoch [1240], train_loss: 925.41 with loss1: 836.22, loss2: 89.19 and loss3: 0.00\n",
      "Epoch [1241], train_loss: 930.48 with loss1: 841.20, loss2: 89.28 and loss3: 0.00\n",
      "Epoch [1242], train_loss: 922.92 with loss1: 833.81, loss2: 89.11 and loss3: 0.00\n",
      "Epoch [1243], train_loss: 932.22 with loss1: 843.07, loss2: 89.15 and loss3: 0.00\n",
      "Epoch [1244], train_loss: 922.90 with loss1: 833.96, loss2: 88.94 and loss3: 0.00\n",
      "Epoch [1245], train_loss: 927.19 with loss1: 838.10, loss2: 89.10 and loss3: 0.00\n",
      "Epoch [1246], train_loss: 922.61 with loss1: 833.78, loss2: 88.83 and loss3: 0.00\n",
      "Epoch [1247], train_loss: 923.59 with loss1: 834.67, loss2: 88.92 and loss3: 0.00\n",
      "Epoch [1248], train_loss: 915.75 with loss1: 826.93, loss2: 88.82 and loss3: 0.00\n",
      "Epoch [1249], train_loss: 920.85 with loss1: 831.79, loss2: 89.05 and loss3: 0.00\n",
      "Epoch [1250], train_loss: 913.70 with loss1: 824.75, loss2: 88.94 and loss3: 0.00\n",
      "Epoch [1251], train_loss: 915.50 with loss1: 826.59, loss2: 88.91 and loss3: 0.00\n",
      "Epoch [1252], train_loss: 908.25 with loss1: 819.49, loss2: 88.76 and loss3: 0.00\n",
      "Epoch [1253], train_loss: 915.03 with loss1: 826.27, loss2: 88.76 and loss3: 0.00\n",
      "Epoch [1254], train_loss: 903.95 with loss1: 815.06, loss2: 88.90 and loss3: 0.00\n",
      "Epoch [1255], train_loss: 907.31 with loss1: 818.41, loss2: 88.90 and loss3: 0.00\n",
      "Epoch [1256], train_loss: 901.76 with loss1: 813.12, loss2: 88.64 and loss3: 0.00\n",
      "Epoch [1257], train_loss: 903.37 with loss1: 814.67, loss2: 88.71 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1258], train_loss: 898.38 with loss1: 809.80, loss2: 88.58 and loss3: 0.00\n",
      "Epoch [1259], train_loss: 903.24 with loss1: 814.45, loss2: 88.79 and loss3: 0.00\n",
      "Epoch [1260], train_loss: 896.19 with loss1: 807.52, loss2: 88.67 and loss3: 0.00\n",
      "Epoch [1261], train_loss: 897.31 with loss1: 808.51, loss2: 88.80 and loss3: 0.00\n",
      "Epoch [1262], train_loss: 893.50 with loss1: 805.03, loss2: 88.47 and loss3: 0.00\n",
      "Epoch [1263], train_loss: 900.01 with loss1: 811.26, loss2: 88.75 and loss3: 0.00\n",
      "Epoch [1264], train_loss: 892.57 with loss1: 803.98, loss2: 88.59 and loss3: 0.00\n",
      "Epoch [1265], train_loss: 897.57 with loss1: 808.96, loss2: 88.61 and loss3: 0.00\n",
      "Epoch [1266], train_loss: 890.60 with loss1: 802.12, loss2: 88.48 and loss3: 0.00\n",
      "Epoch [1267], train_loss: 891.29 with loss1: 802.71, loss2: 88.58 and loss3: 0.00\n",
      "Epoch [1268], train_loss: 889.42 with loss1: 800.84, loss2: 88.58 and loss3: 0.00\n",
      "Epoch [1269], train_loss: 891.86 with loss1: 803.34, loss2: 88.52 and loss3: 0.00\n",
      "Epoch [1270], train_loss: 888.52 with loss1: 799.96, loss2: 88.56 and loss3: 0.00\n",
      "Epoch [1271], train_loss: 889.12 with loss1: 800.56, loss2: 88.56 and loss3: 0.00\n",
      "Epoch [1272], train_loss: 881.97 with loss1: 793.51, loss2: 88.45 and loss3: 0.00\n",
      "Epoch [1273], train_loss: 886.50 with loss1: 797.91, loss2: 88.59 and loss3: 0.00\n",
      "Epoch [1274], train_loss: 887.04 with loss1: 798.61, loss2: 88.43 and loss3: 0.00\n",
      "Epoch [1275], train_loss: 888.32 with loss1: 799.77, loss2: 88.55 and loss3: 0.00\n",
      "Epoch [1276], train_loss: 884.19 with loss1: 795.71, loss2: 88.47 and loss3: 0.00\n",
      "Epoch [1277], train_loss: 883.75 with loss1: 795.21, loss2: 88.54 and loss3: 0.00\n",
      "Epoch [1278], train_loss: 878.83 with loss1: 790.41, loss2: 88.43 and loss3: 0.00\n",
      "Epoch [1279], train_loss: 884.66 with loss1: 796.17, loss2: 88.49 and loss3: 0.00\n",
      "Epoch [1280], train_loss: 877.42 with loss1: 789.01, loss2: 88.41 and loss3: 0.00\n",
      "Epoch [1281], train_loss: 881.39 with loss1: 793.07, loss2: 88.33 and loss3: 0.00\n",
      "Epoch [1282], train_loss: 876.43 with loss1: 788.09, loss2: 88.34 and loss3: 0.00\n",
      "Epoch [1283], train_loss: 882.79 with loss1: 794.41, loss2: 88.38 and loss3: 0.00\n",
      "Epoch [1284], train_loss: 875.50 with loss1: 787.25, loss2: 88.25 and loss3: 0.00\n",
      "Epoch [1285], train_loss: 878.84 with loss1: 790.53, loss2: 88.31 and loss3: 0.00\n",
      "Epoch [1286], train_loss: 876.54 with loss1: 788.35, loss2: 88.19 and loss3: 0.00\n",
      "Epoch [1287], train_loss: 876.78 with loss1: 788.43, loss2: 88.35 and loss3: 0.00\n",
      "Epoch [1288], train_loss: 871.25 with loss1: 782.90, loss2: 88.35 and loss3: 0.00\n",
      "Epoch [1289], train_loss: 875.26 with loss1: 786.85, loss2: 88.40 and loss3: 0.00\n",
      "Epoch [1290], train_loss: 868.60 with loss1: 780.46, loss2: 88.15 and loss3: 0.00\n",
      "Epoch [1291], train_loss: 873.39 with loss1: 785.07, loss2: 88.32 and loss3: 0.00\n",
      "Epoch [1292], train_loss: 866.97 with loss1: 778.93, loss2: 88.04 and loss3: 0.00\n",
      "Epoch [1293], train_loss: 870.44 with loss1: 782.32, loss2: 88.12 and loss3: 0.00\n",
      "Epoch [1294], train_loss: 870.60 with loss1: 782.46, loss2: 88.14 and loss3: 0.00\n",
      "Epoch [1295], train_loss: 870.12 with loss1: 781.87, loss2: 88.25 and loss3: 0.00\n",
      "Epoch [1296], train_loss: 864.40 with loss1: 776.33, loss2: 88.07 and loss3: 0.00\n",
      "Epoch [1297], train_loss: 866.76 with loss1: 778.56, loss2: 88.20 and loss3: 0.00\n",
      "Epoch [1298], train_loss: 859.94 with loss1: 771.91, loss2: 88.03 and loss3: 0.00\n",
      "Epoch [1299], train_loss: 863.50 with loss1: 775.38, loss2: 88.12 and loss3: 0.00\n",
      "Epoch [1300], train_loss: 861.11 with loss1: 773.16, loss2: 87.95 and loss3: 0.00\n",
      "Epoch [1301], train_loss: 864.74 with loss1: 776.76, loss2: 87.98 and loss3: 0.00\n",
      "Epoch [1302], train_loss: 862.08 with loss1: 773.92, loss2: 88.16 and loss3: 0.00\n",
      "Epoch [1303], train_loss: 859.15 with loss1: 771.15, loss2: 88.00 and loss3: 0.00\n",
      "Epoch [1304], train_loss: 859.10 with loss1: 771.20, loss2: 87.90 and loss3: 0.00\n",
      "Epoch [1305], train_loss: 856.68 with loss1: 768.75, loss2: 87.93 and loss3: 0.00\n",
      "Epoch [1306], train_loss: 855.41 with loss1: 767.59, loss2: 87.81 and loss3: 0.00\n",
      "Epoch [1307], train_loss: 856.49 with loss1: 768.51, loss2: 87.99 and loss3: 0.00\n",
      "Epoch [1308], train_loss: 858.40 with loss1: 770.41, loss2: 87.98 and loss3: 0.00\n",
      "Epoch [1309], train_loss: 857.40 with loss1: 769.52, loss2: 87.89 and loss3: 0.00\n",
      "Epoch [1310], train_loss: 855.80 with loss1: 768.12, loss2: 87.68 and loss3: 0.00\n",
      "Epoch [1311], train_loss: 854.61 with loss1: 766.87, loss2: 87.74 and loss3: 0.00\n",
      "Epoch [1312], train_loss: 849.55 with loss1: 761.79, loss2: 87.76 and loss3: 0.00\n",
      "Epoch [1313], train_loss: 853.90 with loss1: 766.21, loss2: 87.69 and loss3: 0.00\n",
      "Epoch [1314], train_loss: 851.23 with loss1: 763.52, loss2: 87.71 and loss3: 0.00\n",
      "Epoch [1315], train_loss: 849.52 with loss1: 761.72, loss2: 87.80 and loss3: 0.00\n",
      "Epoch [1316], train_loss: 853.24 with loss1: 765.54, loss2: 87.70 and loss3: 0.00\n",
      "Epoch [1317], train_loss: 851.89 with loss1: 764.18, loss2: 87.71 and loss3: 0.00\n",
      "Epoch [1318], train_loss: 849.29 with loss1: 761.56, loss2: 87.73 and loss3: 0.00\n",
      "Epoch [1319], train_loss: 851.24 with loss1: 763.49, loss2: 87.75 and loss3: 0.00\n",
      "Epoch [1320], train_loss: 846.24 with loss1: 758.46, loss2: 87.79 and loss3: 0.00\n",
      "Epoch [1321], train_loss: 850.55 with loss1: 762.89, loss2: 87.66 and loss3: 0.00\n",
      "Epoch [1322], train_loss: 848.50 with loss1: 760.85, loss2: 87.65 and loss3: 0.00\n",
      "Epoch [1323], train_loss: 851.31 with loss1: 763.63, loss2: 87.68 and loss3: 0.00\n",
      "Epoch [1324], train_loss: 850.10 with loss1: 762.70, loss2: 87.40 and loss3: 0.00\n",
      "Epoch [1325], train_loss: 851.86 with loss1: 764.18, loss2: 87.69 and loss3: 0.00\n",
      "Epoch [1326], train_loss: 849.38 with loss1: 761.73, loss2: 87.65 and loss3: 0.00\n",
      "Epoch [1327], train_loss: 851.39 with loss1: 763.74, loss2: 87.65 and loss3: 0.00\n",
      "Epoch [1328], train_loss: 849.10 with loss1: 761.46, loss2: 87.63 and loss3: 0.00\n",
      "Epoch [1329], train_loss: 849.51 with loss1: 762.04, loss2: 87.47 and loss3: 0.00\n",
      "Epoch [1330], train_loss: 848.55 with loss1: 761.16, loss2: 87.39 and loss3: 0.00\n",
      "Epoch [1331], train_loss: 852.65 with loss1: 765.29, loss2: 87.36 and loss3: 0.00\n",
      "Epoch [1332], train_loss: 848.70 with loss1: 761.30, loss2: 87.40 and loss3: 0.00\n",
      "Epoch [1333], train_loss: 854.70 with loss1: 767.12, loss2: 87.58 and loss3: 0.00\n",
      "Epoch [1334], train_loss: 851.94 with loss1: 764.52, loss2: 87.42 and loss3: 0.00\n",
      "Epoch [1335], train_loss: 855.12 with loss1: 767.79, loss2: 87.33 and loss3: 0.00\n",
      "Epoch [1336], train_loss: 856.85 with loss1: 769.45, loss2: 87.39 and loss3: 0.00\n",
      "Epoch [1337], train_loss: 856.26 with loss1: 768.75, loss2: 87.50 and loss3: 0.00\n",
      "Epoch [1338], train_loss: 858.40 with loss1: 770.94, loss2: 87.46 and loss3: 0.00\n",
      "Epoch [1339], train_loss: 860.82 with loss1: 773.39, loss2: 87.43 and loss3: 0.00\n",
      "Epoch [1340], train_loss: 855.31 with loss1: 768.01, loss2: 87.30 and loss3: 0.00\n",
      "Epoch [1341], train_loss: 858.16 with loss1: 770.73, loss2: 87.43 and loss3: 0.00\n",
      "Epoch [1342], train_loss: 857.29 with loss1: 769.93, loss2: 87.36 and loss3: 0.00\n",
      "Epoch [1343], train_loss: 862.50 with loss1: 775.11, loss2: 87.40 and loss3: 0.00\n",
      "Epoch [1344], train_loss: 861.53 with loss1: 774.16, loss2: 87.37 and loss3: 0.00\n",
      "Epoch [1345], train_loss: 864.04 with loss1: 776.67, loss2: 87.38 and loss3: 0.00\n",
      "Epoch [1346], train_loss: 858.53 with loss1: 771.44, loss2: 87.09 and loss3: 0.00\n",
      "Epoch [1347], train_loss: 864.68 with loss1: 777.45, loss2: 87.23 and loss3: 0.00\n",
      "Epoch [1348], train_loss: 859.91 with loss1: 772.75, loss2: 87.16 and loss3: 0.00\n",
      "Epoch [1349], train_loss: 865.51 with loss1: 778.40, loss2: 87.11 and loss3: 0.00\n",
      "Epoch [1350], train_loss: 861.43 with loss1: 774.38, loss2: 87.05 and loss3: 0.00\n",
      "Epoch [1351], train_loss: 866.63 with loss1: 779.49, loss2: 87.14 and loss3: 0.00\n",
      "Epoch [1352], train_loss: 863.06 with loss1: 775.99, loss2: 87.07 and loss3: 0.00\n",
      "Epoch [1353], train_loss: 865.29 with loss1: 778.10, loss2: 87.18 and loss3: 0.00\n",
      "Epoch [1354], train_loss: 861.98 with loss1: 775.05, loss2: 86.93 and loss3: 0.00\n",
      "Epoch [1355], train_loss: 864.88 with loss1: 777.83, loss2: 87.05 and loss3: 0.00\n",
      "Epoch [1356], train_loss: 859.55 with loss1: 772.65, loss2: 86.90 and loss3: 0.00\n",
      "Epoch [1357], train_loss: 864.91 with loss1: 777.80, loss2: 87.11 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1358], train_loss: 858.72 with loss1: 771.82, loss2: 86.90 and loss3: 0.00\n",
      "Epoch [1359], train_loss: 864.79 with loss1: 777.83, loss2: 86.96 and loss3: 0.00\n",
      "Epoch [1360], train_loss: 861.05 with loss1: 774.12, loss2: 86.93 and loss3: 0.00\n",
      "Epoch [1361], train_loss: 864.63 with loss1: 777.56, loss2: 87.07 and loss3: 0.00\n",
      "Epoch [1362], train_loss: 865.97 with loss1: 779.20, loss2: 86.76 and loss3: 0.00\n",
      "Epoch [1363], train_loss: 865.00 with loss1: 778.01, loss2: 86.99 and loss3: 0.00\n",
      "Epoch [1364], train_loss: 864.28 with loss1: 777.51, loss2: 86.77 and loss3: 0.00\n",
      "Epoch [1365], train_loss: 869.51 with loss1: 782.64, loss2: 86.86 and loss3: 0.00\n",
      "Epoch [1366], train_loss: 865.91 with loss1: 779.17, loss2: 86.74 and loss3: 0.00\n",
      "Epoch [1367], train_loss: 872.08 with loss1: 785.23, loss2: 86.85 and loss3: 0.00\n",
      "Epoch [1368], train_loss: 867.59 with loss1: 780.93, loss2: 86.66 and loss3: 0.00\n",
      "Epoch [1369], train_loss: 874.63 with loss1: 787.85, loss2: 86.78 and loss3: 0.00\n",
      "Epoch [1370], train_loss: 870.01 with loss1: 783.30, loss2: 86.71 and loss3: 0.00\n",
      "Epoch [1371], train_loss: 873.67 with loss1: 786.91, loss2: 86.76 and loss3: 0.00\n",
      "Epoch [1372], train_loss: 868.43 with loss1: 781.78, loss2: 86.65 and loss3: 0.00\n",
      "Epoch [1373], train_loss: 871.93 with loss1: 785.09, loss2: 86.84 and loss3: 0.00\n",
      "Epoch [1374], train_loss: 871.99 with loss1: 785.36, loss2: 86.63 and loss3: 0.00\n",
      "Epoch [1375], train_loss: 872.31 with loss1: 785.78, loss2: 86.53 and loss3: 0.00\n",
      "Epoch [1376], train_loss: 868.65 with loss1: 782.14, loss2: 86.51 and loss3: 0.00\n",
      "Epoch [1377], train_loss: 872.63 with loss1: 786.13, loss2: 86.50 and loss3: 0.00\n",
      "Epoch [1378], train_loss: 867.30 with loss1: 780.79, loss2: 86.52 and loss3: 0.00\n",
      "Epoch [1379], train_loss: 865.95 with loss1: 779.35, loss2: 86.60 and loss3: 0.00\n",
      "Epoch [1380], train_loss: 864.83 with loss1: 778.34, loss2: 86.49 and loss3: 0.00\n",
      "Epoch [1381], train_loss: 867.44 with loss1: 780.85, loss2: 86.60 and loss3: 0.00\n",
      "Epoch [1382], train_loss: 862.49 with loss1: 775.93, loss2: 86.56 and loss3: 0.00\n",
      "Epoch [1383], train_loss: 865.44 with loss1: 779.07, loss2: 86.38 and loss3: 0.00\n",
      "Epoch [1384], train_loss: 857.16 with loss1: 770.67, loss2: 86.49 and loss3: 0.00\n",
      "Epoch [1385], train_loss: 860.14 with loss1: 773.74, loss2: 86.40 and loss3: 0.00\n",
      "Epoch [1386], train_loss: 854.82 with loss1: 768.51, loss2: 86.31 and loss3: 0.00\n",
      "Epoch [1387], train_loss: 857.14 with loss1: 770.77, loss2: 86.37 and loss3: 0.00\n",
      "Epoch [1388], train_loss: 850.12 with loss1: 763.92, loss2: 86.21 and loss3: 0.00\n",
      "Epoch [1389], train_loss: 854.63 with loss1: 768.27, loss2: 86.36 and loss3: 0.00\n",
      "Epoch [1390], train_loss: 849.51 with loss1: 763.41, loss2: 86.10 and loss3: 0.00\n",
      "Epoch [1391], train_loss: 848.35 with loss1: 762.20, loss2: 86.16 and loss3: 0.00\n",
      "Epoch [1392], train_loss: 845.23 with loss1: 758.99, loss2: 86.24 and loss3: 0.00\n",
      "Epoch [1393], train_loss: 845.92 with loss1: 759.74, loss2: 86.18 and loss3: 0.00\n",
      "Epoch [1394], train_loss: 840.46 with loss1: 754.28, loss2: 86.18 and loss3: 0.00\n",
      "Epoch [1395], train_loss: 841.59 with loss1: 755.20, loss2: 86.38 and loss3: 0.00\n",
      "Epoch [1396], train_loss: 837.21 with loss1: 751.03, loss2: 86.17 and loss3: 0.00\n",
      "Epoch [1397], train_loss: 839.09 with loss1: 752.86, loss2: 86.23 and loss3: 0.00\n",
      "Epoch [1398], train_loss: 834.17 with loss1: 748.09, loss2: 86.08 and loss3: 0.00\n",
      "Epoch [1399], train_loss: 836.99 with loss1: 751.00, loss2: 85.99 and loss3: 0.00\n",
      "Epoch [1400], train_loss: 837.02 with loss1: 750.96, loss2: 86.06 and loss3: 0.00\n",
      "Epoch [1401], train_loss: 837.04 with loss1: 750.95, loss2: 86.09 and loss3: 0.00\n",
      "Epoch [1402], train_loss: 834.39 with loss1: 748.29, loss2: 86.10 and loss3: 0.00\n",
      "Epoch [1403], train_loss: 838.04 with loss1: 752.03, loss2: 86.00 and loss3: 0.00\n",
      "Epoch [1404], train_loss: 832.48 with loss1: 746.45, loss2: 86.02 and loss3: 0.00\n",
      "Epoch [1405], train_loss: 835.93 with loss1: 749.81, loss2: 86.12 and loss3: 0.00\n",
      "Epoch [1406], train_loss: 830.89 with loss1: 744.80, loss2: 86.09 and loss3: 0.00\n",
      "Epoch [1407], train_loss: 835.17 with loss1: 749.16, loss2: 86.00 and loss3: 0.00\n",
      "Epoch [1408], train_loss: 833.69 with loss1: 747.67, loss2: 86.02 and loss3: 0.00\n",
      "Epoch [1409], train_loss: 834.11 with loss1: 748.08, loss2: 86.03 and loss3: 0.00\n",
      "Epoch [1410], train_loss: 834.44 with loss1: 748.51, loss2: 85.93 and loss3: 0.00\n",
      "Epoch [1411], train_loss: 836.32 with loss1: 750.51, loss2: 85.81 and loss3: 0.00\n",
      "Epoch [1412], train_loss: 833.23 with loss1: 747.38, loss2: 85.85 and loss3: 0.00\n",
      "Epoch [1413], train_loss: 836.36 with loss1: 750.54, loss2: 85.82 and loss3: 0.00\n",
      "Epoch [1414], train_loss: 839.10 with loss1: 753.33, loss2: 85.77 and loss3: 0.00\n",
      "Epoch [1415], train_loss: 834.92 with loss1: 749.08, loss2: 85.84 and loss3: 0.00\n",
      "Epoch [1416], train_loss: 834.44 with loss1: 748.66, loss2: 85.79 and loss3: 0.00\n",
      "Epoch [1417], train_loss: 836.95 with loss1: 751.26, loss2: 85.69 and loss3: 0.00\n",
      "Epoch [1418], train_loss: 835.97 with loss1: 750.09, loss2: 85.88 and loss3: 0.00\n",
      "Epoch [1419], train_loss: 839.28 with loss1: 753.49, loss2: 85.79 and loss3: 0.00\n",
      "Epoch [1420], train_loss: 833.60 with loss1: 747.70, loss2: 85.91 and loss3: 0.00\n",
      "Epoch [1421], train_loss: 833.95 with loss1: 748.22, loss2: 85.73 and loss3: 0.00\n",
      "Epoch [1422], train_loss: 829.64 with loss1: 743.90, loss2: 85.74 and loss3: 0.00\n",
      "Epoch [1423], train_loss: 832.04 with loss1: 746.42, loss2: 85.62 and loss3: 0.00\n",
      "Epoch [1424], train_loss: 831.51 with loss1: 745.82, loss2: 85.69 and loss3: 0.00\n",
      "Epoch [1425], train_loss: 834.17 with loss1: 748.63, loss2: 85.54 and loss3: 0.00\n",
      "Epoch [1426], train_loss: 832.42 with loss1: 746.77, loss2: 85.65 and loss3: 0.00\n",
      "Epoch [1427], train_loss: 829.56 with loss1: 744.04, loss2: 85.52 and loss3: 0.00\n",
      "Epoch [1428], train_loss: 828.57 with loss1: 742.83, loss2: 85.74 and loss3: 0.00\n",
      "Epoch [1429], train_loss: 828.03 with loss1: 742.54, loss2: 85.48 and loss3: 0.00\n",
      "Epoch [1430], train_loss: 824.74 with loss1: 739.18, loss2: 85.56 and loss3: 0.00\n",
      "Epoch [1431], train_loss: 824.86 with loss1: 739.20, loss2: 85.66 and loss3: 0.00\n",
      "Epoch [1432], train_loss: 822.96 with loss1: 737.34, loss2: 85.61 and loss3: 0.00\n",
      "Epoch [1433], train_loss: 824.80 with loss1: 739.35, loss2: 85.45 and loss3: 0.00\n",
      "Epoch [1434], train_loss: 819.06 with loss1: 733.54, loss2: 85.52 and loss3: 0.00\n",
      "Epoch [1435], train_loss: 818.41 with loss1: 732.92, loss2: 85.49 and loss3: 0.00\n",
      "Epoch [1436], train_loss: 815.28 with loss1: 729.78, loss2: 85.50 and loss3: 0.00\n",
      "Epoch [1437], train_loss: 818.25 with loss1: 732.90, loss2: 85.36 and loss3: 0.00\n",
      "Epoch [1438], train_loss: 814.61 with loss1: 729.18, loss2: 85.43 and loss3: 0.00\n",
      "Epoch [1439], train_loss: 816.33 with loss1: 731.00, loss2: 85.33 and loss3: 0.00\n",
      "Epoch [1440], train_loss: 811.95 with loss1: 726.51, loss2: 85.43 and loss3: 0.00\n",
      "Epoch [1441], train_loss: 813.25 with loss1: 727.96, loss2: 85.30 and loss3: 0.00\n",
      "Epoch [1442], train_loss: 812.12 with loss1: 726.83, loss2: 85.29 and loss3: 0.00\n",
      "Epoch [1443], train_loss: 808.49 with loss1: 723.35, loss2: 85.14 and loss3: 0.00\n",
      "Epoch [1444], train_loss: 807.87 with loss1: 722.60, loss2: 85.27 and loss3: 0.00\n",
      "Epoch [1445], train_loss: 808.27 with loss1: 723.17, loss2: 85.10 and loss3: 0.00\n",
      "Epoch [1446], train_loss: 808.06 with loss1: 722.81, loss2: 85.24 and loss3: 0.00\n",
      "Epoch [1447], train_loss: 806.00 with loss1: 720.96, loss2: 85.04 and loss3: 0.00\n",
      "Epoch [1448], train_loss: 807.06 with loss1: 721.93, loss2: 85.13 and loss3: 0.00\n",
      "Epoch [1449], train_loss: 804.73 with loss1: 719.58, loss2: 85.15 and loss3: 0.00\n",
      "Epoch [1450], train_loss: 803.82 with loss1: 718.82, loss2: 85.00 and loss3: 0.00\n",
      "Epoch [1451], train_loss: 804.76 with loss1: 719.64, loss2: 85.11 and loss3: 0.00\n",
      "Epoch [1452], train_loss: 804.29 with loss1: 719.01, loss2: 85.28 and loss3: 0.00\n",
      "Epoch [1453], train_loss: 803.79 with loss1: 718.68, loss2: 85.11 and loss3: 0.00\n",
      "Epoch [1454], train_loss: 805.21 with loss1: 720.00, loss2: 85.21 and loss3: 0.00\n",
      "Epoch [1455], train_loss: 805.98 with loss1: 721.01, loss2: 84.97 and loss3: 0.00\n",
      "Epoch [1456], train_loss: 803.92 with loss1: 718.76, loss2: 85.16 and loss3: 0.00\n",
      "Epoch [1457], train_loss: 802.45 with loss1: 717.46, loss2: 84.99 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1458], train_loss: 800.24 with loss1: 715.19, loss2: 85.05 and loss3: 0.00\n",
      "Epoch [1459], train_loss: 802.58 with loss1: 717.79, loss2: 84.80 and loss3: 0.00\n",
      "Epoch [1460], train_loss: 799.00 with loss1: 714.10, loss2: 84.90 and loss3: 0.00\n",
      "Epoch [1461], train_loss: 801.49 with loss1: 716.48, loss2: 85.01 and loss3: 0.00\n",
      "Epoch [1462], train_loss: 800.62 with loss1: 715.64, loss2: 84.98 and loss3: 0.00\n",
      "Epoch [1463], train_loss: 802.04 with loss1: 717.14, loss2: 84.90 and loss3: 0.00\n",
      "Epoch [1464], train_loss: 799.86 with loss1: 714.72, loss2: 85.14 and loss3: 0.00\n",
      "Epoch [1465], train_loss: 799.74 with loss1: 714.92, loss2: 84.82 and loss3: 0.00\n",
      "Epoch [1466], train_loss: 799.29 with loss1: 714.27, loss2: 85.02 and loss3: 0.00\n",
      "Epoch [1467], train_loss: 801.79 with loss1: 716.95, loss2: 84.84 and loss3: 0.00\n",
      "Epoch [1468], train_loss: 801.17 with loss1: 716.18, loss2: 84.99 and loss3: 0.00\n",
      "Epoch [1469], train_loss: 803.85 with loss1: 719.07, loss2: 84.78 and loss3: 0.00\n",
      "Epoch [1470], train_loss: 801.40 with loss1: 716.58, loss2: 84.82 and loss3: 0.00\n",
      "Epoch [1471], train_loss: 804.05 with loss1: 719.30, loss2: 84.75 and loss3: 0.00\n",
      "Epoch [1472], train_loss: 803.45 with loss1: 718.63, loss2: 84.82 and loss3: 0.00\n",
      "Epoch [1473], train_loss: 804.79 with loss1: 720.04, loss2: 84.75 and loss3: 0.00\n",
      "Epoch [1474], train_loss: 801.79 with loss1: 717.10, loss2: 84.69 and loss3: 0.00\n",
      "Epoch [1475], train_loss: 803.92 with loss1: 719.34, loss2: 84.58 and loss3: 0.00\n",
      "Epoch [1476], train_loss: 801.08 with loss1: 716.34, loss2: 84.74 and loss3: 0.00\n",
      "Epoch [1477], train_loss: 802.91 with loss1: 718.23, loss2: 84.68 and loss3: 0.00\n",
      "Epoch [1478], train_loss: 798.05 with loss1: 713.26, loss2: 84.80 and loss3: 0.00\n",
      "Epoch [1479], train_loss: 802.51 with loss1: 717.89, loss2: 84.62 and loss3: 0.00\n",
      "Epoch [1480], train_loss: 800.31 with loss1: 715.55, loss2: 84.76 and loss3: 0.00\n",
      "Epoch [1481], train_loss: 801.28 with loss1: 716.65, loss2: 84.63 and loss3: 0.00\n",
      "Epoch [1482], train_loss: 804.39 with loss1: 719.68, loss2: 84.71 and loss3: 0.00\n",
      "Epoch [1483], train_loss: 805.30 with loss1: 720.82, loss2: 84.49 and loss3: 0.00\n",
      "Epoch [1484], train_loss: 801.08 with loss1: 716.52, loss2: 84.57 and loss3: 0.00\n",
      "Epoch [1485], train_loss: 807.20 with loss1: 722.69, loss2: 84.50 and loss3: 0.00\n",
      "Epoch [1486], train_loss: 803.88 with loss1: 719.26, loss2: 84.63 and loss3: 0.00\n",
      "Epoch [1487], train_loss: 803.80 with loss1: 719.20, loss2: 84.60 and loss3: 0.00\n",
      "Epoch [1488], train_loss: 805.89 with loss1: 721.23, loss2: 84.66 and loss3: 0.00\n",
      "Epoch [1489], train_loss: 807.06 with loss1: 722.64, loss2: 84.43 and loss3: 0.00\n",
      "Epoch [1490], train_loss: 804.91 with loss1: 720.48, loss2: 84.44 and loss3: 0.00\n",
      "Epoch [1491], train_loss: 806.16 with loss1: 721.78, loss2: 84.38 and loss3: 0.00\n",
      "Epoch [1492], train_loss: 805.52 with loss1: 721.04, loss2: 84.48 and loss3: 0.00\n",
      "Epoch [1493], train_loss: 807.20 with loss1: 722.70, loss2: 84.50 and loss3: 0.00\n",
      "Epoch [1494], train_loss: 806.54 with loss1: 722.10, loss2: 84.44 and loss3: 0.00\n",
      "Epoch [1495], train_loss: 806.10 with loss1: 721.71, loss2: 84.39 and loss3: 0.00\n",
      "Epoch [1496], train_loss: 807.24 with loss1: 722.88, loss2: 84.36 and loss3: 0.00\n",
      "Epoch [1497], train_loss: 806.76 with loss1: 722.37, loss2: 84.39 and loss3: 0.00\n",
      "Epoch [1498], train_loss: 805.67 with loss1: 721.29, loss2: 84.38 and loss3: 0.00\n",
      "Epoch [1499], train_loss: 809.93 with loss1: 725.68, loss2: 84.25 and loss3: 0.00\n",
      "Epoch [1500], train_loss: 809.02 with loss1: 724.65, loss2: 84.37 and loss3: 0.00\n",
      "Epoch [1501], train_loss: 807.29 with loss1: 722.91, loss2: 84.38 and loss3: 0.00\n",
      "Epoch [1502], train_loss: 805.76 with loss1: 721.56, loss2: 84.20 and loss3: 0.00\n",
      "Epoch [1503], train_loss: 809.62 with loss1: 725.32, loss2: 84.29 and loss3: 0.00\n",
      "Epoch [1504], train_loss: 805.33 with loss1: 721.08, loss2: 84.25 and loss3: 0.00\n",
      "Epoch [1505], train_loss: 806.84 with loss1: 722.65, loss2: 84.19 and loss3: 0.00\n",
      "Epoch [1506], train_loss: 806.00 with loss1: 721.81, loss2: 84.19 and loss3: 0.00\n",
      "Epoch [1507], train_loss: 806.72 with loss1: 722.46, loss2: 84.25 and loss3: 0.00\n",
      "Epoch [1508], train_loss: 804.75 with loss1: 720.67, loss2: 84.09 and loss3: 0.00\n",
      "Epoch [1509], train_loss: 807.80 with loss1: 723.60, loss2: 84.20 and loss3: 0.00\n",
      "Epoch [1510], train_loss: 806.05 with loss1: 721.89, loss2: 84.16 and loss3: 0.00\n",
      "Epoch [1511], train_loss: 807.38 with loss1: 723.33, loss2: 84.06 and loss3: 0.00\n",
      "Epoch [1512], train_loss: 809.65 with loss1: 725.49, loss2: 84.16 and loss3: 0.00\n",
      "Epoch [1513], train_loss: 815.10 with loss1: 730.94, loss2: 84.16 and loss3: 0.00\n",
      "Epoch [1514], train_loss: 813.93 with loss1: 729.90, loss2: 84.03 and loss3: 0.00\n",
      "Epoch [1515], train_loss: 816.77 with loss1: 732.73, loss2: 84.03 and loss3: 0.00\n",
      "Epoch [1516], train_loss: 814.08 with loss1: 729.93, loss2: 84.14 and loss3: 0.00\n",
      "Epoch [1517], train_loss: 819.97 with loss1: 735.91, loss2: 84.06 and loss3: 0.00\n",
      "Epoch [1518], train_loss: 820.85 with loss1: 736.91, loss2: 83.94 and loss3: 0.00\n",
      "Epoch [1519], train_loss: 826.06 with loss1: 741.96, loss2: 84.10 and loss3: 0.00\n",
      "Epoch [1520], train_loss: 825.89 with loss1: 741.89, loss2: 84.00 and loss3: 0.00\n",
      "Epoch [1521], train_loss: 832.73 with loss1: 748.70, loss2: 84.02 and loss3: 0.00\n",
      "Epoch [1522], train_loss: 829.47 with loss1: 745.47, loss2: 84.00 and loss3: 0.00\n",
      "Epoch [1523], train_loss: 837.07 with loss1: 752.89, loss2: 84.17 and loss3: 0.00\n",
      "Epoch [1524], train_loss: 834.74 with loss1: 750.87, loss2: 83.86 and loss3: 0.00\n",
      "Epoch [1525], train_loss: 837.45 with loss1: 753.42, loss2: 84.03 and loss3: 0.00\n",
      "Epoch [1526], train_loss: 833.76 with loss1: 749.95, loss2: 83.80 and loss3: 0.00\n",
      "Epoch [1527], train_loss: 843.52 with loss1: 759.52, loss2: 83.99 and loss3: 0.00\n",
      "Epoch [1528], train_loss: 836.00 with loss1: 752.19, loss2: 83.81 and loss3: 0.00\n",
      "Epoch [1529], train_loss: 845.16 with loss1: 761.13, loss2: 84.02 and loss3: 0.00\n",
      "Epoch [1530], train_loss: 833.30 with loss1: 749.55, loss2: 83.75 and loss3: 0.00\n",
      "Epoch [1531], train_loss: 838.97 with loss1: 755.09, loss2: 83.89 and loss3: 0.00\n",
      "Epoch [1532], train_loss: 828.83 with loss1: 745.01, loss2: 83.82 and loss3: 0.00\n",
      "Epoch [1533], train_loss: 831.27 with loss1: 747.37, loss2: 83.90 and loss3: 0.00\n",
      "Epoch [1534], train_loss: 820.31 with loss1: 736.71, loss2: 83.60 and loss3: 0.00\n",
      "Epoch [1535], train_loss: 822.65 with loss1: 738.85, loss2: 83.80 and loss3: 0.00\n",
      "Epoch [1536], train_loss: 810.23 with loss1: 726.60, loss2: 83.63 and loss3: 0.00\n",
      "Epoch [1537], train_loss: 810.99 with loss1: 727.29, loss2: 83.70 and loss3: 0.00\n",
      "Epoch [1538], train_loss: 801.66 with loss1: 718.18, loss2: 83.48 and loss3: 0.00\n",
      "Epoch [1539], train_loss: 801.32 with loss1: 717.54, loss2: 83.78 and loss3: 0.00\n",
      "Epoch [1540], train_loss: 790.50 with loss1: 707.04, loss2: 83.47 and loss3: 0.00\n",
      "Epoch [1541], train_loss: 793.33 with loss1: 709.75, loss2: 83.59 and loss3: 0.00\n",
      "Epoch [1542], train_loss: 785.47 with loss1: 701.79, loss2: 83.68 and loss3: 0.00\n",
      "Epoch [1543], train_loss: 783.07 with loss1: 699.46, loss2: 83.61 and loss3: 0.00\n",
      "Epoch [1544], train_loss: 779.10 with loss1: 695.62, loss2: 83.47 and loss3: 0.00\n",
      "Epoch [1545], train_loss: 777.95 with loss1: 694.34, loss2: 83.61 and loss3: 0.00\n",
      "Epoch [1546], train_loss: 775.89 with loss1: 692.45, loss2: 83.44 and loss3: 0.00\n",
      "Epoch [1547], train_loss: 771.24 with loss1: 687.60, loss2: 83.64 and loss3: 0.00\n",
      "Epoch [1548], train_loss: 768.77 with loss1: 685.38, loss2: 83.39 and loss3: 0.00\n",
      "Epoch [1549], train_loss: 770.96 with loss1: 687.28, loss2: 83.67 and loss3: 0.00\n",
      "Epoch [1550], train_loss: 761.84 with loss1: 678.54, loss2: 83.30 and loss3: 0.00\n",
      "Epoch [1551], train_loss: 761.68 with loss1: 678.21, loss2: 83.47 and loss3: 0.00\n",
      "Epoch [1552], train_loss: 761.26 with loss1: 677.94, loss2: 83.31 and loss3: 0.00\n",
      "Epoch [1553], train_loss: 759.50 with loss1: 676.04, loss2: 83.45 and loss3: 0.00\n",
      "Epoch [1554], train_loss: 756.26 with loss1: 672.94, loss2: 83.32 and loss3: 0.00\n",
      "Epoch [1555], train_loss: 759.67 with loss1: 676.27, loss2: 83.40 and loss3: 0.00\n",
      "Epoch [1556], train_loss: 758.67 with loss1: 675.36, loss2: 83.31 and loss3: 0.00\n",
      "Epoch [1557], train_loss: 758.45 with loss1: 675.11, loss2: 83.34 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1558], train_loss: 756.79 with loss1: 673.50, loss2: 83.29 and loss3: 0.00\n",
      "Epoch [1559], train_loss: 756.56 with loss1: 673.19, loss2: 83.37 and loss3: 0.00\n",
      "Epoch [1560], train_loss: 754.46 with loss1: 671.21, loss2: 83.25 and loss3: 0.00\n",
      "Epoch [1561], train_loss: 756.96 with loss1: 673.74, loss2: 83.22 and loss3: 0.00\n",
      "Epoch [1562], train_loss: 754.86 with loss1: 671.82, loss2: 83.04 and loss3: 0.00\n",
      "Epoch [1563], train_loss: 758.79 with loss1: 675.49, loss2: 83.30 and loss3: 0.00\n",
      "Epoch [1564], train_loss: 756.94 with loss1: 673.86, loss2: 83.08 and loss3: 0.00\n",
      "Epoch [1565], train_loss: 761.71 with loss1: 678.55, loss2: 83.16 and loss3: 0.00\n",
      "Epoch [1566], train_loss: 756.76 with loss1: 673.68, loss2: 83.09 and loss3: 0.00\n",
      "Epoch [1567], train_loss: 759.92 with loss1: 676.79, loss2: 83.14 and loss3: 0.00\n",
      "Epoch [1568], train_loss: 764.35 with loss1: 681.33, loss2: 83.02 and loss3: 0.00\n",
      "Epoch [1569], train_loss: 762.22 with loss1: 679.07, loss2: 83.15 and loss3: 0.00\n",
      "Epoch [1570], train_loss: 763.19 with loss1: 680.18, loss2: 83.01 and loss3: 0.00\n",
      "Epoch [1571], train_loss: 767.59 with loss1: 684.36, loss2: 83.22 and loss3: 0.00\n",
      "Epoch [1572], train_loss: 768.05 with loss1: 685.06, loss2: 82.99 and loss3: 0.00\n",
      "Epoch [1573], train_loss: 772.36 with loss1: 689.30, loss2: 83.07 and loss3: 0.00\n",
      "Epoch [1574], train_loss: 774.73 with loss1: 691.82, loss2: 82.91 and loss3: 0.00\n",
      "Epoch [1575], train_loss: 775.07 with loss1: 692.03, loss2: 83.04 and loss3: 0.00\n",
      "Epoch [1576], train_loss: 772.69 with loss1: 689.74, loss2: 82.95 and loss3: 0.00\n",
      "Epoch [1577], train_loss: 776.59 with loss1: 693.48, loss2: 83.11 and loss3: 0.00\n",
      "Epoch [1578], train_loss: 777.95 with loss1: 695.05, loss2: 82.90 and loss3: 0.00\n",
      "Epoch [1579], train_loss: 781.93 with loss1: 699.10, loss2: 82.83 and loss3: 0.00\n",
      "Epoch [1580], train_loss: 780.29 with loss1: 697.34, loss2: 82.94 and loss3: 0.00\n",
      "Epoch [1581], train_loss: 786.25 with loss1: 703.35, loss2: 82.90 and loss3: 0.00\n",
      "Epoch [1582], train_loss: 782.74 with loss1: 699.80, loss2: 82.95 and loss3: 0.00\n",
      "Epoch [1583], train_loss: 787.15 with loss1: 704.20, loss2: 82.95 and loss3: 0.00\n",
      "Epoch [1584], train_loss: 785.76 with loss1: 702.79, loss2: 82.97 and loss3: 0.00\n",
      "Epoch [1585], train_loss: 791.87 with loss1: 709.02, loss2: 82.85 and loss3: 0.00\n",
      "Epoch [1586], train_loss: 784.87 with loss1: 702.07, loss2: 82.80 and loss3: 0.00\n",
      "Epoch [1587], train_loss: 790.27 with loss1: 707.44, loss2: 82.82 and loss3: 0.00\n",
      "Epoch [1588], train_loss: 786.03 with loss1: 703.34, loss2: 82.69 and loss3: 0.00\n",
      "Epoch [1589], train_loss: 790.85 with loss1: 708.04, loss2: 82.81 and loss3: 0.00\n",
      "Epoch [1590], train_loss: 788.12 with loss1: 705.38, loss2: 82.75 and loss3: 0.00\n",
      "Epoch [1591], train_loss: 787.67 with loss1: 704.98, loss2: 82.70 and loss3: 0.00\n",
      "Epoch [1592], train_loss: 783.04 with loss1: 700.32, loss2: 82.73 and loss3: 0.00\n",
      "Epoch [1593], train_loss: 791.85 with loss1: 709.18, loss2: 82.66 and loss3: 0.00\n",
      "Epoch [1594], train_loss: 783.75 with loss1: 701.08, loss2: 82.67 and loss3: 0.00\n",
      "Epoch [1595], train_loss: 783.88 with loss1: 701.13, loss2: 82.75 and loss3: 0.00\n",
      "Epoch [1596], train_loss: 781.00 with loss1: 698.34, loss2: 82.66 and loss3: 0.00\n",
      "Epoch [1597], train_loss: 781.56 with loss1: 698.88, loss2: 82.68 and loss3: 0.00\n",
      "Epoch [1598], train_loss: 779.82 with loss1: 697.15, loss2: 82.67 and loss3: 0.00\n",
      "Epoch [1599], train_loss: 779.08 with loss1: 696.48, loss2: 82.60 and loss3: 0.00\n",
      "Epoch [1600], train_loss: 776.54 with loss1: 693.91, loss2: 82.63 and loss3: 0.00\n",
      "Epoch [1601], train_loss: 776.78 with loss1: 694.17, loss2: 82.61 and loss3: 0.00\n",
      "Epoch [1602], train_loss: 774.72 with loss1: 692.19, loss2: 82.54 and loss3: 0.00\n",
      "Epoch [1603], train_loss: 776.41 with loss1: 693.91, loss2: 82.49 and loss3: 0.00\n",
      "Epoch [1604], train_loss: 774.70 with loss1: 692.15, loss2: 82.56 and loss3: 0.00\n",
      "Epoch [1605], train_loss: 771.53 with loss1: 688.98, loss2: 82.55 and loss3: 0.00\n",
      "Epoch [1606], train_loss: 769.04 with loss1: 686.42, loss2: 82.62 and loss3: 0.00\n",
      "Epoch [1607], train_loss: 770.02 with loss1: 687.50, loss2: 82.52 and loss3: 0.00\n",
      "Epoch [1608], train_loss: 764.85 with loss1: 682.30, loss2: 82.55 and loss3: 0.00\n",
      "Epoch [1609], train_loss: 764.73 with loss1: 682.19, loss2: 82.55 and loss3: 0.00\n",
      "Epoch [1610], train_loss: 761.19 with loss1: 678.61, loss2: 82.58 and loss3: 0.00\n",
      "Epoch [1611], train_loss: 763.25 with loss1: 680.76, loss2: 82.49 and loss3: 0.00\n",
      "Epoch [1612], train_loss: 757.90 with loss1: 675.46, loss2: 82.44 and loss3: 0.00\n",
      "Epoch [1613], train_loss: 758.17 with loss1: 675.74, loss2: 82.43 and loss3: 0.00\n",
      "Epoch [1614], train_loss: 756.75 with loss1: 674.46, loss2: 82.29 and loss3: 0.00\n",
      "Epoch [1615], train_loss: 755.62 with loss1: 673.35, loss2: 82.27 and loss3: 0.00\n",
      "Epoch [1616], train_loss: 753.03 with loss1: 670.70, loss2: 82.33 and loss3: 0.00\n",
      "Epoch [1617], train_loss: 753.64 with loss1: 671.08, loss2: 82.56 and loss3: 0.00\n",
      "Epoch [1618], train_loss: 756.84 with loss1: 674.46, loss2: 82.38 and loss3: 0.00\n",
      "Epoch [1619], train_loss: 752.53 with loss1: 670.19, loss2: 82.34 and loss3: 0.00\n",
      "Epoch [1620], train_loss: 750.02 with loss1: 667.60, loss2: 82.42 and loss3: 0.00\n",
      "Epoch [1621], train_loss: 749.30 with loss1: 666.93, loss2: 82.38 and loss3: 0.00\n",
      "Epoch [1622], train_loss: 748.10 with loss1: 665.75, loss2: 82.35 and loss3: 0.00\n",
      "Epoch [1623], train_loss: 749.09 with loss1: 666.71, loss2: 82.39 and loss3: 0.00\n",
      "Epoch [1624], train_loss: 747.95 with loss1: 665.59, loss2: 82.36 and loss3: 0.00\n",
      "Epoch [1625], train_loss: 747.98 with loss1: 665.69, loss2: 82.30 and loss3: 0.00\n",
      "Epoch [1626], train_loss: 747.68 with loss1: 665.40, loss2: 82.28 and loss3: 0.00\n",
      "Epoch [1627], train_loss: 745.86 with loss1: 663.60, loss2: 82.26 and loss3: 0.00\n",
      "Epoch [1628], train_loss: 743.34 with loss1: 661.22, loss2: 82.12 and loss3: 0.00\n",
      "Epoch [1629], train_loss: 748.26 with loss1: 666.16, loss2: 82.09 and loss3: 0.00\n",
      "Epoch [1630], train_loss: 741.55 with loss1: 659.42, loss2: 82.13 and loss3: 0.00\n",
      "Epoch [1631], train_loss: 742.34 with loss1: 660.26, loss2: 82.08 and loss3: 0.00\n",
      "Epoch [1632], train_loss: 741.28 with loss1: 659.07, loss2: 82.22 and loss3: 0.00\n",
      "Epoch [1633], train_loss: 741.85 with loss1: 659.70, loss2: 82.15 and loss3: 0.00\n",
      "Epoch [1634], train_loss: 737.81 with loss1: 655.75, loss2: 82.06 and loss3: 0.00\n",
      "Epoch [1635], train_loss: 739.66 with loss1: 657.47, loss2: 82.18 and loss3: 0.00\n",
      "Epoch [1636], train_loss: 739.96 with loss1: 657.80, loss2: 82.16 and loss3: 0.00\n",
      "Epoch [1637], train_loss: 737.01 with loss1: 655.03, loss2: 81.98 and loss3: 0.00\n",
      "Epoch [1638], train_loss: 738.81 with loss1: 656.67, loss2: 82.13 and loss3: 0.00\n",
      "Epoch [1639], train_loss: 737.62 with loss1: 655.65, loss2: 81.96 and loss3: 0.00\n",
      "Epoch [1640], train_loss: 734.78 with loss1: 652.70, loss2: 82.08 and loss3: 0.00\n",
      "Epoch [1641], train_loss: 735.19 with loss1: 653.30, loss2: 81.89 and loss3: 0.00\n",
      "Epoch [1642], train_loss: 736.59 with loss1: 654.67, loss2: 81.91 and loss3: 0.00\n",
      "Epoch [1643], train_loss: 737.67 with loss1: 655.76, loss2: 81.91 and loss3: 0.00\n",
      "Epoch [1644], train_loss: 736.69 with loss1: 654.72, loss2: 81.97 and loss3: 0.00\n",
      "Epoch [1645], train_loss: 738.54 with loss1: 656.50, loss2: 82.04 and loss3: 0.00\n",
      "Epoch [1646], train_loss: 735.33 with loss1: 653.37, loss2: 81.95 and loss3: 0.00\n",
      "Epoch [1647], train_loss: 735.32 with loss1: 653.45, loss2: 81.86 and loss3: 0.00\n",
      "Epoch [1648], train_loss: 734.91 with loss1: 653.10, loss2: 81.81 and loss3: 0.00\n",
      "Epoch [1649], train_loss: 736.25 with loss1: 654.34, loss2: 81.91 and loss3: 0.00\n",
      "Epoch [1650], train_loss: 740.69 with loss1: 658.82, loss2: 81.87 and loss3: 0.00\n",
      "Epoch [1651], train_loss: 732.85 with loss1: 651.10, loss2: 81.75 and loss3: 0.00\n",
      "Epoch [1652], train_loss: 734.31 with loss1: 652.44, loss2: 81.87 and loss3: 0.00\n",
      "Epoch [1653], train_loss: 735.80 with loss1: 653.98, loss2: 81.82 and loss3: 0.00\n",
      "Epoch [1654], train_loss: 735.14 with loss1: 653.27, loss2: 81.87 and loss3: 0.00\n",
      "Epoch [1655], train_loss: 736.12 with loss1: 654.40, loss2: 81.71 and loss3: 0.00\n",
      "Epoch [1656], train_loss: 735.74 with loss1: 653.91, loss2: 81.83 and loss3: 0.00\n",
      "Epoch [1657], train_loss: 737.06 with loss1: 655.30, loss2: 81.76 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1658], train_loss: 735.64 with loss1: 653.98, loss2: 81.66 and loss3: 0.00\n",
      "Epoch [1659], train_loss: 736.82 with loss1: 655.17, loss2: 81.65 and loss3: 0.00\n",
      "Epoch [1660], train_loss: 734.86 with loss1: 653.15, loss2: 81.71 and loss3: 0.00\n",
      "Epoch [1661], train_loss: 737.65 with loss1: 656.06, loss2: 81.58 and loss3: 0.00\n",
      "Epoch [1662], train_loss: 738.10 with loss1: 656.36, loss2: 81.73 and loss3: 0.00\n",
      "Epoch [1663], train_loss: 740.72 with loss1: 659.14, loss2: 81.58 and loss3: 0.00\n",
      "Epoch [1664], train_loss: 738.04 with loss1: 656.36, loss2: 81.68 and loss3: 0.00\n",
      "Epoch [1665], train_loss: 742.05 with loss1: 660.42, loss2: 81.64 and loss3: 0.00\n",
      "Epoch [1666], train_loss: 742.24 with loss1: 660.57, loss2: 81.68 and loss3: 0.00\n",
      "Epoch [1667], train_loss: 742.69 with loss1: 661.17, loss2: 81.53 and loss3: 0.00\n",
      "Epoch [1668], train_loss: 743.87 with loss1: 662.36, loss2: 81.51 and loss3: 0.00\n",
      "Epoch [1669], train_loss: 744.48 with loss1: 663.07, loss2: 81.42 and loss3: 0.00\n",
      "Epoch [1670], train_loss: 746.44 with loss1: 664.89, loss2: 81.55 and loss3: 0.00\n",
      "Epoch [1671], train_loss: 748.55 with loss1: 667.21, loss2: 81.34 and loss3: 0.00\n",
      "Epoch [1672], train_loss: 748.53 with loss1: 667.06, loss2: 81.47 and loss3: 0.00\n",
      "Epoch [1673], train_loss: 752.19 with loss1: 670.73, loss2: 81.45 and loss3: 0.00\n",
      "Epoch [1674], train_loss: 752.90 with loss1: 671.41, loss2: 81.49 and loss3: 0.00\n",
      "Epoch [1675], train_loss: 754.77 with loss1: 673.35, loss2: 81.42 and loss3: 0.00\n",
      "Epoch [1676], train_loss: 752.36 with loss1: 671.03, loss2: 81.32 and loss3: 0.00\n",
      "Epoch [1677], train_loss: 757.65 with loss1: 676.24, loss2: 81.42 and loss3: 0.00\n",
      "Epoch [1678], train_loss: 755.58 with loss1: 674.16, loss2: 81.41 and loss3: 0.00\n",
      "Epoch [1679], train_loss: 760.34 with loss1: 678.89, loss2: 81.45 and loss3: 0.00\n",
      "Epoch [1680], train_loss: 758.65 with loss1: 677.41, loss2: 81.25 and loss3: 0.00\n",
      "Epoch [1681], train_loss: 763.34 with loss1: 682.10, loss2: 81.24 and loss3: 0.00\n",
      "Epoch [1682], train_loss: 764.60 with loss1: 683.27, loss2: 81.33 and loss3: 0.00\n",
      "Epoch [1683], train_loss: 768.30 with loss1: 687.02, loss2: 81.28 and loss3: 0.00\n",
      "Epoch [1684], train_loss: 768.54 with loss1: 687.33, loss2: 81.21 and loss3: 0.00\n",
      "Epoch [1685], train_loss: 774.25 with loss1: 692.89, loss2: 81.35 and loss3: 0.00\n",
      "Epoch [1686], train_loss: 774.08 with loss1: 692.83, loss2: 81.25 and loss3: 0.00\n",
      "Epoch [1687], train_loss: 777.92 with loss1: 696.74, loss2: 81.18 and loss3: 0.00\n",
      "Epoch [1688], train_loss: 778.81 with loss1: 697.55, loss2: 81.26 and loss3: 0.00\n",
      "Epoch [1689], train_loss: 783.88 with loss1: 702.78, loss2: 81.10 and loss3: 0.00\n",
      "Epoch [1690], train_loss: 783.91 with loss1: 702.76, loss2: 81.15 and loss3: 0.00\n",
      "Epoch [1691], train_loss: 789.82 with loss1: 708.62, loss2: 81.20 and loss3: 0.00\n",
      "Epoch [1692], train_loss: 787.25 with loss1: 706.12, loss2: 81.13 and loss3: 0.00\n",
      "Epoch [1693], train_loss: 791.91 with loss1: 710.72, loss2: 81.19 and loss3: 0.00\n",
      "Epoch [1694], train_loss: 792.18 with loss1: 711.02, loss2: 81.16 and loss3: 0.00\n",
      "Epoch [1695], train_loss: 799.15 with loss1: 718.03, loss2: 81.12 and loss3: 0.00\n",
      "Epoch [1696], train_loss: 793.32 with loss1: 712.22, loss2: 81.09 and loss3: 0.00\n",
      "Epoch [1697], train_loss: 802.40 with loss1: 721.25, loss2: 81.15 and loss3: 0.00\n",
      "Epoch [1698], train_loss: 795.74 with loss1: 714.64, loss2: 81.10 and loss3: 0.00\n",
      "Epoch [1699], train_loss: 798.94 with loss1: 717.93, loss2: 81.01 and loss3: 0.00\n",
      "Epoch [1700], train_loss: 792.23 with loss1: 711.27, loss2: 80.96 and loss3: 0.00\n",
      "Epoch [1701], train_loss: 795.44 with loss1: 714.58, loss2: 80.86 and loss3: 0.00\n",
      "Epoch [1702], train_loss: 791.58 with loss1: 710.68, loss2: 80.91 and loss3: 0.00\n",
      "Epoch [1703], train_loss: 794.35 with loss1: 713.48, loss2: 80.87 and loss3: 0.00\n",
      "Epoch [1704], train_loss: 789.09 with loss1: 708.11, loss2: 80.98 and loss3: 0.00\n",
      "Epoch [1705], train_loss: 790.95 with loss1: 710.04, loss2: 80.91 and loss3: 0.00\n",
      "Epoch [1706], train_loss: 781.16 with loss1: 700.28, loss2: 80.88 and loss3: 0.00\n",
      "Epoch [1707], train_loss: 786.36 with loss1: 705.37, loss2: 80.99 and loss3: 0.00\n",
      "Epoch [1708], train_loss: 777.56 with loss1: 696.78, loss2: 80.78 and loss3: 0.00\n",
      "Epoch [1709], train_loss: 778.03 with loss1: 697.23, loss2: 80.80 and loss3: 0.00\n",
      "Epoch [1710], train_loss: 774.91 with loss1: 694.15, loss2: 80.75 and loss3: 0.00\n",
      "Epoch [1711], train_loss: 772.89 with loss1: 692.01, loss2: 80.88 and loss3: 0.00\n",
      "Epoch [1712], train_loss: 763.47 with loss1: 682.69, loss2: 80.78 and loss3: 0.00\n",
      "Epoch [1713], train_loss: 765.00 with loss1: 684.12, loss2: 80.88 and loss3: 0.00\n",
      "Epoch [1714], train_loss: 756.92 with loss1: 676.27, loss2: 80.65 and loss3: 0.00\n",
      "Epoch [1715], train_loss: 756.52 with loss1: 675.86, loss2: 80.66 and loss3: 0.00\n",
      "Epoch [1716], train_loss: 752.10 with loss1: 671.53, loss2: 80.57 and loss3: 0.00\n",
      "Epoch [1717], train_loss: 754.06 with loss1: 673.31, loss2: 80.74 and loss3: 0.00\n",
      "Epoch [1718], train_loss: 743.14 with loss1: 662.56, loss2: 80.58 and loss3: 0.00\n",
      "Epoch [1719], train_loss: 746.63 with loss1: 665.96, loss2: 80.67 and loss3: 0.00\n",
      "Epoch [1720], train_loss: 740.27 with loss1: 659.85, loss2: 80.42 and loss3: 0.00\n",
      "Epoch [1721], train_loss: 737.37 with loss1: 656.73, loss2: 80.64 and loss3: 0.00\n",
      "Epoch [1722], train_loss: 733.93 with loss1: 653.44, loss2: 80.48 and loss3: 0.00\n",
      "Epoch [1723], train_loss: 733.75 with loss1: 653.17, loss2: 80.58 and loss3: 0.00\n",
      "Epoch [1724], train_loss: 730.07 with loss1: 649.56, loss2: 80.51 and loss3: 0.00\n",
      "Epoch [1725], train_loss: 731.34 with loss1: 650.73, loss2: 80.61 and loss3: 0.00\n",
      "Epoch [1726], train_loss: 729.02 with loss1: 648.64, loss2: 80.37 and loss3: 0.00\n",
      "Epoch [1727], train_loss: 728.52 with loss1: 647.83, loss2: 80.69 and loss3: 0.00\n",
      "Epoch [1728], train_loss: 724.54 with loss1: 644.05, loss2: 80.49 and loss3: 0.00\n",
      "Epoch [1729], train_loss: 723.61 with loss1: 643.15, loss2: 80.45 and loss3: 0.00\n",
      "Epoch [1730], train_loss: 721.38 with loss1: 640.95, loss2: 80.43 and loss3: 0.00\n",
      "Epoch [1731], train_loss: 721.92 with loss1: 641.52, loss2: 80.40 and loss3: 0.00\n",
      "Epoch [1732], train_loss: 720.29 with loss1: 639.93, loss2: 80.36 and loss3: 0.00\n",
      "Epoch [1733], train_loss: 723.41 with loss1: 643.09, loss2: 80.32 and loss3: 0.00\n",
      "Epoch [1734], train_loss: 720.00 with loss1: 639.68, loss2: 80.32 and loss3: 0.00\n",
      "Epoch [1735], train_loss: 719.07 with loss1: 638.71, loss2: 80.36 and loss3: 0.00\n",
      "Epoch [1736], train_loss: 718.41 with loss1: 638.12, loss2: 80.29 and loss3: 0.00\n",
      "Epoch [1737], train_loss: 717.75 with loss1: 637.28, loss2: 80.47 and loss3: 0.00\n",
      "Epoch [1738], train_loss: 715.54 with loss1: 635.33, loss2: 80.21 and loss3: 0.00\n",
      "Epoch [1739], train_loss: 715.77 with loss1: 635.46, loss2: 80.30 and loss3: 0.00\n",
      "Epoch [1740], train_loss: 715.19 with loss1: 634.96, loss2: 80.23 and loss3: 0.00\n",
      "Epoch [1741], train_loss: 717.14 with loss1: 636.91, loss2: 80.23 and loss3: 0.00\n",
      "Epoch [1742], train_loss: 714.45 with loss1: 634.18, loss2: 80.27 and loss3: 0.00\n",
      "Epoch [1743], train_loss: 714.66 with loss1: 634.36, loss2: 80.30 and loss3: 0.00\n",
      "Epoch [1744], train_loss: 715.39 with loss1: 634.98, loss2: 80.42 and loss3: 0.00\n",
      "Epoch [1745], train_loss: 713.64 with loss1: 633.41, loss2: 80.23 and loss3: 0.00\n",
      "Epoch [1746], train_loss: 713.26 with loss1: 633.13, loss2: 80.13 and loss3: 0.00\n",
      "Epoch [1747], train_loss: 713.92 with loss1: 633.72, loss2: 80.20 and loss3: 0.00\n",
      "Epoch [1748], train_loss: 711.12 with loss1: 631.00, loss2: 80.11 and loss3: 0.00\n",
      "Epoch [1749], train_loss: 713.62 with loss1: 633.38, loss2: 80.24 and loss3: 0.00\n",
      "Epoch [1750], train_loss: 711.20 with loss1: 631.03, loss2: 80.17 and loss3: 0.00\n",
      "Epoch [1751], train_loss: 711.97 with loss1: 631.80, loss2: 80.17 and loss3: 0.00\n",
      "Epoch [1752], train_loss: 712.68 with loss1: 632.54, loss2: 80.14 and loss3: 0.00\n",
      "Epoch [1753], train_loss: 713.96 with loss1: 633.83, loss2: 80.13 and loss3: 0.00\n",
      "Epoch [1754], train_loss: 709.54 with loss1: 629.54, loss2: 80.00 and loss3: 0.00\n",
      "Epoch [1755], train_loss: 713.54 with loss1: 633.38, loss2: 80.16 and loss3: 0.00\n",
      "Epoch [1756], train_loss: 710.23 with loss1: 630.10, loss2: 80.13 and loss3: 0.00\n",
      "Epoch [1757], train_loss: 713.10 with loss1: 632.93, loss2: 80.18 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1758], train_loss: 711.58 with loss1: 631.60, loss2: 79.98 and loss3: 0.00\n",
      "Epoch [1759], train_loss: 715.35 with loss1: 635.37, loss2: 79.98 and loss3: 0.00\n",
      "Epoch [1760], train_loss: 715.09 with loss1: 635.19, loss2: 79.90 and loss3: 0.00\n",
      "Epoch [1761], train_loss: 714.79 with loss1: 634.66, loss2: 80.13 and loss3: 0.00\n",
      "Epoch [1762], train_loss: 713.59 with loss1: 633.64, loss2: 79.95 and loss3: 0.00\n",
      "Epoch [1763], train_loss: 715.42 with loss1: 635.36, loss2: 80.06 and loss3: 0.00\n",
      "Epoch [1764], train_loss: 714.14 with loss1: 634.17, loss2: 79.97 and loss3: 0.00\n",
      "Epoch [1765], train_loss: 719.54 with loss1: 639.64, loss2: 79.91 and loss3: 0.00\n",
      "Epoch [1766], train_loss: 716.18 with loss1: 636.22, loss2: 79.96 and loss3: 0.00\n",
      "Epoch [1767], train_loss: 716.87 with loss1: 636.90, loss2: 79.97 and loss3: 0.00\n",
      "Epoch [1768], train_loss: 715.03 with loss1: 635.13, loss2: 79.91 and loss3: 0.00\n",
      "Epoch [1769], train_loss: 721.09 with loss1: 641.04, loss2: 80.05 and loss3: 0.00\n",
      "Epoch [1770], train_loss: 718.80 with loss1: 638.85, loss2: 79.95 and loss3: 0.00\n",
      "Epoch [1771], train_loss: 721.79 with loss1: 641.85, loss2: 79.94 and loss3: 0.00\n",
      "Epoch [1772], train_loss: 719.84 with loss1: 639.91, loss2: 79.93 and loss3: 0.00\n",
      "Epoch [1773], train_loss: 718.21 with loss1: 638.25, loss2: 79.96 and loss3: 0.00\n",
      "Epoch [1774], train_loss: 717.66 with loss1: 637.72, loss2: 79.94 and loss3: 0.00\n",
      "Epoch [1775], train_loss: 716.88 with loss1: 637.04, loss2: 79.84 and loss3: 0.00\n",
      "Epoch [1776], train_loss: 717.17 with loss1: 637.36, loss2: 79.81 and loss3: 0.00\n",
      "Epoch [1777], train_loss: 718.84 with loss1: 638.92, loss2: 79.93 and loss3: 0.00\n",
      "Epoch [1778], train_loss: 716.08 with loss1: 636.34, loss2: 79.74 and loss3: 0.00\n",
      "Epoch [1779], train_loss: 719.61 with loss1: 639.80, loss2: 79.81 and loss3: 0.00\n",
      "Epoch [1780], train_loss: 716.41 with loss1: 636.62, loss2: 79.79 and loss3: 0.00\n",
      "Epoch [1781], train_loss: 717.36 with loss1: 637.60, loss2: 79.76 and loss3: 0.00\n",
      "Epoch [1782], train_loss: 716.41 with loss1: 636.79, loss2: 79.62 and loss3: 0.00\n",
      "Epoch [1783], train_loss: 715.34 with loss1: 635.52, loss2: 79.82 and loss3: 0.00\n",
      "Epoch [1784], train_loss: 712.32 with loss1: 632.67, loss2: 79.66 and loss3: 0.00\n",
      "Epoch [1785], train_loss: 713.95 with loss1: 634.08, loss2: 79.87 and loss3: 0.00\n",
      "Epoch [1786], train_loss: 710.19 with loss1: 630.57, loss2: 79.62 and loss3: 0.00\n",
      "Epoch [1787], train_loss: 712.80 with loss1: 633.12, loss2: 79.68 and loss3: 0.00\n",
      "Epoch [1788], train_loss: 708.89 with loss1: 629.25, loss2: 79.64 and loss3: 0.00\n",
      "Epoch [1789], train_loss: 710.01 with loss1: 630.43, loss2: 79.58 and loss3: 0.00\n",
      "Epoch [1790], train_loss: 707.66 with loss1: 628.01, loss2: 79.65 and loss3: 0.00\n",
      "Epoch [1791], train_loss: 708.72 with loss1: 629.12, loss2: 79.60 and loss3: 0.00\n",
      "Epoch [1792], train_loss: 709.13 with loss1: 629.55, loss2: 79.58 and loss3: 0.00\n",
      "Epoch [1793], train_loss: 711.28 with loss1: 631.54, loss2: 79.74 and loss3: 0.00\n",
      "Epoch [1794], train_loss: 710.05 with loss1: 630.51, loss2: 79.54 and loss3: 0.00\n",
      "Epoch [1795], train_loss: 709.98 with loss1: 630.44, loss2: 79.53 and loss3: 0.00\n",
      "Epoch [1796], train_loss: 707.62 with loss1: 628.10, loss2: 79.53 and loss3: 0.00\n",
      "Epoch [1797], train_loss: 710.63 with loss1: 631.05, loss2: 79.58 and loss3: 0.00\n",
      "Epoch [1798], train_loss: 708.72 with loss1: 629.17, loss2: 79.54 and loss3: 0.00\n",
      "Epoch [1799], train_loss: 709.33 with loss1: 629.84, loss2: 79.49 and loss3: 0.00\n",
      "Epoch [1800], train_loss: 704.92 with loss1: 625.41, loss2: 79.51 and loss3: 0.00\n",
      "Epoch [1801], train_loss: 708.10 with loss1: 628.66, loss2: 79.44 and loss3: 0.00\n",
      "Epoch [1802], train_loss: 703.78 with loss1: 624.30, loss2: 79.47 and loss3: 0.00\n",
      "Epoch [1803], train_loss: 709.01 with loss1: 629.49, loss2: 79.52 and loss3: 0.00\n",
      "Epoch [1804], train_loss: 704.92 with loss1: 625.52, loss2: 79.39 and loss3: 0.00\n",
      "Epoch [1805], train_loss: 710.55 with loss1: 631.14, loss2: 79.41 and loss3: 0.00\n",
      "Epoch [1806], train_loss: 704.55 with loss1: 625.19, loss2: 79.36 and loss3: 0.00\n",
      "Epoch [1807], train_loss: 704.77 with loss1: 625.46, loss2: 79.31 and loss3: 0.00\n",
      "Epoch [1808], train_loss: 703.97 with loss1: 624.52, loss2: 79.45 and loss3: 0.00\n",
      "Epoch [1809], train_loss: 705.57 with loss1: 626.18, loss2: 79.39 and loss3: 0.00\n",
      "Epoch [1810], train_loss: 705.25 with loss1: 625.92, loss2: 79.34 and loss3: 0.00\n",
      "Epoch [1811], train_loss: 707.47 with loss1: 628.13, loss2: 79.34 and loss3: 0.00\n",
      "Epoch [1812], train_loss: 705.19 with loss1: 625.88, loss2: 79.31 and loss3: 0.00\n",
      "Epoch [1813], train_loss: 703.70 with loss1: 624.37, loss2: 79.33 and loss3: 0.00\n",
      "Epoch [1814], train_loss: 703.99 with loss1: 624.71, loss2: 79.28 and loss3: 0.00\n",
      "Epoch [1815], train_loss: 705.96 with loss1: 626.76, loss2: 79.21 and loss3: 0.00\n",
      "Epoch [1816], train_loss: 701.20 with loss1: 621.90, loss2: 79.30 and loss3: 0.00\n",
      "Epoch [1817], train_loss: 702.39 with loss1: 623.07, loss2: 79.32 and loss3: 0.00\n",
      "Epoch [1818], train_loss: 700.34 with loss1: 621.09, loss2: 79.26 and loss3: 0.00\n",
      "Epoch [1819], train_loss: 702.55 with loss1: 623.46, loss2: 79.09 and loss3: 0.00\n",
      "Epoch [1820], train_loss: 701.05 with loss1: 621.82, loss2: 79.23 and loss3: 0.00\n",
      "Epoch [1821], train_loss: 701.73 with loss1: 622.60, loss2: 79.13 and loss3: 0.00\n",
      "Epoch [1822], train_loss: 702.14 with loss1: 623.02, loss2: 79.12 and loss3: 0.00\n",
      "Epoch [1823], train_loss: 700.79 with loss1: 621.57, loss2: 79.22 and loss3: 0.00\n",
      "Epoch [1824], train_loss: 700.18 with loss1: 621.01, loss2: 79.17 and loss3: 0.00\n",
      "Epoch [1825], train_loss: 698.45 with loss1: 619.28, loss2: 79.17 and loss3: 0.00\n",
      "Epoch [1826], train_loss: 696.19 with loss1: 617.10, loss2: 79.09 and loss3: 0.00\n",
      "Epoch [1827], train_loss: 700.46 with loss1: 621.37, loss2: 79.09 and loss3: 0.00\n",
      "Epoch [1828], train_loss: 698.67 with loss1: 619.65, loss2: 79.03 and loss3: 0.00\n",
      "Epoch [1829], train_loss: 698.34 with loss1: 619.31, loss2: 79.03 and loss3: 0.00\n",
      "Epoch [1830], train_loss: 697.18 with loss1: 618.12, loss2: 79.05 and loss3: 0.00\n",
      "Epoch [1831], train_loss: 699.21 with loss1: 620.23, loss2: 78.98 and loss3: 0.00\n",
      "Epoch [1832], train_loss: 703.81 with loss1: 624.85, loss2: 78.96 and loss3: 0.00\n",
      "Epoch [1833], train_loss: 700.83 with loss1: 622.05, loss2: 78.77 and loss3: 0.00\n",
      "Epoch [1834], train_loss: 701.58 with loss1: 622.60, loss2: 78.98 and loss3: 0.00\n",
      "Epoch [1835], train_loss: 701.34 with loss1: 622.55, loss2: 78.79 and loss3: 0.00\n",
      "Epoch [1836], train_loss: 701.15 with loss1: 622.14, loss2: 79.00 and loss3: 0.00\n",
      "Epoch [1837], train_loss: 701.21 with loss1: 622.36, loss2: 78.85 and loss3: 0.00\n",
      "Epoch [1838], train_loss: 703.17 with loss1: 624.27, loss2: 78.90 and loss3: 0.00\n",
      "Epoch [1839], train_loss: 702.19 with loss1: 623.24, loss2: 78.95 and loss3: 0.00\n",
      "Epoch [1840], train_loss: 700.44 with loss1: 621.54, loss2: 78.90 and loss3: 0.00\n",
      "Epoch [1841], train_loss: 701.69 with loss1: 622.73, loss2: 78.95 and loss3: 0.00\n",
      "Epoch [1842], train_loss: 702.83 with loss1: 624.01, loss2: 78.82 and loss3: 0.00\n",
      "Epoch [1843], train_loss: 703.38 with loss1: 624.43, loss2: 78.95 and loss3: 0.00\n",
      "Epoch [1844], train_loss: 702.41 with loss1: 623.53, loss2: 78.88 and loss3: 0.00\n",
      "Epoch [1845], train_loss: 706.84 with loss1: 628.09, loss2: 78.75 and loss3: 0.00\n",
      "Epoch [1846], train_loss: 707.25 with loss1: 628.48, loss2: 78.77 and loss3: 0.00\n",
      "Epoch [1847], train_loss: 709.48 with loss1: 630.65, loss2: 78.82 and loss3: 0.00\n",
      "Epoch [1848], train_loss: 706.37 with loss1: 627.69, loss2: 78.68 and loss3: 0.00\n",
      "Epoch [1849], train_loss: 708.72 with loss1: 629.92, loss2: 78.80 and loss3: 0.00\n",
      "Epoch [1850], train_loss: 705.11 with loss1: 626.42, loss2: 78.69 and loss3: 0.00\n",
      "Epoch [1851], train_loss: 705.86 with loss1: 627.05, loss2: 78.81 and loss3: 0.00\n",
      "Epoch [1852], train_loss: 703.74 with loss1: 625.04, loss2: 78.70 and loss3: 0.00\n",
      "Epoch [1853], train_loss: 706.31 with loss1: 627.64, loss2: 78.67 and loss3: 0.00\n",
      "Epoch [1854], train_loss: 706.24 with loss1: 627.55, loss2: 78.69 and loss3: 0.00\n",
      "Epoch [1855], train_loss: 708.61 with loss1: 629.89, loss2: 78.73 and loss3: 0.00\n",
      "Epoch [1856], train_loss: 709.12 with loss1: 630.36, loss2: 78.75 and loss3: 0.00\n",
      "Epoch [1857], train_loss: 712.13 with loss1: 633.49, loss2: 78.64 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1858], train_loss: 714.66 with loss1: 636.04, loss2: 78.62 and loss3: 0.00\n",
      "Epoch [1859], train_loss: 715.33 with loss1: 636.82, loss2: 78.51 and loss3: 0.00\n",
      "Epoch [1860], train_loss: 713.91 with loss1: 635.33, loss2: 78.58 and loss3: 0.00\n",
      "Epoch [1861], train_loss: 718.22 with loss1: 639.55, loss2: 78.66 and loss3: 0.00\n",
      "Epoch [1862], train_loss: 719.47 with loss1: 640.92, loss2: 78.54 and loss3: 0.00\n",
      "Epoch [1863], train_loss: 720.36 with loss1: 641.79, loss2: 78.57 and loss3: 0.00\n",
      "Epoch [1864], train_loss: 721.27 with loss1: 642.86, loss2: 78.41 and loss3: 0.00\n",
      "Epoch [1865], train_loss: 724.63 with loss1: 646.12, loss2: 78.51 and loss3: 0.00\n",
      "Epoch [1866], train_loss: 724.38 with loss1: 645.91, loss2: 78.47 and loss3: 0.00\n",
      "Epoch [1867], train_loss: 727.29 with loss1: 648.77, loss2: 78.51 and loss3: 0.00\n",
      "Epoch [1868], train_loss: 730.30 with loss1: 651.83, loss2: 78.47 and loss3: 0.00\n",
      "Epoch [1869], train_loss: 734.96 with loss1: 656.52, loss2: 78.43 and loss3: 0.00\n",
      "Epoch [1870], train_loss: 737.25 with loss1: 658.76, loss2: 78.50 and loss3: 0.00\n",
      "Epoch [1871], train_loss: 743.50 with loss1: 665.07, loss2: 78.43 and loss3: 0.00\n",
      "Epoch [1872], train_loss: 743.51 with loss1: 665.14, loss2: 78.36 and loss3: 0.00\n",
      "Epoch [1873], train_loss: 751.09 with loss1: 672.72, loss2: 78.38 and loss3: 0.00\n",
      "Epoch [1874], train_loss: 750.73 with loss1: 672.48, loss2: 78.25 and loss3: 0.00\n",
      "Epoch [1875], train_loss: 757.60 with loss1: 679.22, loss2: 78.38 and loss3: 0.00\n",
      "Epoch [1876], train_loss: 753.82 with loss1: 675.61, loss2: 78.21 and loss3: 0.00\n",
      "Epoch [1877], train_loss: 762.10 with loss1: 683.70, loss2: 78.40 and loss3: 0.00\n",
      "Epoch [1878], train_loss: 757.30 with loss1: 679.15, loss2: 78.16 and loss3: 0.00\n",
      "Epoch [1879], train_loss: 766.33 with loss1: 688.12, loss2: 78.21 and loss3: 0.00\n",
      "Epoch [1880], train_loss: 761.76 with loss1: 683.55, loss2: 78.21 and loss3: 0.00\n",
      "Epoch [1881], train_loss: 769.27 with loss1: 690.96, loss2: 78.32 and loss3: 0.00\n",
      "Epoch [1882], train_loss: 762.46 with loss1: 684.32, loss2: 78.14 and loss3: 0.00\n",
      "Epoch [1883], train_loss: 766.45 with loss1: 688.29, loss2: 78.16 and loss3: 0.00\n",
      "Epoch [1884], train_loss: 755.57 with loss1: 677.38, loss2: 78.20 and loss3: 0.00\n",
      "Epoch [1885], train_loss: 760.21 with loss1: 681.93, loss2: 78.27 and loss3: 0.00\n",
      "Epoch [1886], train_loss: 750.12 with loss1: 671.79, loss2: 78.33 and loss3: 0.00\n",
      "Epoch [1887], train_loss: 748.64 with loss1: 670.49, loss2: 78.15 and loss3: 0.00\n",
      "Epoch [1888], train_loss: 740.13 with loss1: 662.02, loss2: 78.10 and loss3: 0.00\n",
      "Epoch [1889], train_loss: 740.82 with loss1: 662.67, loss2: 78.15 and loss3: 0.00\n",
      "Epoch [1890], train_loss: 728.44 with loss1: 650.35, loss2: 78.09 and loss3: 0.00\n",
      "Epoch [1891], train_loss: 726.37 with loss1: 648.34, loss2: 78.03 and loss3: 0.00\n",
      "Epoch [1892], train_loss: 715.94 with loss1: 637.93, loss2: 78.01 and loss3: 0.00\n",
      "Epoch [1893], train_loss: 715.91 with loss1: 637.89, loss2: 78.01 and loss3: 0.00\n",
      "Epoch [1894], train_loss: 706.73 with loss1: 628.71, loss2: 78.02 and loss3: 0.00\n",
      "Epoch [1895], train_loss: 705.08 with loss1: 627.04, loss2: 78.04 and loss3: 0.00\n",
      "Epoch [1896], train_loss: 698.89 with loss1: 620.90, loss2: 78.00 and loss3: 0.00\n",
      "Epoch [1897], train_loss: 697.21 with loss1: 619.26, loss2: 77.96 and loss3: 0.00\n",
      "Epoch [1898], train_loss: 693.22 with loss1: 615.26, loss2: 77.96 and loss3: 0.00\n",
      "Epoch [1899], train_loss: 690.58 with loss1: 612.69, loss2: 77.89 and loss3: 0.00\n",
      "Epoch [1900], train_loss: 689.69 with loss1: 611.75, loss2: 77.93 and loss3: 0.00\n",
      "Epoch [1901], train_loss: 687.58 with loss1: 609.65, loss2: 77.93 and loss3: 0.00\n",
      "Epoch [1902], train_loss: 680.97 with loss1: 603.13, loss2: 77.84 and loss3: 0.00\n",
      "Epoch [1903], train_loss: 679.66 with loss1: 601.78, loss2: 77.89 and loss3: 0.00\n",
      "Epoch [1904], train_loss: 678.05 with loss1: 600.17, loss2: 77.88 and loss3: 0.00\n",
      "Epoch [1905], train_loss: 678.79 with loss1: 600.87, loss2: 77.92 and loss3: 0.00\n",
      "Epoch [1906], train_loss: 673.68 with loss1: 595.99, loss2: 77.69 and loss3: 0.00\n",
      "Epoch [1907], train_loss: 673.12 with loss1: 595.22, loss2: 77.90 and loss3: 0.00\n",
      "Epoch [1908], train_loss: 673.85 with loss1: 596.08, loss2: 77.77 and loss3: 0.00\n",
      "Epoch [1909], train_loss: 672.58 with loss1: 594.79, loss2: 77.79 and loss3: 0.00\n",
      "Epoch [1910], train_loss: 668.98 with loss1: 591.16, loss2: 77.82 and loss3: 0.00\n",
      "Epoch [1911], train_loss: 674.05 with loss1: 596.26, loss2: 77.78 and loss3: 0.00\n",
      "Epoch [1912], train_loss: 667.19 with loss1: 589.42, loss2: 77.77 and loss3: 0.00\n",
      "Epoch [1913], train_loss: 667.20 with loss1: 589.53, loss2: 77.67 and loss3: 0.00\n",
      "Epoch [1914], train_loss: 666.79 with loss1: 588.96, loss2: 77.82 and loss3: 0.00\n",
      "Epoch [1915], train_loss: 667.10 with loss1: 589.39, loss2: 77.71 and loss3: 0.00\n",
      "Epoch [1916], train_loss: 666.31 with loss1: 588.68, loss2: 77.63 and loss3: 0.00\n",
      "Epoch [1917], train_loss: 669.24 with loss1: 591.58, loss2: 77.66 and loss3: 0.00\n",
      "Epoch [1918], train_loss: 666.72 with loss1: 589.09, loss2: 77.64 and loss3: 0.00\n",
      "Epoch [1919], train_loss: 665.87 with loss1: 588.23, loss2: 77.64 and loss3: 0.00\n",
      "Epoch [1920], train_loss: 664.59 with loss1: 586.91, loss2: 77.68 and loss3: 0.00\n",
      "Epoch [1921], train_loss: 665.68 with loss1: 587.96, loss2: 77.72 and loss3: 0.00\n",
      "Epoch [1922], train_loss: 665.02 with loss1: 587.44, loss2: 77.58 and loss3: 0.00\n",
      "Epoch [1923], train_loss: 665.38 with loss1: 587.77, loss2: 77.62 and loss3: 0.00\n",
      "Epoch [1924], train_loss: 671.59 with loss1: 593.99, loss2: 77.60 and loss3: 0.00\n",
      "Epoch [1925], train_loss: 665.32 with loss1: 587.83, loss2: 77.48 and loss3: 0.00\n",
      "Epoch [1926], train_loss: 666.51 with loss1: 589.05, loss2: 77.46 and loss3: 0.00\n",
      "Epoch [1927], train_loss: 666.96 with loss1: 589.45, loss2: 77.51 and loss3: 0.00\n",
      "Epoch [1928], train_loss: 665.63 with loss1: 588.14, loss2: 77.48 and loss3: 0.00\n",
      "Epoch [1929], train_loss: 666.52 with loss1: 588.96, loss2: 77.56 and loss3: 0.00\n",
      "Epoch [1930], train_loss: 670.73 with loss1: 593.23, loss2: 77.51 and loss3: 0.00\n",
      "Epoch [1931], train_loss: 670.60 with loss1: 593.15, loss2: 77.45 and loss3: 0.00\n",
      "Epoch [1932], train_loss: 667.45 with loss1: 589.95, loss2: 77.51 and loss3: 0.00\n",
      "Epoch [1933], train_loss: 671.85 with loss1: 594.34, loss2: 77.51 and loss3: 0.00\n",
      "Epoch [1934], train_loss: 669.46 with loss1: 592.04, loss2: 77.42 and loss3: 0.00\n",
      "Epoch [1935], train_loss: 672.23 with loss1: 594.73, loss2: 77.49 and loss3: 0.00\n",
      "Epoch [1936], train_loss: 673.15 with loss1: 595.79, loss2: 77.36 and loss3: 0.00\n",
      "Epoch [1937], train_loss: 674.13 with loss1: 596.75, loss2: 77.37 and loss3: 0.00\n",
      "Epoch [1938], train_loss: 675.95 with loss1: 598.48, loss2: 77.47 and loss3: 0.00\n",
      "Epoch [1939], train_loss: 674.87 with loss1: 597.47, loss2: 77.40 and loss3: 0.00\n",
      "Epoch [1940], train_loss: 674.12 with loss1: 596.76, loss2: 77.36 and loss3: 0.00\n",
      "Epoch [1941], train_loss: 677.30 with loss1: 599.92, loss2: 77.39 and loss3: 0.00\n",
      "Epoch [1942], train_loss: 675.48 with loss1: 598.23, loss2: 77.24 and loss3: 0.00\n",
      "Epoch [1943], train_loss: 679.77 with loss1: 602.37, loss2: 77.40 and loss3: 0.00\n",
      "Epoch [1944], train_loss: 676.58 with loss1: 599.27, loss2: 77.31 and loss3: 0.00\n",
      "Epoch [1945], train_loss: 680.52 with loss1: 603.11, loss2: 77.41 and loss3: 0.00\n",
      "Epoch [1946], train_loss: 680.99 with loss1: 603.67, loss2: 77.32 and loss3: 0.00\n",
      "Epoch [1947], train_loss: 684.03 with loss1: 606.75, loss2: 77.28 and loss3: 0.00\n",
      "Epoch [1948], train_loss: 685.36 with loss1: 608.14, loss2: 77.22 and loss3: 0.00\n",
      "Epoch [1949], train_loss: 688.75 with loss1: 611.49, loss2: 77.26 and loss3: 0.00\n",
      "Epoch [1950], train_loss: 687.50 with loss1: 610.28, loss2: 77.23 and loss3: 0.00\n",
      "Epoch [1951], train_loss: 690.13 with loss1: 612.97, loss2: 77.16 and loss3: 0.00\n",
      "Epoch [1952], train_loss: 690.68 with loss1: 613.38, loss2: 77.30 and loss3: 0.00\n",
      "Epoch [1953], train_loss: 695.13 with loss1: 617.86, loss2: 77.26 and loss3: 0.00\n",
      "Epoch [1954], train_loss: 694.43 with loss1: 617.07, loss2: 77.36 and loss3: 0.00\n",
      "Epoch [1955], train_loss: 697.39 with loss1: 620.21, loss2: 77.18 and loss3: 0.00\n",
      "Epoch [1956], train_loss: 696.95 with loss1: 619.80, loss2: 77.15 and loss3: 0.00\n",
      "Epoch [1957], train_loss: 697.25 with loss1: 620.19, loss2: 77.06 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1958], train_loss: 696.83 with loss1: 619.72, loss2: 77.11 and loss3: 0.00\n",
      "Epoch [1959], train_loss: 700.44 with loss1: 623.31, loss2: 77.14 and loss3: 0.00\n",
      "Epoch [1960], train_loss: 701.83 with loss1: 624.56, loss2: 77.27 and loss3: 0.00\n",
      "Epoch [1961], train_loss: 703.14 with loss1: 626.06, loss2: 77.08 and loss3: 0.00\n",
      "Epoch [1962], train_loss: 701.85 with loss1: 624.73, loss2: 77.12 and loss3: 0.00\n",
      "Epoch [1963], train_loss: 705.38 with loss1: 628.28, loss2: 77.10 and loss3: 0.00\n",
      "Epoch [1964], train_loss: 702.65 with loss1: 625.43, loss2: 77.22 and loss3: 0.00\n",
      "Epoch [1965], train_loss: 704.74 with loss1: 627.68, loss2: 77.06 and loss3: 0.00\n",
      "Epoch [1966], train_loss: 704.00 with loss1: 626.89, loss2: 77.11 and loss3: 0.00\n",
      "Epoch [1967], train_loss: 705.41 with loss1: 628.28, loss2: 77.13 and loss3: 0.00\n",
      "Epoch [1968], train_loss: 701.32 with loss1: 624.13, loss2: 77.18 and loss3: 0.00\n",
      "Epoch [1969], train_loss: 702.41 with loss1: 625.44, loss2: 76.96 and loss3: 0.00\n",
      "Epoch [1970], train_loss: 701.66 with loss1: 624.64, loss2: 77.02 and loss3: 0.00\n",
      "Epoch [1971], train_loss: 700.89 with loss1: 623.94, loss2: 76.96 and loss3: 0.00\n",
      "Epoch [1972], train_loss: 697.94 with loss1: 620.85, loss2: 77.09 and loss3: 0.00\n",
      "Epoch [1973], train_loss: 699.44 with loss1: 622.51, loss2: 76.93 and loss3: 0.00\n",
      "Epoch [1974], train_loss: 695.52 with loss1: 618.48, loss2: 77.04 and loss3: 0.00\n",
      "Epoch [1975], train_loss: 696.01 with loss1: 619.06, loss2: 76.94 and loss3: 0.00\n",
      "Epoch [1976], train_loss: 690.94 with loss1: 613.85, loss2: 77.09 and loss3: 0.00\n",
      "Epoch [1977], train_loss: 689.64 with loss1: 612.76, loss2: 76.87 and loss3: 0.00\n",
      "Epoch [1978], train_loss: 687.04 with loss1: 610.10, loss2: 76.94 and loss3: 0.00\n",
      "Epoch [1979], train_loss: 687.18 with loss1: 610.33, loss2: 76.85 and loss3: 0.00\n",
      "Epoch [1980], train_loss: 682.53 with loss1: 605.57, loss2: 76.97 and loss3: 0.00\n",
      "Epoch [1981], train_loss: 681.31 with loss1: 604.50, loss2: 76.81 and loss3: 0.00\n",
      "Epoch [1982], train_loss: 678.24 with loss1: 601.21, loss2: 77.03 and loss3: 0.00\n",
      "Epoch [1983], train_loss: 677.35 with loss1: 600.59, loss2: 76.76 and loss3: 0.00\n",
      "Epoch [1984], train_loss: 674.34 with loss1: 597.41, loss2: 76.93 and loss3: 0.00\n",
      "Epoch [1985], train_loss: 677.58 with loss1: 600.78, loss2: 76.80 and loss3: 0.00\n",
      "Epoch [1986], train_loss: 672.14 with loss1: 595.26, loss2: 76.88 and loss3: 0.00\n",
      "Epoch [1987], train_loss: 670.28 with loss1: 593.62, loss2: 76.66 and loss3: 0.00\n",
      "Epoch [1988], train_loss: 670.78 with loss1: 594.02, loss2: 76.75 and loss3: 0.00\n",
      "Epoch [1989], train_loss: 669.87 with loss1: 593.19, loss2: 76.67 and loss3: 0.00\n",
      "Epoch [1990], train_loss: 664.60 with loss1: 587.73, loss2: 76.86 and loss3: 0.00\n",
      "Epoch [1991], train_loss: 665.81 with loss1: 589.09, loss2: 76.72 and loss3: 0.00\n",
      "Epoch [1992], train_loss: 664.49 with loss1: 587.76, loss2: 76.73 and loss3: 0.00\n",
      "Epoch [1993], train_loss: 663.33 with loss1: 586.74, loss2: 76.58 and loss3: 0.00\n",
      "Epoch [1994], train_loss: 661.20 with loss1: 584.39, loss2: 76.81 and loss3: 0.00\n",
      "Epoch [1995], train_loss: 664.88 with loss1: 588.25, loss2: 76.63 and loss3: 0.00\n",
      "Epoch [1996], train_loss: 662.84 with loss1: 586.15, loss2: 76.69 and loss3: 0.00\n",
      "Epoch [1997], train_loss: 658.45 with loss1: 581.86, loss2: 76.58 and loss3: 0.00\n",
      "Epoch [1998], train_loss: 658.05 with loss1: 581.37, loss2: 76.69 and loss3: 0.00\n",
      "Epoch [1999], train_loss: 657.44 with loss1: 580.89, loss2: 76.54 and loss3: 0.00\n",
      "Epoch [2000], train_loss: 659.01 with loss1: 582.29, loss2: 76.72 and loss3: 0.00\n",
      "Epoch [2001], train_loss: 657.72 with loss1: 581.12, loss2: 76.60 and loss3: 0.00\n",
      "Epoch [2002], train_loss: 656.02 with loss1: 579.43, loss2: 76.59 and loss3: 0.00\n",
      "Epoch [2003], train_loss: 652.37 with loss1: 575.87, loss2: 76.50 and loss3: 0.00\n",
      "Epoch [2004], train_loss: 654.08 with loss1: 577.55, loss2: 76.53 and loss3: 0.00\n",
      "Epoch [2005], train_loss: 655.17 with loss1: 578.73, loss2: 76.44 and loss3: 0.00\n",
      "Epoch [2006], train_loss: 650.51 with loss1: 574.01, loss2: 76.50 and loss3: 0.00\n",
      "Epoch [2007], train_loss: 652.02 with loss1: 575.55, loss2: 76.47 and loss3: 0.00\n",
      "Epoch [2008], train_loss: 652.08 with loss1: 575.60, loss2: 76.48 and loss3: 0.00\n",
      "Epoch [2009], train_loss: 649.19 with loss1: 572.71, loss2: 76.49 and loss3: 0.00\n",
      "Epoch [2010], train_loss: 648.21 with loss1: 571.78, loss2: 76.43 and loss3: 0.00\n",
      "Epoch [2011], train_loss: 646.71 with loss1: 570.30, loss2: 76.41 and loss3: 0.00\n",
      "Epoch [2012], train_loss: 647.22 with loss1: 570.74, loss2: 76.48 and loss3: 0.00\n",
      "Epoch [2013], train_loss: 647.05 with loss1: 570.78, loss2: 76.27 and loss3: 0.00\n",
      "Epoch [2014], train_loss: 646.80 with loss1: 570.37, loss2: 76.42 and loss3: 0.00\n",
      "Epoch [2015], train_loss: 645.31 with loss1: 568.93, loss2: 76.38 and loss3: 0.00\n",
      "Epoch [2016], train_loss: 647.88 with loss1: 571.52, loss2: 76.35 and loss3: 0.00\n",
      "Epoch [2017], train_loss: 643.79 with loss1: 567.60, loss2: 76.20 and loss3: 0.00\n",
      "Epoch [2018], train_loss: 644.92 with loss1: 568.35, loss2: 76.57 and loss3: 0.00\n",
      "Epoch [2019], train_loss: 643.79 with loss1: 567.46, loss2: 76.33 and loss3: 0.00\n",
      "Epoch [2020], train_loss: 644.15 with loss1: 567.73, loss2: 76.42 and loss3: 0.00\n",
      "Epoch [2021], train_loss: 644.57 with loss1: 568.33, loss2: 76.24 and loss3: 0.00\n",
      "Epoch [2022], train_loss: 643.14 with loss1: 566.91, loss2: 76.23 and loss3: 0.00\n",
      "Epoch [2023], train_loss: 642.11 with loss1: 565.97, loss2: 76.13 and loss3: 0.00\n",
      "Epoch [2024], train_loss: 642.78 with loss1: 566.45, loss2: 76.33 and loss3: 0.00\n",
      "Epoch [2025], train_loss: 643.60 with loss1: 567.43, loss2: 76.17 and loss3: 0.00\n",
      "Epoch [2026], train_loss: 644.49 with loss1: 568.25, loss2: 76.24 and loss3: 0.00\n",
      "Epoch [2027], train_loss: 641.26 with loss1: 565.20, loss2: 76.05 and loss3: 0.00\n",
      "Epoch [2028], train_loss: 641.54 with loss1: 565.33, loss2: 76.22 and loss3: 0.00\n",
      "Epoch [2029], train_loss: 642.21 with loss1: 566.06, loss2: 76.16 and loss3: 0.00\n",
      "Epoch [2030], train_loss: 643.14 with loss1: 566.90, loss2: 76.24 and loss3: 0.00\n",
      "Epoch [2031], train_loss: 643.19 with loss1: 567.10, loss2: 76.08 and loss3: 0.00\n",
      "Epoch [2032], train_loss: 639.45 with loss1: 563.32, loss2: 76.13 and loss3: 0.00\n",
      "Epoch [2033], train_loss: 643.26 with loss1: 567.19, loss2: 76.07 and loss3: 0.00\n",
      "Epoch [2034], train_loss: 641.06 with loss1: 564.95, loss2: 76.12 and loss3: 0.00\n",
      "Epoch [2035], train_loss: 640.86 with loss1: 564.80, loss2: 76.07 and loss3: 0.00\n",
      "Epoch [2036], train_loss: 641.21 with loss1: 565.29, loss2: 75.92 and loss3: 0.00\n",
      "Epoch [2037], train_loss: 637.97 with loss1: 562.04, loss2: 75.93 and loss3: 0.00\n",
      "Epoch [2038], train_loss: 638.36 with loss1: 562.33, loss2: 76.03 and loss3: 0.00\n",
      "Epoch [2039], train_loss: 638.41 with loss1: 562.48, loss2: 75.93 and loss3: 0.00\n",
      "Epoch [2040], train_loss: 638.49 with loss1: 562.46, loss2: 76.03 and loss3: 0.00\n",
      "Epoch [2041], train_loss: 638.54 with loss1: 562.56, loss2: 75.98 and loss3: 0.00\n",
      "Epoch [2042], train_loss: 640.62 with loss1: 564.61, loss2: 76.01 and loss3: 0.00\n",
      "Epoch [2043], train_loss: 638.82 with loss1: 562.92, loss2: 75.90 and loss3: 0.00\n",
      "Epoch [2044], train_loss: 638.33 with loss1: 562.28, loss2: 76.05 and loss3: 0.00\n",
      "Epoch [2045], train_loss: 637.50 with loss1: 561.63, loss2: 75.87 and loss3: 0.00\n",
      "Epoch [2046], train_loss: 638.16 with loss1: 562.15, loss2: 76.02 and loss3: 0.00\n",
      "Epoch [2047], train_loss: 637.59 with loss1: 561.75, loss2: 75.85 and loss3: 0.00\n",
      "Epoch [2048], train_loss: 640.03 with loss1: 564.16, loss2: 75.86 and loss3: 0.00\n",
      "Epoch [2049], train_loss: 642.95 with loss1: 567.10, loss2: 75.85 and loss3: 0.00\n",
      "Epoch [2050], train_loss: 640.93 with loss1: 565.02, loss2: 75.91 and loss3: 0.00\n",
      "Epoch [2051], train_loss: 644.35 with loss1: 568.52, loss2: 75.83 and loss3: 0.00\n",
      "Epoch [2052], train_loss: 643.39 with loss1: 567.51, loss2: 75.88 and loss3: 0.00\n",
      "Epoch [2053], train_loss: 643.85 with loss1: 568.08, loss2: 75.77 and loss3: 0.00\n",
      "Epoch [2054], train_loss: 646.64 with loss1: 570.76, loss2: 75.88 and loss3: 0.00\n",
      "Epoch [2055], train_loss: 644.86 with loss1: 568.98, loss2: 75.89 and loss3: 0.00\n",
      "Epoch [2056], train_loss: 650.83 with loss1: 574.93, loss2: 75.90 and loss3: 0.00\n",
      "Epoch [2057], train_loss: 646.65 with loss1: 570.84, loss2: 75.80 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2058], train_loss: 649.21 with loss1: 573.40, loss2: 75.81 and loss3: 0.00\n",
      "Epoch [2059], train_loss: 649.94 with loss1: 574.09, loss2: 75.85 and loss3: 0.00\n",
      "Epoch [2060], train_loss: 653.31 with loss1: 577.45, loss2: 75.87 and loss3: 0.00\n",
      "Epoch [2061], train_loss: 656.64 with loss1: 580.92, loss2: 75.72 and loss3: 0.00\n",
      "Epoch [2062], train_loss: 658.83 with loss1: 583.01, loss2: 75.82 and loss3: 0.00\n",
      "Epoch [2063], train_loss: 659.69 with loss1: 583.92, loss2: 75.78 and loss3: 0.00\n",
      "Epoch [2064], train_loss: 665.52 with loss1: 589.64, loss2: 75.88 and loss3: 0.00\n",
      "Epoch [2065], train_loss: 668.54 with loss1: 592.87, loss2: 75.68 and loss3: 0.00\n",
      "Epoch [2066], train_loss: 672.05 with loss1: 596.23, loss2: 75.82 and loss3: 0.00\n",
      "Epoch [2067], train_loss: 675.16 with loss1: 599.54, loss2: 75.62 and loss3: 0.00\n",
      "Epoch [2068], train_loss: 681.73 with loss1: 605.94, loss2: 75.79 and loss3: 0.00\n",
      "Epoch [2069], train_loss: 681.26 with loss1: 605.69, loss2: 75.58 and loss3: 0.00\n",
      "Epoch [2070], train_loss: 694.95 with loss1: 619.13, loss2: 75.82 and loss3: 0.00\n",
      "Epoch [2071], train_loss: 695.81 with loss1: 620.15, loss2: 75.66 and loss3: 0.00\n",
      "Epoch [2072], train_loss: 704.10 with loss1: 628.29, loss2: 75.81 and loss3: 0.00\n",
      "Epoch [2073], train_loss: 704.68 with loss1: 628.99, loss2: 75.68 and loss3: 0.00\n",
      "Epoch [2074], train_loss: 716.59 with loss1: 640.88, loss2: 75.71 and loss3: 0.00\n",
      "Epoch [2075], train_loss: 712.87 with loss1: 637.30, loss2: 75.56 and loss3: 0.00\n",
      "Epoch [2076], train_loss: 725.82 with loss1: 650.11, loss2: 75.70 and loss3: 0.00\n",
      "Epoch [2077], train_loss: 722.48 with loss1: 646.87, loss2: 75.61 and loss3: 0.00\n",
      "Epoch [2078], train_loss: 730.43 with loss1: 654.76, loss2: 75.67 and loss3: 0.00\n",
      "Epoch [2079], train_loss: 726.73 with loss1: 651.24, loss2: 75.49 and loss3: 0.00\n",
      "Epoch [2080], train_loss: 732.30 with loss1: 656.64, loss2: 75.66 and loss3: 0.00\n",
      "Epoch [2081], train_loss: 725.70 with loss1: 650.15, loss2: 75.54 and loss3: 0.00\n",
      "Epoch [2082], train_loss: 730.82 with loss1: 655.18, loss2: 75.64 and loss3: 0.00\n",
      "Epoch [2083], train_loss: 725.60 with loss1: 650.10, loss2: 75.50 and loss3: 0.00\n",
      "Epoch [2084], train_loss: 730.37 with loss1: 654.77, loss2: 75.60 and loss3: 0.00\n",
      "Epoch [2085], train_loss: 720.64 with loss1: 645.19, loss2: 75.45 and loss3: 0.00\n",
      "Epoch [2086], train_loss: 726.00 with loss1: 650.56, loss2: 75.44 and loss3: 0.00\n",
      "Epoch [2087], train_loss: 720.19 with loss1: 644.80, loss2: 75.39 and loss3: 0.00\n",
      "Epoch [2088], train_loss: 721.60 with loss1: 646.20, loss2: 75.40 and loss3: 0.00\n",
      "Epoch [2089], train_loss: 714.59 with loss1: 639.11, loss2: 75.48 and loss3: 0.00\n",
      "Epoch [2090], train_loss: 716.33 with loss1: 640.95, loss2: 75.38 and loss3: 0.00\n",
      "Epoch [2091], train_loss: 709.50 with loss1: 634.14, loss2: 75.36 and loss3: 0.00\n",
      "Epoch [2092], train_loss: 711.09 with loss1: 635.71, loss2: 75.38 and loss3: 0.00\n",
      "Epoch [2093], train_loss: 702.19 with loss1: 626.90, loss2: 75.29 and loss3: 0.00\n",
      "Epoch [2094], train_loss: 704.14 with loss1: 628.92, loss2: 75.22 and loss3: 0.00\n",
      "Epoch [2095], train_loss: 696.77 with loss1: 621.54, loss2: 75.23 and loss3: 0.00\n",
      "Epoch [2096], train_loss: 696.44 with loss1: 621.25, loss2: 75.19 and loss3: 0.00\n",
      "Epoch [2097], train_loss: 688.53 with loss1: 613.32, loss2: 75.21 and loss3: 0.00\n",
      "Epoch [2098], train_loss: 688.15 with loss1: 612.95, loss2: 75.20 and loss3: 0.00\n",
      "Epoch [2099], train_loss: 685.45 with loss1: 610.25, loss2: 75.20 and loss3: 0.00\n",
      "Epoch [2100], train_loss: 682.33 with loss1: 607.15, loss2: 75.18 and loss3: 0.00\n",
      "Epoch [2101], train_loss: 677.59 with loss1: 602.53, loss2: 75.05 and loss3: 0.00\n",
      "Epoch [2102], train_loss: 676.45 with loss1: 601.32, loss2: 75.14 and loss3: 0.00\n",
      "Epoch [2103], train_loss: 673.02 with loss1: 597.94, loss2: 75.08 and loss3: 0.00\n",
      "Epoch [2104], train_loss: 674.42 with loss1: 599.42, loss2: 75.00 and loss3: 0.00\n",
      "Epoch [2105], train_loss: 669.66 with loss1: 594.63, loss2: 75.03 and loss3: 0.00\n",
      "Epoch [2106], train_loss: 670.58 with loss1: 595.49, loss2: 75.09 and loss3: 0.00\n",
      "Epoch [2107], train_loss: 668.20 with loss1: 593.13, loss2: 75.07 and loss3: 0.00\n",
      "Epoch [2108], train_loss: 664.88 with loss1: 589.95, loss2: 74.93 and loss3: 0.00\n",
      "Epoch [2109], train_loss: 662.17 with loss1: 587.31, loss2: 74.85 and loss3: 0.00\n",
      "Epoch [2110], train_loss: 664.07 with loss1: 589.17, loss2: 74.90 and loss3: 0.00\n",
      "Epoch [2111], train_loss: 658.05 with loss1: 583.09, loss2: 74.96 and loss3: 0.00\n",
      "Epoch [2112], train_loss: 659.53 with loss1: 584.64, loss2: 74.89 and loss3: 0.00\n",
      "Epoch [2113], train_loss: 656.06 with loss1: 581.06, loss2: 75.00 and loss3: 0.00\n",
      "Epoch [2114], train_loss: 655.80 with loss1: 580.95, loss2: 74.85 and loss3: 0.00\n",
      "Epoch [2115], train_loss: 652.19 with loss1: 577.36, loss2: 74.83 and loss3: 0.00\n",
      "Epoch [2116], train_loss: 652.04 with loss1: 577.13, loss2: 74.91 and loss3: 0.00\n",
      "Epoch [2117], train_loss: 648.87 with loss1: 573.98, loss2: 74.89 and loss3: 0.00\n",
      "Epoch [2118], train_loss: 650.92 with loss1: 576.11, loss2: 74.82 and loss3: 0.00\n",
      "Epoch [2119], train_loss: 648.76 with loss1: 574.03, loss2: 74.74 and loss3: 0.00\n",
      "Epoch [2120], train_loss: 646.75 with loss1: 571.91, loss2: 74.85 and loss3: 0.00\n",
      "Epoch [2121], train_loss: 644.20 with loss1: 569.45, loss2: 74.75 and loss3: 0.00\n",
      "Epoch [2122], train_loss: 644.20 with loss1: 569.41, loss2: 74.79 and loss3: 0.00\n",
      "Epoch [2123], train_loss: 644.18 with loss1: 569.33, loss2: 74.85 and loss3: 0.00\n",
      "Epoch [2124], train_loss: 643.92 with loss1: 569.10, loss2: 74.82 and loss3: 0.00\n",
      "Epoch [2125], train_loss: 644.34 with loss1: 569.68, loss2: 74.66 and loss3: 0.00\n",
      "Epoch [2126], train_loss: 642.34 with loss1: 567.74, loss2: 74.60 and loss3: 0.00\n",
      "Epoch [2127], train_loss: 641.85 with loss1: 567.16, loss2: 74.70 and loss3: 0.00\n",
      "Epoch [2128], train_loss: 641.98 with loss1: 567.22, loss2: 74.76 and loss3: 0.00\n",
      "Epoch [2129], train_loss: 638.63 with loss1: 564.08, loss2: 74.55 and loss3: 0.00\n",
      "Epoch [2130], train_loss: 642.27 with loss1: 567.54, loss2: 74.73 and loss3: 0.00\n",
      "Epoch [2131], train_loss: 640.98 with loss1: 566.26, loss2: 74.73 and loss3: 0.00\n",
      "Epoch [2132], train_loss: 642.02 with loss1: 567.44, loss2: 74.58 and loss3: 0.00\n",
      "Epoch [2133], train_loss: 639.76 with loss1: 565.09, loss2: 74.67 and loss3: 0.00\n",
      "Epoch [2134], train_loss: 641.65 with loss1: 567.01, loss2: 74.63 and loss3: 0.00\n",
      "Epoch [2135], train_loss: 639.79 with loss1: 565.14, loss2: 74.64 and loss3: 0.00\n",
      "Epoch [2136], train_loss: 640.89 with loss1: 566.24, loss2: 74.65 and loss3: 0.00\n",
      "Epoch [2137], train_loss: 640.05 with loss1: 565.43, loss2: 74.61 and loss3: 0.00\n",
      "Epoch [2138], train_loss: 640.81 with loss1: 566.32, loss2: 74.48 and loss3: 0.00\n",
      "Epoch [2139], train_loss: 639.13 with loss1: 564.57, loss2: 74.57 and loss3: 0.00\n",
      "Epoch [2140], train_loss: 639.35 with loss1: 564.82, loss2: 74.53 and loss3: 0.00\n",
      "Epoch [2141], train_loss: 638.98 with loss1: 564.53, loss2: 74.45 and loss3: 0.00\n",
      "Epoch [2142], train_loss: 640.77 with loss1: 566.22, loss2: 74.55 and loss3: 0.00\n",
      "Epoch [2143], train_loss: 638.46 with loss1: 563.96, loss2: 74.50 and loss3: 0.00\n",
      "Epoch [2144], train_loss: 638.39 with loss1: 563.92, loss2: 74.48 and loss3: 0.00\n",
      "Epoch [2145], train_loss: 638.52 with loss1: 564.08, loss2: 74.45 and loss3: 0.00\n",
      "Epoch [2146], train_loss: 641.63 with loss1: 567.21, loss2: 74.43 and loss3: 0.00\n",
      "Epoch [2147], train_loss: 637.38 with loss1: 562.94, loss2: 74.43 and loss3: 0.00\n",
      "Epoch [2148], train_loss: 641.73 with loss1: 567.31, loss2: 74.42 and loss3: 0.00\n",
      "Epoch [2149], train_loss: 639.56 with loss1: 565.20, loss2: 74.36 and loss3: 0.00\n",
      "Epoch [2150], train_loss: 640.01 with loss1: 565.60, loss2: 74.42 and loss3: 0.00\n",
      "Epoch [2151], train_loss: 637.02 with loss1: 562.68, loss2: 74.34 and loss3: 0.00\n",
      "Epoch [2152], train_loss: 639.15 with loss1: 564.75, loss2: 74.40 and loss3: 0.00\n",
      "Epoch [2153], train_loss: 640.32 with loss1: 565.89, loss2: 74.43 and loss3: 0.00\n",
      "Epoch [2154], train_loss: 639.98 with loss1: 565.75, loss2: 74.23 and loss3: 0.00\n",
      "Epoch [2155], train_loss: 638.43 with loss1: 564.04, loss2: 74.39 and loss3: 0.00\n",
      "Epoch [2156], train_loss: 639.05 with loss1: 564.69, loss2: 74.36 and loss3: 0.00\n",
      "Epoch [2157], train_loss: 637.76 with loss1: 563.44, loss2: 74.32 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2158], train_loss: 637.44 with loss1: 563.07, loss2: 74.37 and loss3: 0.00\n",
      "Epoch [2159], train_loss: 636.35 with loss1: 562.09, loss2: 74.26 and loss3: 0.00\n",
      "Epoch [2160], train_loss: 638.29 with loss1: 563.97, loss2: 74.31 and loss3: 0.00\n",
      "Epoch [2161], train_loss: 634.94 with loss1: 560.57, loss2: 74.37 and loss3: 0.00\n",
      "Epoch [2162], train_loss: 636.93 with loss1: 562.72, loss2: 74.21 and loss3: 0.00\n",
      "Epoch [2163], train_loss: 638.48 with loss1: 564.25, loss2: 74.23 and loss3: 0.00\n",
      "Epoch [2164], train_loss: 637.43 with loss1: 563.22, loss2: 74.21 and loss3: 0.00\n",
      "Epoch [2165], train_loss: 635.09 with loss1: 560.94, loss2: 74.15 and loss3: 0.00\n",
      "Epoch [2166], train_loss: 637.27 with loss1: 563.11, loss2: 74.15 and loss3: 0.00\n",
      "Epoch [2167], train_loss: 634.84 with loss1: 560.65, loss2: 74.20 and loss3: 0.00\n",
      "Epoch [2168], train_loss: 637.28 with loss1: 563.01, loss2: 74.26 and loss3: 0.00\n",
      "Epoch [2169], train_loss: 634.96 with loss1: 560.78, loss2: 74.18 and loss3: 0.00\n",
      "Epoch [2170], train_loss: 638.36 with loss1: 564.19, loss2: 74.17 and loss3: 0.00\n",
      "Epoch [2171], train_loss: 633.50 with loss1: 559.27, loss2: 74.23 and loss3: 0.00\n",
      "Epoch [2172], train_loss: 639.39 with loss1: 565.11, loss2: 74.28 and loss3: 0.00\n",
      "Epoch [2173], train_loss: 634.37 with loss1: 560.30, loss2: 74.07 and loss3: 0.00\n",
      "Epoch [2174], train_loss: 635.15 with loss1: 560.98, loss2: 74.17 and loss3: 0.00\n",
      "Epoch [2175], train_loss: 635.58 with loss1: 561.42, loss2: 74.17 and loss3: 0.00\n",
      "Epoch [2176], train_loss: 634.38 with loss1: 560.33, loss2: 74.05 and loss3: 0.00\n",
      "Epoch [2177], train_loss: 634.12 with loss1: 559.92, loss2: 74.21 and loss3: 0.00\n",
      "Epoch [2178], train_loss: 636.85 with loss1: 562.78, loss2: 74.06 and loss3: 0.00\n",
      "Epoch [2179], train_loss: 635.74 with loss1: 561.66, loss2: 74.08 and loss3: 0.00\n",
      "Epoch [2180], train_loss: 636.72 with loss1: 562.70, loss2: 74.03 and loss3: 0.00\n",
      "Epoch [2181], train_loss: 634.59 with loss1: 560.54, loss2: 74.04 and loss3: 0.00\n",
      "Epoch [2182], train_loss: 636.65 with loss1: 562.61, loss2: 74.04 and loss3: 0.00\n",
      "Epoch [2183], train_loss: 633.89 with loss1: 559.79, loss2: 74.10 and loss3: 0.00\n",
      "Epoch [2184], train_loss: 635.13 with loss1: 561.12, loss2: 74.01 and loss3: 0.00\n",
      "Epoch [2185], train_loss: 633.34 with loss1: 559.30, loss2: 74.05 and loss3: 0.00\n",
      "Epoch [2186], train_loss: 634.30 with loss1: 560.28, loss2: 74.02 and loss3: 0.00\n",
      "Epoch [2187], train_loss: 632.43 with loss1: 558.40, loss2: 74.02 and loss3: 0.00\n",
      "Epoch [2188], train_loss: 632.12 with loss1: 558.19, loss2: 73.92 and loss3: 0.00\n",
      "Epoch [2189], train_loss: 633.88 with loss1: 559.98, loss2: 73.90 and loss3: 0.00\n",
      "Epoch [2190], train_loss: 635.59 with loss1: 561.53, loss2: 74.06 and loss3: 0.00\n",
      "Epoch [2191], train_loss: 632.32 with loss1: 558.38, loss2: 73.94 and loss3: 0.00\n",
      "Epoch [2192], train_loss: 634.48 with loss1: 560.61, loss2: 73.87 and loss3: 0.00\n",
      "Epoch [2193], train_loss: 633.08 with loss1: 559.14, loss2: 73.93 and loss3: 0.00\n",
      "Epoch [2194], train_loss: 631.43 with loss1: 557.60, loss2: 73.83 and loss3: 0.00\n",
      "Epoch [2195], train_loss: 631.84 with loss1: 557.96, loss2: 73.88 and loss3: 0.00\n",
      "Epoch [2196], train_loss: 634.47 with loss1: 560.66, loss2: 73.81 and loss3: 0.00\n",
      "Epoch [2197], train_loss: 634.53 with loss1: 560.80, loss2: 73.73 and loss3: 0.00\n",
      "Epoch [2198], train_loss: 636.20 with loss1: 562.41, loss2: 73.79 and loss3: 0.00\n",
      "Epoch [2199], train_loss: 633.69 with loss1: 559.85, loss2: 73.84 and loss3: 0.00\n",
      "Epoch [2200], train_loss: 634.63 with loss1: 560.82, loss2: 73.82 and loss3: 0.00\n",
      "Epoch [2201], train_loss: 632.81 with loss1: 558.97, loss2: 73.84 and loss3: 0.00\n",
      "Epoch [2202], train_loss: 633.87 with loss1: 560.07, loss2: 73.81 and loss3: 0.00\n",
      "Epoch [2203], train_loss: 632.29 with loss1: 558.44, loss2: 73.86 and loss3: 0.00\n",
      "Epoch [2204], train_loss: 633.26 with loss1: 559.41, loss2: 73.84 and loss3: 0.00\n",
      "Epoch [2205], train_loss: 634.12 with loss1: 560.33, loss2: 73.79 and loss3: 0.00\n",
      "Epoch [2206], train_loss: 635.98 with loss1: 562.27, loss2: 73.71 and loss3: 0.00\n",
      "Epoch [2207], train_loss: 633.66 with loss1: 559.97, loss2: 73.68 and loss3: 0.00\n",
      "Epoch [2208], train_loss: 634.90 with loss1: 561.23, loss2: 73.67 and loss3: 0.00\n",
      "Epoch [2209], train_loss: 634.14 with loss1: 560.49, loss2: 73.65 and loss3: 0.00\n",
      "Epoch [2210], train_loss: 636.37 with loss1: 562.68, loss2: 73.69 and loss3: 0.00\n",
      "Epoch [2211], train_loss: 635.04 with loss1: 561.39, loss2: 73.65 and loss3: 0.00\n",
      "Epoch [2212], train_loss: 637.82 with loss1: 564.16, loss2: 73.66 and loss3: 0.00\n",
      "Epoch [2213], train_loss: 636.70 with loss1: 563.06, loss2: 73.64 and loss3: 0.00\n",
      "Epoch [2214], train_loss: 638.84 with loss1: 565.17, loss2: 73.67 and loss3: 0.00\n",
      "Epoch [2215], train_loss: 637.36 with loss1: 563.73, loss2: 73.63 and loss3: 0.00\n",
      "Epoch [2216], train_loss: 638.40 with loss1: 564.79, loss2: 73.61 and loss3: 0.00\n",
      "Epoch [2217], train_loss: 636.38 with loss1: 562.78, loss2: 73.59 and loss3: 0.00\n",
      "Epoch [2218], train_loss: 637.65 with loss1: 564.00, loss2: 73.66 and loss3: 0.00\n",
      "Epoch [2219], train_loss: 636.98 with loss1: 563.44, loss2: 73.53 and loss3: 0.00\n",
      "Epoch [2220], train_loss: 638.41 with loss1: 564.84, loss2: 73.57 and loss3: 0.00\n",
      "Epoch [2221], train_loss: 635.94 with loss1: 562.41, loss2: 73.53 and loss3: 0.00\n",
      "Epoch [2222], train_loss: 637.23 with loss1: 563.71, loss2: 73.52 and loss3: 0.00\n",
      "Epoch [2223], train_loss: 639.63 with loss1: 566.08, loss2: 73.54 and loss3: 0.00\n",
      "Epoch [2224], train_loss: 639.97 with loss1: 566.45, loss2: 73.52 and loss3: 0.00\n",
      "Epoch [2225], train_loss: 635.81 with loss1: 562.34, loss2: 73.47 and loss3: 0.00\n",
      "Epoch [2226], train_loss: 640.49 with loss1: 566.98, loss2: 73.51 and loss3: 0.00\n",
      "Epoch [2227], train_loss: 639.97 with loss1: 566.55, loss2: 73.42 and loss3: 0.00\n",
      "Epoch [2228], train_loss: 640.77 with loss1: 567.24, loss2: 73.53 and loss3: 0.00\n",
      "Epoch [2229], train_loss: 638.51 with loss1: 565.06, loss2: 73.45 and loss3: 0.00\n",
      "Epoch [2230], train_loss: 639.96 with loss1: 566.50, loss2: 73.46 and loss3: 0.00\n",
      "Epoch [2231], train_loss: 638.63 with loss1: 565.19, loss2: 73.44 and loss3: 0.00\n",
      "Epoch [2232], train_loss: 641.75 with loss1: 568.38, loss2: 73.37 and loss3: 0.00\n",
      "Epoch [2233], train_loss: 638.82 with loss1: 565.34, loss2: 73.48 and loss3: 0.00\n",
      "Epoch [2234], train_loss: 640.84 with loss1: 567.56, loss2: 73.29 and loss3: 0.00\n",
      "Epoch [2235], train_loss: 641.03 with loss1: 567.66, loss2: 73.37 and loss3: 0.00\n",
      "Epoch [2236], train_loss: 641.31 with loss1: 567.92, loss2: 73.38 and loss3: 0.00\n",
      "Epoch [2237], train_loss: 643.12 with loss1: 569.84, loss2: 73.28 and loss3: 0.00\n",
      "Epoch [2238], train_loss: 644.79 with loss1: 571.46, loss2: 73.33 and loss3: 0.00\n",
      "Epoch [2239], train_loss: 640.97 with loss1: 567.69, loss2: 73.28 and loss3: 0.00\n",
      "Epoch [2240], train_loss: 644.39 with loss1: 571.14, loss2: 73.25 and loss3: 0.00\n",
      "Epoch [2241], train_loss: 642.14 with loss1: 568.85, loss2: 73.29 and loss3: 0.00\n",
      "Epoch [2242], train_loss: 642.76 with loss1: 569.48, loss2: 73.28 and loss3: 0.00\n",
      "Epoch [2243], train_loss: 643.68 with loss1: 570.41, loss2: 73.27 and loss3: 0.00\n",
      "Epoch [2244], train_loss: 646.42 with loss1: 573.20, loss2: 73.22 and loss3: 0.00\n",
      "Epoch [2245], train_loss: 641.84 with loss1: 568.64, loss2: 73.21 and loss3: 0.00\n",
      "Epoch [2246], train_loss: 643.97 with loss1: 570.65, loss2: 73.32 and loss3: 0.00\n",
      "Epoch [2247], train_loss: 642.34 with loss1: 569.12, loss2: 73.22 and loss3: 0.00\n",
      "Epoch [2248], train_loss: 643.18 with loss1: 570.01, loss2: 73.17 and loss3: 0.00\n",
      "Epoch [2249], train_loss: 640.34 with loss1: 567.18, loss2: 73.17 and loss3: 0.00\n",
      "Epoch [2250], train_loss: 641.66 with loss1: 568.48, loss2: 73.18 and loss3: 0.00\n",
      "Epoch [2251], train_loss: 639.16 with loss1: 566.06, loss2: 73.10 and loss3: 0.00\n",
      "Epoch [2252], train_loss: 643.05 with loss1: 569.78, loss2: 73.27 and loss3: 0.00\n",
      "Epoch [2253], train_loss: 638.76 with loss1: 565.66, loss2: 73.09 and loss3: 0.00\n",
      "Epoch [2254], train_loss: 642.24 with loss1: 569.05, loss2: 73.19 and loss3: 0.00\n",
      "Epoch [2255], train_loss: 640.18 with loss1: 567.06, loss2: 73.12 and loss3: 0.00\n",
      "Epoch [2256], train_loss: 641.81 with loss1: 568.57, loss2: 73.24 and loss3: 0.00\n",
      "Epoch [2257], train_loss: 637.07 with loss1: 564.04, loss2: 73.03 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2258], train_loss: 641.92 with loss1: 568.74, loss2: 73.18 and loss3: 0.00\n",
      "Epoch [2259], train_loss: 639.74 with loss1: 566.63, loss2: 73.11 and loss3: 0.00\n",
      "Epoch [2260], train_loss: 644.26 with loss1: 571.15, loss2: 73.11 and loss3: 0.00\n",
      "Epoch [2261], train_loss: 643.59 with loss1: 570.52, loss2: 73.06 and loss3: 0.00\n",
      "Epoch [2262], train_loss: 646.97 with loss1: 573.91, loss2: 73.07 and loss3: 0.00\n",
      "Epoch [2263], train_loss: 638.70 with loss1: 565.74, loss2: 72.95 and loss3: 0.00\n",
      "Epoch [2264], train_loss: 643.64 with loss1: 570.53, loss2: 73.11 and loss3: 0.00\n",
      "Epoch [2265], train_loss: 643.85 with loss1: 570.84, loss2: 73.01 and loss3: 0.00\n",
      "Epoch [2266], train_loss: 647.71 with loss1: 574.66, loss2: 73.05 and loss3: 0.00\n",
      "Epoch [2267], train_loss: 645.77 with loss1: 572.84, loss2: 72.93 and loss3: 0.00\n",
      "Epoch [2268], train_loss: 649.38 with loss1: 576.39, loss2: 72.99 and loss3: 0.00\n",
      "Epoch [2269], train_loss: 649.61 with loss1: 576.65, loss2: 72.97 and loss3: 0.00\n",
      "Epoch [2270], train_loss: 649.35 with loss1: 576.34, loss2: 73.02 and loss3: 0.00\n",
      "Epoch [2271], train_loss: 644.57 with loss1: 571.64, loss2: 72.93 and loss3: 0.00\n",
      "Epoch [2272], train_loss: 649.07 with loss1: 576.06, loss2: 73.01 and loss3: 0.00\n",
      "Epoch [2273], train_loss: 643.13 with loss1: 570.31, loss2: 72.82 and loss3: 0.00\n",
      "Epoch [2274], train_loss: 647.72 with loss1: 574.74, loss2: 72.98 and loss3: 0.00\n",
      "Epoch [2275], train_loss: 641.88 with loss1: 569.03, loss2: 72.85 and loss3: 0.00\n",
      "Epoch [2276], train_loss: 645.77 with loss1: 572.88, loss2: 72.88 and loss3: 0.00\n",
      "Epoch [2277], train_loss: 639.87 with loss1: 567.02, loss2: 72.86 and loss3: 0.00\n",
      "Epoch [2278], train_loss: 641.35 with loss1: 568.47, loss2: 72.88 and loss3: 0.00\n",
      "Epoch [2279], train_loss: 635.60 with loss1: 562.75, loss2: 72.85 and loss3: 0.00\n",
      "Epoch [2280], train_loss: 636.49 with loss1: 563.55, loss2: 72.93 and loss3: 0.00\n",
      "Epoch [2281], train_loss: 630.15 with loss1: 557.38, loss2: 72.77 and loss3: 0.00\n",
      "Epoch [2282], train_loss: 633.02 with loss1: 560.23, loss2: 72.79 and loss3: 0.00\n",
      "Epoch [2283], train_loss: 629.77 with loss1: 557.07, loss2: 72.70 and loss3: 0.00\n",
      "Epoch [2284], train_loss: 630.73 with loss1: 557.97, loss2: 72.76 and loss3: 0.00\n",
      "Epoch [2285], train_loss: 628.20 with loss1: 555.48, loss2: 72.72 and loss3: 0.00\n",
      "Epoch [2286], train_loss: 632.91 with loss1: 560.07, loss2: 72.83 and loss3: 0.00\n",
      "Epoch [2287], train_loss: 626.46 with loss1: 553.79, loss2: 72.67 and loss3: 0.00\n",
      "Epoch [2288], train_loss: 627.83 with loss1: 555.12, loss2: 72.71 and loss3: 0.00\n",
      "Epoch [2289], train_loss: 622.80 with loss1: 550.08, loss2: 72.72 and loss3: 0.00\n",
      "Epoch [2290], train_loss: 624.84 with loss1: 552.22, loss2: 72.62 and loss3: 0.00\n",
      "Epoch [2291], train_loss: 620.90 with loss1: 548.24, loss2: 72.66 and loss3: 0.00\n",
      "Epoch [2292], train_loss: 622.14 with loss1: 549.34, loss2: 72.80 and loss3: 0.00\n",
      "Epoch [2293], train_loss: 621.61 with loss1: 549.03, loss2: 72.58 and loss3: 0.00\n",
      "Epoch [2294], train_loss: 619.72 with loss1: 547.02, loss2: 72.70 and loss3: 0.00\n",
      "Epoch [2295], train_loss: 617.32 with loss1: 544.68, loss2: 72.64 and loss3: 0.00\n",
      "Epoch [2296], train_loss: 616.21 with loss1: 543.52, loss2: 72.69 and loss3: 0.00\n",
      "Epoch [2297], train_loss: 613.58 with loss1: 540.84, loss2: 72.73 and loss3: 0.00\n",
      "Epoch [2298], train_loss: 613.82 with loss1: 541.15, loss2: 72.66 and loss3: 0.00\n",
      "Epoch [2299], train_loss: 611.62 with loss1: 539.03, loss2: 72.59 and loss3: 0.00\n",
      "Epoch [2300], train_loss: 613.11 with loss1: 540.47, loss2: 72.64 and loss3: 0.00\n",
      "Epoch [2301], train_loss: 612.16 with loss1: 539.49, loss2: 72.67 and loss3: 0.00\n",
      "Epoch [2302], train_loss: 611.50 with loss1: 538.86, loss2: 72.64 and loss3: 0.00\n",
      "Epoch [2303], train_loss: 611.29 with loss1: 538.77, loss2: 72.52 and loss3: 0.00\n",
      "Epoch [2304], train_loss: 612.95 with loss1: 540.24, loss2: 72.71 and loss3: 0.00\n",
      "Epoch [2305], train_loss: 607.46 with loss1: 534.88, loss2: 72.59 and loss3: 0.00\n",
      "Epoch [2306], train_loss: 606.76 with loss1: 534.21, loss2: 72.55 and loss3: 0.00\n",
      "Epoch [2307], train_loss: 608.02 with loss1: 535.46, loss2: 72.55 and loss3: 0.00\n",
      "Epoch [2308], train_loss: 608.82 with loss1: 536.21, loss2: 72.61 and loss3: 0.00\n",
      "Epoch [2309], train_loss: 604.99 with loss1: 532.52, loss2: 72.46 and loss3: 0.00\n",
      "Epoch [2310], train_loss: 606.49 with loss1: 533.94, loss2: 72.55 and loss3: 0.00\n",
      "Epoch [2311], train_loss: 605.07 with loss1: 532.54, loss2: 72.53 and loss3: 0.00\n",
      "Epoch [2312], train_loss: 607.12 with loss1: 534.71, loss2: 72.41 and loss3: 0.00\n",
      "Epoch [2313], train_loss: 605.06 with loss1: 532.61, loss2: 72.45 and loss3: 0.00\n",
      "Epoch [2314], train_loss: 605.10 with loss1: 532.53, loss2: 72.57 and loss3: 0.00\n",
      "Epoch [2315], train_loss: 605.20 with loss1: 532.93, loss2: 72.27 and loss3: 0.00\n",
      "Epoch [2316], train_loss: 602.16 with loss1: 529.75, loss2: 72.40 and loss3: 0.00\n",
      "Epoch [2317], train_loss: 603.14 with loss1: 530.80, loss2: 72.34 and loss3: 0.00\n",
      "Epoch [2318], train_loss: 602.58 with loss1: 530.12, loss2: 72.46 and loss3: 0.00\n",
      "Epoch [2319], train_loss: 604.00 with loss1: 531.65, loss2: 72.35 and loss3: 0.00\n",
      "Epoch [2320], train_loss: 601.44 with loss1: 528.94, loss2: 72.50 and loss3: 0.00\n",
      "Epoch [2321], train_loss: 602.92 with loss1: 530.47, loss2: 72.45 and loss3: 0.00\n",
      "Epoch [2322], train_loss: 599.96 with loss1: 527.52, loss2: 72.44 and loss3: 0.00\n",
      "Epoch [2323], train_loss: 599.70 with loss1: 527.32, loss2: 72.38 and loss3: 0.00\n",
      "Epoch [2324], train_loss: 600.31 with loss1: 528.00, loss2: 72.31 and loss3: 0.00\n",
      "Epoch [2325], train_loss: 601.23 with loss1: 528.93, loss2: 72.30 and loss3: 0.00\n",
      "Epoch [2326], train_loss: 602.44 with loss1: 530.06, loss2: 72.39 and loss3: 0.00\n",
      "Epoch [2327], train_loss: 600.04 with loss1: 527.76, loss2: 72.28 and loss3: 0.00\n",
      "Epoch [2328], train_loss: 598.81 with loss1: 526.53, loss2: 72.29 and loss3: 0.00\n",
      "Epoch [2329], train_loss: 598.36 with loss1: 526.06, loss2: 72.29 and loss3: 0.00\n",
      "Epoch [2330], train_loss: 598.91 with loss1: 526.56, loss2: 72.35 and loss3: 0.00\n",
      "Epoch [2331], train_loss: 596.60 with loss1: 524.25, loss2: 72.35 and loss3: 0.00\n",
      "Epoch [2332], train_loss: 599.08 with loss1: 526.75, loss2: 72.33 and loss3: 0.00\n",
      "Epoch [2333], train_loss: 597.21 with loss1: 524.93, loss2: 72.29 and loss3: 0.00\n",
      "Epoch [2334], train_loss: 596.73 with loss1: 524.43, loss2: 72.30 and loss3: 0.00\n",
      "Epoch [2335], train_loss: 597.94 with loss1: 525.66, loss2: 72.28 and loss3: 0.00\n",
      "Epoch [2336], train_loss: 597.44 with loss1: 525.23, loss2: 72.21 and loss3: 0.00\n",
      "Epoch [2337], train_loss: 598.15 with loss1: 525.93, loss2: 72.23 and loss3: 0.00\n",
      "Epoch [2338], train_loss: 597.68 with loss1: 525.40, loss2: 72.28 and loss3: 0.00\n",
      "Epoch [2339], train_loss: 596.42 with loss1: 524.26, loss2: 72.16 and loss3: 0.00\n",
      "Epoch [2340], train_loss: 595.90 with loss1: 523.67, loss2: 72.23 and loss3: 0.00\n",
      "Epoch [2341], train_loss: 594.28 with loss1: 522.05, loss2: 72.23 and loss3: 0.00\n",
      "Epoch [2342], train_loss: 596.25 with loss1: 524.05, loss2: 72.20 and loss3: 0.00\n",
      "Epoch [2343], train_loss: 593.80 with loss1: 521.73, loss2: 72.07 and loss3: 0.00\n",
      "Epoch [2344], train_loss: 596.14 with loss1: 524.10, loss2: 72.03 and loss3: 0.00\n",
      "Epoch [2345], train_loss: 593.85 with loss1: 521.73, loss2: 72.12 and loss3: 0.00\n",
      "Epoch [2346], train_loss: 594.52 with loss1: 522.38, loss2: 72.14 and loss3: 0.00\n",
      "Epoch [2347], train_loss: 594.29 with loss1: 522.26, loss2: 72.03 and loss3: 0.00\n",
      "Epoch [2348], train_loss: 596.18 with loss1: 524.06, loss2: 72.12 and loss3: 0.00\n",
      "Epoch [2349], train_loss: 593.65 with loss1: 521.63, loss2: 72.02 and loss3: 0.00\n",
      "Epoch [2350], train_loss: 596.19 with loss1: 524.16, loss2: 72.03 and loss3: 0.00\n",
      "Epoch [2351], train_loss: 593.91 with loss1: 521.90, loss2: 72.00 and loss3: 0.00\n",
      "Epoch [2352], train_loss: 595.75 with loss1: 523.74, loss2: 72.02 and loss3: 0.00\n",
      "Epoch [2353], train_loss: 594.19 with loss1: 522.23, loss2: 71.96 and loss3: 0.00\n",
      "Epoch [2354], train_loss: 593.93 with loss1: 521.96, loss2: 71.97 and loss3: 0.00\n",
      "Epoch [2355], train_loss: 594.45 with loss1: 522.50, loss2: 71.95 and loss3: 0.00\n",
      "Epoch [2356], train_loss: 591.25 with loss1: 519.24, loss2: 72.01 and loss3: 0.00\n",
      "Epoch [2357], train_loss: 591.47 with loss1: 519.55, loss2: 71.92 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2358], train_loss: 593.43 with loss1: 521.54, loss2: 71.89 and loss3: 0.00\n",
      "Epoch [2359], train_loss: 594.13 with loss1: 522.22, loss2: 71.90 and loss3: 0.00\n",
      "Epoch [2360], train_loss: 591.64 with loss1: 519.69, loss2: 71.95 and loss3: 0.00\n",
      "Epoch [2361], train_loss: 592.16 with loss1: 520.32, loss2: 71.84 and loss3: 0.00\n",
      "Epoch [2362], train_loss: 592.04 with loss1: 520.09, loss2: 71.95 and loss3: 0.00\n",
      "Epoch [2363], train_loss: 591.06 with loss1: 519.07, loss2: 71.99 and loss3: 0.00\n",
      "Epoch [2364], train_loss: 592.29 with loss1: 520.42, loss2: 71.87 and loss3: 0.00\n",
      "Epoch [2365], train_loss: 590.33 with loss1: 518.44, loss2: 71.89 and loss3: 0.00\n",
      "Epoch [2366], train_loss: 592.33 with loss1: 520.52, loss2: 71.81 and loss3: 0.00\n",
      "Epoch [2367], train_loss: 591.50 with loss1: 519.63, loss2: 71.88 and loss3: 0.00\n",
      "Epoch [2368], train_loss: 591.85 with loss1: 519.98, loss2: 71.87 and loss3: 0.00\n",
      "Epoch [2369], train_loss: 589.03 with loss1: 517.28, loss2: 71.75 and loss3: 0.00\n",
      "Epoch [2370], train_loss: 589.02 with loss1: 517.17, loss2: 71.85 and loss3: 0.00\n",
      "Epoch [2371], train_loss: 588.96 with loss1: 517.20, loss2: 71.76 and loss3: 0.00\n",
      "Epoch [2372], train_loss: 589.79 with loss1: 517.97, loss2: 71.82 and loss3: 0.00\n",
      "Epoch [2373], train_loss: 589.19 with loss1: 517.46, loss2: 71.74 and loss3: 0.00\n",
      "Epoch [2374], train_loss: 589.62 with loss1: 517.90, loss2: 71.72 and loss3: 0.00\n",
      "Epoch [2375], train_loss: 590.13 with loss1: 518.43, loss2: 71.70 and loss3: 0.00\n",
      "Epoch [2376], train_loss: 589.67 with loss1: 517.93, loss2: 71.73 and loss3: 0.00\n",
      "Epoch [2377], train_loss: 590.21 with loss1: 518.58, loss2: 71.63 and loss3: 0.00\n",
      "Epoch [2378], train_loss: 589.97 with loss1: 518.22, loss2: 71.75 and loss3: 0.00\n",
      "Epoch [2379], train_loss: 589.81 with loss1: 518.24, loss2: 71.57 and loss3: 0.00\n",
      "Epoch [2380], train_loss: 590.80 with loss1: 519.13, loss2: 71.67 and loss3: 0.00\n",
      "Epoch [2381], train_loss: 592.14 with loss1: 520.59, loss2: 71.55 and loss3: 0.00\n",
      "Epoch [2382], train_loss: 591.30 with loss1: 519.57, loss2: 71.73 and loss3: 0.00\n",
      "Epoch [2383], train_loss: 592.85 with loss1: 521.25, loss2: 71.60 and loss3: 0.00\n",
      "Epoch [2384], train_loss: 591.16 with loss1: 519.55, loss2: 71.60 and loss3: 0.00\n",
      "Epoch [2385], train_loss: 590.29 with loss1: 518.79, loss2: 71.50 and loss3: 0.00\n",
      "Epoch [2386], train_loss: 593.48 with loss1: 521.88, loss2: 71.60 and loss3: 0.00\n",
      "Epoch [2387], train_loss: 591.84 with loss1: 520.32, loss2: 71.52 and loss3: 0.00\n",
      "Epoch [2388], train_loss: 596.68 with loss1: 525.04, loss2: 71.64 and loss3: 0.00\n",
      "Epoch [2389], train_loss: 593.11 with loss1: 521.59, loss2: 71.51 and loss3: 0.00\n",
      "Epoch [2390], train_loss: 595.95 with loss1: 524.33, loss2: 71.63 and loss3: 0.00\n",
      "Epoch [2391], train_loss: 594.82 with loss1: 523.36, loss2: 71.46 and loss3: 0.00\n",
      "Epoch [2392], train_loss: 597.36 with loss1: 525.84, loss2: 71.52 and loss3: 0.00\n",
      "Epoch [2393], train_loss: 596.95 with loss1: 525.45, loss2: 71.50 and loss3: 0.00\n",
      "Epoch [2394], train_loss: 600.62 with loss1: 529.13, loss2: 71.49 and loss3: 0.00\n",
      "Epoch [2395], train_loss: 601.22 with loss1: 529.74, loss2: 71.48 and loss3: 0.00\n",
      "Epoch [2396], train_loss: 603.67 with loss1: 532.15, loss2: 71.52 and loss3: 0.00\n",
      "Epoch [2397], train_loss: 603.41 with loss1: 531.89, loss2: 71.51 and loss3: 0.00\n",
      "Epoch [2398], train_loss: 608.07 with loss1: 536.51, loss2: 71.56 and loss3: 0.00\n",
      "Epoch [2399], train_loss: 609.89 with loss1: 538.51, loss2: 71.38 and loss3: 0.00\n",
      "Epoch [2400], train_loss: 613.39 with loss1: 541.93, loss2: 71.46 and loss3: 0.00\n",
      "Epoch [2401], train_loss: 613.99 with loss1: 542.56, loss2: 71.43 and loss3: 0.00\n",
      "Epoch [2402], train_loss: 616.13 with loss1: 544.61, loss2: 71.51 and loss3: 0.00\n",
      "Epoch [2403], train_loss: 618.20 with loss1: 546.82, loss2: 71.39 and loss3: 0.00\n",
      "Epoch [2404], train_loss: 622.69 with loss1: 551.14, loss2: 71.55 and loss3: 0.00\n",
      "Epoch [2405], train_loss: 624.28 with loss1: 552.95, loss2: 71.33 and loss3: 0.00\n",
      "Epoch [2406], train_loss: 628.24 with loss1: 556.75, loss2: 71.49 and loss3: 0.00\n",
      "Epoch [2407], train_loss: 626.81 with loss1: 555.63, loss2: 71.19 and loss3: 0.00\n",
      "Epoch [2408], train_loss: 632.75 with loss1: 561.39, loss2: 71.36 and loss3: 0.00\n",
      "Epoch [2409], train_loss: 632.86 with loss1: 561.68, loss2: 71.18 and loss3: 0.00\n",
      "Epoch [2410], train_loss: 639.64 with loss1: 568.19, loss2: 71.44 and loss3: 0.00\n",
      "Epoch [2411], train_loss: 637.99 with loss1: 566.77, loss2: 71.22 and loss3: 0.00\n",
      "Epoch [2412], train_loss: 645.38 with loss1: 574.02, loss2: 71.36 and loss3: 0.00\n",
      "Epoch [2413], train_loss: 644.35 with loss1: 573.18, loss2: 71.17 and loss3: 0.00\n",
      "Epoch [2414], train_loss: 650.87 with loss1: 579.49, loss2: 71.38 and loss3: 0.00\n",
      "Epoch [2415], train_loss: 647.30 with loss1: 576.14, loss2: 71.16 and loss3: 0.00\n",
      "Epoch [2416], train_loss: 653.70 with loss1: 582.37, loss2: 71.34 and loss3: 0.00\n",
      "Epoch [2417], train_loss: 652.35 with loss1: 581.12, loss2: 71.22 and loss3: 0.00\n",
      "Epoch [2418], train_loss: 657.23 with loss1: 585.92, loss2: 71.31 and loss3: 0.00\n",
      "Epoch [2419], train_loss: 653.40 with loss1: 582.20, loss2: 71.20 and loss3: 0.00\n",
      "Epoch [2420], train_loss: 658.20 with loss1: 587.04, loss2: 71.16 and loss3: 0.00\n",
      "Epoch [2421], train_loss: 653.05 with loss1: 581.95, loss2: 71.10 and loss3: 0.00\n",
      "Epoch [2422], train_loss: 660.36 with loss1: 589.13, loss2: 71.23 and loss3: 0.00\n",
      "Epoch [2423], train_loss: 652.68 with loss1: 581.66, loss2: 71.02 and loss3: 0.00\n",
      "Epoch [2424], train_loss: 654.45 with loss1: 583.31, loss2: 71.14 and loss3: 0.00\n",
      "Epoch [2425], train_loss: 648.49 with loss1: 577.37, loss2: 71.12 and loss3: 0.00\n",
      "Epoch [2426], train_loss: 649.00 with loss1: 577.87, loss2: 71.13 and loss3: 0.00\n",
      "Epoch [2427], train_loss: 643.95 with loss1: 572.86, loss2: 71.09 and loss3: 0.00\n",
      "Epoch [2428], train_loss: 646.29 with loss1: 575.08, loss2: 71.21 and loss3: 0.00\n",
      "Epoch [2429], train_loss: 639.54 with loss1: 568.49, loss2: 71.05 and loss3: 0.00\n",
      "Epoch [2430], train_loss: 639.79 with loss1: 568.68, loss2: 71.10 and loss3: 0.00\n",
      "Epoch [2431], train_loss: 634.84 with loss1: 563.77, loss2: 71.07 and loss3: 0.00\n",
      "Epoch [2432], train_loss: 634.83 with loss1: 563.78, loss2: 71.05 and loss3: 0.00\n",
      "Epoch [2433], train_loss: 629.98 with loss1: 559.00, loss2: 70.98 and loss3: 0.00\n",
      "Epoch [2434], train_loss: 631.56 with loss1: 560.60, loss2: 70.95 and loss3: 0.00\n",
      "Epoch [2435], train_loss: 627.29 with loss1: 556.27, loss2: 71.02 and loss3: 0.00\n",
      "Epoch [2436], train_loss: 627.84 with loss1: 556.82, loss2: 71.02 and loss3: 0.00\n",
      "Epoch [2437], train_loss: 622.40 with loss1: 551.47, loss2: 70.93 and loss3: 0.00\n",
      "Epoch [2438], train_loss: 624.84 with loss1: 553.92, loss2: 70.92 and loss3: 0.00\n",
      "Epoch [2439], train_loss: 620.74 with loss1: 549.89, loss2: 70.85 and loss3: 0.00\n",
      "Epoch [2440], train_loss: 623.16 with loss1: 552.24, loss2: 70.92 and loss3: 0.00\n",
      "Epoch [2441], train_loss: 620.87 with loss1: 550.05, loss2: 70.82 and loss3: 0.00\n",
      "Epoch [2442], train_loss: 622.19 with loss1: 551.35, loss2: 70.83 and loss3: 0.00\n",
      "Epoch [2443], train_loss: 616.63 with loss1: 545.78, loss2: 70.85 and loss3: 0.00\n",
      "Epoch [2444], train_loss: 617.21 with loss1: 546.33, loss2: 70.88 and loss3: 0.00\n",
      "Epoch [2445], train_loss: 614.92 with loss1: 544.18, loss2: 70.75 and loss3: 0.00\n",
      "Epoch [2446], train_loss: 618.27 with loss1: 547.57, loss2: 70.70 and loss3: 0.00\n",
      "Epoch [2447], train_loss: 614.84 with loss1: 544.07, loss2: 70.77 and loss3: 0.00\n",
      "Epoch [2448], train_loss: 616.05 with loss1: 545.27, loss2: 70.79 and loss3: 0.00\n",
      "Epoch [2449], train_loss: 614.77 with loss1: 543.90, loss2: 70.87 and loss3: 0.00\n",
      "Epoch [2450], train_loss: 615.83 with loss1: 545.19, loss2: 70.63 and loss3: 0.00\n",
      "Epoch [2451], train_loss: 611.82 with loss1: 541.23, loss2: 70.58 and loss3: 0.00\n",
      "Epoch [2452], train_loss: 614.88 with loss1: 544.23, loss2: 70.65 and loss3: 0.00\n",
      "Epoch [2453], train_loss: 612.51 with loss1: 541.89, loss2: 70.62 and loss3: 0.00\n",
      "Epoch [2454], train_loss: 612.74 with loss1: 542.11, loss2: 70.62 and loss3: 0.00\n",
      "Epoch [2455], train_loss: 611.91 with loss1: 541.27, loss2: 70.64 and loss3: 0.00\n",
      "Epoch [2456], train_loss: 621.22 with loss1: 550.58, loss2: 70.64 and loss3: 0.00\n",
      "Epoch [2457], train_loss: 612.35 with loss1: 541.75, loss2: 70.60 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2458], train_loss: 614.83 with loss1: 544.28, loss2: 70.55 and loss3: 0.00\n",
      "Epoch [2459], train_loss: 614.73 with loss1: 544.07, loss2: 70.66 and loss3: 0.00\n",
      "Epoch [2460], train_loss: 614.27 with loss1: 543.67, loss2: 70.59 and loss3: 0.00\n",
      "Epoch [2461], train_loss: 611.31 with loss1: 540.70, loss2: 70.61 and loss3: 0.00\n",
      "Epoch [2462], train_loss: 615.85 with loss1: 545.28, loss2: 70.57 and loss3: 0.00\n",
      "Epoch [2463], train_loss: 612.17 with loss1: 541.63, loss2: 70.55 and loss3: 0.00\n",
      "Epoch [2464], train_loss: 614.36 with loss1: 543.82, loss2: 70.54 and loss3: 0.00\n",
      "Epoch [2465], train_loss: 613.16 with loss1: 542.65, loss2: 70.51 and loss3: 0.00\n",
      "Epoch [2466], train_loss: 614.20 with loss1: 543.78, loss2: 70.42 and loss3: 0.00\n",
      "Epoch [2467], train_loss: 612.94 with loss1: 542.48, loss2: 70.45 and loss3: 0.00\n",
      "Epoch [2468], train_loss: 615.07 with loss1: 544.71, loss2: 70.36 and loss3: 0.00\n",
      "Epoch [2469], train_loss: 610.03 with loss1: 539.46, loss2: 70.57 and loss3: 0.00\n",
      "Epoch [2470], train_loss: 614.70 with loss1: 544.22, loss2: 70.49 and loss3: 0.00\n",
      "Epoch [2471], train_loss: 612.58 with loss1: 542.08, loss2: 70.50 and loss3: 0.00\n",
      "Epoch [2472], train_loss: 613.72 with loss1: 543.23, loss2: 70.49 and loss3: 0.00\n",
      "Epoch [2473], train_loss: 611.18 with loss1: 540.70, loss2: 70.48 and loss3: 0.00\n",
      "Epoch [2474], train_loss: 614.48 with loss1: 544.11, loss2: 70.38 and loss3: 0.00\n",
      "Epoch [2475], train_loss: 609.52 with loss1: 539.09, loss2: 70.43 and loss3: 0.00\n",
      "Epoch [2476], train_loss: 614.20 with loss1: 543.84, loss2: 70.37 and loss3: 0.00\n",
      "Epoch [2477], train_loss: 609.93 with loss1: 539.54, loss2: 70.39 and loss3: 0.00\n",
      "Epoch [2478], train_loss: 611.32 with loss1: 540.97, loss2: 70.35 and loss3: 0.00\n",
      "Epoch [2479], train_loss: 608.15 with loss1: 537.67, loss2: 70.48 and loss3: 0.00\n",
      "Epoch [2480], train_loss: 611.71 with loss1: 541.40, loss2: 70.31 and loss3: 0.00\n",
      "Epoch [2481], train_loss: 609.64 with loss1: 539.26, loss2: 70.38 and loss3: 0.00\n",
      "Epoch [2482], train_loss: 609.44 with loss1: 539.08, loss2: 70.36 and loss3: 0.00\n",
      "Epoch [2483], train_loss: 604.90 with loss1: 534.51, loss2: 70.39 and loss3: 0.00\n",
      "Epoch [2484], train_loss: 606.42 with loss1: 536.06, loss2: 70.36 and loss3: 0.00\n",
      "Epoch [2485], train_loss: 603.48 with loss1: 533.11, loss2: 70.37 and loss3: 0.00\n",
      "Epoch [2486], train_loss: 605.19 with loss1: 534.84, loss2: 70.35 and loss3: 0.00\n",
      "Epoch [2487], train_loss: 601.39 with loss1: 530.98, loss2: 70.40 and loss3: 0.00\n",
      "Epoch [2488], train_loss: 602.74 with loss1: 532.37, loss2: 70.37 and loss3: 0.00\n",
      "Epoch [2489], train_loss: 597.97 with loss1: 527.68, loss2: 70.28 and loss3: 0.00\n",
      "Epoch [2490], train_loss: 599.04 with loss1: 528.84, loss2: 70.20 and loss3: 0.00\n",
      "Epoch [2491], train_loss: 596.96 with loss1: 526.66, loss2: 70.30 and loss3: 0.00\n",
      "Epoch [2492], train_loss: 598.28 with loss1: 528.08, loss2: 70.20 and loss3: 0.00\n",
      "Epoch [2493], train_loss: 594.35 with loss1: 524.08, loss2: 70.27 and loss3: 0.00\n",
      "Epoch [2494], train_loss: 593.65 with loss1: 523.43, loss2: 70.21 and loss3: 0.00\n",
      "Epoch [2495], train_loss: 595.28 with loss1: 525.01, loss2: 70.27 and loss3: 0.00\n",
      "Epoch [2496], train_loss: 595.67 with loss1: 525.49, loss2: 70.19 and loss3: 0.00\n",
      "Epoch [2497], train_loss: 590.47 with loss1: 520.25, loss2: 70.22 and loss3: 0.00\n",
      "Epoch [2498], train_loss: 592.26 with loss1: 522.09, loss2: 70.17 and loss3: 0.00\n",
      "Epoch [2499], train_loss: 588.69 with loss1: 518.49, loss2: 70.20 and loss3: 0.00\n",
      "Epoch [2500], train_loss: 587.99 with loss1: 517.86, loss2: 70.14 and loss3: 0.00\n",
      "Epoch [2501], train_loss: 589.11 with loss1: 518.94, loss2: 70.17 and loss3: 0.00\n",
      "Epoch [2502], train_loss: 585.93 with loss1: 515.85, loss2: 70.08 and loss3: 0.00\n",
      "Epoch [2503], train_loss: 584.79 with loss1: 514.65, loss2: 70.15 and loss3: 0.00\n",
      "Epoch [2504], train_loss: 585.66 with loss1: 515.56, loss2: 70.09 and loss3: 0.00\n",
      "Epoch [2505], train_loss: 584.34 with loss1: 514.19, loss2: 70.16 and loss3: 0.00\n",
      "Epoch [2506], train_loss: 584.37 with loss1: 514.26, loss2: 70.11 and loss3: 0.00\n",
      "Epoch [2507], train_loss: 584.59 with loss1: 514.52, loss2: 70.07 and loss3: 0.00\n",
      "Epoch [2508], train_loss: 586.60 with loss1: 516.44, loss2: 70.15 and loss3: 0.00\n",
      "Epoch [2509], train_loss: 583.22 with loss1: 513.18, loss2: 70.04 and loss3: 0.00\n",
      "Epoch [2510], train_loss: 582.74 with loss1: 512.75, loss2: 69.99 and loss3: 0.00\n",
      "Epoch [2511], train_loss: 581.84 with loss1: 511.86, loss2: 69.98 and loss3: 0.00\n",
      "Epoch [2512], train_loss: 581.67 with loss1: 511.70, loss2: 69.98 and loss3: 0.00\n",
      "Epoch [2513], train_loss: 581.37 with loss1: 511.39, loss2: 69.98 and loss3: 0.00\n",
      "Epoch [2514], train_loss: 581.85 with loss1: 511.92, loss2: 69.93 and loss3: 0.00\n",
      "Epoch [2515], train_loss: 582.61 with loss1: 512.67, loss2: 69.94 and loss3: 0.00\n",
      "Epoch [2516], train_loss: 581.98 with loss1: 512.04, loss2: 69.94 and loss3: 0.00\n",
      "Epoch [2517], train_loss: 581.09 with loss1: 511.09, loss2: 70.00 and loss3: 0.00\n",
      "Epoch [2518], train_loss: 584.09 with loss1: 514.22, loss2: 69.86 and loss3: 0.00\n",
      "Epoch [2519], train_loss: 581.14 with loss1: 511.25, loss2: 69.89 and loss3: 0.00\n",
      "Epoch [2520], train_loss: 582.93 with loss1: 513.01, loss2: 69.92 and loss3: 0.00\n",
      "Epoch [2521], train_loss: 581.05 with loss1: 511.15, loss2: 69.90 and loss3: 0.00\n",
      "Epoch [2522], train_loss: 583.31 with loss1: 513.47, loss2: 69.84 and loss3: 0.00\n",
      "Epoch [2523], train_loss: 581.42 with loss1: 511.53, loss2: 69.89 and loss3: 0.00\n",
      "Epoch [2524], train_loss: 580.46 with loss1: 510.57, loss2: 69.89 and loss3: 0.00\n",
      "Epoch [2525], train_loss: 581.72 with loss1: 511.84, loss2: 69.87 and loss3: 0.00\n",
      "Epoch [2526], train_loss: 580.83 with loss1: 510.94, loss2: 69.89 and loss3: 0.00\n",
      "Epoch [2527], train_loss: 581.62 with loss1: 511.66, loss2: 69.96 and loss3: 0.00\n",
      "Epoch [2528], train_loss: 582.39 with loss1: 512.53, loss2: 69.85 and loss3: 0.00\n",
      "Epoch [2529], train_loss: 582.64 with loss1: 512.87, loss2: 69.78 and loss3: 0.00\n",
      "Epoch [2530], train_loss: 580.63 with loss1: 510.78, loss2: 69.84 and loss3: 0.00\n",
      "Epoch [2531], train_loss: 582.46 with loss1: 512.71, loss2: 69.75 and loss3: 0.00\n",
      "Epoch [2532], train_loss: 581.44 with loss1: 511.66, loss2: 69.78 and loss3: 0.00\n",
      "Epoch [2533], train_loss: 580.93 with loss1: 511.20, loss2: 69.73 and loss3: 0.00\n",
      "Epoch [2534], train_loss: 583.75 with loss1: 513.99, loss2: 69.77 and loss3: 0.00\n",
      "Epoch [2535], train_loss: 581.71 with loss1: 511.91, loss2: 69.79 and loss3: 0.00\n",
      "Epoch [2536], train_loss: 583.60 with loss1: 513.89, loss2: 69.71 and loss3: 0.00\n",
      "Epoch [2537], train_loss: 581.15 with loss1: 511.40, loss2: 69.75 and loss3: 0.00\n",
      "Epoch [2538], train_loss: 583.55 with loss1: 513.78, loss2: 69.77 and loss3: 0.00\n",
      "Epoch [2539], train_loss: 583.72 with loss1: 513.97, loss2: 69.75 and loss3: 0.00\n",
      "Epoch [2540], train_loss: 585.68 with loss1: 515.95, loss2: 69.73 and loss3: 0.00\n",
      "Epoch [2541], train_loss: 583.64 with loss1: 513.87, loss2: 69.76 and loss3: 0.00\n",
      "Epoch [2542], train_loss: 583.01 with loss1: 513.34, loss2: 69.67 and loss3: 0.00\n",
      "Epoch [2543], train_loss: 585.59 with loss1: 515.86, loss2: 69.73 and loss3: 0.00\n",
      "Epoch [2544], train_loss: 586.00 with loss1: 516.31, loss2: 69.69 and loss3: 0.00\n",
      "Epoch [2545], train_loss: 586.56 with loss1: 516.88, loss2: 69.68 and loss3: 0.00\n",
      "Epoch [2546], train_loss: 586.97 with loss1: 517.42, loss2: 69.55 and loss3: 0.00\n",
      "Epoch [2547], train_loss: 584.75 with loss1: 515.16, loss2: 69.60 and loss3: 0.00\n",
      "Epoch [2548], train_loss: 587.42 with loss1: 517.83, loss2: 69.59 and loss3: 0.00\n",
      "Epoch [2549], train_loss: 586.10 with loss1: 516.49, loss2: 69.61 and loss3: 0.00\n",
      "Epoch [2550], train_loss: 588.83 with loss1: 519.33, loss2: 69.50 and loss3: 0.00\n",
      "Epoch [2551], train_loss: 589.05 with loss1: 519.51, loss2: 69.54 and loss3: 0.00\n",
      "Epoch [2552], train_loss: 590.85 with loss1: 521.30, loss2: 69.54 and loss3: 0.00\n",
      "Epoch [2553], train_loss: 588.36 with loss1: 518.80, loss2: 69.56 and loss3: 0.00\n",
      "Epoch [2554], train_loss: 590.48 with loss1: 520.93, loss2: 69.55 and loss3: 0.00\n",
      "Epoch [2555], train_loss: 589.04 with loss1: 519.52, loss2: 69.51 and loss3: 0.00\n",
      "Epoch [2556], train_loss: 594.43 with loss1: 524.84, loss2: 69.60 and loss3: 0.00\n",
      "Epoch [2557], train_loss: 591.14 with loss1: 521.49, loss2: 69.65 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2558], train_loss: 592.38 with loss1: 522.74, loss2: 69.64 and loss3: 0.00\n",
      "Epoch [2559], train_loss: 589.92 with loss1: 520.40, loss2: 69.52 and loss3: 0.00\n",
      "Epoch [2560], train_loss: 592.52 with loss1: 522.97, loss2: 69.55 and loss3: 0.00\n",
      "Epoch [2561], train_loss: 590.06 with loss1: 520.52, loss2: 69.53 and loss3: 0.00\n",
      "Epoch [2562], train_loss: 592.09 with loss1: 522.63, loss2: 69.46 and loss3: 0.00\n",
      "Epoch [2563], train_loss: 591.65 with loss1: 522.22, loss2: 69.43 and loss3: 0.00\n",
      "Epoch [2564], train_loss: 592.90 with loss1: 523.48, loss2: 69.42 and loss3: 0.00\n",
      "Epoch [2565], train_loss: 593.25 with loss1: 523.79, loss2: 69.46 and loss3: 0.00\n",
      "Epoch [2566], train_loss: 593.73 with loss1: 524.26, loss2: 69.47 and loss3: 0.00\n",
      "Epoch [2567], train_loss: 591.32 with loss1: 521.96, loss2: 69.37 and loss3: 0.00\n",
      "Epoch [2568], train_loss: 593.15 with loss1: 523.74, loss2: 69.41 and loss3: 0.00\n",
      "Epoch [2569], train_loss: 593.53 with loss1: 524.13, loss2: 69.40 and loss3: 0.00\n",
      "Epoch [2570], train_loss: 593.59 with loss1: 524.21, loss2: 69.37 and loss3: 0.00\n",
      "Epoch [2571], train_loss: 592.96 with loss1: 523.63, loss2: 69.33 and loss3: 0.00\n",
      "Epoch [2572], train_loss: 595.25 with loss1: 525.92, loss2: 69.33 and loss3: 0.00\n",
      "Epoch [2573], train_loss: 591.56 with loss1: 522.26, loss2: 69.29 and loss3: 0.00\n",
      "Epoch [2574], train_loss: 594.40 with loss1: 525.09, loss2: 69.31 and loss3: 0.00\n",
      "Epoch [2575], train_loss: 591.25 with loss1: 521.98, loss2: 69.27 and loss3: 0.00\n",
      "Epoch [2576], train_loss: 591.30 with loss1: 521.99, loss2: 69.31 and loss3: 0.00\n",
      "Epoch [2577], train_loss: 590.98 with loss1: 521.66, loss2: 69.32 and loss3: 0.00\n",
      "Epoch [2578], train_loss: 591.84 with loss1: 522.57, loss2: 69.27 and loss3: 0.00\n",
      "Epoch [2579], train_loss: 592.33 with loss1: 523.06, loss2: 69.27 and loss3: 0.00\n",
      "Epoch [2580], train_loss: 593.31 with loss1: 524.09, loss2: 69.21 and loss3: 0.00\n",
      "Epoch [2581], train_loss: 591.63 with loss1: 522.40, loss2: 69.24 and loss3: 0.00\n",
      "Epoch [2582], train_loss: 593.85 with loss1: 524.65, loss2: 69.20 and loss3: 0.00\n",
      "Epoch [2583], train_loss: 588.98 with loss1: 519.78, loss2: 69.19 and loss3: 0.00\n",
      "Epoch [2584], train_loss: 593.42 with loss1: 524.12, loss2: 69.30 and loss3: 0.00\n",
      "Epoch [2585], train_loss: 592.90 with loss1: 523.63, loss2: 69.26 and loss3: 0.00\n",
      "Epoch [2586], train_loss: 592.26 with loss1: 523.11, loss2: 69.15 and loss3: 0.00\n",
      "Epoch [2587], train_loss: 588.32 with loss1: 519.14, loss2: 69.17 and loss3: 0.00\n",
      "Epoch [2588], train_loss: 589.37 with loss1: 520.14, loss2: 69.22 and loss3: 0.00\n",
      "Epoch [2589], train_loss: 585.82 with loss1: 516.67, loss2: 69.15 and loss3: 0.00\n",
      "Epoch [2590], train_loss: 586.40 with loss1: 517.25, loss2: 69.15 and loss3: 0.00\n",
      "Epoch [2591], train_loss: 585.39 with loss1: 516.27, loss2: 69.11 and loss3: 0.00\n",
      "Epoch [2592], train_loss: 585.06 with loss1: 515.99, loss2: 69.07 and loss3: 0.00\n",
      "Epoch [2593], train_loss: 582.70 with loss1: 513.61, loss2: 69.09 and loss3: 0.00\n",
      "Epoch [2594], train_loss: 584.74 with loss1: 515.64, loss2: 69.09 and loss3: 0.00\n",
      "Epoch [2595], train_loss: 579.69 with loss1: 510.67, loss2: 69.02 and loss3: 0.00\n",
      "Epoch [2596], train_loss: 582.76 with loss1: 513.66, loss2: 69.11 and loss3: 0.00\n",
      "Epoch [2597], train_loss: 578.29 with loss1: 509.29, loss2: 69.00 and loss3: 0.00\n",
      "Epoch [2598], train_loss: 580.05 with loss1: 510.98, loss2: 69.07 and loss3: 0.00\n",
      "Epoch [2599], train_loss: 577.25 with loss1: 508.22, loss2: 69.03 and loss3: 0.00\n",
      "Epoch [2600], train_loss: 577.31 with loss1: 508.30, loss2: 69.01 and loss3: 0.00\n",
      "Epoch [2601], train_loss: 578.28 with loss1: 509.33, loss2: 68.95 and loss3: 0.00\n",
      "Epoch [2602], train_loss: 577.65 with loss1: 508.59, loss2: 69.06 and loss3: 0.00\n",
      "Epoch [2603], train_loss: 575.96 with loss1: 507.05, loss2: 68.91 and loss3: 0.00\n",
      "Epoch [2604], train_loss: 576.22 with loss1: 507.16, loss2: 69.06 and loss3: 0.00\n",
      "Epoch [2605], train_loss: 572.34 with loss1: 503.41, loss2: 68.93 and loss3: 0.00\n",
      "Epoch [2606], train_loss: 573.17 with loss1: 504.18, loss2: 68.99 and loss3: 0.00\n",
      "Epoch [2607], train_loss: 572.60 with loss1: 503.57, loss2: 69.02 and loss3: 0.00\n",
      "Epoch [2608], train_loss: 573.37 with loss1: 504.42, loss2: 68.96 and loss3: 0.00\n",
      "Epoch [2609], train_loss: 569.41 with loss1: 500.57, loss2: 68.84 and loss3: 0.00\n",
      "Epoch [2610], train_loss: 570.87 with loss1: 501.98, loss2: 68.90 and loss3: 0.00\n",
      "Epoch [2611], train_loss: 570.65 with loss1: 501.77, loss2: 68.88 and loss3: 0.00\n",
      "Epoch [2612], train_loss: 571.48 with loss1: 502.58, loss2: 68.90 and loss3: 0.00\n",
      "Epoch [2613], train_loss: 570.00 with loss1: 501.15, loss2: 68.85 and loss3: 0.00\n",
      "Epoch [2614], train_loss: 570.53 with loss1: 501.67, loss2: 68.86 and loss3: 0.00\n",
      "Epoch [2615], train_loss: 570.18 with loss1: 501.28, loss2: 68.89 and loss3: 0.00\n",
      "Epoch [2616], train_loss: 571.73 with loss1: 502.92, loss2: 68.82 and loss3: 0.00\n",
      "Epoch [2617], train_loss: 569.97 with loss1: 501.07, loss2: 68.90 and loss3: 0.00\n",
      "Epoch [2618], train_loss: 569.84 with loss1: 500.94, loss2: 68.90 and loss3: 0.00\n",
      "Epoch [2619], train_loss: 569.03 with loss1: 500.17, loss2: 68.86 and loss3: 0.00\n",
      "Epoch [2620], train_loss: 570.19 with loss1: 501.28, loss2: 68.92 and loss3: 0.00\n",
      "Epoch [2621], train_loss: 567.22 with loss1: 498.38, loss2: 68.84 and loss3: 0.00\n",
      "Epoch [2622], train_loss: 568.02 with loss1: 499.20, loss2: 68.83 and loss3: 0.00\n",
      "Epoch [2623], train_loss: 568.12 with loss1: 499.38, loss2: 68.74 and loss3: 0.00\n",
      "Epoch [2624], train_loss: 567.33 with loss1: 498.55, loss2: 68.79 and loss3: 0.00\n",
      "Epoch [2625], train_loss: 566.12 with loss1: 497.43, loss2: 68.69 and loss3: 0.00\n",
      "Epoch [2626], train_loss: 567.10 with loss1: 498.33, loss2: 68.77 and loss3: 0.00\n",
      "Epoch [2627], train_loss: 566.38 with loss1: 497.63, loss2: 68.74 and loss3: 0.00\n",
      "Epoch [2628], train_loss: 566.13 with loss1: 497.37, loss2: 68.76 and loss3: 0.00\n",
      "Epoch [2629], train_loss: 565.10 with loss1: 496.43, loss2: 68.66 and loss3: 0.00\n",
      "Epoch [2630], train_loss: 566.33 with loss1: 497.59, loss2: 68.74 and loss3: 0.00\n",
      "Epoch [2631], train_loss: 568.29 with loss1: 499.61, loss2: 68.68 and loss3: 0.00\n",
      "Epoch [2632], train_loss: 565.27 with loss1: 496.59, loss2: 68.68 and loss3: 0.00\n",
      "Epoch [2633], train_loss: 563.53 with loss1: 494.91, loss2: 68.62 and loss3: 0.00\n",
      "Epoch [2634], train_loss: 565.14 with loss1: 496.50, loss2: 68.64 and loss3: 0.00\n",
      "Epoch [2635], train_loss: 565.46 with loss1: 496.83, loss2: 68.63 and loss3: 0.00\n",
      "Epoch [2636], train_loss: 564.18 with loss1: 495.49, loss2: 68.69 and loss3: 0.00\n",
      "Epoch [2637], train_loss: 564.86 with loss1: 496.23, loss2: 68.63 and loss3: 0.00\n",
      "Epoch [2638], train_loss: 565.20 with loss1: 496.53, loss2: 68.68 and loss3: 0.00\n",
      "Epoch [2639], train_loss: 564.19 with loss1: 495.56, loss2: 68.63 and loss3: 0.00\n",
      "Epoch [2640], train_loss: 563.66 with loss1: 495.08, loss2: 68.58 and loss3: 0.00\n",
      "Epoch [2641], train_loss: 564.31 with loss1: 495.71, loss2: 68.60 and loss3: 0.00\n",
      "Epoch [2642], train_loss: 564.12 with loss1: 495.53, loss2: 68.59 and loss3: 0.00\n",
      "Epoch [2643], train_loss: 564.14 with loss1: 495.51, loss2: 68.62 and loss3: 0.00\n",
      "Epoch [2644], train_loss: 565.16 with loss1: 496.67, loss2: 68.49 and loss3: 0.00\n",
      "Epoch [2645], train_loss: 564.59 with loss1: 495.96, loss2: 68.62 and loss3: 0.00\n",
      "Epoch [2646], train_loss: 566.97 with loss1: 498.46, loss2: 68.51 and loss3: 0.00\n",
      "Epoch [2647], train_loss: 565.61 with loss1: 497.11, loss2: 68.50 and loss3: 0.00\n",
      "Epoch [2648], train_loss: 567.12 with loss1: 498.63, loss2: 68.49 and loss3: 0.00\n",
      "Epoch [2649], train_loss: 565.82 with loss1: 497.39, loss2: 68.43 and loss3: 0.00\n",
      "Epoch [2650], train_loss: 568.30 with loss1: 499.83, loss2: 68.47 and loss3: 0.00\n",
      "Epoch [2651], train_loss: 565.78 with loss1: 497.32, loss2: 68.46 and loss3: 0.00\n",
      "Epoch [2652], train_loss: 569.13 with loss1: 500.64, loss2: 68.49 and loss3: 0.00\n",
      "Epoch [2653], train_loss: 568.64 with loss1: 500.15, loss2: 68.49 and loss3: 0.00\n",
      "Epoch [2654], train_loss: 570.91 with loss1: 502.40, loss2: 68.51 and loss3: 0.00\n",
      "Epoch [2655], train_loss: 569.40 with loss1: 501.01, loss2: 68.40 and loss3: 0.00\n",
      "Epoch [2656], train_loss: 569.16 with loss1: 500.73, loss2: 68.43 and loss3: 0.00\n",
      "Epoch [2657], train_loss: 567.83 with loss1: 499.41, loss2: 68.43 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2658], train_loss: 572.06 with loss1: 503.65, loss2: 68.40 and loss3: 0.00\n",
      "Epoch [2659], train_loss: 571.23 with loss1: 502.84, loss2: 68.38 and loss3: 0.00\n",
      "Epoch [2660], train_loss: 572.18 with loss1: 503.78, loss2: 68.40 and loss3: 0.00\n",
      "Epoch [2661], train_loss: 570.82 with loss1: 502.53, loss2: 68.30 and loss3: 0.00\n",
      "Epoch [2662], train_loss: 574.50 with loss1: 506.17, loss2: 68.33 and loss3: 0.00\n",
      "Epoch [2663], train_loss: 573.36 with loss1: 505.08, loss2: 68.28 and loss3: 0.00\n",
      "Epoch [2664], train_loss: 575.85 with loss1: 507.51, loss2: 68.34 and loss3: 0.00\n",
      "Epoch [2665], train_loss: 576.88 with loss1: 508.58, loss2: 68.29 and loss3: 0.00\n",
      "Epoch [2666], train_loss: 579.97 with loss1: 511.63, loss2: 68.34 and loss3: 0.00\n",
      "Epoch [2667], train_loss: 582.42 with loss1: 514.06, loss2: 68.35 and loss3: 0.00\n",
      "Epoch [2668], train_loss: 584.05 with loss1: 515.75, loss2: 68.30 and loss3: 0.00\n",
      "Epoch [2669], train_loss: 583.49 with loss1: 515.11, loss2: 68.37 and loss3: 0.00\n",
      "Epoch [2670], train_loss: 586.07 with loss1: 517.77, loss2: 68.30 and loss3: 0.00\n",
      "Epoch [2671], train_loss: 585.61 with loss1: 517.34, loss2: 68.27 and loss3: 0.00\n",
      "Epoch [2672], train_loss: 588.59 with loss1: 520.30, loss2: 68.29 and loss3: 0.00\n",
      "Epoch [2673], train_loss: 585.86 with loss1: 517.64, loss2: 68.22 and loss3: 0.00\n",
      "Epoch [2674], train_loss: 592.49 with loss1: 524.22, loss2: 68.27 and loss3: 0.00\n",
      "Epoch [2675], train_loss: 586.22 with loss1: 518.07, loss2: 68.15 and loss3: 0.00\n",
      "Epoch [2676], train_loss: 589.00 with loss1: 520.69, loss2: 68.31 and loss3: 0.00\n",
      "Epoch [2677], train_loss: 587.65 with loss1: 519.36, loss2: 68.29 and loss3: 0.00\n",
      "Epoch [2678], train_loss: 590.01 with loss1: 521.80, loss2: 68.21 and loss3: 0.00\n",
      "Epoch [2679], train_loss: 589.91 with loss1: 521.77, loss2: 68.14 and loss3: 0.00\n",
      "Epoch [2680], train_loss: 593.82 with loss1: 525.60, loss2: 68.22 and loss3: 0.00\n",
      "Epoch [2681], train_loss: 590.98 with loss1: 522.79, loss2: 68.19 and loss3: 0.00\n",
      "Epoch [2682], train_loss: 595.16 with loss1: 526.94, loss2: 68.23 and loss3: 0.00\n",
      "Epoch [2683], train_loss: 596.82 with loss1: 528.69, loss2: 68.13 and loss3: 0.00\n",
      "Epoch [2684], train_loss: 598.73 with loss1: 530.58, loss2: 68.15 and loss3: 0.00\n",
      "Epoch [2685], train_loss: 594.98 with loss1: 526.81, loss2: 68.17 and loss3: 0.00\n",
      "Epoch [2686], train_loss: 600.31 with loss1: 532.14, loss2: 68.17 and loss3: 0.00\n",
      "Epoch [2687], train_loss: 596.71 with loss1: 528.62, loss2: 68.10 and loss3: 0.00\n",
      "Epoch [2688], train_loss: 602.00 with loss1: 533.85, loss2: 68.15 and loss3: 0.00\n",
      "Epoch [2689], train_loss: 599.44 with loss1: 531.34, loss2: 68.11 and loss3: 0.00\n",
      "Epoch [2690], train_loss: 603.83 with loss1: 535.76, loss2: 68.07 and loss3: 0.00\n",
      "Epoch [2691], train_loss: 599.38 with loss1: 531.23, loss2: 68.15 and loss3: 0.00\n",
      "Epoch [2692], train_loss: 604.81 with loss1: 536.80, loss2: 68.00 and loss3: 0.00\n",
      "Epoch [2693], train_loss: 599.67 with loss1: 531.61, loss2: 68.05 and loss3: 0.00\n",
      "Epoch [2694], train_loss: 603.35 with loss1: 535.28, loss2: 68.06 and loss3: 0.00\n",
      "Epoch [2695], train_loss: 598.73 with loss1: 530.78, loss2: 67.95 and loss3: 0.00\n",
      "Epoch [2696], train_loss: 600.79 with loss1: 532.80, loss2: 68.00 and loss3: 0.00\n",
      "Epoch [2697], train_loss: 595.94 with loss1: 527.94, loss2: 67.99 and loss3: 0.00\n",
      "Epoch [2698], train_loss: 599.37 with loss1: 531.35, loss2: 68.02 and loss3: 0.00\n",
      "Epoch [2699], train_loss: 594.17 with loss1: 526.18, loss2: 67.99 and loss3: 0.00\n",
      "Epoch [2700], train_loss: 594.59 with loss1: 526.55, loss2: 68.04 and loss3: 0.00\n",
      "Epoch [2701], train_loss: 591.00 with loss1: 523.01, loss2: 67.99 and loss3: 0.00\n",
      "Epoch [2702], train_loss: 590.42 with loss1: 522.47, loss2: 67.95 and loss3: 0.00\n",
      "Epoch [2703], train_loss: 586.96 with loss1: 518.96, loss2: 68.00 and loss3: 0.00\n",
      "Epoch [2704], train_loss: 588.05 with loss1: 520.08, loss2: 67.97 and loss3: 0.00\n",
      "Epoch [2705], train_loss: 583.52 with loss1: 515.60, loss2: 67.92 and loss3: 0.00\n",
      "Epoch [2706], train_loss: 586.59 with loss1: 518.69, loss2: 67.90 and loss3: 0.00\n",
      "Epoch [2707], train_loss: 581.55 with loss1: 513.67, loss2: 67.88 and loss3: 0.00\n",
      "Epoch [2708], train_loss: 584.41 with loss1: 516.54, loss2: 67.87 and loss3: 0.00\n",
      "Epoch [2709], train_loss: 579.01 with loss1: 511.14, loss2: 67.87 and loss3: 0.00\n",
      "Epoch [2710], train_loss: 580.60 with loss1: 512.71, loss2: 67.89 and loss3: 0.00\n",
      "Epoch [2711], train_loss: 575.32 with loss1: 507.49, loss2: 67.83 and loss3: 0.00\n",
      "Epoch [2712], train_loss: 574.97 with loss1: 507.06, loss2: 67.91 and loss3: 0.00\n",
      "Epoch [2713], train_loss: 572.31 with loss1: 504.50, loss2: 67.81 and loss3: 0.00\n",
      "Epoch [2714], train_loss: 573.26 with loss1: 505.41, loss2: 67.85 and loss3: 0.00\n",
      "Epoch [2715], train_loss: 570.02 with loss1: 502.23, loss2: 67.79 and loss3: 0.00\n",
      "Epoch [2716], train_loss: 570.42 with loss1: 502.59, loss2: 67.83 and loss3: 0.00\n",
      "Epoch [2717], train_loss: 568.38 with loss1: 500.56, loss2: 67.82 and loss3: 0.00\n",
      "Epoch [2718], train_loss: 569.65 with loss1: 501.84, loss2: 67.81 and loss3: 0.00\n",
      "Epoch [2719], train_loss: 568.79 with loss1: 501.07, loss2: 67.72 and loss3: 0.00\n",
      "Epoch [2720], train_loss: 570.73 with loss1: 502.94, loss2: 67.79 and loss3: 0.00\n",
      "Epoch [2721], train_loss: 565.83 with loss1: 498.09, loss2: 67.74 and loss3: 0.00\n",
      "Epoch [2722], train_loss: 566.64 with loss1: 498.81, loss2: 67.82 and loss3: 0.00\n",
      "Epoch [2723], train_loss: 566.24 with loss1: 498.54, loss2: 67.70 and loss3: 0.00\n",
      "Epoch [2724], train_loss: 567.05 with loss1: 499.28, loss2: 67.77 and loss3: 0.00\n",
      "Epoch [2725], train_loss: 565.86 with loss1: 498.14, loss2: 67.71 and loss3: 0.00\n",
      "Epoch [2726], train_loss: 568.59 with loss1: 500.72, loss2: 67.87 and loss3: 0.00\n",
      "Epoch [2727], train_loss: 566.16 with loss1: 498.46, loss2: 67.71 and loss3: 0.00\n",
      "Epoch [2728], train_loss: 569.15 with loss1: 501.40, loss2: 67.75 and loss3: 0.00\n",
      "Epoch [2729], train_loss: 565.15 with loss1: 497.43, loss2: 67.71 and loss3: 0.00\n",
      "Epoch [2730], train_loss: 564.04 with loss1: 496.23, loss2: 67.80 and loss3: 0.00\n",
      "Epoch [2731], train_loss: 562.81 with loss1: 495.09, loss2: 67.72 and loss3: 0.00\n",
      "Epoch [2732], train_loss: 564.93 with loss1: 497.19, loss2: 67.74 and loss3: 0.00\n",
      "Epoch [2733], train_loss: 563.17 with loss1: 495.55, loss2: 67.61 and loss3: 0.00\n",
      "Epoch [2734], train_loss: 565.23 with loss1: 497.53, loss2: 67.70 and loss3: 0.00\n",
      "Epoch [2735], train_loss: 562.95 with loss1: 495.26, loss2: 67.69 and loss3: 0.00\n",
      "Epoch [2736], train_loss: 564.71 with loss1: 496.98, loss2: 67.73 and loss3: 0.00\n",
      "Epoch [2737], train_loss: 562.88 with loss1: 495.25, loss2: 67.63 and loss3: 0.00\n",
      "Epoch [2738], train_loss: 567.57 with loss1: 499.90, loss2: 67.67 and loss3: 0.00\n",
      "Epoch [2739], train_loss: 564.77 with loss1: 497.08, loss2: 67.68 and loss3: 0.00\n",
      "Epoch [2740], train_loss: 567.19 with loss1: 499.53, loss2: 67.66 and loss3: 0.00\n",
      "Epoch [2741], train_loss: 565.95 with loss1: 498.40, loss2: 67.55 and loss3: 0.00\n",
      "Epoch [2742], train_loss: 565.19 with loss1: 497.44, loss2: 67.75 and loss3: 0.00\n",
      "Epoch [2743], train_loss: 563.53 with loss1: 495.97, loss2: 67.56 and loss3: 0.00\n",
      "Epoch [2744], train_loss: 564.45 with loss1: 496.78, loss2: 67.67 and loss3: 0.00\n",
      "Epoch [2745], train_loss: 562.56 with loss1: 495.00, loss2: 67.55 and loss3: 0.00\n",
      "Epoch [2746], train_loss: 564.19 with loss1: 496.69, loss2: 67.50 and loss3: 0.00\n",
      "Epoch [2747], train_loss: 562.16 with loss1: 494.62, loss2: 67.54 and loss3: 0.00\n",
      "Epoch [2748], train_loss: 563.33 with loss1: 495.74, loss2: 67.59 and loss3: 0.00\n",
      "Epoch [2749], train_loss: 561.39 with loss1: 493.86, loss2: 67.53 and loss3: 0.00\n",
      "Epoch [2750], train_loss: 562.44 with loss1: 494.80, loss2: 67.64 and loss3: 0.00\n",
      "Epoch [2751], train_loss: 561.96 with loss1: 494.44, loss2: 67.51 and loss3: 0.00\n",
      "Epoch [2752], train_loss: 560.39 with loss1: 492.80, loss2: 67.59 and loss3: 0.00\n",
      "Epoch [2753], train_loss: 559.44 with loss1: 491.96, loss2: 67.48 and loss3: 0.00\n",
      "Epoch [2754], train_loss: 560.35 with loss1: 492.80, loss2: 67.55 and loss3: 0.00\n",
      "Epoch [2755], train_loss: 559.08 with loss1: 491.59, loss2: 67.48 and loss3: 0.00\n",
      "Epoch [2756], train_loss: 560.89 with loss1: 493.35, loss2: 67.54 and loss3: 0.00\n",
      "Epoch [2757], train_loss: 557.97 with loss1: 490.50, loss2: 67.47 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2758], train_loss: 559.86 with loss1: 492.29, loss2: 67.56 and loss3: 0.00\n",
      "Epoch [2759], train_loss: 556.77 with loss1: 489.35, loss2: 67.43 and loss3: 0.00\n",
      "Epoch [2760], train_loss: 556.53 with loss1: 488.99, loss2: 67.54 and loss3: 0.00\n",
      "Epoch [2761], train_loss: 557.08 with loss1: 489.61, loss2: 67.47 and loss3: 0.00\n",
      "Epoch [2762], train_loss: 556.96 with loss1: 489.44, loss2: 67.53 and loss3: 0.00\n",
      "Epoch [2763], train_loss: 553.23 with loss1: 485.78, loss2: 67.45 and loss3: 0.00\n",
      "Epoch [2764], train_loss: 557.49 with loss1: 489.95, loss2: 67.53 and loss3: 0.00\n",
      "Epoch [2765], train_loss: 552.84 with loss1: 485.47, loss2: 67.38 and loss3: 0.00\n",
      "Epoch [2766], train_loss: 554.69 with loss1: 487.30, loss2: 67.39 and loss3: 0.00\n",
      "Epoch [2767], train_loss: 551.29 with loss1: 483.92, loss2: 67.37 and loss3: 0.00\n",
      "Epoch [2768], train_loss: 550.36 with loss1: 482.91, loss2: 67.45 and loss3: 0.00\n",
      "Epoch [2769], train_loss: 548.72 with loss1: 481.40, loss2: 67.32 and loss3: 0.00\n",
      "Epoch [2770], train_loss: 549.74 with loss1: 482.33, loss2: 67.41 and loss3: 0.00\n",
      "Epoch [2771], train_loss: 548.27 with loss1: 480.98, loss2: 67.30 and loss3: 0.00\n",
      "Epoch [2772], train_loss: 546.68 with loss1: 479.32, loss2: 67.36 and loss3: 0.00\n",
      "Epoch [2773], train_loss: 544.07 with loss1: 476.73, loss2: 67.34 and loss3: 0.00\n",
      "Epoch [2774], train_loss: 546.10 with loss1: 478.70, loss2: 67.40 and loss3: 0.00\n",
      "Epoch [2775], train_loss: 543.97 with loss1: 476.70, loss2: 67.27 and loss3: 0.00\n",
      "Epoch [2776], train_loss: 545.61 with loss1: 478.23, loss2: 67.38 and loss3: 0.00\n",
      "Epoch [2777], train_loss: 544.85 with loss1: 477.59, loss2: 67.26 and loss3: 0.00\n",
      "Epoch [2778], train_loss: 545.23 with loss1: 477.95, loss2: 67.28 and loss3: 0.00\n",
      "Epoch [2779], train_loss: 543.93 with loss1: 476.73, loss2: 67.20 and loss3: 0.00\n",
      "Epoch [2780], train_loss: 544.94 with loss1: 477.59, loss2: 67.36 and loss3: 0.00\n",
      "Epoch [2781], train_loss: 544.81 with loss1: 477.49, loss2: 67.32 and loss3: 0.00\n",
      "Epoch [2782], train_loss: 542.78 with loss1: 475.52, loss2: 67.26 and loss3: 0.00\n",
      "Epoch [2783], train_loss: 542.26 with loss1: 475.06, loss2: 67.20 and loss3: 0.00\n",
      "Epoch [2784], train_loss: 545.80 with loss1: 478.55, loss2: 67.25 and loss3: 0.00\n",
      "Epoch [2785], train_loss: 542.14 with loss1: 475.00, loss2: 67.13 and loss3: 0.00\n",
      "Epoch [2786], train_loss: 542.87 with loss1: 475.56, loss2: 67.31 and loss3: 0.00\n",
      "Epoch [2787], train_loss: 541.14 with loss1: 473.95, loss2: 67.19 and loss3: 0.00\n",
      "Epoch [2788], train_loss: 543.00 with loss1: 475.69, loss2: 67.30 and loss3: 0.00\n",
      "Epoch [2789], train_loss: 541.59 with loss1: 474.41, loss2: 67.18 and loss3: 0.00\n",
      "Epoch [2790], train_loss: 541.96 with loss1: 474.72, loss2: 67.24 and loss3: 0.00\n",
      "Epoch [2791], train_loss: 540.68 with loss1: 473.57, loss2: 67.11 and loss3: 0.00\n",
      "Epoch [2792], train_loss: 541.71 with loss1: 474.52, loss2: 67.19 and loss3: 0.00\n",
      "Epoch [2793], train_loss: 540.74 with loss1: 473.63, loss2: 67.11 and loss3: 0.00\n",
      "Epoch [2794], train_loss: 543.27 with loss1: 476.12, loss2: 67.15 and loss3: 0.00\n",
      "Epoch [2795], train_loss: 541.12 with loss1: 474.05, loss2: 67.07 and loss3: 0.00\n",
      "Epoch [2796], train_loss: 541.85 with loss1: 474.70, loss2: 67.15 and loss3: 0.00\n",
      "Epoch [2797], train_loss: 542.57 with loss1: 475.51, loss2: 67.06 and loss3: 0.00\n",
      "Epoch [2798], train_loss: 542.73 with loss1: 475.60, loss2: 67.13 and loss3: 0.00\n",
      "Epoch [2799], train_loss: 541.25 with loss1: 474.23, loss2: 67.02 and loss3: 0.00\n",
      "Epoch [2800], train_loss: 543.39 with loss1: 476.26, loss2: 67.13 and loss3: 0.00\n",
      "Epoch [2801], train_loss: 545.79 with loss1: 478.71, loss2: 67.08 and loss3: 0.00\n",
      "Epoch [2802], train_loss: 542.82 with loss1: 475.74, loss2: 67.08 and loss3: 0.00\n",
      "Epoch [2803], train_loss: 542.65 with loss1: 475.69, loss2: 66.96 and loss3: 0.00\n",
      "Epoch [2804], train_loss: 543.45 with loss1: 476.34, loss2: 67.11 and loss3: 0.00\n",
      "Epoch [2805], train_loss: 542.80 with loss1: 475.87, loss2: 66.93 and loss3: 0.00\n",
      "Epoch [2806], train_loss: 545.40 with loss1: 478.32, loss2: 67.08 and loss3: 0.00\n",
      "Epoch [2807], train_loss: 544.33 with loss1: 477.29, loss2: 67.04 and loss3: 0.00\n",
      "Epoch [2808], train_loss: 544.12 with loss1: 477.10, loss2: 67.02 and loss3: 0.00\n",
      "Epoch [2809], train_loss: 544.66 with loss1: 477.72, loss2: 66.94 and loss3: 0.00\n",
      "Epoch [2810], train_loss: 547.14 with loss1: 480.12, loss2: 67.03 and loss3: 0.00\n",
      "Epoch [2811], train_loss: 544.88 with loss1: 477.93, loss2: 66.96 and loss3: 0.00\n",
      "Epoch [2812], train_loss: 548.87 with loss1: 481.81, loss2: 67.06 and loss3: 0.00\n",
      "Epoch [2813], train_loss: 546.54 with loss1: 479.61, loss2: 66.93 and loss3: 0.00\n",
      "Epoch [2814], train_loss: 546.29 with loss1: 479.30, loss2: 67.00 and loss3: 0.00\n",
      "Epoch [2815], train_loss: 546.60 with loss1: 479.73, loss2: 66.88 and loss3: 0.00\n",
      "Epoch [2816], train_loss: 549.17 with loss1: 482.23, loss2: 66.94 and loss3: 0.00\n",
      "Epoch [2817], train_loss: 546.13 with loss1: 479.32, loss2: 66.81 and loss3: 0.00\n",
      "Epoch [2818], train_loss: 546.87 with loss1: 479.99, loss2: 66.88 and loss3: 0.00\n",
      "Epoch [2819], train_loss: 546.47 with loss1: 479.74, loss2: 66.73 and loss3: 0.00\n",
      "Epoch [2820], train_loss: 546.95 with loss1: 480.08, loss2: 66.87 and loss3: 0.00\n",
      "Epoch [2821], train_loss: 547.34 with loss1: 480.62, loss2: 66.73 and loss3: 0.00\n",
      "Epoch [2822], train_loss: 548.23 with loss1: 481.36, loss2: 66.87 and loss3: 0.00\n",
      "Epoch [2823], train_loss: 546.51 with loss1: 479.76, loss2: 66.75 and loss3: 0.00\n",
      "Epoch [2824], train_loss: 550.00 with loss1: 483.10, loss2: 66.90 and loss3: 0.00\n",
      "Epoch [2825], train_loss: 547.54 with loss1: 480.75, loss2: 66.79 and loss3: 0.00\n",
      "Epoch [2826], train_loss: 547.60 with loss1: 480.78, loss2: 66.82 and loss3: 0.00\n",
      "Epoch [2827], train_loss: 547.45 with loss1: 480.69, loss2: 66.76 and loss3: 0.00\n",
      "Epoch [2828], train_loss: 548.80 with loss1: 481.95, loss2: 66.85 and loss3: 0.00\n",
      "Epoch [2829], train_loss: 547.38 with loss1: 480.64, loss2: 66.74 and loss3: 0.00\n",
      "Epoch [2830], train_loss: 546.82 with loss1: 479.99, loss2: 66.83 and loss3: 0.00\n",
      "Epoch [2831], train_loss: 545.95 with loss1: 479.29, loss2: 66.67 and loss3: 0.00\n",
      "Epoch [2832], train_loss: 546.79 with loss1: 480.01, loss2: 66.78 and loss3: 0.00\n",
      "Epoch [2833], train_loss: 544.20 with loss1: 477.52, loss2: 66.68 and loss3: 0.00\n",
      "Epoch [2834], train_loss: 545.36 with loss1: 478.57, loss2: 66.78 and loss3: 0.00\n",
      "Epoch [2835], train_loss: 543.81 with loss1: 477.17, loss2: 66.64 and loss3: 0.00\n",
      "Epoch [2836], train_loss: 545.25 with loss1: 478.48, loss2: 66.77 and loss3: 0.00\n",
      "Epoch [2837], train_loss: 542.92 with loss1: 476.29, loss2: 66.63 and loss3: 0.00\n",
      "Epoch [2838], train_loss: 543.43 with loss1: 476.79, loss2: 66.64 and loss3: 0.00\n",
      "Epoch [2839], train_loss: 540.35 with loss1: 473.71, loss2: 66.64 and loss3: 0.00\n",
      "Epoch [2840], train_loss: 541.20 with loss1: 474.46, loss2: 66.74 and loss3: 0.00\n",
      "Epoch [2841], train_loss: 542.23 with loss1: 475.73, loss2: 66.50 and loss3: 0.00\n",
      "Epoch [2842], train_loss: 541.49 with loss1: 474.84, loss2: 66.65 and loss3: 0.00\n",
      "Epoch [2843], train_loss: 538.30 with loss1: 471.70, loss2: 66.60 and loss3: 0.00\n",
      "Epoch [2844], train_loss: 537.96 with loss1: 471.26, loss2: 66.70 and loss3: 0.00\n",
      "Epoch [2845], train_loss: 537.74 with loss1: 471.12, loss2: 66.62 and loss3: 0.00\n",
      "Epoch [2846], train_loss: 538.98 with loss1: 472.31, loss2: 66.67 and loss3: 0.00\n",
      "Epoch [2847], train_loss: 535.90 with loss1: 469.38, loss2: 66.53 and loss3: 0.00\n",
      "Epoch [2848], train_loss: 538.00 with loss1: 471.42, loss2: 66.58 and loss3: 0.00\n",
      "Epoch [2849], train_loss: 539.56 with loss1: 473.03, loss2: 66.53 and loss3: 0.00\n",
      "Epoch [2850], train_loss: 538.56 with loss1: 471.98, loss2: 66.58 and loss3: 0.00\n",
      "Epoch [2851], train_loss: 538.08 with loss1: 471.48, loss2: 66.61 and loss3: 0.00\n",
      "Epoch [2852], train_loss: 538.97 with loss1: 472.40, loss2: 66.57 and loss3: 0.00\n",
      "Epoch [2853], train_loss: 537.38 with loss1: 470.79, loss2: 66.60 and loss3: 0.00\n",
      "Epoch [2854], train_loss: 539.41 with loss1: 472.89, loss2: 66.52 and loss3: 0.00\n",
      "Epoch [2855], train_loss: 539.57 with loss1: 473.01, loss2: 66.55 and loss3: 0.00\n",
      "Epoch [2856], train_loss: 541.61 with loss1: 475.09, loss2: 66.52 and loss3: 0.00\n",
      "Epoch [2857], train_loss: 545.19 with loss1: 478.75, loss2: 66.43 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2858], train_loss: 540.61 with loss1: 474.12, loss2: 66.50 and loss3: 0.00\n",
      "Epoch [2859], train_loss: 541.99 with loss1: 475.66, loss2: 66.33 and loss3: 0.00\n",
      "Epoch [2860], train_loss: 542.94 with loss1: 476.45, loss2: 66.50 and loss3: 0.00\n",
      "Epoch [2861], train_loss: 541.36 with loss1: 474.92, loss2: 66.44 and loss3: 0.00\n",
      "Epoch [2862], train_loss: 545.82 with loss1: 479.34, loss2: 66.48 and loss3: 0.00\n",
      "Epoch [2863], train_loss: 545.63 with loss1: 479.26, loss2: 66.37 and loss3: 0.00\n",
      "Epoch [2864], train_loss: 548.08 with loss1: 481.72, loss2: 66.35 and loss3: 0.00\n",
      "Epoch [2865], train_loss: 549.40 with loss1: 482.95, loss2: 66.45 and loss3: 0.00\n",
      "Epoch [2866], train_loss: 553.14 with loss1: 486.69, loss2: 66.45 and loss3: 0.00\n",
      "Epoch [2867], train_loss: 554.83 with loss1: 488.41, loss2: 66.42 and loss3: 0.00\n",
      "Epoch [2868], train_loss: 558.84 with loss1: 492.47, loss2: 66.38 and loss3: 0.00\n",
      "Epoch [2869], train_loss: 561.31 with loss1: 494.94, loss2: 66.37 and loss3: 0.00\n",
      "Epoch [2870], train_loss: 565.08 with loss1: 498.71, loss2: 66.37 and loss3: 0.00\n",
      "Epoch [2871], train_loss: 567.14 with loss1: 500.81, loss2: 66.33 and loss3: 0.00\n",
      "Epoch [2872], train_loss: 573.93 with loss1: 507.53, loss2: 66.40 and loss3: 0.00\n",
      "Epoch [2873], train_loss: 576.91 with loss1: 510.59, loss2: 66.32 and loss3: 0.00\n",
      "Epoch [2874], train_loss: 581.91 with loss1: 515.47, loss2: 66.44 and loss3: 0.00\n",
      "Epoch [2875], train_loss: 582.70 with loss1: 516.36, loss2: 66.34 and loss3: 0.00\n",
      "Epoch [2876], train_loss: 592.74 with loss1: 526.41, loss2: 66.33 and loss3: 0.00\n",
      "Epoch [2877], train_loss: 594.72 with loss1: 528.36, loss2: 66.36 and loss3: 0.00\n",
      "Epoch [2878], train_loss: 602.38 with loss1: 536.11, loss2: 66.27 and loss3: 0.00\n",
      "Epoch [2879], train_loss: 601.98 with loss1: 535.62, loss2: 66.36 and loss3: 0.00\n",
      "Epoch [2880], train_loss: 613.03 with loss1: 546.70, loss2: 66.33 and loss3: 0.00\n",
      "Epoch [2881], train_loss: 610.45 with loss1: 544.16, loss2: 66.29 and loss3: 0.00\n",
      "Epoch [2882], train_loss: 621.54 with loss1: 555.34, loss2: 66.21 and loss3: 0.00\n",
      "Epoch [2883], train_loss: 619.68 with loss1: 553.45, loss2: 66.24 and loss3: 0.00\n",
      "Epoch [2884], train_loss: 627.33 with loss1: 561.17, loss2: 66.16 and loss3: 0.00\n",
      "Epoch [2885], train_loss: 621.98 with loss1: 555.75, loss2: 66.22 and loss3: 0.00\n",
      "Epoch [2886], train_loss: 632.85 with loss1: 566.68, loss2: 66.16 and loss3: 0.00\n",
      "Epoch [2887], train_loss: 624.58 with loss1: 558.30, loss2: 66.28 and loss3: 0.00\n",
      "Epoch [2888], train_loss: 631.16 with loss1: 565.07, loss2: 66.08 and loss3: 0.00\n",
      "Epoch [2889], train_loss: 624.96 with loss1: 558.74, loss2: 66.22 and loss3: 0.00\n",
      "Epoch [2890], train_loss: 626.24 with loss1: 560.06, loss2: 66.18 and loss3: 0.00\n",
      "Epoch [2891], train_loss: 619.25 with loss1: 553.06, loss2: 66.19 and loss3: 0.00\n",
      "Epoch [2892], train_loss: 620.72 with loss1: 554.61, loss2: 66.11 and loss3: 0.00\n",
      "Epoch [2893], train_loss: 612.59 with loss1: 546.40, loss2: 66.19 and loss3: 0.00\n",
      "Epoch [2894], train_loss: 612.42 with loss1: 546.31, loss2: 66.11 and loss3: 0.00\n",
      "Epoch [2895], train_loss: 604.33 with loss1: 538.20, loss2: 66.13 and loss3: 0.00\n",
      "Epoch [2896], train_loss: 605.29 with loss1: 539.23, loss2: 66.07 and loss3: 0.00\n",
      "Epoch [2897], train_loss: 599.37 with loss1: 533.26, loss2: 66.11 and loss3: 0.00\n",
      "Epoch [2898], train_loss: 597.44 with loss1: 531.28, loss2: 66.16 and loss3: 0.00\n",
      "Epoch [2899], train_loss: 589.39 with loss1: 523.26, loss2: 66.13 and loss3: 0.00\n",
      "Epoch [2900], train_loss: 589.17 with loss1: 523.11, loss2: 66.06 and loss3: 0.00\n",
      "Epoch [2901], train_loss: 583.09 with loss1: 517.00, loss2: 66.09 and loss3: 0.00\n",
      "Epoch [2902], train_loss: 580.77 with loss1: 514.81, loss2: 65.96 and loss3: 0.00\n",
      "Epoch [2903], train_loss: 575.93 with loss1: 509.92, loss2: 66.01 and loss3: 0.00\n",
      "Epoch [2904], train_loss: 573.67 with loss1: 507.75, loss2: 65.92 and loss3: 0.00\n",
      "Epoch [2905], train_loss: 567.30 with loss1: 501.32, loss2: 65.98 and loss3: 0.00\n",
      "Epoch [2906], train_loss: 565.93 with loss1: 499.99, loss2: 65.94 and loss3: 0.00\n",
      "Epoch [2907], train_loss: 563.53 with loss1: 497.58, loss2: 65.95 and loss3: 0.00\n",
      "Epoch [2908], train_loss: 561.74 with loss1: 495.85, loss2: 65.90 and loss3: 0.00\n",
      "Epoch [2909], train_loss: 557.67 with loss1: 491.68, loss2: 65.99 and loss3: 0.00\n",
      "Epoch [2910], train_loss: 557.07 with loss1: 491.17, loss2: 65.90 and loss3: 0.00\n",
      "Epoch [2911], train_loss: 553.17 with loss1: 487.31, loss2: 65.85 and loss3: 0.00\n",
      "Epoch [2912], train_loss: 551.88 with loss1: 486.00, loss2: 65.88 and loss3: 0.00\n",
      "Epoch [2913], train_loss: 551.55 with loss1: 485.66, loss2: 65.90 and loss3: 0.00\n",
      "Epoch [2914], train_loss: 546.92 with loss1: 481.01, loss2: 65.90 and loss3: 0.00\n",
      "Epoch [2915], train_loss: 545.20 with loss1: 479.28, loss2: 65.91 and loss3: 0.00\n",
      "Epoch [2916], train_loss: 546.04 with loss1: 480.17, loss2: 65.87 and loss3: 0.00\n",
      "Epoch [2917], train_loss: 542.85 with loss1: 476.99, loss2: 65.86 and loss3: 0.00\n",
      "Epoch [2918], train_loss: 541.68 with loss1: 475.88, loss2: 65.80 and loss3: 0.00\n",
      "Epoch [2919], train_loss: 539.02 with loss1: 473.22, loss2: 65.80 and loss3: 0.00\n",
      "Epoch [2920], train_loss: 539.19 with loss1: 473.42, loss2: 65.78 and loss3: 0.00\n",
      "Epoch [2921], train_loss: 537.56 with loss1: 471.80, loss2: 65.76 and loss3: 0.00\n",
      "Epoch [2922], train_loss: 539.03 with loss1: 473.27, loss2: 65.76 and loss3: 0.00\n",
      "Epoch [2923], train_loss: 537.44 with loss1: 471.61, loss2: 65.83 and loss3: 0.00\n",
      "Epoch [2924], train_loss: 535.99 with loss1: 470.21, loss2: 65.78 and loss3: 0.00\n",
      "Epoch [2925], train_loss: 536.26 with loss1: 470.49, loss2: 65.77 and loss3: 0.00\n",
      "Epoch [2926], train_loss: 535.84 with loss1: 470.08, loss2: 65.75 and loss3: 0.00\n",
      "Epoch [2927], train_loss: 534.94 with loss1: 469.13, loss2: 65.81 and loss3: 0.00\n",
      "Epoch [2928], train_loss: 535.07 with loss1: 469.33, loss2: 65.74 and loss3: 0.00\n",
      "Epoch [2929], train_loss: 534.39 with loss1: 468.64, loss2: 65.75 and loss3: 0.00\n",
      "Epoch [2930], train_loss: 533.47 with loss1: 467.80, loss2: 65.68 and loss3: 0.00\n",
      "Epoch [2931], train_loss: 534.39 with loss1: 468.73, loss2: 65.66 and loss3: 0.00\n",
      "Epoch [2932], train_loss: 534.17 with loss1: 468.47, loss2: 65.70 and loss3: 0.00\n",
      "Epoch [2933], train_loss: 532.60 with loss1: 466.91, loss2: 65.69 and loss3: 0.00\n",
      "Epoch [2934], train_loss: 532.36 with loss1: 466.72, loss2: 65.64 and loss3: 0.00\n",
      "Epoch [2935], train_loss: 530.47 with loss1: 464.85, loss2: 65.62 and loss3: 0.00\n",
      "Epoch [2936], train_loss: 531.95 with loss1: 466.32, loss2: 65.62 and loss3: 0.00\n",
      "Epoch [2937], train_loss: 530.66 with loss1: 465.03, loss2: 65.63 and loss3: 0.00\n",
      "Epoch [2938], train_loss: 530.35 with loss1: 464.71, loss2: 65.64 and loss3: 0.00\n",
      "Epoch [2939], train_loss: 531.88 with loss1: 466.22, loss2: 65.65 and loss3: 0.00\n",
      "Epoch [2940], train_loss: 532.69 with loss1: 467.07, loss2: 65.62 and loss3: 0.00\n",
      "Epoch [2941], train_loss: 532.05 with loss1: 466.46, loss2: 65.59 and loss3: 0.00\n",
      "Epoch [2942], train_loss: 532.77 with loss1: 467.17, loss2: 65.60 and loss3: 0.00\n",
      "Epoch [2943], train_loss: 532.35 with loss1: 466.71, loss2: 65.64 and loss3: 0.00\n",
      "Epoch [2944], train_loss: 533.66 with loss1: 468.14, loss2: 65.51 and loss3: 0.00\n",
      "Epoch [2945], train_loss: 532.82 with loss1: 467.23, loss2: 65.59 and loss3: 0.00\n",
      "Epoch [2946], train_loss: 535.09 with loss1: 469.56, loss2: 65.53 and loss3: 0.00\n",
      "Epoch [2947], train_loss: 534.59 with loss1: 468.99, loss2: 65.60 and loss3: 0.00\n",
      "Epoch [2948], train_loss: 535.00 with loss1: 469.54, loss2: 65.46 and loss3: 0.00\n",
      "Epoch [2949], train_loss: 535.86 with loss1: 470.32, loss2: 65.54 and loss3: 0.00\n",
      "Epoch [2950], train_loss: 535.10 with loss1: 469.61, loss2: 65.48 and loss3: 0.00\n",
      "Epoch [2951], train_loss: 535.42 with loss1: 469.97, loss2: 65.45 and loss3: 0.00\n",
      "Epoch [2952], train_loss: 536.20 with loss1: 470.68, loss2: 65.53 and loss3: 0.00\n",
      "Epoch [2953], train_loss: 534.02 with loss1: 468.51, loss2: 65.51 and loss3: 0.00\n",
      "Epoch [2954], train_loss: 534.49 with loss1: 468.94, loss2: 65.55 and loss3: 0.00\n",
      "Epoch [2955], train_loss: 534.10 with loss1: 468.66, loss2: 65.44 and loss3: 0.00\n",
      "Epoch [2956], train_loss: 535.98 with loss1: 470.54, loss2: 65.44 and loss3: 0.00\n",
      "Epoch [2957], train_loss: 533.78 with loss1: 468.33, loss2: 65.45 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2958], train_loss: 535.10 with loss1: 469.72, loss2: 65.38 and loss3: 0.00\n",
      "Epoch [2959], train_loss: 535.57 with loss1: 470.10, loss2: 65.47 and loss3: 0.00\n",
      "Epoch [2960], train_loss: 536.32 with loss1: 470.87, loss2: 65.44 and loss3: 0.00\n",
      "Epoch [2961], train_loss: 536.80 with loss1: 471.33, loss2: 65.47 and loss3: 0.00\n",
      "Epoch [2962], train_loss: 539.35 with loss1: 473.90, loss2: 65.45 and loss3: 0.00\n",
      "Epoch [2963], train_loss: 536.66 with loss1: 471.20, loss2: 65.46 and loss3: 0.00\n",
      "Epoch [2964], train_loss: 539.28 with loss1: 473.89, loss2: 65.39 and loss3: 0.00\n",
      "Epoch [2965], train_loss: 538.77 with loss1: 473.42, loss2: 65.35 and loss3: 0.00\n",
      "Epoch [2966], train_loss: 539.94 with loss1: 474.54, loss2: 65.40 and loss3: 0.00\n",
      "Epoch [2967], train_loss: 540.44 with loss1: 475.05, loss2: 65.40 and loss3: 0.00\n",
      "Epoch [2968], train_loss: 540.73 with loss1: 475.47, loss2: 65.26 and loss3: 0.00\n",
      "Epoch [2969], train_loss: 541.06 with loss1: 475.69, loss2: 65.36 and loss3: 0.00\n",
      "Epoch [2970], train_loss: 543.47 with loss1: 478.11, loss2: 65.36 and loss3: 0.00\n",
      "Epoch [2971], train_loss: 540.21 with loss1: 474.88, loss2: 65.33 and loss3: 0.00\n",
      "Epoch [2972], train_loss: 542.31 with loss1: 477.04, loss2: 65.28 and loss3: 0.00\n",
      "Epoch [2973], train_loss: 543.53 with loss1: 478.24, loss2: 65.29 and loss3: 0.00\n",
      "Epoch [2974], train_loss: 542.04 with loss1: 476.72, loss2: 65.33 and loss3: 0.00\n",
      "Epoch [2975], train_loss: 541.97 with loss1: 476.66, loss2: 65.32 and loss3: 0.00\n",
      "Epoch [2976], train_loss: 542.64 with loss1: 477.40, loss2: 65.24 and loss3: 0.00\n",
      "Epoch [2977], train_loss: 541.71 with loss1: 476.43, loss2: 65.29 and loss3: 0.00\n",
      "Epoch [2978], train_loss: 542.98 with loss1: 477.76, loss2: 65.22 and loss3: 0.00\n",
      "Epoch [2979], train_loss: 544.07 with loss1: 478.86, loss2: 65.21 and loss3: 0.00\n",
      "Epoch [2980], train_loss: 545.66 with loss1: 480.46, loss2: 65.20 and loss3: 0.00\n",
      "Epoch [2981], train_loss: 543.77 with loss1: 478.53, loss2: 65.24 and loss3: 0.00\n",
      "Epoch [2982], train_loss: 546.98 with loss1: 481.79, loss2: 65.19 and loss3: 0.00\n",
      "Epoch [2983], train_loss: 545.20 with loss1: 480.01, loss2: 65.19 and loss3: 0.00\n",
      "Epoch [2984], train_loss: 546.84 with loss1: 481.65, loss2: 65.19 and loss3: 0.00\n",
      "Epoch [2985], train_loss: 546.10 with loss1: 480.96, loss2: 65.14 and loss3: 0.00\n",
      "Epoch [2986], train_loss: 547.69 with loss1: 482.57, loss2: 65.13 and loss3: 0.00\n",
      "Epoch [2987], train_loss: 545.45 with loss1: 480.31, loss2: 65.14 and loss3: 0.00\n",
      "Epoch [2988], train_loss: 547.02 with loss1: 481.90, loss2: 65.12 and loss3: 0.00\n",
      "Epoch [2989], train_loss: 546.53 with loss1: 481.33, loss2: 65.20 and loss3: 0.00\n",
      "Epoch [2990], train_loss: 546.17 with loss1: 481.09, loss2: 65.08 and loss3: 0.00\n",
      "Epoch [2991], train_loss: 546.10 with loss1: 480.96, loss2: 65.14 and loss3: 0.00\n",
      "Epoch [2992], train_loss: 548.44 with loss1: 483.43, loss2: 65.01 and loss3: 0.00\n",
      "Epoch [2993], train_loss: 546.75 with loss1: 481.66, loss2: 65.09 and loss3: 0.00\n",
      "Epoch [2994], train_loss: 546.01 with loss1: 480.91, loss2: 65.10 and loss3: 0.00\n",
      "Epoch [2995], train_loss: 544.70 with loss1: 479.61, loss2: 65.09 and loss3: 0.00\n",
      "Epoch [2996], train_loss: 544.65 with loss1: 479.58, loss2: 65.07 and loss3: 0.00\n",
      "Epoch [2997], train_loss: 542.46 with loss1: 477.35, loss2: 65.10 and loss3: 0.00\n",
      "Epoch [2998], train_loss: 544.55 with loss1: 479.52, loss2: 65.03 and loss3: 0.00\n",
      "Epoch [2999], train_loss: 539.65 with loss1: 474.59, loss2: 65.06 and loss3: 0.00\n",
      "Epoch [3000], train_loss: 541.31 with loss1: 476.30, loss2: 65.01 and loss3: 0.00\n",
      "Epoch [3001], train_loss: 539.51 with loss1: 474.55, loss2: 64.96 and loss3: 0.00\n",
      "Epoch [3002], train_loss: 538.91 with loss1: 473.93, loss2: 64.98 and loss3: 0.00\n",
      "Epoch [3003], train_loss: 538.75 with loss1: 473.79, loss2: 64.96 and loss3: 0.00\n",
      "Epoch [3004], train_loss: 539.47 with loss1: 474.49, loss2: 64.98 and loss3: 0.00\n",
      "Epoch [3005], train_loss: 537.35 with loss1: 472.39, loss2: 64.96 and loss3: 0.00\n",
      "Epoch [3006], train_loss: 539.76 with loss1: 474.80, loss2: 64.96 and loss3: 0.00\n",
      "Epoch [3007], train_loss: 537.15 with loss1: 472.22, loss2: 64.94 and loss3: 0.00\n",
      "Epoch [3008], train_loss: 538.75 with loss1: 473.81, loss2: 64.94 and loss3: 0.00\n",
      "Epoch [3009], train_loss: 537.49 with loss1: 472.57, loss2: 64.92 and loss3: 0.00\n",
      "Epoch [3010], train_loss: 537.64 with loss1: 472.71, loss2: 64.93 and loss3: 0.00\n",
      "Epoch [3011], train_loss: 537.34 with loss1: 472.42, loss2: 64.92 and loss3: 0.00\n",
      "Epoch [3012], train_loss: 539.05 with loss1: 474.12, loss2: 64.93 and loss3: 0.00\n",
      "Epoch [3013], train_loss: 536.62 with loss1: 471.67, loss2: 64.95 and loss3: 0.00\n",
      "Epoch [3014], train_loss: 536.81 with loss1: 471.96, loss2: 64.85 and loss3: 0.00\n",
      "Epoch [3015], train_loss: 536.02 with loss1: 471.21, loss2: 64.81 and loss3: 0.00\n",
      "Epoch [3016], train_loss: 538.63 with loss1: 473.78, loss2: 64.85 and loss3: 0.00\n",
      "Epoch [3017], train_loss: 536.23 with loss1: 471.34, loss2: 64.89 and loss3: 0.00\n",
      "Epoch [3018], train_loss: 537.34 with loss1: 472.50, loss2: 64.84 and loss3: 0.00\n",
      "Epoch [3019], train_loss: 536.01 with loss1: 471.17, loss2: 64.84 and loss3: 0.00\n",
      "Epoch [3020], train_loss: 535.68 with loss1: 470.78, loss2: 64.90 and loss3: 0.00\n",
      "Epoch [3021], train_loss: 534.25 with loss1: 469.48, loss2: 64.77 and loss3: 0.00\n",
      "Epoch [3022], train_loss: 534.32 with loss1: 469.57, loss2: 64.75 and loss3: 0.00\n",
      "Epoch [3023], train_loss: 532.71 with loss1: 467.91, loss2: 64.80 and loss3: 0.00\n",
      "Epoch [3024], train_loss: 534.56 with loss1: 469.74, loss2: 64.82 and loss3: 0.00\n",
      "Epoch [3025], train_loss: 532.74 with loss1: 467.95, loss2: 64.79 and loss3: 0.00\n",
      "Epoch [3026], train_loss: 532.78 with loss1: 468.07, loss2: 64.71 and loss3: 0.00\n",
      "Epoch [3027], train_loss: 532.13 with loss1: 467.31, loss2: 64.82 and loss3: 0.00\n",
      "Epoch [3028], train_loss: 534.55 with loss1: 469.73, loss2: 64.82 and loss3: 0.00\n",
      "Epoch [3029], train_loss: 533.17 with loss1: 468.49, loss2: 64.68 and loss3: 0.00\n",
      "Epoch [3030], train_loss: 533.13 with loss1: 468.45, loss2: 64.67 and loss3: 0.00\n",
      "Epoch [3031], train_loss: 532.14 with loss1: 467.40, loss2: 64.74 and loss3: 0.00\n",
      "Epoch [3032], train_loss: 533.29 with loss1: 468.57, loss2: 64.72 and loss3: 0.00\n",
      "Epoch [3033], train_loss: 531.94 with loss1: 467.25, loss2: 64.69 and loss3: 0.00\n",
      "Epoch [3034], train_loss: 532.80 with loss1: 468.13, loss2: 64.68 and loss3: 0.00\n",
      "Epoch [3035], train_loss: 532.65 with loss1: 467.99, loss2: 64.66 and loss3: 0.00\n",
      "Epoch [3036], train_loss: 533.58 with loss1: 468.81, loss2: 64.77 and loss3: 0.00\n",
      "Epoch [3037], train_loss: 533.62 with loss1: 468.99, loss2: 64.63 and loss3: 0.00\n",
      "Epoch [3038], train_loss: 535.29 with loss1: 470.66, loss2: 64.63 and loss3: 0.00\n",
      "Epoch [3039], train_loss: 534.78 with loss1: 470.13, loss2: 64.65 and loss3: 0.00\n",
      "Epoch [3040], train_loss: 535.43 with loss1: 470.75, loss2: 64.67 and loss3: 0.00\n",
      "Epoch [3041], train_loss: 532.73 with loss1: 468.15, loss2: 64.58 and loss3: 0.00\n",
      "Epoch [3042], train_loss: 534.54 with loss1: 469.90, loss2: 64.64 and loss3: 0.00\n",
      "Epoch [3043], train_loss: 533.33 with loss1: 468.73, loss2: 64.60 and loss3: 0.00\n",
      "Epoch [3044], train_loss: 536.02 with loss1: 471.44, loss2: 64.58 and loss3: 0.00\n",
      "Epoch [3045], train_loss: 533.55 with loss1: 469.04, loss2: 64.51 and loss3: 0.00\n",
      "Epoch [3046], train_loss: 537.22 with loss1: 472.67, loss2: 64.55 and loss3: 0.00\n",
      "Epoch [3047], train_loss: 536.80 with loss1: 472.26, loss2: 64.55 and loss3: 0.00\n",
      "Epoch [3048], train_loss: 537.51 with loss1: 472.97, loss2: 64.55 and loss3: 0.00\n",
      "Epoch [3049], train_loss: 534.11 with loss1: 469.57, loss2: 64.54 and loss3: 0.00\n",
      "Epoch [3050], train_loss: 536.18 with loss1: 471.65, loss2: 64.54 and loss3: 0.00\n",
      "Epoch [3051], train_loss: 533.75 with loss1: 469.25, loss2: 64.50 and loss3: 0.00\n",
      "Epoch [3052], train_loss: 535.35 with loss1: 470.82, loss2: 64.53 and loss3: 0.00\n",
      "Epoch [3053], train_loss: 534.33 with loss1: 469.84, loss2: 64.49 and loss3: 0.00\n",
      "Epoch [3054], train_loss: 534.60 with loss1: 470.10, loss2: 64.50 and loss3: 0.00\n",
      "Epoch [3055], train_loss: 535.41 with loss1: 470.97, loss2: 64.44 and loss3: 0.00\n",
      "Epoch [3056], train_loss: 536.15 with loss1: 471.67, loss2: 64.48 and loss3: 0.00\n",
      "Epoch [3057], train_loss: 534.72 with loss1: 470.36, loss2: 64.36 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3058], train_loss: 537.82 with loss1: 473.31, loss2: 64.51 and loss3: 0.00\n",
      "Epoch [3059], train_loss: 535.52 with loss1: 471.17, loss2: 64.35 and loss3: 0.00\n",
      "Epoch [3060], train_loss: 536.96 with loss1: 472.49, loss2: 64.48 and loss3: 0.00\n",
      "Epoch [3061], train_loss: 538.46 with loss1: 474.02, loss2: 64.45 and loss3: 0.00\n",
      "Epoch [3062], train_loss: 536.50 with loss1: 472.05, loss2: 64.45 and loss3: 0.00\n",
      "Epoch [3063], train_loss: 536.84 with loss1: 472.48, loss2: 64.36 and loss3: 0.00\n",
      "Epoch [3064], train_loss: 538.13 with loss1: 473.70, loss2: 64.42 and loss3: 0.00\n",
      "Epoch [3065], train_loss: 540.78 with loss1: 476.47, loss2: 64.31 and loss3: 0.00\n",
      "Epoch [3066], train_loss: 541.20 with loss1: 476.71, loss2: 64.49 and loss3: 0.00\n",
      "Epoch [3067], train_loss: 536.68 with loss1: 472.26, loss2: 64.43 and loss3: 0.00\n",
      "Epoch [3068], train_loss: 539.46 with loss1: 475.02, loss2: 64.43 and loss3: 0.00\n",
      "Epoch [3069], train_loss: 535.74 with loss1: 471.38, loss2: 64.35 and loss3: 0.00\n",
      "Epoch [3070], train_loss: 538.61 with loss1: 474.19, loss2: 64.42 and loss3: 0.00\n",
      "Epoch [3071], train_loss: 536.57 with loss1: 472.21, loss2: 64.36 and loss3: 0.00\n",
      "Epoch [3072], train_loss: 539.92 with loss1: 475.58, loss2: 64.34 and loss3: 0.00\n",
      "Epoch [3073], train_loss: 536.35 with loss1: 471.99, loss2: 64.37 and loss3: 0.00\n",
      "Epoch [3074], train_loss: 539.37 with loss1: 474.97, loss2: 64.40 and loss3: 0.00\n",
      "Epoch [3075], train_loss: 538.70 with loss1: 474.35, loss2: 64.35 and loss3: 0.00\n",
      "Epoch [3076], train_loss: 540.60 with loss1: 476.22, loss2: 64.38 and loss3: 0.00\n",
      "Epoch [3077], train_loss: 538.95 with loss1: 474.65, loss2: 64.30 and loss3: 0.00\n",
      "Epoch [3078], train_loss: 540.28 with loss1: 475.98, loss2: 64.30 and loss3: 0.00\n",
      "Epoch [3079], train_loss: 537.51 with loss1: 473.26, loss2: 64.25 and loss3: 0.00\n",
      "Epoch [3080], train_loss: 539.55 with loss1: 475.21, loss2: 64.34 and loss3: 0.00\n",
      "Epoch [3081], train_loss: 538.96 with loss1: 474.72, loss2: 64.24 and loss3: 0.00\n",
      "Epoch [3082], train_loss: 539.51 with loss1: 475.24, loss2: 64.27 and loss3: 0.00\n",
      "Epoch [3083], train_loss: 537.18 with loss1: 472.89, loss2: 64.29 and loss3: 0.00\n",
      "Epoch [3084], train_loss: 536.97 with loss1: 472.67, loss2: 64.31 and loss3: 0.00\n",
      "Epoch [3085], train_loss: 533.41 with loss1: 469.15, loss2: 64.26 and loss3: 0.00\n",
      "Epoch [3086], train_loss: 539.80 with loss1: 475.54, loss2: 64.26 and loss3: 0.00\n",
      "Epoch [3087], train_loss: 534.71 with loss1: 470.47, loss2: 64.24 and loss3: 0.00\n",
      "Epoch [3088], train_loss: 535.11 with loss1: 470.81, loss2: 64.30 and loss3: 0.00\n",
      "Epoch [3089], train_loss: 533.04 with loss1: 468.88, loss2: 64.16 and loss3: 0.00\n",
      "Epoch [3090], train_loss: 536.44 with loss1: 472.18, loss2: 64.26 and loss3: 0.00\n",
      "Epoch [3091], train_loss: 533.89 with loss1: 469.58, loss2: 64.32 and loss3: 0.00\n",
      "Epoch [3092], train_loss: 536.24 with loss1: 471.99, loss2: 64.25 and loss3: 0.00\n",
      "Epoch [3093], train_loss: 532.67 with loss1: 468.52, loss2: 64.15 and loss3: 0.00\n",
      "Epoch [3094], train_loss: 536.09 with loss1: 471.90, loss2: 64.19 and loss3: 0.00\n",
      "Epoch [3095], train_loss: 533.51 with loss1: 469.34, loss2: 64.18 and loss3: 0.00\n",
      "Epoch [3096], train_loss: 535.65 with loss1: 471.44, loss2: 64.21 and loss3: 0.00\n",
      "Epoch [3097], train_loss: 535.69 with loss1: 471.54, loss2: 64.15 and loss3: 0.00\n",
      "Epoch [3098], train_loss: 537.82 with loss1: 473.55, loss2: 64.27 and loss3: 0.00\n",
      "Epoch [3099], train_loss: 535.15 with loss1: 470.93, loss2: 64.22 and loss3: 0.00\n",
      "Epoch [3100], train_loss: 537.30 with loss1: 473.06, loss2: 64.24 and loss3: 0.00\n",
      "Epoch [3101], train_loss: 534.38 with loss1: 470.20, loss2: 64.18 and loss3: 0.00\n",
      "Epoch [3102], train_loss: 537.61 with loss1: 473.39, loss2: 64.22 and loss3: 0.00\n",
      "Epoch [3103], train_loss: 535.05 with loss1: 470.98, loss2: 64.07 and loss3: 0.00\n",
      "Epoch [3104], train_loss: 538.39 with loss1: 474.19, loss2: 64.20 and loss3: 0.00\n",
      "Epoch [3105], train_loss: 538.25 with loss1: 474.17, loss2: 64.08 and loss3: 0.00\n",
      "Epoch [3106], train_loss: 536.99 with loss1: 472.86, loss2: 64.13 and loss3: 0.00\n",
      "Epoch [3107], train_loss: 534.52 with loss1: 470.44, loss2: 64.08 and loss3: 0.00\n",
      "Epoch [3108], train_loss: 537.78 with loss1: 473.60, loss2: 64.19 and loss3: 0.00\n",
      "Epoch [3109], train_loss: 534.46 with loss1: 470.32, loss2: 64.14 and loss3: 0.00\n",
      "Epoch [3110], train_loss: 536.10 with loss1: 472.04, loss2: 64.06 and loss3: 0.00\n",
      "Epoch [3111], train_loss: 534.87 with loss1: 470.70, loss2: 64.17 and loss3: 0.00\n",
      "Epoch [3112], train_loss: 536.81 with loss1: 472.72, loss2: 64.09 and loss3: 0.00\n",
      "Epoch [3113], train_loss: 535.13 with loss1: 470.99, loss2: 64.14 and loss3: 0.00\n",
      "Epoch [3114], train_loss: 536.37 with loss1: 472.30, loss2: 64.07 and loss3: 0.00\n",
      "Epoch [3115], train_loss: 536.69 with loss1: 472.54, loss2: 64.15 and loss3: 0.00\n",
      "Epoch [3116], train_loss: 534.97 with loss1: 470.87, loss2: 64.10 and loss3: 0.00\n",
      "Epoch [3117], train_loss: 533.94 with loss1: 469.86, loss2: 64.08 and loss3: 0.00\n",
      "Epoch [3118], train_loss: 535.62 with loss1: 471.52, loss2: 64.10 and loss3: 0.00\n",
      "Epoch [3119], train_loss: 535.19 with loss1: 471.11, loss2: 64.08 and loss3: 0.00\n",
      "Epoch [3120], train_loss: 534.51 with loss1: 470.48, loss2: 64.03 and loss3: 0.00\n",
      "Epoch [3121], train_loss: 533.40 with loss1: 469.40, loss2: 64.01 and loss3: 0.00\n",
      "Epoch [3122], train_loss: 533.35 with loss1: 469.29, loss2: 64.05 and loss3: 0.00\n",
      "Epoch [3123], train_loss: 532.31 with loss1: 468.24, loss2: 64.07 and loss3: 0.00\n",
      "Epoch [3124], train_loss: 534.91 with loss1: 470.87, loss2: 64.05 and loss3: 0.00\n",
      "Epoch [3125], train_loss: 532.76 with loss1: 468.81, loss2: 63.96 and loss3: 0.00\n",
      "Epoch [3126], train_loss: 531.21 with loss1: 467.17, loss2: 64.05 and loss3: 0.00\n",
      "Epoch [3127], train_loss: 528.09 with loss1: 464.07, loss2: 64.02 and loss3: 0.00\n",
      "Epoch [3128], train_loss: 528.69 with loss1: 464.72, loss2: 63.97 and loss3: 0.00\n",
      "Epoch [3129], train_loss: 525.82 with loss1: 461.82, loss2: 64.01 and loss3: 0.00\n",
      "Epoch [3130], train_loss: 526.24 with loss1: 462.24, loss2: 63.99 and loss3: 0.00\n",
      "Epoch [3131], train_loss: 524.29 with loss1: 460.32, loss2: 63.97 and loss3: 0.00\n",
      "Epoch [3132], train_loss: 523.03 with loss1: 459.05, loss2: 63.99 and loss3: 0.00\n",
      "Epoch [3133], train_loss: 521.60 with loss1: 457.64, loss2: 63.96 and loss3: 0.00\n",
      "Epoch [3134], train_loss: 519.77 with loss1: 455.79, loss2: 63.98 and loss3: 0.00\n",
      "Epoch [3135], train_loss: 519.52 with loss1: 455.56, loss2: 63.96 and loss3: 0.00\n",
      "Epoch [3136], train_loss: 517.51 with loss1: 453.48, loss2: 64.02 and loss3: 0.00\n",
      "Epoch [3137], train_loss: 515.67 with loss1: 451.82, loss2: 63.86 and loss3: 0.00\n",
      "Epoch [3138], train_loss: 515.45 with loss1: 451.52, loss2: 63.93 and loss3: 0.00\n",
      "Epoch [3139], train_loss: 513.73 with loss1: 449.82, loss2: 63.91 and loss3: 0.00\n",
      "Epoch [3140], train_loss: 512.51 with loss1: 448.61, loss2: 63.90 and loss3: 0.00\n",
      "Epoch [3141], train_loss: 512.38 with loss1: 448.43, loss2: 63.94 and loss3: 0.00\n",
      "Epoch [3142], train_loss: 510.77 with loss1: 446.90, loss2: 63.87 and loss3: 0.00\n",
      "Epoch [3143], train_loss: 510.75 with loss1: 446.80, loss2: 63.95 and loss3: 0.00\n",
      "Epoch [3144], train_loss: 509.94 with loss1: 446.12, loss2: 63.82 and loss3: 0.00\n",
      "Epoch [3145], train_loss: 508.97 with loss1: 445.15, loss2: 63.82 and loss3: 0.00\n",
      "Epoch [3146], train_loss: 509.27 with loss1: 445.43, loss2: 63.84 and loss3: 0.00\n",
      "Epoch [3147], train_loss: 508.42 with loss1: 444.62, loss2: 63.80 and loss3: 0.00\n",
      "Epoch [3148], train_loss: 507.40 with loss1: 443.57, loss2: 63.83 and loss3: 0.00\n",
      "Epoch [3149], train_loss: 507.21 with loss1: 443.39, loss2: 63.82 and loss3: 0.00\n",
      "Epoch [3150], train_loss: 506.60 with loss1: 442.73, loss2: 63.87 and loss3: 0.00\n",
      "Epoch [3151], train_loss: 506.67 with loss1: 442.88, loss2: 63.79 and loss3: 0.00\n",
      "Epoch [3152], train_loss: 506.90 with loss1: 443.10, loss2: 63.80 and loss3: 0.00\n",
      "Epoch [3153], train_loss: 504.99 with loss1: 441.19, loss2: 63.80 and loss3: 0.00\n",
      "Epoch [3154], train_loss: 504.09 with loss1: 440.29, loss2: 63.80 and loss3: 0.00\n",
      "Epoch [3155], train_loss: 505.01 with loss1: 441.23, loss2: 63.77 and loss3: 0.00\n",
      "Epoch [3156], train_loss: 504.89 with loss1: 441.10, loss2: 63.78 and loss3: 0.00\n",
      "Epoch [3157], train_loss: 507.57 with loss1: 443.83, loss2: 63.74 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3158], train_loss: 505.34 with loss1: 441.63, loss2: 63.71 and loss3: 0.00\n",
      "Epoch [3159], train_loss: 505.49 with loss1: 441.77, loss2: 63.72 and loss3: 0.00\n",
      "Epoch [3160], train_loss: 506.21 with loss1: 442.50, loss2: 63.72 and loss3: 0.00\n",
      "Epoch [3161], train_loss: 504.72 with loss1: 441.08, loss2: 63.64 and loss3: 0.00\n",
      "Epoch [3162], train_loss: 504.88 with loss1: 441.20, loss2: 63.67 and loss3: 0.00\n",
      "Epoch [3163], train_loss: 506.64 with loss1: 442.98, loss2: 63.67 and loss3: 0.00\n",
      "Epoch [3164], train_loss: 507.09 with loss1: 443.45, loss2: 63.65 and loss3: 0.00\n",
      "Epoch [3165], train_loss: 507.58 with loss1: 443.92, loss2: 63.66 and loss3: 0.00\n",
      "Epoch [3166], train_loss: 509.80 with loss1: 446.23, loss2: 63.57 and loss3: 0.00\n",
      "Epoch [3167], train_loss: 508.50 with loss1: 444.86, loss2: 63.64 and loss3: 0.00\n",
      "Epoch [3168], train_loss: 509.62 with loss1: 446.03, loss2: 63.58 and loss3: 0.00\n",
      "Epoch [3169], train_loss: 510.08 with loss1: 446.49, loss2: 63.59 and loss3: 0.00\n",
      "Epoch [3170], train_loss: 510.43 with loss1: 446.78, loss2: 63.65 and loss3: 0.00\n",
      "Epoch [3171], train_loss: 512.66 with loss1: 449.14, loss2: 63.52 and loss3: 0.00\n",
      "Epoch [3172], train_loss: 511.94 with loss1: 448.33, loss2: 63.60 and loss3: 0.00\n",
      "Epoch [3173], train_loss: 514.95 with loss1: 451.46, loss2: 63.50 and loss3: 0.00\n",
      "Epoch [3174], train_loss: 515.32 with loss1: 451.77, loss2: 63.54 and loss3: 0.00\n",
      "Epoch [3175], train_loss: 517.63 with loss1: 454.12, loss2: 63.51 and loss3: 0.00\n",
      "Epoch [3176], train_loss: 517.72 with loss1: 454.21, loss2: 63.51 and loss3: 0.00\n",
      "Epoch [3177], train_loss: 521.05 with loss1: 457.56, loss2: 63.49 and loss3: 0.00\n",
      "Epoch [3178], train_loss: 520.01 with loss1: 456.52, loss2: 63.49 and loss3: 0.00\n",
      "Epoch [3179], train_loss: 524.11 with loss1: 460.62, loss2: 63.49 and loss3: 0.00\n",
      "Epoch [3180], train_loss: 524.93 with loss1: 461.36, loss2: 63.56 and loss3: 0.00\n",
      "Epoch [3181], train_loss: 527.84 with loss1: 464.36, loss2: 63.48 and loss3: 0.00\n",
      "Epoch [3182], train_loss: 526.29 with loss1: 462.80, loss2: 63.49 and loss3: 0.00\n",
      "Epoch [3183], train_loss: 534.78 with loss1: 471.36, loss2: 63.42 and loss3: 0.00\n",
      "Epoch [3184], train_loss: 531.60 with loss1: 468.16, loss2: 63.45 and loss3: 0.00\n",
      "Epoch [3185], train_loss: 534.15 with loss1: 470.65, loss2: 63.50 and loss3: 0.00\n",
      "Epoch [3186], train_loss: 535.01 with loss1: 471.51, loss2: 63.49 and loss3: 0.00\n",
      "Epoch [3187], train_loss: 539.50 with loss1: 476.04, loss2: 63.46 and loss3: 0.00\n",
      "Epoch [3188], train_loss: 540.47 with loss1: 477.04, loss2: 63.43 and loss3: 0.00\n",
      "Epoch [3189], train_loss: 546.22 with loss1: 482.85, loss2: 63.37 and loss3: 0.00\n",
      "Epoch [3190], train_loss: 545.78 with loss1: 482.30, loss2: 63.48 and loss3: 0.00\n",
      "Epoch [3191], train_loss: 549.97 with loss1: 486.52, loss2: 63.45 and loss3: 0.00\n",
      "Epoch [3192], train_loss: 551.05 with loss1: 487.64, loss2: 63.41 and loss3: 0.00\n",
      "Epoch [3193], train_loss: 555.10 with loss1: 491.78, loss2: 63.33 and loss3: 0.00\n",
      "Epoch [3194], train_loss: 553.21 with loss1: 489.77, loss2: 63.44 and loss3: 0.00\n",
      "Epoch [3195], train_loss: 558.14 with loss1: 494.82, loss2: 63.33 and loss3: 0.00\n",
      "Epoch [3196], train_loss: 556.88 with loss1: 493.48, loss2: 63.40 and loss3: 0.00\n",
      "Epoch [3197], train_loss: 561.10 with loss1: 497.81, loss2: 63.29 and loss3: 0.00\n",
      "Epoch [3198], train_loss: 559.46 with loss1: 496.05, loss2: 63.41 and loss3: 0.00\n",
      "Epoch [3199], train_loss: 565.79 with loss1: 502.52, loss2: 63.27 and loss3: 0.00\n",
      "Epoch [3200], train_loss: 560.98 with loss1: 497.64, loss2: 63.34 and loss3: 0.00\n",
      "Epoch [3201], train_loss: 564.57 with loss1: 501.30, loss2: 63.27 and loss3: 0.00\n",
      "Epoch [3202], train_loss: 560.18 with loss1: 496.91, loss2: 63.27 and loss3: 0.00\n",
      "Epoch [3203], train_loss: 562.25 with loss1: 499.06, loss2: 63.20 and loss3: 0.00\n",
      "Epoch [3204], train_loss: 558.80 with loss1: 495.58, loss2: 63.22 and loss3: 0.00\n",
      "Epoch [3205], train_loss: 561.19 with loss1: 497.98, loss2: 63.21 and loss3: 0.00\n",
      "Epoch [3206], train_loss: 558.57 with loss1: 495.30, loss2: 63.27 and loss3: 0.00\n",
      "Epoch [3207], train_loss: 560.26 with loss1: 497.05, loss2: 63.20 and loss3: 0.00\n",
      "Epoch [3208], train_loss: 556.45 with loss1: 493.29, loss2: 63.16 and loss3: 0.00\n",
      "Epoch [3209], train_loss: 557.13 with loss1: 493.96, loss2: 63.17 and loss3: 0.00\n",
      "Epoch [3210], train_loss: 553.41 with loss1: 490.24, loss2: 63.18 and loss3: 0.00\n",
      "Epoch [3211], train_loss: 555.20 with loss1: 492.09, loss2: 63.11 and loss3: 0.00\n",
      "Epoch [3212], train_loss: 550.96 with loss1: 487.74, loss2: 63.23 and loss3: 0.00\n",
      "Epoch [3213], train_loss: 550.91 with loss1: 487.73, loss2: 63.18 and loss3: 0.00\n",
      "Epoch [3214], train_loss: 546.04 with loss1: 482.84, loss2: 63.20 and loss3: 0.00\n",
      "Epoch [3215], train_loss: 547.98 with loss1: 484.84, loss2: 63.14 and loss3: 0.00\n",
      "Epoch [3216], train_loss: 544.11 with loss1: 480.96, loss2: 63.15 and loss3: 0.00\n",
      "Epoch [3217], train_loss: 544.30 with loss1: 481.18, loss2: 63.12 and loss3: 0.00\n",
      "Epoch [3218], train_loss: 539.19 with loss1: 476.07, loss2: 63.12 and loss3: 0.00\n",
      "Epoch [3219], train_loss: 539.48 with loss1: 476.35, loss2: 63.13 and loss3: 0.00\n",
      "Epoch [3220], train_loss: 532.55 with loss1: 469.44, loss2: 63.11 and loss3: 0.00\n",
      "Epoch [3221], train_loss: 533.97 with loss1: 470.87, loss2: 63.10 and loss3: 0.00\n",
      "Epoch [3222], train_loss: 530.33 with loss1: 467.27, loss2: 63.06 and loss3: 0.00\n",
      "Epoch [3223], train_loss: 530.98 with loss1: 467.84, loss2: 63.14 and loss3: 0.00\n",
      "Epoch [3224], train_loss: 528.64 with loss1: 465.54, loss2: 63.10 and loss3: 0.00\n",
      "Epoch [3225], train_loss: 526.76 with loss1: 463.65, loss2: 63.10 and loss3: 0.00\n",
      "Epoch [3226], train_loss: 525.05 with loss1: 461.97, loss2: 63.08 and loss3: 0.00\n",
      "Epoch [3227], train_loss: 524.88 with loss1: 461.80, loss2: 63.08 and loss3: 0.00\n",
      "Epoch [3228], train_loss: 520.45 with loss1: 457.42, loss2: 63.02 and loss3: 0.00\n",
      "Epoch [3229], train_loss: 522.31 with loss1: 459.32, loss2: 62.98 and loss3: 0.00\n",
      "Epoch [3230], train_loss: 519.42 with loss1: 456.34, loss2: 63.08 and loss3: 0.00\n",
      "Epoch [3231], train_loss: 520.10 with loss1: 457.15, loss2: 62.95 and loss3: 0.00\n",
      "Epoch [3232], train_loss: 518.99 with loss1: 456.08, loss2: 62.91 and loss3: 0.00\n",
      "Epoch [3233], train_loss: 520.15 with loss1: 457.19, loss2: 62.96 and loss3: 0.00\n",
      "Epoch [3234], train_loss: 517.45 with loss1: 454.51, loss2: 62.95 and loss3: 0.00\n",
      "Epoch [3235], train_loss: 518.20 with loss1: 455.31, loss2: 62.89 and loss3: 0.00\n",
      "Epoch [3236], train_loss: 516.31 with loss1: 453.37, loss2: 62.95 and loss3: 0.00\n",
      "Epoch [3237], train_loss: 515.10 with loss1: 452.09, loss2: 63.01 and loss3: 0.00\n",
      "Epoch [3238], train_loss: 515.43 with loss1: 452.52, loss2: 62.91 and loss3: 0.00\n",
      "Epoch [3239], train_loss: 515.91 with loss1: 453.02, loss2: 62.90 and loss3: 0.00\n",
      "Epoch [3240], train_loss: 512.26 with loss1: 449.39, loss2: 62.87 and loss3: 0.00\n",
      "Epoch [3241], train_loss: 512.56 with loss1: 449.73, loss2: 62.82 and loss3: 0.00\n",
      "Epoch [3242], train_loss: 510.24 with loss1: 447.37, loss2: 62.87 and loss3: 0.00\n",
      "Epoch [3243], train_loss: 510.66 with loss1: 447.74, loss2: 62.92 and loss3: 0.00\n",
      "Epoch [3244], train_loss: 511.69 with loss1: 448.80, loss2: 62.89 and loss3: 0.00\n",
      "Epoch [3245], train_loss: 510.90 with loss1: 448.03, loss2: 62.87 and loss3: 0.00\n",
      "Epoch [3246], train_loss: 508.50 with loss1: 445.59, loss2: 62.91 and loss3: 0.00\n",
      "Epoch [3247], train_loss: 508.84 with loss1: 445.93, loss2: 62.91 and loss3: 0.00\n",
      "Epoch [3248], train_loss: 510.25 with loss1: 447.44, loss2: 62.82 and loss3: 0.00\n",
      "Epoch [3249], train_loss: 507.43 with loss1: 444.58, loss2: 62.84 and loss3: 0.00\n",
      "Epoch [3250], train_loss: 507.26 with loss1: 444.49, loss2: 62.77 and loss3: 0.00\n",
      "Epoch [3251], train_loss: 507.79 with loss1: 445.05, loss2: 62.74 and loss3: 0.00\n",
      "Epoch [3252], train_loss: 506.34 with loss1: 443.52, loss2: 62.82 and loss3: 0.00\n",
      "Epoch [3253], train_loss: 508.12 with loss1: 445.36, loss2: 62.76 and loss3: 0.00\n",
      "Epoch [3254], train_loss: 506.27 with loss1: 443.48, loss2: 62.79 and loss3: 0.00\n",
      "Epoch [3255], train_loss: 509.06 with loss1: 446.40, loss2: 62.66 and loss3: 0.00\n",
      "Epoch [3256], train_loss: 507.18 with loss1: 444.43, loss2: 62.75 and loss3: 0.00\n",
      "Epoch [3257], train_loss: 507.01 with loss1: 444.30, loss2: 62.71 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3258], train_loss: 507.23 with loss1: 444.49, loss2: 62.74 and loss3: 0.00\n",
      "Epoch [3259], train_loss: 506.77 with loss1: 444.00, loss2: 62.77 and loss3: 0.00\n",
      "Epoch [3260], train_loss: 506.40 with loss1: 443.65, loss2: 62.74 and loss3: 0.00\n",
      "Epoch [3261], train_loss: 507.46 with loss1: 444.76, loss2: 62.70 and loss3: 0.00\n",
      "Epoch [3262], train_loss: 505.81 with loss1: 443.09, loss2: 62.72 and loss3: 0.00\n",
      "Epoch [3263], train_loss: 507.58 with loss1: 444.94, loss2: 62.63 and loss3: 0.00\n",
      "Epoch [3264], train_loss: 507.65 with loss1: 444.95, loss2: 62.70 and loss3: 0.00\n",
      "Epoch [3265], train_loss: 508.41 with loss1: 445.83, loss2: 62.59 and loss3: 0.00\n",
      "Epoch [3266], train_loss: 507.29 with loss1: 444.60, loss2: 62.69 and loss3: 0.00\n",
      "Epoch [3267], train_loss: 508.64 with loss1: 446.02, loss2: 62.63 and loss3: 0.00\n",
      "Epoch [3268], train_loss: 508.22 with loss1: 445.63, loss2: 62.59 and loss3: 0.00\n",
      "Epoch [3269], train_loss: 509.15 with loss1: 446.59, loss2: 62.55 and loss3: 0.00\n",
      "Epoch [3270], train_loss: 507.93 with loss1: 445.30, loss2: 62.63 and loss3: 0.00\n",
      "Epoch [3271], train_loss: 509.95 with loss1: 447.33, loss2: 62.62 and loss3: 0.00\n",
      "Epoch [3272], train_loss: 508.67 with loss1: 446.05, loss2: 62.62 and loss3: 0.00\n",
      "Epoch [3273], train_loss: 510.08 with loss1: 447.46, loss2: 62.62 and loss3: 0.00\n",
      "Epoch [3274], train_loss: 512.20 with loss1: 449.56, loss2: 62.64 and loss3: 0.00\n",
      "Epoch [3275], train_loss: 513.16 with loss1: 450.60, loss2: 62.56 and loss3: 0.00\n",
      "Epoch [3276], train_loss: 509.57 with loss1: 446.96, loss2: 62.62 and loss3: 0.00\n",
      "Epoch [3277], train_loss: 510.91 with loss1: 448.34, loss2: 62.57 and loss3: 0.00\n",
      "Epoch [3278], train_loss: 509.95 with loss1: 447.32, loss2: 62.63 and loss3: 0.00\n",
      "Epoch [3279], train_loss: 515.12 with loss1: 452.60, loss2: 62.52 and loss3: 0.00\n",
      "Epoch [3280], train_loss: 510.55 with loss1: 448.00, loss2: 62.55 and loss3: 0.00\n",
      "Epoch [3281], train_loss: 510.89 with loss1: 448.38, loss2: 62.51 and loss3: 0.00\n",
      "Epoch [3282], train_loss: 511.81 with loss1: 449.23, loss2: 62.58 and loss3: 0.00\n",
      "Epoch [3283], train_loss: 514.18 with loss1: 451.65, loss2: 62.53 and loss3: 0.00\n",
      "Epoch [3284], train_loss: 510.40 with loss1: 447.76, loss2: 62.64 and loss3: 0.00\n",
      "Epoch [3285], train_loss: 513.48 with loss1: 450.98, loss2: 62.50 and loss3: 0.00\n",
      "Epoch [3286], train_loss: 510.83 with loss1: 448.35, loss2: 62.48 and loss3: 0.00\n",
      "Epoch [3287], train_loss: 510.72 with loss1: 448.23, loss2: 62.48 and loss3: 0.00\n",
      "Epoch [3288], train_loss: 511.42 with loss1: 448.82, loss2: 62.60 and loss3: 0.00\n",
      "Epoch [3289], train_loss: 513.83 with loss1: 451.28, loss2: 62.55 and loss3: 0.00\n",
      "Epoch [3290], train_loss: 510.81 with loss1: 448.26, loss2: 62.54 and loss3: 0.00\n",
      "Epoch [3291], train_loss: 512.75 with loss1: 450.29, loss2: 62.45 and loss3: 0.00\n",
      "Epoch [3292], train_loss: 510.37 with loss1: 447.94, loss2: 62.43 and loss3: 0.00\n",
      "Epoch [3293], train_loss: 511.67 with loss1: 449.21, loss2: 62.46 and loss3: 0.00\n",
      "Epoch [3294], train_loss: 510.18 with loss1: 447.77, loss2: 62.41 and loss3: 0.00\n",
      "Epoch [3295], train_loss: 511.65 with loss1: 449.24, loss2: 62.41 and loss3: 0.00\n",
      "Epoch [3296], train_loss: 509.73 with loss1: 447.27, loss2: 62.46 and loss3: 0.00\n",
      "Epoch [3297], train_loss: 512.26 with loss1: 449.82, loss2: 62.43 and loss3: 0.00\n",
      "Epoch [3298], train_loss: 510.53 with loss1: 448.09, loss2: 62.44 and loss3: 0.00\n",
      "Epoch [3299], train_loss: 512.44 with loss1: 450.08, loss2: 62.36 and loss3: 0.00\n",
      "Epoch [3300], train_loss: 511.39 with loss1: 448.93, loss2: 62.45 and loss3: 0.00\n",
      "Epoch [3301], train_loss: 510.66 with loss1: 448.25, loss2: 62.41 and loss3: 0.00\n",
      "Epoch [3302], train_loss: 510.68 with loss1: 448.23, loss2: 62.45 and loss3: 0.00\n",
      "Epoch [3303], train_loss: 510.93 with loss1: 448.56, loss2: 62.37 and loss3: 0.00\n",
      "Epoch [3304], train_loss: 511.25 with loss1: 448.83, loss2: 62.43 and loss3: 0.00\n",
      "Epoch [3305], train_loss: 512.08 with loss1: 449.69, loss2: 62.39 and loss3: 0.00\n",
      "Epoch [3306], train_loss: 508.26 with loss1: 445.93, loss2: 62.33 and loss3: 0.00\n",
      "Epoch [3307], train_loss: 509.34 with loss1: 446.99, loss2: 62.35 and loss3: 0.00\n",
      "Epoch [3308], train_loss: 509.30 with loss1: 446.98, loss2: 62.32 and loss3: 0.00\n",
      "Epoch [3309], train_loss: 508.66 with loss1: 446.28, loss2: 62.38 and loss3: 0.00\n",
      "Epoch [3310], train_loss: 507.67 with loss1: 445.29, loss2: 62.37 and loss3: 0.00\n",
      "Epoch [3311], train_loss: 507.17 with loss1: 444.82, loss2: 62.35 and loss3: 0.00\n",
      "Epoch [3312], train_loss: 506.63 with loss1: 444.30, loss2: 62.33 and loss3: 0.00\n",
      "Epoch [3313], train_loss: 509.72 with loss1: 447.44, loss2: 62.29 and loss3: 0.00\n",
      "Epoch [3314], train_loss: 507.94 with loss1: 445.65, loss2: 62.29 and loss3: 0.00\n",
      "Epoch [3315], train_loss: 509.61 with loss1: 447.35, loss2: 62.25 and loss3: 0.00\n",
      "Epoch [3316], train_loss: 508.01 with loss1: 445.72, loss2: 62.29 and loss3: 0.00\n",
      "Epoch [3317], train_loss: 510.18 with loss1: 447.87, loss2: 62.30 and loss3: 0.00\n",
      "Epoch [3318], train_loss: 507.39 with loss1: 445.11, loss2: 62.27 and loss3: 0.00\n",
      "Epoch [3319], train_loss: 509.01 with loss1: 446.77, loss2: 62.25 and loss3: 0.00\n",
      "Epoch [3320], train_loss: 507.19 with loss1: 444.94, loss2: 62.24 and loss3: 0.00\n",
      "Epoch [3321], train_loss: 508.78 with loss1: 446.54, loss2: 62.24 and loss3: 0.00\n",
      "Epoch [3322], train_loss: 507.98 with loss1: 445.70, loss2: 62.28 and loss3: 0.00\n",
      "Epoch [3323], train_loss: 509.39 with loss1: 447.19, loss2: 62.20 and loss3: 0.00\n",
      "Epoch [3324], train_loss: 508.34 with loss1: 446.11, loss2: 62.23 and loss3: 0.00\n",
      "Epoch [3325], train_loss: 508.65 with loss1: 446.48, loss2: 62.18 and loss3: 0.00\n",
      "Epoch [3326], train_loss: 508.87 with loss1: 446.69, loss2: 62.18 and loss3: 0.00\n",
      "Epoch [3327], train_loss: 510.75 with loss1: 448.57, loss2: 62.18 and loss3: 0.00\n",
      "Epoch [3328], train_loss: 510.82 with loss1: 448.60, loss2: 62.22 and loss3: 0.00\n",
      "Epoch [3329], train_loss: 509.49 with loss1: 447.42, loss2: 62.07 and loss3: 0.00\n",
      "Epoch [3330], train_loss: 508.89 with loss1: 446.79, loss2: 62.11 and loss3: 0.00\n",
      "Epoch [3331], train_loss: 511.18 with loss1: 449.07, loss2: 62.10 and loss3: 0.00\n",
      "Epoch [3332], train_loss: 508.40 with loss1: 446.33, loss2: 62.07 and loss3: 0.00\n",
      "Epoch [3333], train_loss: 512.36 with loss1: 450.33, loss2: 62.04 and loss3: 0.00\n",
      "Epoch [3334], train_loss: 511.02 with loss1: 448.89, loss2: 62.13 and loss3: 0.00\n",
      "Epoch [3335], train_loss: 511.26 with loss1: 449.13, loss2: 62.13 and loss3: 0.00\n",
      "Epoch [3336], train_loss: 511.30 with loss1: 449.19, loss2: 62.11 and loss3: 0.00\n",
      "Epoch [3337], train_loss: 512.51 with loss1: 450.42, loss2: 62.09 and loss3: 0.00\n",
      "Epoch [3338], train_loss: 511.19 with loss1: 449.06, loss2: 62.12 and loss3: 0.00\n",
      "Epoch [3339], train_loss: 512.99 with loss1: 450.98, loss2: 62.01 and loss3: 0.00\n",
      "Epoch [3340], train_loss: 512.27 with loss1: 450.22, loss2: 62.05 and loss3: 0.00\n",
      "Epoch [3341], train_loss: 515.36 with loss1: 453.27, loss2: 62.10 and loss3: 0.00\n",
      "Epoch [3342], train_loss: 513.93 with loss1: 451.86, loss2: 62.07 and loss3: 0.00\n",
      "Epoch [3343], train_loss: 516.45 with loss1: 454.46, loss2: 61.99 and loss3: 0.00\n",
      "Epoch [3344], train_loss: 517.64 with loss1: 455.60, loss2: 62.04 and loss3: 0.00\n",
      "Epoch [3345], train_loss: 518.28 with loss1: 456.28, loss2: 61.99 and loss3: 0.00\n",
      "Epoch [3346], train_loss: 518.35 with loss1: 456.32, loss2: 62.03 and loss3: 0.00\n",
      "Epoch [3347], train_loss: 518.53 with loss1: 456.54, loss2: 61.99 and loss3: 0.00\n",
      "Epoch [3348], train_loss: 516.27 with loss1: 454.32, loss2: 61.95 and loss3: 0.00\n",
      "Epoch [3349], train_loss: 519.78 with loss1: 457.84, loss2: 61.94 and loss3: 0.00\n",
      "Epoch [3350], train_loss: 519.54 with loss1: 457.56, loss2: 61.98 and loss3: 0.00\n",
      "Epoch [3351], train_loss: 523.06 with loss1: 461.10, loss2: 61.96 and loss3: 0.00\n",
      "Epoch [3352], train_loss: 521.06 with loss1: 459.15, loss2: 61.91 and loss3: 0.00\n",
      "Epoch [3353], train_loss: 521.89 with loss1: 459.96, loss2: 61.93 and loss3: 0.00\n",
      "Epoch [3354], train_loss: 521.53 with loss1: 459.58, loss2: 61.94 and loss3: 0.00\n",
      "Epoch [3355], train_loss: 524.09 with loss1: 462.24, loss2: 61.84 and loss3: 0.00\n",
      "Epoch [3356], train_loss: 522.85 with loss1: 460.92, loss2: 61.92 and loss3: 0.00\n",
      "Epoch [3357], train_loss: 526.09 with loss1: 464.19, loss2: 61.91 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3358], train_loss: 524.47 with loss1: 462.61, loss2: 61.86 and loss3: 0.00\n",
      "Epoch [3359], train_loss: 527.73 with loss1: 465.85, loss2: 61.88 and loss3: 0.00\n",
      "Epoch [3360], train_loss: 524.58 with loss1: 462.71, loss2: 61.87 and loss3: 0.00\n",
      "Epoch [3361], train_loss: 529.22 with loss1: 467.34, loss2: 61.88 and loss3: 0.00\n",
      "Epoch [3362], train_loss: 526.86 with loss1: 465.03, loss2: 61.82 and loss3: 0.00\n",
      "Epoch [3363], train_loss: 528.17 with loss1: 466.40, loss2: 61.77 and loss3: 0.00\n",
      "Epoch [3364], train_loss: 528.61 with loss1: 466.73, loss2: 61.88 and loss3: 0.00\n",
      "Epoch [3365], train_loss: 529.91 with loss1: 468.03, loss2: 61.88 and loss3: 0.00\n",
      "Epoch [3366], train_loss: 529.80 with loss1: 467.95, loss2: 61.85 and loss3: 0.00\n",
      "Epoch [3367], train_loss: 530.62 with loss1: 468.80, loss2: 61.82 and loss3: 0.00\n",
      "Epoch [3368], train_loss: 530.73 with loss1: 468.99, loss2: 61.75 and loss3: 0.00\n",
      "Epoch [3369], train_loss: 531.45 with loss1: 469.68, loss2: 61.77 and loss3: 0.00\n",
      "Epoch [3370], train_loss: 529.93 with loss1: 468.18, loss2: 61.75 and loss3: 0.00\n",
      "Epoch [3371], train_loss: 529.54 with loss1: 467.80, loss2: 61.74 and loss3: 0.00\n",
      "Epoch [3372], train_loss: 526.72 with loss1: 464.97, loss2: 61.75 and loss3: 0.00\n",
      "Epoch [3373], train_loss: 526.85 with loss1: 465.03, loss2: 61.82 and loss3: 0.00\n",
      "Epoch [3374], train_loss: 524.92 with loss1: 463.17, loss2: 61.75 and loss3: 0.00\n",
      "Epoch [3375], train_loss: 525.45 with loss1: 463.70, loss2: 61.76 and loss3: 0.00\n",
      "Epoch [3376], train_loss: 522.56 with loss1: 460.78, loss2: 61.78 and loss3: 0.00\n",
      "Epoch [3377], train_loss: 522.57 with loss1: 460.86, loss2: 61.71 and loss3: 0.00\n",
      "Epoch [3378], train_loss: 518.83 with loss1: 457.03, loss2: 61.80 and loss3: 0.00\n",
      "Epoch [3379], train_loss: 519.21 with loss1: 457.49, loss2: 61.72 and loss3: 0.00\n",
      "Epoch [3380], train_loss: 515.86 with loss1: 454.15, loss2: 61.71 and loss3: 0.00\n",
      "Epoch [3381], train_loss: 516.14 with loss1: 454.43, loss2: 61.71 and loss3: 0.00\n",
      "Epoch [3382], train_loss: 513.82 with loss1: 452.14, loss2: 61.68 and loss3: 0.00\n",
      "Epoch [3383], train_loss: 514.41 with loss1: 452.74, loss2: 61.67 and loss3: 0.00\n",
      "Epoch [3384], train_loss: 509.69 with loss1: 448.01, loss2: 61.69 and loss3: 0.00\n",
      "Epoch [3385], train_loss: 509.50 with loss1: 447.74, loss2: 61.76 and loss3: 0.00\n",
      "Epoch [3386], train_loss: 506.47 with loss1: 444.72, loss2: 61.75 and loss3: 0.00\n",
      "Epoch [3387], train_loss: 506.68 with loss1: 445.02, loss2: 61.66 and loss3: 0.00\n",
      "Epoch [3388], train_loss: 503.41 with loss1: 441.80, loss2: 61.61 and loss3: 0.00\n",
      "Epoch [3389], train_loss: 502.75 with loss1: 441.04, loss2: 61.71 and loss3: 0.00\n",
      "Epoch [3390], train_loss: 500.70 with loss1: 439.06, loss2: 61.64 and loss3: 0.00\n",
      "Epoch [3391], train_loss: 500.11 with loss1: 438.48, loss2: 61.64 and loss3: 0.00\n",
      "Epoch [3392], train_loss: 498.35 with loss1: 436.76, loss2: 61.59 and loss3: 0.00\n",
      "Epoch [3393], train_loss: 497.27 with loss1: 435.64, loss2: 61.62 and loss3: 0.00\n",
      "Epoch [3394], train_loss: 497.34 with loss1: 435.79, loss2: 61.55 and loss3: 0.00\n",
      "Epoch [3395], train_loss: 497.52 with loss1: 435.88, loss2: 61.64 and loss3: 0.00\n",
      "Epoch [3396], train_loss: 495.86 with loss1: 434.37, loss2: 61.49 and loss3: 0.00\n",
      "Epoch [3397], train_loss: 496.04 with loss1: 434.45, loss2: 61.60 and loss3: 0.00\n",
      "Epoch [3398], train_loss: 494.42 with loss1: 432.84, loss2: 61.58 and loss3: 0.00\n",
      "Epoch [3399], train_loss: 497.59 with loss1: 435.99, loss2: 61.60 and loss3: 0.00\n",
      "Epoch [3400], train_loss: 493.33 with loss1: 431.82, loss2: 61.52 and loss3: 0.00\n",
      "Epoch [3401], train_loss: 493.23 with loss1: 431.68, loss2: 61.55 and loss3: 0.00\n",
      "Epoch [3402], train_loss: 492.95 with loss1: 431.43, loss2: 61.52 and loss3: 0.00\n",
      "Epoch [3403], train_loss: 493.30 with loss1: 431.80, loss2: 61.49 and loss3: 0.00\n",
      "Epoch [3404], train_loss: 490.76 with loss1: 429.24, loss2: 61.52 and loss3: 0.00\n",
      "Epoch [3405], train_loss: 491.65 with loss1: 430.07, loss2: 61.58 and loss3: 0.00\n",
      "Epoch [3406], train_loss: 489.23 with loss1: 427.78, loss2: 61.45 and loss3: 0.00\n",
      "Epoch [3407], train_loss: 490.13 with loss1: 428.70, loss2: 61.43 and loss3: 0.00\n",
      "Epoch [3408], train_loss: 490.71 with loss1: 429.29, loss2: 61.43 and loss3: 0.00\n",
      "Epoch [3409], train_loss: 490.46 with loss1: 428.99, loss2: 61.47 and loss3: 0.00\n",
      "Epoch [3410], train_loss: 487.95 with loss1: 426.58, loss2: 61.37 and loss3: 0.00\n",
      "Epoch [3411], train_loss: 489.72 with loss1: 428.28, loss2: 61.44 and loss3: 0.00\n",
      "Epoch [3412], train_loss: 488.36 with loss1: 426.94, loss2: 61.42 and loss3: 0.00\n",
      "Epoch [3413], train_loss: 490.27 with loss1: 428.86, loss2: 61.41 and loss3: 0.00\n",
      "Epoch [3414], train_loss: 488.90 with loss1: 427.51, loss2: 61.38 and loss3: 0.00\n",
      "Epoch [3415], train_loss: 490.73 with loss1: 429.29, loss2: 61.44 and loss3: 0.00\n",
      "Epoch [3416], train_loss: 490.33 with loss1: 428.91, loss2: 61.42 and loss3: 0.00\n",
      "Epoch [3417], train_loss: 491.61 with loss1: 430.24, loss2: 61.37 and loss3: 0.00\n",
      "Epoch [3418], train_loss: 491.98 with loss1: 430.61, loss2: 61.37 and loss3: 0.00\n",
      "Epoch [3419], train_loss: 491.49 with loss1: 430.08, loss2: 61.41 and loss3: 0.00\n",
      "Epoch [3420], train_loss: 493.36 with loss1: 432.00, loss2: 61.36 and loss3: 0.00\n",
      "Epoch [3421], train_loss: 492.72 with loss1: 431.33, loss2: 61.38 and loss3: 0.00\n",
      "Epoch [3422], train_loss: 491.98 with loss1: 430.67, loss2: 61.31 and loss3: 0.00\n",
      "Epoch [3423], train_loss: 493.96 with loss1: 432.65, loss2: 61.32 and loss3: 0.00\n",
      "Epoch [3424], train_loss: 493.77 with loss1: 432.40, loss2: 61.38 and loss3: 0.00\n",
      "Epoch [3425], train_loss: 496.12 with loss1: 434.76, loss2: 61.36 and loss3: 0.00\n",
      "Epoch [3426], train_loss: 494.83 with loss1: 433.50, loss2: 61.33 and loss3: 0.00\n",
      "Epoch [3427], train_loss: 497.26 with loss1: 435.93, loss2: 61.32 and loss3: 0.00\n",
      "Epoch [3428], train_loss: 498.24 with loss1: 436.89, loss2: 61.35 and loss3: 0.00\n",
      "Epoch [3429], train_loss: 497.84 with loss1: 436.51, loss2: 61.33 and loss3: 0.00\n",
      "Epoch [3430], train_loss: 498.35 with loss1: 437.03, loss2: 61.31 and loss3: 0.00\n",
      "Epoch [3431], train_loss: 500.48 with loss1: 439.16, loss2: 61.32 and loss3: 0.00\n",
      "Epoch [3432], train_loss: 499.50 with loss1: 438.25, loss2: 61.25 and loss3: 0.00\n",
      "Epoch [3433], train_loss: 500.76 with loss1: 439.47, loss2: 61.29 and loss3: 0.00\n",
      "Epoch [3434], train_loss: 501.11 with loss1: 439.84, loss2: 61.27 and loss3: 0.00\n",
      "Epoch [3435], train_loss: 501.14 with loss1: 439.93, loss2: 61.21 and loss3: 0.00\n",
      "Epoch [3436], train_loss: 501.85 with loss1: 440.59, loss2: 61.26 and loss3: 0.00\n",
      "Epoch [3437], train_loss: 503.77 with loss1: 442.47, loss2: 61.30 and loss3: 0.00\n",
      "Epoch [3438], train_loss: 503.89 with loss1: 442.72, loss2: 61.17 and loss3: 0.00\n",
      "Epoch [3439], train_loss: 503.14 with loss1: 441.84, loss2: 61.30 and loss3: 0.00\n",
      "Epoch [3440], train_loss: 504.02 with loss1: 442.77, loss2: 61.25 and loss3: 0.00\n",
      "Epoch [3441], train_loss: 505.74 with loss1: 444.40, loss2: 61.34 and loss3: 0.00\n",
      "Epoch [3442], train_loss: 505.50 with loss1: 444.29, loss2: 61.21 and loss3: 0.00\n",
      "Epoch [3443], train_loss: 506.28 with loss1: 444.98, loss2: 61.29 and loss3: 0.00\n",
      "Epoch [3444], train_loss: 505.40 with loss1: 444.15, loss2: 61.25 and loss3: 0.00\n",
      "Epoch [3445], train_loss: 506.53 with loss1: 445.25, loss2: 61.28 and loss3: 0.00\n",
      "Epoch [3446], train_loss: 506.86 with loss1: 445.64, loss2: 61.22 and loss3: 0.00\n",
      "Epoch [3447], train_loss: 506.52 with loss1: 445.29, loss2: 61.23 and loss3: 0.00\n",
      "Epoch [3448], train_loss: 505.19 with loss1: 444.03, loss2: 61.16 and loss3: 0.00\n",
      "Epoch [3449], train_loss: 508.70 with loss1: 447.52, loss2: 61.17 and loss3: 0.00\n",
      "Epoch [3450], train_loss: 507.90 with loss1: 446.72, loss2: 61.18 and loss3: 0.00\n",
      "Epoch [3451], train_loss: 509.83 with loss1: 448.62, loss2: 61.21 and loss3: 0.00\n",
      "Epoch [3452], train_loss: 509.91 with loss1: 448.72, loss2: 61.18 and loss3: 0.00\n",
      "Epoch [3453], train_loss: 512.97 with loss1: 451.75, loss2: 61.22 and loss3: 0.00\n",
      "Epoch [3454], train_loss: 511.32 with loss1: 450.14, loss2: 61.18 and loss3: 0.00\n",
      "Epoch [3455], train_loss: 512.40 with loss1: 451.29, loss2: 61.11 and loss3: 0.00\n",
      "Epoch [3456], train_loss: 510.70 with loss1: 449.59, loss2: 61.11 and loss3: 0.00\n",
      "Epoch [3457], train_loss: 513.29 with loss1: 452.07, loss2: 61.22 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3458], train_loss: 511.37 with loss1: 450.22, loss2: 61.15 and loss3: 0.00\n",
      "Epoch [3459], train_loss: 515.15 with loss1: 454.02, loss2: 61.13 and loss3: 0.00\n",
      "Epoch [3460], train_loss: 514.49 with loss1: 453.46, loss2: 61.03 and loss3: 0.00\n",
      "Epoch [3461], train_loss: 515.87 with loss1: 454.76, loss2: 61.11 and loss3: 0.00\n",
      "Epoch [3462], train_loss: 513.80 with loss1: 452.77, loss2: 61.03 and loss3: 0.00\n",
      "Epoch [3463], train_loss: 517.17 with loss1: 456.06, loss2: 61.10 and loss3: 0.00\n",
      "Epoch [3464], train_loss: 514.06 with loss1: 452.99, loss2: 61.07 and loss3: 0.00\n",
      "Epoch [3465], train_loss: 516.96 with loss1: 455.85, loss2: 61.12 and loss3: 0.00\n",
      "Epoch [3466], train_loss: 514.53 with loss1: 453.52, loss2: 61.01 and loss3: 0.00\n",
      "Epoch [3467], train_loss: 518.53 with loss1: 457.43, loss2: 61.10 and loss3: 0.00\n",
      "Epoch [3468], train_loss: 516.61 with loss1: 455.52, loss2: 61.08 and loss3: 0.00\n",
      "Epoch [3469], train_loss: 517.96 with loss1: 456.82, loss2: 61.14 and loss3: 0.00\n",
      "Epoch [3470], train_loss: 517.33 with loss1: 456.29, loss2: 61.04 and loss3: 0.00\n",
      "Epoch [3471], train_loss: 520.97 with loss1: 459.87, loss2: 61.10 and loss3: 0.00\n",
      "Epoch [3472], train_loss: 516.70 with loss1: 455.72, loss2: 60.98 and loss3: 0.00\n",
      "Epoch [3473], train_loss: 520.92 with loss1: 459.92, loss2: 61.00 and loss3: 0.00\n",
      "Epoch [3474], train_loss: 516.94 with loss1: 455.98, loss2: 60.96 and loss3: 0.00\n",
      "Epoch [3475], train_loss: 519.80 with loss1: 458.74, loss2: 61.06 and loss3: 0.00\n",
      "Epoch [3476], train_loss: 517.89 with loss1: 456.96, loss2: 60.93 and loss3: 0.00\n",
      "Epoch [3477], train_loss: 517.95 with loss1: 456.92, loss2: 61.03 and loss3: 0.00\n",
      "Epoch [3478], train_loss: 514.50 with loss1: 453.54, loss2: 60.96 and loss3: 0.00\n",
      "Epoch [3479], train_loss: 519.48 with loss1: 458.46, loss2: 61.03 and loss3: 0.00\n",
      "Epoch [3480], train_loss: 514.14 with loss1: 453.25, loss2: 60.90 and loss3: 0.00\n",
      "Epoch [3481], train_loss: 517.53 with loss1: 456.56, loss2: 60.98 and loss3: 0.00\n",
      "Epoch [3482], train_loss: 515.85 with loss1: 454.87, loss2: 60.98 and loss3: 0.00\n",
      "Epoch [3483], train_loss: 516.45 with loss1: 455.49, loss2: 60.96 and loss3: 0.00\n",
      "Epoch [3484], train_loss: 512.34 with loss1: 451.40, loss2: 60.94 and loss3: 0.00\n",
      "Epoch [3485], train_loss: 514.02 with loss1: 453.03, loss2: 60.99 and loss3: 0.00\n",
      "Epoch [3486], train_loss: 511.21 with loss1: 450.26, loss2: 60.95 and loss3: 0.00\n",
      "Epoch [3487], train_loss: 511.79 with loss1: 450.84, loss2: 60.95 and loss3: 0.00\n",
      "Epoch [3488], train_loss: 508.24 with loss1: 447.32, loss2: 60.92 and loss3: 0.00\n",
      "Epoch [3489], train_loss: 509.31 with loss1: 448.40, loss2: 60.91 and loss3: 0.00\n",
      "Epoch [3490], train_loss: 505.28 with loss1: 444.40, loss2: 60.88 and loss3: 0.00\n",
      "Epoch [3491], train_loss: 506.62 with loss1: 445.66, loss2: 60.96 and loss3: 0.00\n",
      "Epoch [3492], train_loss: 502.17 with loss1: 441.30, loss2: 60.87 and loss3: 0.00\n",
      "Epoch [3493], train_loss: 506.56 with loss1: 445.73, loss2: 60.83 and loss3: 0.00\n",
      "Epoch [3494], train_loss: 501.67 with loss1: 440.84, loss2: 60.82 and loss3: 0.00\n",
      "Epoch [3495], train_loss: 501.89 with loss1: 441.00, loss2: 60.89 and loss3: 0.00\n",
      "Epoch [3496], train_loss: 500.91 with loss1: 440.05, loss2: 60.86 and loss3: 0.00\n",
      "Epoch [3497], train_loss: 504.50 with loss1: 443.65, loss2: 60.85 and loss3: 0.00\n",
      "Epoch [3498], train_loss: 503.20 with loss1: 442.30, loss2: 60.90 and loss3: 0.00\n",
      "Epoch [3499], train_loss: 501.80 with loss1: 440.88, loss2: 60.92 and loss3: 0.00\n",
      "Epoch [3500], train_loss: 501.10 with loss1: 440.28, loss2: 60.82 and loss3: 0.00\n",
      "Epoch [3501], train_loss: 499.91 with loss1: 439.02, loss2: 60.89 and loss3: 0.00\n",
      "Epoch [3502], train_loss: 497.89 with loss1: 437.02, loss2: 60.87 and loss3: 0.00\n",
      "Epoch [3503], train_loss: 498.14 with loss1: 437.24, loss2: 60.90 and loss3: 0.00\n",
      "Epoch [3504], train_loss: 498.76 with loss1: 437.90, loss2: 60.86 and loss3: 0.00\n",
      "Epoch [3505], train_loss: 496.58 with loss1: 435.82, loss2: 60.77 and loss3: 0.00\n",
      "Epoch [3506], train_loss: 495.47 with loss1: 434.63, loss2: 60.84 and loss3: 0.00\n",
      "Epoch [3507], train_loss: 496.65 with loss1: 435.87, loss2: 60.78 and loss3: 0.00\n",
      "Epoch [3508], train_loss: 495.46 with loss1: 434.62, loss2: 60.83 and loss3: 0.00\n",
      "Epoch [3509], train_loss: 496.75 with loss1: 435.94, loss2: 60.81 and loss3: 0.00\n",
      "Epoch [3510], train_loss: 496.44 with loss1: 435.61, loss2: 60.82 and loss3: 0.00\n",
      "Epoch [3511], train_loss: 496.41 with loss1: 435.62, loss2: 60.79 and loss3: 0.00\n",
      "Epoch [3512], train_loss: 494.56 with loss1: 433.75, loss2: 60.81 and loss3: 0.00\n",
      "Epoch [3513], train_loss: 494.83 with loss1: 434.03, loss2: 60.80 and loss3: 0.00\n",
      "Epoch [3514], train_loss: 493.77 with loss1: 432.97, loss2: 60.80 and loss3: 0.00\n",
      "Epoch [3515], train_loss: 496.00 with loss1: 435.20, loss2: 60.81 and loss3: 0.00\n",
      "Epoch [3516], train_loss: 493.92 with loss1: 433.10, loss2: 60.82 and loss3: 0.00\n",
      "Epoch [3517], train_loss: 496.04 with loss1: 435.18, loss2: 60.86 and loss3: 0.00\n",
      "Epoch [3518], train_loss: 494.56 with loss1: 433.74, loss2: 60.82 and loss3: 0.00\n",
      "Epoch [3519], train_loss: 493.55 with loss1: 432.78, loss2: 60.77 and loss3: 0.00\n",
      "Epoch [3520], train_loss: 491.21 with loss1: 430.41, loss2: 60.80 and loss3: 0.00\n",
      "Epoch [3521], train_loss: 494.28 with loss1: 433.49, loss2: 60.79 and loss3: 0.00\n",
      "Epoch [3522], train_loss: 491.69 with loss1: 430.92, loss2: 60.77 and loss3: 0.00\n",
      "Epoch [3523], train_loss: 492.13 with loss1: 431.31, loss2: 60.81 and loss3: 0.00\n",
      "Epoch [3524], train_loss: 491.68 with loss1: 430.87, loss2: 60.81 and loss3: 0.00\n",
      "Epoch [3525], train_loss: 491.68 with loss1: 430.92, loss2: 60.76 and loss3: 0.00\n",
      "Epoch [3526], train_loss: 489.40 with loss1: 428.63, loss2: 60.76 and loss3: 0.00\n",
      "Epoch [3527], train_loss: 490.00 with loss1: 429.29, loss2: 60.71 and loss3: 0.00\n",
      "Epoch [3528], train_loss: 489.43 with loss1: 428.65, loss2: 60.78 and loss3: 0.00\n",
      "Epoch [3529], train_loss: 489.35 with loss1: 428.61, loss2: 60.74 and loss3: 0.00\n",
      "Epoch [3530], train_loss: 488.10 with loss1: 427.35, loss2: 60.75 and loss3: 0.00\n",
      "Epoch [3531], train_loss: 486.87 with loss1: 426.14, loss2: 60.73 and loss3: 0.00\n",
      "Epoch [3532], train_loss: 486.26 with loss1: 425.58, loss2: 60.68 and loss3: 0.00\n",
      "Epoch [3533], train_loss: 486.17 with loss1: 425.49, loss2: 60.68 and loss3: 0.00\n",
      "Epoch [3534], train_loss: 484.65 with loss1: 424.00, loss2: 60.65 and loss3: 0.00\n",
      "Epoch [3535], train_loss: 486.07 with loss1: 425.38, loss2: 60.69 and loss3: 0.00\n",
      "Epoch [3536], train_loss: 483.16 with loss1: 422.52, loss2: 60.64 and loss3: 0.00\n",
      "Epoch [3537], train_loss: 485.60 with loss1: 424.96, loss2: 60.64 and loss3: 0.00\n",
      "Epoch [3538], train_loss: 483.28 with loss1: 422.67, loss2: 60.60 and loss3: 0.00\n",
      "Epoch [3539], train_loss: 484.35 with loss1: 423.73, loss2: 60.62 and loss3: 0.00\n",
      "Epoch [3540], train_loss: 484.00 with loss1: 423.39, loss2: 60.60 and loss3: 0.00\n",
      "Epoch [3541], train_loss: 484.59 with loss1: 424.01, loss2: 60.58 and loss3: 0.00\n",
      "Epoch [3542], train_loss: 483.20 with loss1: 422.62, loss2: 60.58 and loss3: 0.00\n",
      "Epoch [3543], train_loss: 484.32 with loss1: 423.74, loss2: 60.58 and loss3: 0.00\n",
      "Epoch [3544], train_loss: 484.11 with loss1: 423.55, loss2: 60.57 and loss3: 0.00\n",
      "Epoch [3545], train_loss: 483.67 with loss1: 423.14, loss2: 60.53 and loss3: 0.00\n",
      "Epoch [3546], train_loss: 481.58 with loss1: 420.99, loss2: 60.59 and loss3: 0.00\n",
      "Epoch [3547], train_loss: 481.38 with loss1: 420.79, loss2: 60.58 and loss3: 0.00\n",
      "Epoch [3548], train_loss: 480.73 with loss1: 420.17, loss2: 60.56 and loss3: 0.00\n",
      "Epoch [3549], train_loss: 482.58 with loss1: 422.03, loss2: 60.55 and loss3: 0.00\n",
      "Epoch [3550], train_loss: 481.59 with loss1: 421.09, loss2: 60.50 and loss3: 0.00\n",
      "Epoch [3551], train_loss: 481.72 with loss1: 421.18, loss2: 60.54 and loss3: 0.00\n",
      "Epoch [3552], train_loss: 479.35 with loss1: 418.85, loss2: 60.50 and loss3: 0.00\n",
      "Epoch [3553], train_loss: 479.90 with loss1: 419.39, loss2: 60.51 and loss3: 0.00\n",
      "Epoch [3554], train_loss: 479.00 with loss1: 418.51, loss2: 60.50 and loss3: 0.00\n",
      "Epoch [3555], train_loss: 480.17 with loss1: 419.64, loss2: 60.52 and loss3: 0.00\n",
      "Epoch [3556], train_loss: 477.92 with loss1: 417.47, loss2: 60.45 and loss3: 0.00\n",
      "Epoch [3557], train_loss: 479.25 with loss1: 418.82, loss2: 60.43 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3558], train_loss: 479.06 with loss1: 418.68, loss2: 60.38 and loss3: 0.00\n",
      "Epoch [3559], train_loss: 479.22 with loss1: 418.78, loss2: 60.44 and loss3: 0.00\n",
      "Epoch [3560], train_loss: 479.22 with loss1: 418.78, loss2: 60.44 and loss3: 0.00\n",
      "Epoch [3561], train_loss: 483.20 with loss1: 422.77, loss2: 60.43 and loss3: 0.00\n",
      "Epoch [3562], train_loss: 478.85 with loss1: 418.48, loss2: 60.36 and loss3: 0.00\n",
      "Epoch [3563], train_loss: 479.48 with loss1: 419.15, loss2: 60.34 and loss3: 0.00\n",
      "Epoch [3564], train_loss: 478.15 with loss1: 417.76, loss2: 60.39 and loss3: 0.00\n",
      "Epoch [3565], train_loss: 479.54 with loss1: 419.19, loss2: 60.36 and loss3: 0.00\n",
      "Epoch [3566], train_loss: 479.24 with loss1: 418.90, loss2: 60.34 and loss3: 0.00\n",
      "Epoch [3567], train_loss: 478.70 with loss1: 418.28, loss2: 60.42 and loss3: 0.00\n",
      "Epoch [3568], train_loss: 479.38 with loss1: 419.11, loss2: 60.27 and loss3: 0.00\n",
      "Epoch [3569], train_loss: 480.36 with loss1: 420.04, loss2: 60.33 and loss3: 0.00\n",
      "Epoch [3570], train_loss: 478.80 with loss1: 418.43, loss2: 60.37 and loss3: 0.00\n",
      "Epoch [3571], train_loss: 480.35 with loss1: 420.02, loss2: 60.33 and loss3: 0.00\n",
      "Epoch [3572], train_loss: 479.09 with loss1: 418.73, loss2: 60.35 and loss3: 0.00\n",
      "Epoch [3573], train_loss: 481.02 with loss1: 420.74, loss2: 60.29 and loss3: 0.00\n",
      "Epoch [3574], train_loss: 480.80 with loss1: 420.48, loss2: 60.32 and loss3: 0.00\n",
      "Epoch [3575], train_loss: 482.47 with loss1: 422.15, loss2: 60.31 and loss3: 0.00\n",
      "Epoch [3576], train_loss: 482.48 with loss1: 422.26, loss2: 60.21 and loss3: 0.00\n",
      "Epoch [3577], train_loss: 483.78 with loss1: 423.55, loss2: 60.24 and loss3: 0.00\n",
      "Epoch [3578], train_loss: 481.91 with loss1: 421.61, loss2: 60.30 and loss3: 0.00\n",
      "Epoch [3579], train_loss: 484.91 with loss1: 424.67, loss2: 60.23 and loss3: 0.00\n",
      "Epoch [3580], train_loss: 483.28 with loss1: 423.01, loss2: 60.27 and loss3: 0.00\n",
      "Epoch [3581], train_loss: 482.70 with loss1: 422.44, loss2: 60.26 and loss3: 0.00\n",
      "Epoch [3582], train_loss: 484.01 with loss1: 423.73, loss2: 60.29 and loss3: 0.00\n",
      "Epoch [3583], train_loss: 484.84 with loss1: 424.62, loss2: 60.22 and loss3: 0.00\n",
      "Epoch [3584], train_loss: 483.68 with loss1: 423.45, loss2: 60.22 and loss3: 0.00\n",
      "Epoch [3585], train_loss: 485.58 with loss1: 425.40, loss2: 60.18 and loss3: 0.00\n",
      "Epoch [3586], train_loss: 485.11 with loss1: 424.85, loss2: 60.26 and loss3: 0.00\n",
      "Epoch [3587], train_loss: 484.91 with loss1: 424.72, loss2: 60.18 and loss3: 0.00\n",
      "Epoch [3588], train_loss: 485.25 with loss1: 425.05, loss2: 60.20 and loss3: 0.00\n",
      "Epoch [3589], train_loss: 487.28 with loss1: 427.11, loss2: 60.16 and loss3: 0.00\n",
      "Epoch [3590], train_loss: 489.66 with loss1: 429.45, loss2: 60.21 and loss3: 0.00\n",
      "Epoch [3591], train_loss: 489.49 with loss1: 429.36, loss2: 60.14 and loss3: 0.00\n",
      "Epoch [3592], train_loss: 489.78 with loss1: 429.61, loss2: 60.16 and loss3: 0.00\n",
      "Epoch [3593], train_loss: 491.03 with loss1: 430.89, loss2: 60.14 and loss3: 0.00\n",
      "Epoch [3594], train_loss: 491.85 with loss1: 431.72, loss2: 60.13 and loss3: 0.00\n",
      "Epoch [3595], train_loss: 493.78 with loss1: 433.65, loss2: 60.13 and loss3: 0.00\n",
      "Epoch [3596], train_loss: 493.17 with loss1: 432.96, loss2: 60.21 and loss3: 0.00\n",
      "Epoch [3597], train_loss: 495.65 with loss1: 435.50, loss2: 60.15 and loss3: 0.00\n",
      "Epoch [3598], train_loss: 497.39 with loss1: 437.22, loss2: 60.17 and loss3: 0.00\n",
      "Epoch [3599], train_loss: 498.58 with loss1: 438.52, loss2: 60.06 and loss3: 0.00\n",
      "Epoch [3600], train_loss: 498.15 with loss1: 438.03, loss2: 60.12 and loss3: 0.00\n",
      "Epoch [3601], train_loss: 501.93 with loss1: 441.85, loss2: 60.07 and loss3: 0.00\n",
      "Epoch [3602], train_loss: 501.86 with loss1: 441.74, loss2: 60.11 and loss3: 0.00\n",
      "Epoch [3603], train_loss: 507.38 with loss1: 447.31, loss2: 60.07 and loss3: 0.00\n",
      "Epoch [3604], train_loss: 506.41 with loss1: 446.32, loss2: 60.09 and loss3: 0.00\n",
      "Epoch [3605], train_loss: 512.10 with loss1: 452.00, loss2: 60.10 and loss3: 0.00\n",
      "Epoch [3606], train_loss: 510.52 with loss1: 450.42, loss2: 60.11 and loss3: 0.00\n",
      "Epoch [3607], train_loss: 516.38 with loss1: 456.36, loss2: 60.02 and loss3: 0.00\n",
      "Epoch [3608], train_loss: 515.80 with loss1: 455.67, loss2: 60.13 and loss3: 0.00\n",
      "Epoch [3609], train_loss: 523.89 with loss1: 463.89, loss2: 60.01 and loss3: 0.00\n",
      "Epoch [3610], train_loss: 520.77 with loss1: 460.66, loss2: 60.12 and loss3: 0.00\n",
      "Epoch [3611], train_loss: 528.23 with loss1: 468.20, loss2: 60.02 and loss3: 0.00\n",
      "Epoch [3612], train_loss: 526.15 with loss1: 466.10, loss2: 60.05 and loss3: 0.00\n",
      "Epoch [3613], train_loss: 532.23 with loss1: 472.23, loss2: 60.00 and loss3: 0.00\n",
      "Epoch [3614], train_loss: 531.38 with loss1: 471.37, loss2: 60.01 and loss3: 0.00\n",
      "Epoch [3615], train_loss: 536.60 with loss1: 476.59, loss2: 60.02 and loss3: 0.00\n",
      "Epoch [3616], train_loss: 534.25 with loss1: 474.19, loss2: 60.06 and loss3: 0.00\n",
      "Epoch [3617], train_loss: 537.86 with loss1: 477.86, loss2: 60.00 and loss3: 0.00\n",
      "Epoch [3618], train_loss: 535.82 with loss1: 475.86, loss2: 59.97 and loss3: 0.00\n",
      "Epoch [3619], train_loss: 540.76 with loss1: 480.77, loss2: 59.99 and loss3: 0.00\n",
      "Epoch [3620], train_loss: 538.68 with loss1: 478.68, loss2: 60.00 and loss3: 0.00\n",
      "Epoch [3621], train_loss: 542.64 with loss1: 482.62, loss2: 60.02 and loss3: 0.00\n",
      "Epoch [3622], train_loss: 538.99 with loss1: 478.99, loss2: 59.99 and loss3: 0.00\n",
      "Epoch [3623], train_loss: 542.22 with loss1: 482.28, loss2: 59.95 and loss3: 0.00\n",
      "Epoch [3624], train_loss: 536.88 with loss1: 476.85, loss2: 60.03 and loss3: 0.00\n",
      "Epoch [3625], train_loss: 538.74 with loss1: 478.80, loss2: 59.93 and loss3: 0.00\n",
      "Epoch [3626], train_loss: 533.25 with loss1: 473.28, loss2: 59.97 and loss3: 0.00\n",
      "Epoch [3627], train_loss: 534.24 with loss1: 474.31, loss2: 59.93 and loss3: 0.00\n",
      "Epoch [3628], train_loss: 530.13 with loss1: 470.18, loss2: 59.95 and loss3: 0.00\n",
      "Epoch [3629], train_loss: 530.57 with loss1: 470.60, loss2: 59.97 and loss3: 0.00\n",
      "Epoch [3630], train_loss: 523.45 with loss1: 463.54, loss2: 59.91 and loss3: 0.00\n",
      "Epoch [3631], train_loss: 524.36 with loss1: 464.49, loss2: 59.88 and loss3: 0.00\n",
      "Epoch [3632], train_loss: 519.59 with loss1: 459.74, loss2: 59.86 and loss3: 0.00\n",
      "Epoch [3633], train_loss: 518.88 with loss1: 458.99, loss2: 59.89 and loss3: 0.00\n",
      "Epoch [3634], train_loss: 513.74 with loss1: 453.87, loss2: 59.87 and loss3: 0.00\n",
      "Epoch [3635], train_loss: 513.05 with loss1: 453.11, loss2: 59.93 and loss3: 0.00\n",
      "Epoch [3636], train_loss: 510.25 with loss1: 450.41, loss2: 59.85 and loss3: 0.00\n",
      "Epoch [3637], train_loss: 507.44 with loss1: 447.62, loss2: 59.82 and loss3: 0.00\n",
      "Epoch [3638], train_loss: 503.98 with loss1: 444.09, loss2: 59.89 and loss3: 0.00\n",
      "Epoch [3639], train_loss: 503.62 with loss1: 443.81, loss2: 59.81 and loss3: 0.00\n",
      "Epoch [3640], train_loss: 500.39 with loss1: 440.54, loss2: 59.85 and loss3: 0.00\n",
      "Epoch [3641], train_loss: 499.79 with loss1: 440.00, loss2: 59.79 and loss3: 0.00\n",
      "Epoch [3642], train_loss: 498.40 with loss1: 438.58, loss2: 59.82 and loss3: 0.00\n",
      "Epoch [3643], train_loss: 496.48 with loss1: 436.69, loss2: 59.78 and loss3: 0.00\n",
      "Epoch [3644], train_loss: 493.05 with loss1: 433.28, loss2: 59.77 and loss3: 0.00\n",
      "Epoch [3645], train_loss: 492.88 with loss1: 433.06, loss2: 59.82 and loss3: 0.00\n",
      "Epoch [3646], train_loss: 491.57 with loss1: 431.79, loss2: 59.78 and loss3: 0.00\n",
      "Epoch [3647], train_loss: 491.65 with loss1: 431.89, loss2: 59.75 and loss3: 0.00\n",
      "Epoch [3648], train_loss: 489.47 with loss1: 429.73, loss2: 59.74 and loss3: 0.00\n",
      "Epoch [3649], train_loss: 489.35 with loss1: 429.64, loss2: 59.71 and loss3: 0.00\n",
      "Epoch [3650], train_loss: 487.30 with loss1: 427.52, loss2: 59.77 and loss3: 0.00\n",
      "Epoch [3651], train_loss: 488.07 with loss1: 428.36, loss2: 59.71 and loss3: 0.00\n",
      "Epoch [3652], train_loss: 487.59 with loss1: 427.86, loss2: 59.72 and loss3: 0.00\n",
      "Epoch [3653], train_loss: 487.84 with loss1: 428.14, loss2: 59.70 and loss3: 0.00\n",
      "Epoch [3654], train_loss: 486.19 with loss1: 426.44, loss2: 59.74 and loss3: 0.00\n",
      "Epoch [3655], train_loss: 485.48 with loss1: 425.78, loss2: 59.69 and loss3: 0.00\n",
      "Epoch [3656], train_loss: 483.74 with loss1: 424.09, loss2: 59.65 and loss3: 0.00\n",
      "Epoch [3657], train_loss: 484.49 with loss1: 424.79, loss2: 59.70 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3658], train_loss: 482.73 with loss1: 423.06, loss2: 59.67 and loss3: 0.00\n",
      "Epoch [3659], train_loss: 482.38 with loss1: 422.77, loss2: 59.60 and loss3: 0.00\n",
      "Epoch [3660], train_loss: 481.96 with loss1: 422.33, loss2: 59.63 and loss3: 0.00\n",
      "Epoch [3661], train_loss: 482.73 with loss1: 423.10, loss2: 59.62 and loss3: 0.00\n",
      "Epoch [3662], train_loss: 481.87 with loss1: 422.23, loss2: 59.65 and loss3: 0.00\n",
      "Epoch [3663], train_loss: 481.31 with loss1: 421.69, loss2: 59.62 and loss3: 0.00\n",
      "Epoch [3664], train_loss: 480.39 with loss1: 420.79, loss2: 59.60 and loss3: 0.00\n",
      "Epoch [3665], train_loss: 483.21 with loss1: 423.65, loss2: 59.56 and loss3: 0.00\n",
      "Epoch [3666], train_loss: 479.80 with loss1: 420.22, loss2: 59.58 and loss3: 0.00\n",
      "Epoch [3667], train_loss: 480.92 with loss1: 421.41, loss2: 59.51 and loss3: 0.00\n",
      "Epoch [3668], train_loss: 480.60 with loss1: 421.03, loss2: 59.58 and loss3: 0.00\n",
      "Epoch [3669], train_loss: 479.55 with loss1: 420.06, loss2: 59.49 and loss3: 0.00\n",
      "Epoch [3670], train_loss: 479.41 with loss1: 419.85, loss2: 59.56 and loss3: 0.00\n",
      "Epoch [3671], train_loss: 479.62 with loss1: 420.08, loss2: 59.54 and loss3: 0.00\n",
      "Epoch [3672], train_loss: 480.12 with loss1: 420.62, loss2: 59.50 and loss3: 0.00\n",
      "Epoch [3673], train_loss: 480.52 with loss1: 421.04, loss2: 59.47 and loss3: 0.00\n",
      "Epoch [3674], train_loss: 477.78 with loss1: 418.26, loss2: 59.53 and loss3: 0.00\n",
      "Epoch [3675], train_loss: 479.51 with loss1: 420.05, loss2: 59.46 and loss3: 0.00\n",
      "Epoch [3676], train_loss: 479.25 with loss1: 419.72, loss2: 59.53 and loss3: 0.00\n",
      "Epoch [3677], train_loss: 481.37 with loss1: 421.89, loss2: 59.48 and loss3: 0.00\n",
      "Epoch [3678], train_loss: 478.38 with loss1: 418.90, loss2: 59.48 and loss3: 0.00\n",
      "Epoch [3679], train_loss: 479.67 with loss1: 420.22, loss2: 59.44 and loss3: 0.00\n",
      "Epoch [3680], train_loss: 478.58 with loss1: 419.16, loss2: 59.42 and loss3: 0.00\n",
      "Epoch [3681], train_loss: 478.95 with loss1: 419.49, loss2: 59.46 and loss3: 0.00\n",
      "Epoch [3682], train_loss: 478.01 with loss1: 418.52, loss2: 59.49 and loss3: 0.00\n",
      "Epoch [3683], train_loss: 480.15 with loss1: 420.74, loss2: 59.41 and loss3: 0.00\n",
      "Epoch [3684], train_loss: 478.17 with loss1: 418.79, loss2: 59.37 and loss3: 0.00\n",
      "Epoch [3685], train_loss: 479.63 with loss1: 420.17, loss2: 59.46 and loss3: 0.00\n",
      "Epoch [3686], train_loss: 479.17 with loss1: 419.78, loss2: 59.39 and loss3: 0.00\n",
      "Epoch [3687], train_loss: 480.01 with loss1: 420.61, loss2: 59.41 and loss3: 0.00\n",
      "Epoch [3688], train_loss: 479.91 with loss1: 420.56, loss2: 59.35 and loss3: 0.00\n",
      "Epoch [3689], train_loss: 480.61 with loss1: 421.19, loss2: 59.41 and loss3: 0.00\n",
      "Epoch [3690], train_loss: 480.39 with loss1: 420.98, loss2: 59.40 and loss3: 0.00\n",
      "Epoch [3691], train_loss: 481.89 with loss1: 422.52, loss2: 59.37 and loss3: 0.00\n",
      "Epoch [3692], train_loss: 480.50 with loss1: 421.13, loss2: 59.36 and loss3: 0.00\n",
      "Epoch [3693], train_loss: 482.83 with loss1: 423.41, loss2: 59.42 and loss3: 0.00\n",
      "Epoch [3694], train_loss: 482.72 with loss1: 423.36, loss2: 59.36 and loss3: 0.00\n",
      "Epoch [3695], train_loss: 484.99 with loss1: 425.65, loss2: 59.34 and loss3: 0.00\n",
      "Epoch [3696], train_loss: 484.08 with loss1: 424.76, loss2: 59.33 and loss3: 0.00\n",
      "Epoch [3697], train_loss: 484.71 with loss1: 425.42, loss2: 59.29 and loss3: 0.00\n",
      "Epoch [3698], train_loss: 482.77 with loss1: 423.40, loss2: 59.37 and loss3: 0.00\n",
      "Epoch [3699], train_loss: 483.28 with loss1: 424.03, loss2: 59.25 and loss3: 0.00\n",
      "Epoch [3700], train_loss: 482.33 with loss1: 423.10, loss2: 59.23 and loss3: 0.00\n",
      "Epoch [3701], train_loss: 484.93 with loss1: 425.66, loss2: 59.27 and loss3: 0.00\n",
      "Epoch [3702], train_loss: 482.37 with loss1: 423.08, loss2: 59.29 and loss3: 0.00\n",
      "Epoch [3703], train_loss: 483.75 with loss1: 424.50, loss2: 59.24 and loss3: 0.00\n",
      "Epoch [3704], train_loss: 482.31 with loss1: 423.10, loss2: 59.21 and loss3: 0.00\n",
      "Epoch [3705], train_loss: 484.04 with loss1: 424.78, loss2: 59.26 and loss3: 0.00\n",
      "Epoch [3706], train_loss: 483.53 with loss1: 424.30, loss2: 59.23 and loss3: 0.00\n",
      "Epoch [3707], train_loss: 484.81 with loss1: 425.55, loss2: 59.26 and loss3: 0.00\n",
      "Epoch [3708], train_loss: 483.80 with loss1: 424.54, loss2: 59.25 and loss3: 0.00\n",
      "Epoch [3709], train_loss: 486.67 with loss1: 427.48, loss2: 59.19 and loss3: 0.00\n",
      "Epoch [3710], train_loss: 484.49 with loss1: 425.31, loss2: 59.18 and loss3: 0.00\n",
      "Epoch [3711], train_loss: 486.68 with loss1: 427.45, loss2: 59.23 and loss3: 0.00\n",
      "Epoch [3712], train_loss: 483.59 with loss1: 424.40, loss2: 59.19 and loss3: 0.00\n",
      "Epoch [3713], train_loss: 484.10 with loss1: 424.91, loss2: 59.20 and loss3: 0.00\n",
      "Epoch [3714], train_loss: 484.12 with loss1: 424.92, loss2: 59.19 and loss3: 0.00\n",
      "Epoch [3715], train_loss: 484.30 with loss1: 425.12, loss2: 59.18 and loss3: 0.00\n",
      "Epoch [3716], train_loss: 484.52 with loss1: 425.36, loss2: 59.16 and loss3: 0.00\n",
      "Epoch [3717], train_loss: 486.40 with loss1: 427.26, loss2: 59.14 and loss3: 0.00\n",
      "Epoch [3718], train_loss: 484.71 with loss1: 425.58, loss2: 59.13 and loss3: 0.00\n",
      "Epoch [3719], train_loss: 486.78 with loss1: 427.67, loss2: 59.11 and loss3: 0.00\n",
      "Epoch [3720], train_loss: 485.53 with loss1: 426.47, loss2: 59.07 and loss3: 0.00\n",
      "Epoch [3721], train_loss: 485.55 with loss1: 426.38, loss2: 59.16 and loss3: 0.00\n",
      "Epoch [3722], train_loss: 484.85 with loss1: 425.76, loss2: 59.09 and loss3: 0.00\n",
      "Epoch [3723], train_loss: 485.84 with loss1: 426.65, loss2: 59.19 and loss3: 0.00\n",
      "Epoch [3724], train_loss: 484.73 with loss1: 425.61, loss2: 59.12 and loss3: 0.00\n",
      "Epoch [3725], train_loss: 483.73 with loss1: 424.60, loss2: 59.13 and loss3: 0.00\n",
      "Epoch [3726], train_loss: 482.31 with loss1: 423.25, loss2: 59.06 and loss3: 0.00\n",
      "Epoch [3727], train_loss: 484.54 with loss1: 425.46, loss2: 59.08 and loss3: 0.00\n",
      "Epoch [3728], train_loss: 482.82 with loss1: 423.77, loss2: 59.05 and loss3: 0.00\n",
      "Epoch [3729], train_loss: 483.27 with loss1: 424.18, loss2: 59.09 and loss3: 0.00\n",
      "Epoch [3730], train_loss: 482.39 with loss1: 423.34, loss2: 59.05 and loss3: 0.00\n",
      "Epoch [3731], train_loss: 483.47 with loss1: 424.43, loss2: 59.05 and loss3: 0.00\n",
      "Epoch [3732], train_loss: 482.55 with loss1: 423.56, loss2: 58.99 and loss3: 0.00\n",
      "Epoch [3733], train_loss: 482.47 with loss1: 423.48, loss2: 58.99 and loss3: 0.00\n",
      "Epoch [3734], train_loss: 481.10 with loss1: 422.03, loss2: 59.08 and loss3: 0.00\n",
      "Epoch [3735], train_loss: 480.60 with loss1: 421.51, loss2: 59.09 and loss3: 0.00\n",
      "Epoch [3736], train_loss: 482.09 with loss1: 423.15, loss2: 58.95 and loss3: 0.00\n",
      "Epoch [3737], train_loss: 481.81 with loss1: 422.84, loss2: 58.97 and loss3: 0.00\n",
      "Epoch [3738], train_loss: 483.59 with loss1: 424.61, loss2: 58.98 and loss3: 0.00\n",
      "Epoch [3739], train_loss: 482.87 with loss1: 423.84, loss2: 59.03 and loss3: 0.00\n",
      "Epoch [3740], train_loss: 481.65 with loss1: 422.67, loss2: 58.98 and loss3: 0.00\n",
      "Epoch [3741], train_loss: 483.06 with loss1: 424.05, loss2: 59.01 and loss3: 0.00\n",
      "Epoch [3742], train_loss: 482.54 with loss1: 423.58, loss2: 58.96 and loss3: 0.00\n",
      "Epoch [3743], train_loss: 482.35 with loss1: 423.33, loss2: 59.02 and loss3: 0.00\n",
      "Epoch [3744], train_loss: 480.08 with loss1: 421.11, loss2: 58.98 and loss3: 0.00\n",
      "Epoch [3745], train_loss: 479.86 with loss1: 420.84, loss2: 59.02 and loss3: 0.00\n",
      "Epoch [3746], train_loss: 478.89 with loss1: 419.94, loss2: 58.95 and loss3: 0.00\n",
      "Epoch [3747], train_loss: 478.37 with loss1: 419.42, loss2: 58.95 and loss3: 0.00\n",
      "Epoch [3748], train_loss: 477.38 with loss1: 418.57, loss2: 58.81 and loss3: 0.00\n",
      "Epoch [3749], train_loss: 479.28 with loss1: 420.28, loss2: 59.00 and loss3: 0.00\n",
      "Epoch [3750], train_loss: 478.14 with loss1: 419.20, loss2: 58.94 and loss3: 0.00\n",
      "Epoch [3751], train_loss: 477.89 with loss1: 419.02, loss2: 58.87 and loss3: 0.00\n",
      "Epoch [3752], train_loss: 480.13 with loss1: 421.26, loss2: 58.86 and loss3: 0.00\n",
      "Epoch [3753], train_loss: 483.18 with loss1: 424.28, loss2: 58.90 and loss3: 0.00\n",
      "Epoch [3754], train_loss: 479.58 with loss1: 420.69, loss2: 58.89 and loss3: 0.00\n",
      "Epoch [3755], train_loss: 479.13 with loss1: 420.20, loss2: 58.93 and loss3: 0.00\n",
      "Epoch [3756], train_loss: 479.11 with loss1: 420.30, loss2: 58.81 and loss3: 0.00\n",
      "Epoch [3757], train_loss: 478.62 with loss1: 419.72, loss2: 58.90 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3758], train_loss: 478.96 with loss1: 420.14, loss2: 58.82 and loss3: 0.00\n",
      "Epoch [3759], train_loss: 479.45 with loss1: 420.55, loss2: 58.90 and loss3: 0.00\n",
      "Epoch [3760], train_loss: 478.18 with loss1: 419.41, loss2: 58.76 and loss3: 0.00\n",
      "Epoch [3761], train_loss: 479.83 with loss1: 420.95, loss2: 58.87 and loss3: 0.00\n",
      "Epoch [3762], train_loss: 478.67 with loss1: 419.82, loss2: 58.84 and loss3: 0.00\n",
      "Epoch [3763], train_loss: 478.17 with loss1: 419.32, loss2: 58.85 and loss3: 0.00\n",
      "Epoch [3764], train_loss: 477.95 with loss1: 419.19, loss2: 58.77 and loss3: 0.00\n",
      "Epoch [3765], train_loss: 479.03 with loss1: 420.16, loss2: 58.87 and loss3: 0.00\n",
      "Epoch [3766], train_loss: 480.55 with loss1: 421.74, loss2: 58.82 and loss3: 0.00\n",
      "Epoch [3767], train_loss: 479.15 with loss1: 420.32, loss2: 58.83 and loss3: 0.00\n",
      "Epoch [3768], train_loss: 478.60 with loss1: 419.81, loss2: 58.79 and loss3: 0.00\n",
      "Epoch [3769], train_loss: 479.81 with loss1: 420.99, loss2: 58.83 and loss3: 0.00\n",
      "Epoch [3770], train_loss: 477.90 with loss1: 419.14, loss2: 58.76 and loss3: 0.00\n",
      "Epoch [3771], train_loss: 478.39 with loss1: 419.56, loss2: 58.83 and loss3: 0.00\n",
      "Epoch [3772], train_loss: 477.15 with loss1: 418.39, loss2: 58.76 and loss3: 0.00\n",
      "Epoch [3773], train_loss: 478.52 with loss1: 419.71, loss2: 58.81 and loss3: 0.00\n",
      "Epoch [3774], train_loss: 478.05 with loss1: 419.32, loss2: 58.74 and loss3: 0.00\n",
      "Epoch [3775], train_loss: 480.42 with loss1: 421.65, loss2: 58.77 and loss3: 0.00\n",
      "Epoch [3776], train_loss: 479.39 with loss1: 420.71, loss2: 58.68 and loss3: 0.00\n",
      "Epoch [3777], train_loss: 479.11 with loss1: 420.34, loss2: 58.77 and loss3: 0.00\n",
      "Epoch [3778], train_loss: 479.39 with loss1: 420.69, loss2: 58.71 and loss3: 0.00\n",
      "Epoch [3779], train_loss: 478.91 with loss1: 420.16, loss2: 58.75 and loss3: 0.00\n",
      "Epoch [3780], train_loss: 479.29 with loss1: 420.61, loss2: 58.69 and loss3: 0.00\n",
      "Epoch [3781], train_loss: 478.75 with loss1: 420.01, loss2: 58.74 and loss3: 0.00\n",
      "Epoch [3782], train_loss: 477.49 with loss1: 418.86, loss2: 58.63 and loss3: 0.00\n",
      "Epoch [3783], train_loss: 478.21 with loss1: 419.48, loss2: 58.73 and loss3: 0.00\n",
      "Epoch [3784], train_loss: 478.50 with loss1: 419.86, loss2: 58.64 and loss3: 0.00\n",
      "Epoch [3785], train_loss: 478.67 with loss1: 420.03, loss2: 58.64 and loss3: 0.00\n",
      "Epoch [3786], train_loss: 479.47 with loss1: 420.82, loss2: 58.64 and loss3: 0.00\n",
      "Epoch [3787], train_loss: 480.07 with loss1: 421.46, loss2: 58.61 and loss3: 0.00\n",
      "Epoch [3788], train_loss: 476.55 with loss1: 417.93, loss2: 58.62 and loss3: 0.00\n",
      "Epoch [3789], train_loss: 478.93 with loss1: 420.28, loss2: 58.64 and loss3: 0.00\n",
      "Epoch [3790], train_loss: 479.72 with loss1: 421.14, loss2: 58.58 and loss3: 0.00\n",
      "Epoch [3791], train_loss: 479.18 with loss1: 420.57, loss2: 58.61 and loss3: 0.00\n",
      "Epoch [3792], train_loss: 478.50 with loss1: 419.96, loss2: 58.54 and loss3: 0.00\n",
      "Epoch [3793], train_loss: 480.43 with loss1: 421.84, loss2: 58.59 and loss3: 0.00\n",
      "Epoch [3794], train_loss: 479.70 with loss1: 421.13, loss2: 58.57 and loss3: 0.00\n",
      "Epoch [3795], train_loss: 481.32 with loss1: 422.70, loss2: 58.62 and loss3: 0.00\n",
      "Epoch [3796], train_loss: 479.47 with loss1: 420.89, loss2: 58.58 and loss3: 0.00\n",
      "Epoch [3797], train_loss: 480.09 with loss1: 421.47, loss2: 58.62 and loss3: 0.00\n",
      "Epoch [3798], train_loss: 477.70 with loss1: 419.14, loss2: 58.55 and loss3: 0.00\n",
      "Epoch [3799], train_loss: 479.91 with loss1: 421.34, loss2: 58.57 and loss3: 0.00\n",
      "Epoch [3800], train_loss: 478.57 with loss1: 419.95, loss2: 58.62 and loss3: 0.00\n",
      "Epoch [3801], train_loss: 480.05 with loss1: 421.45, loss2: 58.60 and loss3: 0.00\n",
      "Epoch [3802], train_loss: 479.82 with loss1: 421.25, loss2: 58.58 and loss3: 0.00\n",
      "Epoch [3803], train_loss: 478.80 with loss1: 420.20, loss2: 58.59 and loss3: 0.00\n",
      "Epoch [3804], train_loss: 480.09 with loss1: 421.56, loss2: 58.53 and loss3: 0.00\n",
      "Epoch [3805], train_loss: 479.18 with loss1: 420.61, loss2: 58.56 and loss3: 0.00\n",
      "Epoch [3806], train_loss: 477.64 with loss1: 419.11, loss2: 58.53 and loss3: 0.00\n",
      "Epoch [3807], train_loss: 478.62 with loss1: 420.04, loss2: 58.58 and loss3: 0.00\n",
      "Epoch [3808], train_loss: 476.72 with loss1: 418.23, loss2: 58.49 and loss3: 0.00\n",
      "Epoch [3809], train_loss: 477.54 with loss1: 419.08, loss2: 58.46 and loss3: 0.00\n",
      "Epoch [3810], train_loss: 478.03 with loss1: 419.52, loss2: 58.51 and loss3: 0.00\n",
      "Epoch [3811], train_loss: 479.17 with loss1: 420.58, loss2: 58.59 and loss3: 0.00\n",
      "Epoch [3812], train_loss: 477.85 with loss1: 419.44, loss2: 58.41 and loss3: 0.00\n",
      "Epoch [3813], train_loss: 479.53 with loss1: 420.96, loss2: 58.57 and loss3: 0.00\n",
      "Epoch [3814], train_loss: 477.71 with loss1: 419.29, loss2: 58.42 and loss3: 0.00\n",
      "Epoch [3815], train_loss: 479.35 with loss1: 420.90, loss2: 58.45 and loss3: 0.00\n",
      "Epoch [3816], train_loss: 478.17 with loss1: 419.69, loss2: 58.48 and loss3: 0.00\n",
      "Epoch [3817], train_loss: 480.68 with loss1: 422.21, loss2: 58.48 and loss3: 0.00\n",
      "Epoch [3818], train_loss: 477.82 with loss1: 419.42, loss2: 58.40 and loss3: 0.00\n",
      "Epoch [3819], train_loss: 477.95 with loss1: 419.44, loss2: 58.50 and loss3: 0.00\n",
      "Epoch [3820], train_loss: 478.14 with loss1: 419.75, loss2: 58.39 and loss3: 0.00\n",
      "Epoch [3821], train_loss: 479.14 with loss1: 420.63, loss2: 58.51 and loss3: 0.00\n",
      "Epoch [3822], train_loss: 479.05 with loss1: 420.62, loss2: 58.43 and loss3: 0.00\n",
      "Epoch [3823], train_loss: 478.01 with loss1: 419.65, loss2: 58.37 and loss3: 0.00\n",
      "Epoch [3824], train_loss: 476.31 with loss1: 417.96, loss2: 58.35 and loss3: 0.00\n",
      "Epoch [3825], train_loss: 478.40 with loss1: 419.94, loss2: 58.46 and loss3: 0.00\n",
      "Epoch [3826], train_loss: 477.11 with loss1: 418.69, loss2: 58.42 and loss3: 0.00\n",
      "Epoch [3827], train_loss: 478.38 with loss1: 420.05, loss2: 58.33 and loss3: 0.00\n",
      "Epoch [3828], train_loss: 479.77 with loss1: 421.37, loss2: 58.40 and loss3: 0.00\n",
      "Epoch [3829], train_loss: 480.90 with loss1: 422.54, loss2: 58.36 and loss3: 0.00\n",
      "Epoch [3830], train_loss: 479.78 with loss1: 421.34, loss2: 58.44 and loss3: 0.00\n",
      "Epoch [3831], train_loss: 479.82 with loss1: 421.42, loss2: 58.40 and loss3: 0.00\n",
      "Epoch [3832], train_loss: 479.29 with loss1: 420.90, loss2: 58.39 and loss3: 0.00\n",
      "Epoch [3833], train_loss: 480.72 with loss1: 422.35, loss2: 58.38 and loss3: 0.00\n",
      "Epoch [3834], train_loss: 479.29 with loss1: 420.92, loss2: 58.37 and loss3: 0.00\n",
      "Epoch [3835], train_loss: 481.58 with loss1: 423.21, loss2: 58.36 and loss3: 0.00\n",
      "Epoch [3836], train_loss: 480.31 with loss1: 421.93, loss2: 58.38 and loss3: 0.00\n",
      "Epoch [3837], train_loss: 481.24 with loss1: 422.89, loss2: 58.35 and loss3: 0.00\n",
      "Epoch [3838], train_loss: 480.30 with loss1: 421.98, loss2: 58.32 and loss3: 0.00\n",
      "Epoch [3839], train_loss: 482.89 with loss1: 424.56, loss2: 58.33 and loss3: 0.00\n",
      "Epoch [3840], train_loss: 479.90 with loss1: 421.61, loss2: 58.29 and loss3: 0.00\n",
      "Epoch [3841], train_loss: 482.84 with loss1: 424.55, loss2: 58.29 and loss3: 0.00\n",
      "Epoch [3842], train_loss: 481.49 with loss1: 423.22, loss2: 58.27 and loss3: 0.00\n",
      "Epoch [3843], train_loss: 484.19 with loss1: 425.93, loss2: 58.26 and loss3: 0.00\n",
      "Epoch [3844], train_loss: 481.52 with loss1: 423.18, loss2: 58.34 and loss3: 0.00\n",
      "Epoch [3845], train_loss: 483.26 with loss1: 424.94, loss2: 58.32 and loss3: 0.00\n",
      "Epoch [3846], train_loss: 483.54 with loss1: 425.19, loss2: 58.35 and loss3: 0.00\n",
      "Epoch [3847], train_loss: 482.36 with loss1: 424.11, loss2: 58.25 and loss3: 0.00\n",
      "Epoch [3848], train_loss: 481.92 with loss1: 423.60, loss2: 58.32 and loss3: 0.00\n",
      "Epoch [3849], train_loss: 482.87 with loss1: 424.58, loss2: 58.29 and loss3: 0.00\n",
      "Epoch [3850], train_loss: 482.09 with loss1: 423.86, loss2: 58.23 and loss3: 0.00\n",
      "Epoch [3851], train_loss: 480.93 with loss1: 422.63, loss2: 58.29 and loss3: 0.00\n",
      "Epoch [3852], train_loss: 479.58 with loss1: 421.27, loss2: 58.31 and loss3: 0.00\n",
      "Epoch [3853], train_loss: 481.06 with loss1: 422.82, loss2: 58.24 and loss3: 0.00\n",
      "Epoch [3854], train_loss: 479.76 with loss1: 421.55, loss2: 58.21 and loss3: 0.00\n",
      "Epoch [3855], train_loss: 480.36 with loss1: 422.20, loss2: 58.16 and loss3: 0.00\n",
      "Epoch [3856], train_loss: 479.19 with loss1: 420.86, loss2: 58.32 and loss3: 0.00\n",
      "Epoch [3857], train_loss: 478.37 with loss1: 420.22, loss2: 58.15 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3858], train_loss: 478.16 with loss1: 419.94, loss2: 58.22 and loss3: 0.00\n",
      "Epoch [3859], train_loss: 477.36 with loss1: 419.21, loss2: 58.15 and loss3: 0.00\n",
      "Epoch [3860], train_loss: 476.65 with loss1: 418.41, loss2: 58.23 and loss3: 0.00\n",
      "Epoch [3861], train_loss: 475.62 with loss1: 417.44, loss2: 58.18 and loss3: 0.00\n",
      "Epoch [3862], train_loss: 474.65 with loss1: 416.48, loss2: 58.17 and loss3: 0.00\n",
      "Epoch [3863], train_loss: 475.46 with loss1: 417.28, loss2: 58.18 and loss3: 0.00\n",
      "Epoch [3864], train_loss: 472.83 with loss1: 414.69, loss2: 58.14 and loss3: 0.00\n",
      "Epoch [3865], train_loss: 473.80 with loss1: 415.69, loss2: 58.11 and loss3: 0.00\n",
      "Epoch [3866], train_loss: 470.82 with loss1: 412.72, loss2: 58.09 and loss3: 0.00\n",
      "Epoch [3867], train_loss: 471.90 with loss1: 413.77, loss2: 58.13 and loss3: 0.00\n",
      "Epoch [3868], train_loss: 470.47 with loss1: 412.35, loss2: 58.13 and loss3: 0.00\n",
      "Epoch [3869], train_loss: 469.76 with loss1: 411.69, loss2: 58.07 and loss3: 0.00\n",
      "Epoch [3870], train_loss: 470.18 with loss1: 412.03, loss2: 58.15 and loss3: 0.00\n",
      "Epoch [3871], train_loss: 469.41 with loss1: 411.28, loss2: 58.13 and loss3: 0.00\n",
      "Epoch [3872], train_loss: 468.05 with loss1: 409.97, loss2: 58.09 and loss3: 0.00\n",
      "Epoch [3873], train_loss: 468.59 with loss1: 410.52, loss2: 58.07 and loss3: 0.00\n",
      "Epoch [3874], train_loss: 469.03 with loss1: 410.98, loss2: 58.06 and loss3: 0.00\n",
      "Epoch [3875], train_loss: 468.02 with loss1: 409.94, loss2: 58.08 and loss3: 0.00\n",
      "Epoch [3876], train_loss: 466.84 with loss1: 408.82, loss2: 58.02 and loss3: 0.00\n",
      "Epoch [3877], train_loss: 467.83 with loss1: 409.78, loss2: 58.05 and loss3: 0.00\n",
      "Epoch [3878], train_loss: 466.30 with loss1: 408.14, loss2: 58.15 and loss3: 0.00\n",
      "Epoch [3879], train_loss: 467.36 with loss1: 409.32, loss2: 58.04 and loss3: 0.00\n",
      "Epoch [3880], train_loss: 466.82 with loss1: 408.82, loss2: 57.99 and loss3: 0.00\n",
      "Epoch [3881], train_loss: 466.98 with loss1: 408.99, loss2: 57.99 and loss3: 0.00\n",
      "Epoch [3882], train_loss: 465.62 with loss1: 407.52, loss2: 58.10 and loss3: 0.00\n",
      "Epoch [3883], train_loss: 467.28 with loss1: 409.33, loss2: 57.95 and loss3: 0.00\n",
      "Epoch [3884], train_loss: 467.97 with loss1: 409.97, loss2: 57.99 and loss3: 0.00\n",
      "Epoch [3885], train_loss: 469.44 with loss1: 411.50, loss2: 57.94 and loss3: 0.00\n",
      "Epoch [3886], train_loss: 469.86 with loss1: 411.79, loss2: 58.07 and loss3: 0.00\n",
      "Epoch [3887], train_loss: 469.21 with loss1: 411.23, loss2: 57.97 and loss3: 0.00\n",
      "Epoch [3888], train_loss: 469.75 with loss1: 411.76, loss2: 57.99 and loss3: 0.00\n",
      "Epoch [3889], train_loss: 470.37 with loss1: 412.43, loss2: 57.94 and loss3: 0.00\n",
      "Epoch [3890], train_loss: 470.17 with loss1: 412.16, loss2: 58.01 and loss3: 0.00\n",
      "Epoch [3891], train_loss: 470.99 with loss1: 413.02, loss2: 57.97 and loss3: 0.00\n",
      "Epoch [3892], train_loss: 472.08 with loss1: 414.13, loss2: 57.95 and loss3: 0.00\n",
      "Epoch [3893], train_loss: 473.75 with loss1: 415.81, loss2: 57.94 and loss3: 0.00\n",
      "Epoch [3894], train_loss: 474.45 with loss1: 416.38, loss2: 58.07 and loss3: 0.00\n",
      "Epoch [3895], train_loss: 477.05 with loss1: 419.12, loss2: 57.93 and loss3: 0.00\n",
      "Epoch [3896], train_loss: 477.76 with loss1: 419.77, loss2: 57.99 and loss3: 0.00\n",
      "Epoch [3897], train_loss: 481.94 with loss1: 424.05, loss2: 57.89 and loss3: 0.00\n",
      "Epoch [3898], train_loss: 482.26 with loss1: 424.33, loss2: 57.94 and loss3: 0.00\n",
      "Epoch [3899], train_loss: 485.96 with loss1: 428.05, loss2: 57.91 and loss3: 0.00\n",
      "Epoch [3900], train_loss: 485.75 with loss1: 427.79, loss2: 57.97 and loss3: 0.00\n",
      "Epoch [3901], train_loss: 488.34 with loss1: 430.46, loss2: 57.88 and loss3: 0.00\n",
      "Epoch [3902], train_loss: 489.58 with loss1: 431.63, loss2: 57.95 and loss3: 0.00\n",
      "Epoch [3903], train_loss: 494.38 with loss1: 436.53, loss2: 57.85 and loss3: 0.00\n",
      "Epoch [3904], train_loss: 493.36 with loss1: 435.36, loss2: 58.01 and loss3: 0.00\n",
      "Epoch [3905], train_loss: 497.69 with loss1: 439.83, loss2: 57.86 and loss3: 0.00\n",
      "Epoch [3906], train_loss: 496.98 with loss1: 439.02, loss2: 57.96 and loss3: 0.00\n",
      "Epoch [3907], train_loss: 503.31 with loss1: 445.41, loss2: 57.90 and loss3: 0.00\n",
      "Epoch [3908], train_loss: 501.19 with loss1: 443.28, loss2: 57.90 and loss3: 0.00\n",
      "Epoch [3909], train_loss: 503.53 with loss1: 445.68, loss2: 57.85 and loss3: 0.00\n",
      "Epoch [3910], train_loss: 501.28 with loss1: 443.35, loss2: 57.92 and loss3: 0.00\n",
      "Epoch [3911], train_loss: 504.65 with loss1: 446.78, loss2: 57.86 and loss3: 0.00\n",
      "Epoch [3912], train_loss: 503.17 with loss1: 445.18, loss2: 57.99 and loss3: 0.00\n",
      "Epoch [3913], train_loss: 506.68 with loss1: 448.83, loss2: 57.85 and loss3: 0.00\n",
      "Epoch [3914], train_loss: 503.45 with loss1: 445.57, loss2: 57.89 and loss3: 0.00\n",
      "Epoch [3915], train_loss: 504.45 with loss1: 446.56, loss2: 57.89 and loss3: 0.00\n",
      "Epoch [3916], train_loss: 502.43 with loss1: 444.47, loss2: 57.96 and loss3: 0.00\n",
      "Epoch [3917], train_loss: 502.58 with loss1: 444.77, loss2: 57.80 and loss3: 0.00\n",
      "Epoch [3918], train_loss: 500.20 with loss1: 442.30, loss2: 57.90 and loss3: 0.00\n",
      "Epoch [3919], train_loss: 498.99 with loss1: 441.15, loss2: 57.84 and loss3: 0.00\n",
      "Epoch [3920], train_loss: 494.59 with loss1: 436.71, loss2: 57.88 and loss3: 0.00\n",
      "Epoch [3921], train_loss: 494.27 with loss1: 436.42, loss2: 57.84 and loss3: 0.00\n",
      "Epoch [3922], train_loss: 491.74 with loss1: 433.92, loss2: 57.82 and loss3: 0.00\n",
      "Epoch [3923], train_loss: 492.52 with loss1: 434.78, loss2: 57.73 and loss3: 0.00\n",
      "Epoch [3924], train_loss: 487.18 with loss1: 429.34, loss2: 57.84 and loss3: 0.00\n",
      "Epoch [3925], train_loss: 486.74 with loss1: 428.97, loss2: 57.76 and loss3: 0.00\n",
      "Epoch [3926], train_loss: 485.47 with loss1: 427.66, loss2: 57.81 and loss3: 0.00\n",
      "Epoch [3927], train_loss: 483.73 with loss1: 425.98, loss2: 57.75 and loss3: 0.00\n",
      "Epoch [3928], train_loss: 482.04 with loss1: 424.21, loss2: 57.83 and loss3: 0.00\n",
      "Epoch [3929], train_loss: 480.48 with loss1: 422.68, loss2: 57.80 and loss3: 0.00\n",
      "Epoch [3930], train_loss: 480.35 with loss1: 422.54, loss2: 57.82 and loss3: 0.00\n",
      "Epoch [3931], train_loss: 476.99 with loss1: 419.20, loss2: 57.79 and loss3: 0.00\n",
      "Epoch [3932], train_loss: 477.07 with loss1: 419.28, loss2: 57.80 and loss3: 0.00\n",
      "Epoch [3933], train_loss: 474.17 with loss1: 416.37, loss2: 57.81 and loss3: 0.00\n",
      "Epoch [3934], train_loss: 472.98 with loss1: 415.22, loss2: 57.76 and loss3: 0.00\n",
      "Epoch [3935], train_loss: 472.27 with loss1: 414.49, loss2: 57.77 and loss3: 0.00\n",
      "Epoch [3936], train_loss: 471.07 with loss1: 413.31, loss2: 57.76 and loss3: 0.00\n",
      "Epoch [3937], train_loss: 471.04 with loss1: 413.33, loss2: 57.70 and loss3: 0.00\n",
      "Epoch [3938], train_loss: 468.00 with loss1: 410.29, loss2: 57.71 and loss3: 0.00\n",
      "Epoch [3939], train_loss: 467.72 with loss1: 410.04, loss2: 57.69 and loss3: 0.00\n",
      "Epoch [3940], train_loss: 465.79 with loss1: 408.03, loss2: 57.76 and loss3: 0.00\n",
      "Epoch [3941], train_loss: 466.07 with loss1: 408.40, loss2: 57.66 and loss3: 0.00\n",
      "Epoch [3942], train_loss: 465.13 with loss1: 407.43, loss2: 57.70 and loss3: 0.00\n",
      "Epoch [3943], train_loss: 463.65 with loss1: 405.99, loss2: 57.66 and loss3: 0.00\n",
      "Epoch [3944], train_loss: 464.43 with loss1: 406.81, loss2: 57.61 and loss3: 0.00\n",
      "Epoch [3945], train_loss: 464.32 with loss1: 406.66, loss2: 57.66 and loss3: 0.00\n",
      "Epoch [3946], train_loss: 463.05 with loss1: 405.46, loss2: 57.60 and loss3: 0.00\n",
      "Epoch [3947], train_loss: 461.16 with loss1: 403.51, loss2: 57.65 and loss3: 0.00\n",
      "Epoch [3948], train_loss: 461.85 with loss1: 404.27, loss2: 57.57 and loss3: 0.00\n",
      "Epoch [3949], train_loss: 461.62 with loss1: 403.99, loss2: 57.63 and loss3: 0.00\n",
      "Epoch [3950], train_loss: 461.51 with loss1: 403.92, loss2: 57.59 and loss3: 0.00\n",
      "Epoch [3951], train_loss: 462.33 with loss1: 404.69, loss2: 57.64 and loss3: 0.00\n",
      "Epoch [3952], train_loss: 461.77 with loss1: 404.16, loss2: 57.61 and loss3: 0.00\n",
      "Epoch [3953], train_loss: 459.76 with loss1: 402.20, loss2: 57.56 and loss3: 0.00\n",
      "Epoch [3954], train_loss: 460.15 with loss1: 402.58, loss2: 57.57 and loss3: 0.00\n",
      "Epoch [3955], train_loss: 460.54 with loss1: 403.00, loss2: 57.54 and loss3: 0.00\n",
      "Epoch [3956], train_loss: 460.26 with loss1: 402.67, loss2: 57.59 and loss3: 0.00\n",
      "Epoch [3957], train_loss: 461.19 with loss1: 403.70, loss2: 57.49 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3958], train_loss: 461.20 with loss1: 403.66, loss2: 57.54 and loss3: 0.00\n",
      "Epoch [3959], train_loss: 460.55 with loss1: 403.02, loss2: 57.54 and loss3: 0.00\n",
      "Epoch [3960], train_loss: 460.10 with loss1: 402.57, loss2: 57.54 and loss3: 0.00\n",
      "Epoch [3961], train_loss: 459.46 with loss1: 401.98, loss2: 57.48 and loss3: 0.00\n",
      "Epoch [3962], train_loss: 459.17 with loss1: 401.70, loss2: 57.47 and loss3: 0.00\n",
      "Epoch [3963], train_loss: 460.48 with loss1: 403.00, loss2: 57.49 and loss3: 0.00\n",
      "Epoch [3964], train_loss: 460.02 with loss1: 402.50, loss2: 57.52 and loss3: 0.00\n",
      "Epoch [3965], train_loss: 461.74 with loss1: 404.29, loss2: 57.45 and loss3: 0.00\n",
      "Epoch [3966], train_loss: 461.30 with loss1: 403.89, loss2: 57.41 and loss3: 0.00\n",
      "Epoch [3967], train_loss: 463.48 with loss1: 406.01, loss2: 57.47 and loss3: 0.00\n",
      "Epoch [3968], train_loss: 462.65 with loss1: 405.18, loss2: 57.47 and loss3: 0.00\n",
      "Epoch [3969], train_loss: 463.69 with loss1: 406.26, loss2: 57.43 and loss3: 0.00\n",
      "Epoch [3970], train_loss: 463.43 with loss1: 405.97, loss2: 57.45 and loss3: 0.00\n",
      "Epoch [3971], train_loss: 463.77 with loss1: 406.32, loss2: 57.45 and loss3: 0.00\n",
      "Epoch [3972], train_loss: 462.60 with loss1: 405.18, loss2: 57.43 and loss3: 0.00\n",
      "Epoch [3973], train_loss: 463.68 with loss1: 406.20, loss2: 57.49 and loss3: 0.00\n",
      "Epoch [3974], train_loss: 462.70 with loss1: 405.33, loss2: 57.37 and loss3: 0.00\n",
      "Epoch [3975], train_loss: 464.44 with loss1: 407.04, loss2: 57.40 and loss3: 0.00\n",
      "Epoch [3976], train_loss: 464.68 with loss1: 407.30, loss2: 57.38 and loss3: 0.00\n",
      "Epoch [3977], train_loss: 465.68 with loss1: 408.29, loss2: 57.39 and loss3: 0.00\n",
      "Epoch [3978], train_loss: 464.46 with loss1: 407.05, loss2: 57.41 and loss3: 0.00\n",
      "Epoch [3979], train_loss: 466.84 with loss1: 409.47, loss2: 57.37 and loss3: 0.00\n",
      "Epoch [3980], train_loss: 464.57 with loss1: 407.18, loss2: 57.38 and loss3: 0.00\n",
      "Epoch [3981], train_loss: 466.72 with loss1: 409.39, loss2: 57.33 and loss3: 0.00\n",
      "Epoch [3982], train_loss: 465.63 with loss1: 408.27, loss2: 57.35 and loss3: 0.00\n",
      "Epoch [3983], train_loss: 468.94 with loss1: 411.63, loss2: 57.31 and loss3: 0.00\n",
      "Epoch [3984], train_loss: 466.26 with loss1: 408.96, loss2: 57.30 and loss3: 0.00\n",
      "Epoch [3985], train_loss: 467.66 with loss1: 410.33, loss2: 57.32 and loss3: 0.00\n",
      "Epoch [3986], train_loss: 467.84 with loss1: 410.54, loss2: 57.30 and loss3: 0.00\n",
      "Epoch [3987], train_loss: 467.53 with loss1: 410.16, loss2: 57.37 and loss3: 0.00\n",
      "Epoch [3988], train_loss: 465.81 with loss1: 408.55, loss2: 57.27 and loss3: 0.00\n",
      "Epoch [3989], train_loss: 468.53 with loss1: 411.23, loss2: 57.29 and loss3: 0.00\n",
      "Epoch [3990], train_loss: 467.43 with loss1: 410.17, loss2: 57.26 and loss3: 0.00\n",
      "Epoch [3991], train_loss: 468.09 with loss1: 410.80, loss2: 57.29 and loss3: 0.00\n",
      "Epoch [3992], train_loss: 469.23 with loss1: 411.94, loss2: 57.28 and loss3: 0.00\n",
      "Epoch [3993], train_loss: 469.51 with loss1: 412.29, loss2: 57.22 and loss3: 0.00\n",
      "Epoch [3994], train_loss: 469.02 with loss1: 411.80, loss2: 57.22 and loss3: 0.00\n",
      "Epoch [3995], train_loss: 469.59 with loss1: 412.34, loss2: 57.25 and loss3: 0.00\n",
      "Epoch [3996], train_loss: 467.55 with loss1: 410.32, loss2: 57.23 and loss3: 0.00\n",
      "Epoch [3997], train_loss: 470.45 with loss1: 413.23, loss2: 57.23 and loss3: 0.00\n",
      "Epoch [3998], train_loss: 469.96 with loss1: 412.72, loss2: 57.24 and loss3: 0.00\n",
      "Epoch [3999], train_loss: 472.48 with loss1: 415.31, loss2: 57.17 and loss3: 0.00\n",
      "Epoch [4000], train_loss: 468.25 with loss1: 411.04, loss2: 57.21 and loss3: 0.00\n",
      "Epoch [4001], train_loss: 468.92 with loss1: 411.66, loss2: 57.25 and loss3: 0.00\n",
      "Epoch [4002], train_loss: 469.32 with loss1: 412.08, loss2: 57.24 and loss3: 0.00\n",
      "Epoch [4003], train_loss: 469.78 with loss1: 412.56, loss2: 57.22 and loss3: 0.00\n",
      "Epoch [4004], train_loss: 468.87 with loss1: 411.68, loss2: 57.19 and loss3: 0.00\n",
      "Epoch [4005], train_loss: 470.06 with loss1: 412.86, loss2: 57.20 and loss3: 0.00\n",
      "Epoch [4006], train_loss: 469.18 with loss1: 412.08, loss2: 57.10 and loss3: 0.00\n",
      "Epoch [4007], train_loss: 470.38 with loss1: 413.25, loss2: 57.13 and loss3: 0.00\n",
      "Epoch [4008], train_loss: 470.13 with loss1: 412.95, loss2: 57.18 and loss3: 0.00\n",
      "Epoch [4009], train_loss: 470.93 with loss1: 413.76, loss2: 57.17 and loss3: 0.00\n",
      "Epoch [4010], train_loss: 470.87 with loss1: 413.72, loss2: 57.15 and loss3: 0.00\n",
      "Epoch [4011], train_loss: 471.27 with loss1: 414.09, loss2: 57.18 and loss3: 0.00\n",
      "Epoch [4012], train_loss: 468.44 with loss1: 411.26, loss2: 57.18 and loss3: 0.00\n",
      "Epoch [4013], train_loss: 470.46 with loss1: 413.31, loss2: 57.15 and loss3: 0.00\n",
      "Epoch [4014], train_loss: 470.41 with loss1: 413.29, loss2: 57.12 and loss3: 0.00\n",
      "Epoch [4015], train_loss: 471.34 with loss1: 414.19, loss2: 57.15 and loss3: 0.00\n",
      "Epoch [4016], train_loss: 468.90 with loss1: 411.80, loss2: 57.10 and loss3: 0.00\n",
      "Epoch [4017], train_loss: 470.98 with loss1: 413.88, loss2: 57.10 and loss3: 0.00\n",
      "Epoch [4018], train_loss: 469.43 with loss1: 412.30, loss2: 57.12 and loss3: 0.00\n",
      "Epoch [4019], train_loss: 468.23 with loss1: 411.06, loss2: 57.17 and loss3: 0.00\n",
      "Epoch [4020], train_loss: 468.04 with loss1: 410.96, loss2: 57.07 and loss3: 0.00\n",
      "Epoch [4021], train_loss: 469.86 with loss1: 412.82, loss2: 57.04 and loss3: 0.00\n",
      "Epoch [4022], train_loss: 467.09 with loss1: 410.08, loss2: 57.01 and loss3: 0.00\n",
      "Epoch [4023], train_loss: 467.63 with loss1: 410.58, loss2: 57.06 and loss3: 0.00\n",
      "Epoch [4024], train_loss: 466.47 with loss1: 409.42, loss2: 57.05 and loss3: 0.00\n",
      "Epoch [4025], train_loss: 467.37 with loss1: 410.38, loss2: 56.99 and loss3: 0.00\n",
      "Epoch [4026], train_loss: 466.34 with loss1: 409.32, loss2: 57.02 and loss3: 0.00\n",
      "Epoch [4027], train_loss: 466.42 with loss1: 409.38, loss2: 57.04 and loss3: 0.00\n",
      "Epoch [4028], train_loss: 464.40 with loss1: 407.43, loss2: 56.97 and loss3: 0.00\n",
      "Epoch [4029], train_loss: 465.10 with loss1: 408.07, loss2: 57.03 and loss3: 0.00\n",
      "Epoch [4030], train_loss: 466.30 with loss1: 409.29, loss2: 57.01 and loss3: 0.00\n",
      "Epoch [4031], train_loss: 465.42 with loss1: 408.40, loss2: 57.03 and loss3: 0.00\n",
      "Epoch [4032], train_loss: 463.89 with loss1: 406.88, loss2: 57.01 and loss3: 0.00\n",
      "Epoch [4033], train_loss: 464.21 with loss1: 407.24, loss2: 56.97 and loss3: 0.00\n",
      "Epoch [4034], train_loss: 462.80 with loss1: 405.88, loss2: 56.92 and loss3: 0.00\n",
      "Epoch [4035], train_loss: 463.80 with loss1: 406.87, loss2: 56.92 and loss3: 0.00\n",
      "Epoch [4036], train_loss: 463.73 with loss1: 406.83, loss2: 56.90 and loss3: 0.00\n",
      "Epoch [4037], train_loss: 463.65 with loss1: 406.66, loss2: 56.99 and loss3: 0.00\n",
      "Epoch [4038], train_loss: 462.55 with loss1: 405.57, loss2: 56.98 and loss3: 0.00\n",
      "Epoch [4039], train_loss: 464.59 with loss1: 407.67, loss2: 56.91 and loss3: 0.00\n",
      "Epoch [4040], train_loss: 463.27 with loss1: 406.33, loss2: 56.94 and loss3: 0.00\n",
      "Epoch [4041], train_loss: 462.31 with loss1: 405.37, loss2: 56.94 and loss3: 0.00\n",
      "Epoch [4042], train_loss: 461.13 with loss1: 404.21, loss2: 56.93 and loss3: 0.00\n",
      "Epoch [4043], train_loss: 462.08 with loss1: 405.15, loss2: 56.93 and loss3: 0.00\n",
      "Epoch [4044], train_loss: 462.20 with loss1: 405.29, loss2: 56.91 and loss3: 0.00\n",
      "Epoch [4045], train_loss: 461.91 with loss1: 404.99, loss2: 56.92 and loss3: 0.00\n",
      "Epoch [4046], train_loss: 459.46 with loss1: 402.52, loss2: 56.94 and loss3: 0.00\n",
      "Epoch [4047], train_loss: 460.69 with loss1: 403.77, loss2: 56.91 and loss3: 0.00\n",
      "Epoch [4048], train_loss: 460.05 with loss1: 403.19, loss2: 56.87 and loss3: 0.00\n",
      "Epoch [4049], train_loss: 460.81 with loss1: 403.98, loss2: 56.83 and loss3: 0.00\n",
      "Epoch [4050], train_loss: 460.09 with loss1: 403.26, loss2: 56.83 and loss3: 0.00\n",
      "Epoch [4051], train_loss: 459.50 with loss1: 402.59, loss2: 56.90 and loss3: 0.00\n",
      "Epoch [4052], train_loss: 460.19 with loss1: 403.35, loss2: 56.84 and loss3: 0.00\n",
      "Epoch [4053], train_loss: 460.54 with loss1: 403.65, loss2: 56.89 and loss3: 0.00\n",
      "Epoch [4054], train_loss: 460.99 with loss1: 404.20, loss2: 56.79 and loss3: 0.00\n",
      "Epoch [4055], train_loss: 459.68 with loss1: 402.84, loss2: 56.83 and loss3: 0.00\n",
      "Epoch [4056], train_loss: 459.23 with loss1: 402.36, loss2: 56.88 and loss3: 0.00\n",
      "Epoch [4057], train_loss: 461.96 with loss1: 405.18, loss2: 56.78 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4058], train_loss: 458.45 with loss1: 401.63, loss2: 56.83 and loss3: 0.00\n",
      "Epoch [4059], train_loss: 458.84 with loss1: 402.06, loss2: 56.79 and loss3: 0.00\n",
      "Epoch [4060], train_loss: 459.94 with loss1: 403.09, loss2: 56.84 and loss3: 0.00\n",
      "Epoch [4061], train_loss: 459.48 with loss1: 402.69, loss2: 56.79 and loss3: 0.00\n",
      "Epoch [4062], train_loss: 457.89 with loss1: 401.08, loss2: 56.81 and loss3: 0.00\n",
      "Epoch [4063], train_loss: 458.94 with loss1: 402.19, loss2: 56.74 and loss3: 0.00\n",
      "Epoch [4064], train_loss: 460.12 with loss1: 403.38, loss2: 56.74 and loss3: 0.00\n",
      "Epoch [4065], train_loss: 459.92 with loss1: 403.16, loss2: 56.76 and loss3: 0.00\n",
      "Epoch [4066], train_loss: 459.08 with loss1: 402.33, loss2: 56.75 and loss3: 0.00\n",
      "Epoch [4067], train_loss: 461.14 with loss1: 404.41, loss2: 56.73 and loss3: 0.00\n",
      "Epoch [4068], train_loss: 460.09 with loss1: 403.31, loss2: 56.78 and loss3: 0.00\n",
      "Epoch [4069], train_loss: 461.06 with loss1: 404.35, loss2: 56.71 and loss3: 0.00\n",
      "Epoch [4070], train_loss: 460.28 with loss1: 403.52, loss2: 56.76 and loss3: 0.00\n",
      "Epoch [4071], train_loss: 460.66 with loss1: 403.92, loss2: 56.74 and loss3: 0.00\n",
      "Epoch [4072], train_loss: 460.16 with loss1: 403.39, loss2: 56.77 and loss3: 0.00\n",
      "Epoch [4073], train_loss: 462.99 with loss1: 406.32, loss2: 56.67 and loss3: 0.00\n",
      "Epoch [4074], train_loss: 460.87 with loss1: 404.08, loss2: 56.80 and loss3: 0.00\n",
      "Epoch [4075], train_loss: 462.43 with loss1: 405.74, loss2: 56.69 and loss3: 0.00\n",
      "Epoch [4076], train_loss: 462.74 with loss1: 406.00, loss2: 56.75 and loss3: 0.00\n",
      "Epoch [4077], train_loss: 464.04 with loss1: 407.36, loss2: 56.68 and loss3: 0.00\n",
      "Epoch [4078], train_loss: 463.24 with loss1: 406.47, loss2: 56.77 and loss3: 0.00\n",
      "Epoch [4079], train_loss: 464.15 with loss1: 407.47, loss2: 56.68 and loss3: 0.00\n",
      "Epoch [4080], train_loss: 463.44 with loss1: 406.76, loss2: 56.68 and loss3: 0.00\n",
      "Epoch [4081], train_loss: 465.81 with loss1: 409.20, loss2: 56.62 and loss3: 0.00\n",
      "Epoch [4082], train_loss: 464.40 with loss1: 407.72, loss2: 56.67 and loss3: 0.00\n",
      "Epoch [4083], train_loss: 467.74 with loss1: 411.13, loss2: 56.61 and loss3: 0.00\n",
      "Epoch [4084], train_loss: 464.96 with loss1: 408.32, loss2: 56.65 and loss3: 0.00\n",
      "Epoch [4085], train_loss: 466.70 with loss1: 410.11, loss2: 56.59 and loss3: 0.00\n",
      "Epoch [4086], train_loss: 464.56 with loss1: 407.96, loss2: 56.61 and loss3: 0.00\n",
      "Epoch [4087], train_loss: 466.16 with loss1: 409.54, loss2: 56.62 and loss3: 0.00\n",
      "Epoch [4088], train_loss: 466.33 with loss1: 409.65, loss2: 56.68 and loss3: 0.00\n",
      "Epoch [4089], train_loss: 467.13 with loss1: 410.50, loss2: 56.62 and loss3: 0.00\n",
      "Epoch [4090], train_loss: 465.89 with loss1: 409.24, loss2: 56.65 and loss3: 0.00\n",
      "Epoch [4091], train_loss: 467.83 with loss1: 411.24, loss2: 56.59 and loss3: 0.00\n",
      "Epoch [4092], train_loss: 467.44 with loss1: 410.84, loss2: 56.59 and loss3: 0.00\n",
      "Epoch [4093], train_loss: 468.33 with loss1: 411.73, loss2: 56.59 and loss3: 0.00\n",
      "Epoch [4094], train_loss: 468.89 with loss1: 412.25, loss2: 56.64 and loss3: 0.00\n",
      "Epoch [4095], train_loss: 469.09 with loss1: 412.47, loss2: 56.62 and loss3: 0.00\n",
      "Epoch [4096], train_loss: 467.53 with loss1: 410.98, loss2: 56.56 and loss3: 0.00\n",
      "Epoch [4097], train_loss: 469.46 with loss1: 412.96, loss2: 56.50 and loss3: 0.00\n",
      "Epoch [4098], train_loss: 468.09 with loss1: 411.51, loss2: 56.58 and loss3: 0.00\n",
      "Epoch [4099], train_loss: 472.75 with loss1: 416.24, loss2: 56.52 and loss3: 0.00\n",
      "Epoch [4100], train_loss: 469.44 with loss1: 412.91, loss2: 56.53 and loss3: 0.00\n",
      "Epoch [4101], train_loss: 471.32 with loss1: 414.86, loss2: 56.46 and loss3: 0.00\n",
      "Epoch [4102], train_loss: 469.65 with loss1: 413.16, loss2: 56.50 and loss3: 0.00\n",
      "Epoch [4103], train_loss: 470.14 with loss1: 413.63, loss2: 56.51 and loss3: 0.00\n",
      "Epoch [4104], train_loss: 468.74 with loss1: 412.20, loss2: 56.54 and loss3: 0.00\n",
      "Epoch [4105], train_loss: 470.58 with loss1: 414.10, loss2: 56.48 and loss3: 0.00\n",
      "Epoch [4106], train_loss: 470.47 with loss1: 413.95, loss2: 56.51 and loss3: 0.00\n",
      "Epoch [4107], train_loss: 471.58 with loss1: 415.10, loss2: 56.48 and loss3: 0.00\n",
      "Epoch [4108], train_loss: 470.94 with loss1: 414.43, loss2: 56.51 and loss3: 0.00\n",
      "Epoch [4109], train_loss: 471.14 with loss1: 414.64, loss2: 56.50 and loss3: 0.00\n",
      "Epoch [4110], train_loss: 469.65 with loss1: 413.11, loss2: 56.53 and loss3: 0.00\n",
      "Epoch [4111], train_loss: 471.34 with loss1: 414.96, loss2: 56.38 and loss3: 0.00\n",
      "Epoch [4112], train_loss: 468.78 with loss1: 412.22, loss2: 56.56 and loss3: 0.00\n",
      "Epoch [4113], train_loss: 470.28 with loss1: 413.85, loss2: 56.42 and loss3: 0.00\n",
      "Epoch [4114], train_loss: 468.66 with loss1: 412.12, loss2: 56.53 and loss3: 0.00\n",
      "Epoch [4115], train_loss: 471.87 with loss1: 415.40, loss2: 56.47 and loss3: 0.00\n",
      "Epoch [4116], train_loss: 467.29 with loss1: 410.81, loss2: 56.47 and loss3: 0.00\n",
      "Epoch [4117], train_loss: 469.09 with loss1: 412.61, loss2: 56.47 and loss3: 0.00\n",
      "Epoch [4118], train_loss: 468.95 with loss1: 412.48, loss2: 56.47 and loss3: 0.00\n",
      "Epoch [4119], train_loss: 468.57 with loss1: 412.17, loss2: 56.40 and loss3: 0.00\n",
      "Epoch [4120], train_loss: 467.89 with loss1: 411.42, loss2: 56.47 and loss3: 0.00\n",
      "Epoch [4121], train_loss: 469.11 with loss1: 412.74, loss2: 56.37 and loss3: 0.00\n",
      "Epoch [4122], train_loss: 466.33 with loss1: 409.85, loss2: 56.48 and loss3: 0.00\n",
      "Epoch [4123], train_loss: 469.90 with loss1: 413.53, loss2: 56.37 and loss3: 0.00\n",
      "Epoch [4124], train_loss: 466.36 with loss1: 409.91, loss2: 56.45 and loss3: 0.00\n",
      "Epoch [4125], train_loss: 466.30 with loss1: 409.92, loss2: 56.39 and loss3: 0.00\n",
      "Epoch [4126], train_loss: 465.74 with loss1: 409.29, loss2: 56.44 and loss3: 0.00\n",
      "Epoch [4127], train_loss: 465.07 with loss1: 408.71, loss2: 56.36 and loss3: 0.00\n",
      "Epoch [4128], train_loss: 462.58 with loss1: 406.11, loss2: 56.46 and loss3: 0.00\n",
      "Epoch [4129], train_loss: 463.10 with loss1: 406.77, loss2: 56.33 and loss3: 0.00\n",
      "Epoch [4130], train_loss: 461.21 with loss1: 404.73, loss2: 56.48 and loss3: 0.00\n",
      "Epoch [4131], train_loss: 462.28 with loss1: 405.90, loss2: 56.39 and loss3: 0.00\n",
      "Epoch [4132], train_loss: 460.96 with loss1: 404.56, loss2: 56.41 and loss3: 0.00\n",
      "Epoch [4133], train_loss: 458.91 with loss1: 402.56, loss2: 56.35 and loss3: 0.00\n",
      "Epoch [4134], train_loss: 457.49 with loss1: 401.12, loss2: 56.37 and loss3: 0.00\n",
      "Epoch [4135], train_loss: 457.68 with loss1: 401.31, loss2: 56.38 and loss3: 0.00\n",
      "Epoch [4136], train_loss: 456.77 with loss1: 400.35, loss2: 56.42 and loss3: 0.00\n",
      "Epoch [4137], train_loss: 456.34 with loss1: 399.99, loss2: 56.35 and loss3: 0.00\n",
      "Epoch [4138], train_loss: 454.46 with loss1: 398.13, loss2: 56.32 and loss3: 0.00\n",
      "Epoch [4139], train_loss: 454.69 with loss1: 398.38, loss2: 56.31 and loss3: 0.00\n",
      "Epoch [4140], train_loss: 452.86 with loss1: 396.47, loss2: 56.39 and loss3: 0.00\n",
      "Epoch [4141], train_loss: 453.26 with loss1: 397.04, loss2: 56.23 and loss3: 0.00\n",
      "Epoch [4142], train_loss: 452.23 with loss1: 395.96, loss2: 56.28 and loss3: 0.00\n",
      "Epoch [4143], train_loss: 451.34 with loss1: 395.08, loss2: 56.26 and loss3: 0.00\n",
      "Epoch [4144], train_loss: 451.79 with loss1: 395.52, loss2: 56.26 and loss3: 0.00\n",
      "Epoch [4145], train_loss: 452.21 with loss1: 396.00, loss2: 56.21 and loss3: 0.00\n",
      "Epoch [4146], train_loss: 450.58 with loss1: 394.30, loss2: 56.28 and loss3: 0.00\n",
      "Epoch [4147], train_loss: 450.88 with loss1: 394.67, loss2: 56.21 and loss3: 0.00\n",
      "Epoch [4148], train_loss: 450.24 with loss1: 394.00, loss2: 56.24 and loss3: 0.00\n",
      "Epoch [4149], train_loss: 450.41 with loss1: 394.17, loss2: 56.25 and loss3: 0.00\n",
      "Epoch [4150], train_loss: 449.14 with loss1: 392.82, loss2: 56.32 and loss3: 0.00\n",
      "Epoch [4151], train_loss: 448.93 with loss1: 392.76, loss2: 56.17 and loss3: 0.00\n",
      "Epoch [4152], train_loss: 450.20 with loss1: 393.90, loss2: 56.29 and loss3: 0.00\n",
      "Epoch [4153], train_loss: 448.85 with loss1: 392.66, loss2: 56.19 and loss3: 0.00\n",
      "Epoch [4154], train_loss: 448.11 with loss1: 391.84, loss2: 56.27 and loss3: 0.00\n",
      "Epoch [4155], train_loss: 448.94 with loss1: 392.77, loss2: 56.17 and loss3: 0.00\n",
      "Epoch [4156], train_loss: 449.28 with loss1: 393.11, loss2: 56.17 and loss3: 0.00\n",
      "Epoch [4157], train_loss: 447.97 with loss1: 391.85, loss2: 56.12 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4158], train_loss: 448.51 with loss1: 392.31, loss2: 56.20 and loss3: 0.00\n",
      "Epoch [4159], train_loss: 449.88 with loss1: 393.71, loss2: 56.16 and loss3: 0.00\n",
      "Epoch [4160], train_loss: 447.54 with loss1: 391.36, loss2: 56.18 and loss3: 0.00\n",
      "Epoch [4161], train_loss: 447.54 with loss1: 391.32, loss2: 56.22 and loss3: 0.00\n",
      "Epoch [4162], train_loss: 448.17 with loss1: 391.96, loss2: 56.21 and loss3: 0.00\n",
      "Epoch [4163], train_loss: 447.42 with loss1: 391.33, loss2: 56.09 and loss3: 0.00\n",
      "Epoch [4164], train_loss: 446.85 with loss1: 390.66, loss2: 56.19 and loss3: 0.00\n",
      "Epoch [4165], train_loss: 447.48 with loss1: 391.30, loss2: 56.17 and loss3: 0.00\n",
      "Epoch [4166], train_loss: 447.46 with loss1: 391.28, loss2: 56.18 and loss3: 0.00\n",
      "Epoch [4167], train_loss: 447.01 with loss1: 390.85, loss2: 56.16 and loss3: 0.00\n",
      "Epoch [4168], train_loss: 446.49 with loss1: 390.29, loss2: 56.20 and loss3: 0.00\n",
      "Epoch [4169], train_loss: 447.82 with loss1: 391.74, loss2: 56.08 and loss3: 0.00\n",
      "Epoch [4170], train_loss: 448.12 with loss1: 391.97, loss2: 56.14 and loss3: 0.00\n",
      "Epoch [4171], train_loss: 449.03 with loss1: 392.97, loss2: 56.06 and loss3: 0.00\n",
      "Epoch [4172], train_loss: 450.61 with loss1: 394.46, loss2: 56.15 and loss3: 0.00\n",
      "Epoch [4173], train_loss: 448.21 with loss1: 392.16, loss2: 56.06 and loss3: 0.00\n",
      "Epoch [4174], train_loss: 451.32 with loss1: 395.16, loss2: 56.15 and loss3: 0.00\n",
      "Epoch [4175], train_loss: 449.75 with loss1: 393.70, loss2: 56.05 and loss3: 0.00\n",
      "Epoch [4176], train_loss: 450.42 with loss1: 394.28, loss2: 56.14 and loss3: 0.00\n",
      "Epoch [4177], train_loss: 449.99 with loss1: 393.94, loss2: 56.05 and loss3: 0.00\n",
      "Epoch [4178], train_loss: 450.07 with loss1: 393.97, loss2: 56.10 and loss3: 0.00\n",
      "Epoch [4179], train_loss: 450.35 with loss1: 394.33, loss2: 56.02 and loss3: 0.00\n",
      "Epoch [4180], train_loss: 451.42 with loss1: 395.31, loss2: 56.11 and loss3: 0.00\n",
      "Epoch [4181], train_loss: 451.55 with loss1: 395.53, loss2: 56.02 and loss3: 0.00\n",
      "Epoch [4182], train_loss: 452.55 with loss1: 396.46, loss2: 56.09 and loss3: 0.00\n",
      "Epoch [4183], train_loss: 450.99 with loss1: 395.00, loss2: 55.99 and loss3: 0.00\n",
      "Epoch [4184], train_loss: 451.66 with loss1: 395.60, loss2: 56.06 and loss3: 0.00\n",
      "Epoch [4185], train_loss: 454.16 with loss1: 398.18, loss2: 55.98 and loss3: 0.00\n",
      "Epoch [4186], train_loss: 451.70 with loss1: 395.65, loss2: 56.05 and loss3: 0.00\n",
      "Epoch [4187], train_loss: 452.64 with loss1: 396.63, loss2: 56.01 and loss3: 0.00\n",
      "Epoch [4188], train_loss: 453.30 with loss1: 397.25, loss2: 56.05 and loss3: 0.00\n",
      "Epoch [4189], train_loss: 454.86 with loss1: 398.95, loss2: 55.90 and loss3: 0.00\n",
      "Epoch [4190], train_loss: 454.53 with loss1: 398.54, loss2: 55.99 and loss3: 0.00\n",
      "Epoch [4191], train_loss: 453.38 with loss1: 397.49, loss2: 55.89 and loss3: 0.00\n",
      "Epoch [4192], train_loss: 456.20 with loss1: 400.19, loss2: 56.01 and loss3: 0.00\n",
      "Epoch [4193], train_loss: 454.95 with loss1: 398.96, loss2: 56.00 and loss3: 0.00\n",
      "Epoch [4194], train_loss: 455.04 with loss1: 399.07, loss2: 55.97 and loss3: 0.00\n",
      "Epoch [4195], train_loss: 456.29 with loss1: 400.37, loss2: 55.92 and loss3: 0.00\n",
      "Epoch [4196], train_loss: 457.95 with loss1: 401.95, loss2: 56.00 and loss3: 0.00\n",
      "Epoch [4197], train_loss: 457.56 with loss1: 401.60, loss2: 55.96 and loss3: 0.00\n",
      "Epoch [4198], train_loss: 458.53 with loss1: 402.47, loss2: 56.06 and loss3: 0.00\n",
      "Epoch [4199], train_loss: 457.75 with loss1: 401.80, loss2: 55.95 and loss3: 0.00\n",
      "Epoch [4200], train_loss: 458.81 with loss1: 402.83, loss2: 55.98 and loss3: 0.00\n",
      "Epoch [4201], train_loss: 458.87 with loss1: 402.89, loss2: 55.98 and loss3: 0.00\n",
      "Epoch [4202], train_loss: 460.56 with loss1: 404.61, loss2: 55.95 and loss3: 0.00\n",
      "Epoch [4203], train_loss: 460.07 with loss1: 404.07, loss2: 56.01 and loss3: 0.00\n",
      "Epoch [4204], train_loss: 460.67 with loss1: 404.66, loss2: 56.00 and loss3: 0.00\n",
      "Epoch [4205], train_loss: 461.04 with loss1: 405.10, loss2: 55.93 and loss3: 0.00\n",
      "Epoch [4206], train_loss: 461.88 with loss1: 405.94, loss2: 55.94 and loss3: 0.00\n",
      "Epoch [4207], train_loss: 462.80 with loss1: 406.86, loss2: 55.93 and loss3: 0.00\n",
      "Epoch [4208], train_loss: 462.38 with loss1: 406.40, loss2: 55.98 and loss3: 0.00\n",
      "Epoch [4209], train_loss: 462.57 with loss1: 406.61, loss2: 55.96 and loss3: 0.00\n",
      "Epoch [4210], train_loss: 463.17 with loss1: 407.25, loss2: 55.91 and loss3: 0.00\n",
      "Epoch [4211], train_loss: 461.72 with loss1: 405.75, loss2: 55.97 and loss3: 0.00\n",
      "Epoch [4212], train_loss: 463.64 with loss1: 407.72, loss2: 55.92 and loss3: 0.00\n",
      "Epoch [4213], train_loss: 463.12 with loss1: 407.20, loss2: 55.91 and loss3: 0.00\n",
      "Epoch [4214], train_loss: 463.68 with loss1: 407.70, loss2: 55.98 and loss3: 0.00\n",
      "Epoch [4215], train_loss: 462.16 with loss1: 406.18, loss2: 55.98 and loss3: 0.00\n",
      "Epoch [4216], train_loss: 462.12 with loss1: 406.18, loss2: 55.95 and loss3: 0.00\n",
      "Epoch [4217], train_loss: 462.71 with loss1: 406.79, loss2: 55.93 and loss3: 0.00\n",
      "Epoch [4218], train_loss: 462.34 with loss1: 406.38, loss2: 55.95 and loss3: 0.00\n",
      "Epoch [4219], train_loss: 463.30 with loss1: 407.36, loss2: 55.94 and loss3: 0.00\n",
      "Epoch [4220], train_loss: 462.00 with loss1: 406.03, loss2: 55.97 and loss3: 0.00\n",
      "Epoch [4221], train_loss: 461.51 with loss1: 405.55, loss2: 55.96 and loss3: 0.00\n",
      "Epoch [4222], train_loss: 461.76 with loss1: 405.90, loss2: 55.86 and loss3: 0.00\n",
      "Epoch [4223], train_loss: 462.75 with loss1: 406.79, loss2: 55.96 and loss3: 0.00\n",
      "Epoch [4224], train_loss: 462.24 with loss1: 406.34, loss2: 55.90 and loss3: 0.00\n",
      "Epoch [4225], train_loss: 460.97 with loss1: 404.98, loss2: 55.99 and loss3: 0.00\n",
      "Epoch [4226], train_loss: 460.15 with loss1: 404.21, loss2: 55.94 and loss3: 0.00\n",
      "Epoch [4227], train_loss: 458.91 with loss1: 402.93, loss2: 55.98 and loss3: 0.00\n",
      "Epoch [4228], train_loss: 457.48 with loss1: 401.55, loss2: 55.93 and loss3: 0.00\n",
      "Epoch [4229], train_loss: 455.23 with loss1: 399.31, loss2: 55.92 and loss3: 0.00\n",
      "Epoch [4230], train_loss: 452.98 with loss1: 397.10, loss2: 55.88 and loss3: 0.00\n",
      "Epoch [4231], train_loss: 452.58 with loss1: 396.67, loss2: 55.90 and loss3: 0.00\n",
      "Epoch [4232], train_loss: 450.69 with loss1: 394.82, loss2: 55.87 and loss3: 0.00\n",
      "Epoch [4233], train_loss: 449.28 with loss1: 393.31, loss2: 55.97 and loss3: 0.00\n",
      "Epoch [4234], train_loss: 448.83 with loss1: 392.99, loss2: 55.84 and loss3: 0.00\n",
      "Epoch [4235], train_loss: 446.82 with loss1: 391.00, loss2: 55.82 and loss3: 0.00\n",
      "Epoch [4236], train_loss: 444.61 with loss1: 388.80, loss2: 55.82 and loss3: 0.00\n",
      "Epoch [4237], train_loss: 443.84 with loss1: 387.99, loss2: 55.85 and loss3: 0.00\n",
      "Epoch [4238], train_loss: 442.80 with loss1: 387.02, loss2: 55.78 and loss3: 0.00\n",
      "Epoch [4239], train_loss: 441.87 with loss1: 386.05, loss2: 55.82 and loss3: 0.00\n",
      "Epoch [4240], train_loss: 442.31 with loss1: 386.50, loss2: 55.80 and loss3: 0.00\n",
      "Epoch [4241], train_loss: 441.80 with loss1: 385.95, loss2: 55.85 and loss3: 0.00\n",
      "Epoch [4242], train_loss: 439.82 with loss1: 384.09, loss2: 55.74 and loss3: 0.00\n",
      "Epoch [4243], train_loss: 440.37 with loss1: 384.58, loss2: 55.79 and loss3: 0.00\n",
      "Epoch [4244], train_loss: 439.81 with loss1: 384.02, loss2: 55.79 and loss3: 0.00\n",
      "Epoch [4245], train_loss: 439.76 with loss1: 383.92, loss2: 55.84 and loss3: 0.00\n",
      "Epoch [4246], train_loss: 438.58 with loss1: 382.82, loss2: 55.76 and loss3: 0.00\n",
      "Epoch [4247], train_loss: 438.01 with loss1: 382.25, loss2: 55.75 and loss3: 0.00\n",
      "Epoch [4248], train_loss: 437.65 with loss1: 381.94, loss2: 55.72 and loss3: 0.00\n",
      "Epoch [4249], train_loss: 437.62 with loss1: 381.88, loss2: 55.73 and loss3: 0.00\n",
      "Epoch [4250], train_loss: 437.00 with loss1: 381.26, loss2: 55.74 and loss3: 0.00\n",
      "Epoch [4251], train_loss: 437.72 with loss1: 381.96, loss2: 55.76 and loss3: 0.00\n",
      "Epoch [4252], train_loss: 436.69 with loss1: 380.93, loss2: 55.76 and loss3: 0.00\n",
      "Epoch [4253], train_loss: 436.41 with loss1: 380.73, loss2: 55.69 and loss3: 0.00\n",
      "Epoch [4254], train_loss: 436.16 with loss1: 380.45, loss2: 55.71 and loss3: 0.00\n",
      "Epoch [4255], train_loss: 437.87 with loss1: 382.13, loss2: 55.74 and loss3: 0.00\n",
      "Epoch [4256], train_loss: 437.31 with loss1: 381.67, loss2: 55.64 and loss3: 0.00\n",
      "Epoch [4257], train_loss: 438.19 with loss1: 382.46, loss2: 55.73 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4258], train_loss: 439.58 with loss1: 383.95, loss2: 55.63 and loss3: 0.00\n",
      "Epoch [4259], train_loss: 438.89 with loss1: 383.22, loss2: 55.67 and loss3: 0.00\n",
      "Epoch [4260], train_loss: 440.03 with loss1: 384.38, loss2: 55.64 and loss3: 0.00\n",
      "Epoch [4261], train_loss: 441.14 with loss1: 385.53, loss2: 55.61 and loss3: 0.00\n",
      "Epoch [4262], train_loss: 440.83 with loss1: 385.21, loss2: 55.62 and loss3: 0.00\n",
      "Epoch [4263], train_loss: 442.36 with loss1: 386.73, loss2: 55.63 and loss3: 0.00\n",
      "Epoch [4264], train_loss: 442.77 with loss1: 387.15, loss2: 55.62 and loss3: 0.00\n",
      "Epoch [4265], train_loss: 443.23 with loss1: 387.60, loss2: 55.63 and loss3: 0.00\n",
      "Epoch [4266], train_loss: 443.51 with loss1: 388.01, loss2: 55.50 and loss3: 0.00\n",
      "Epoch [4267], train_loss: 445.46 with loss1: 389.92, loss2: 55.54 and loss3: 0.00\n",
      "Epoch [4268], train_loss: 446.85 with loss1: 391.30, loss2: 55.55 and loss3: 0.00\n",
      "Epoch [4269], train_loss: 448.26 with loss1: 392.70, loss2: 55.56 and loss3: 0.00\n",
      "Epoch [4270], train_loss: 450.37 with loss1: 394.88, loss2: 55.49 and loss3: 0.00\n",
      "Epoch [4271], train_loss: 452.12 with loss1: 396.61, loss2: 55.51 and loss3: 0.00\n",
      "Epoch [4272], train_loss: 453.87 with loss1: 398.34, loss2: 55.54 and loss3: 0.00\n",
      "Epoch [4273], train_loss: 456.98 with loss1: 401.46, loss2: 55.52 and loss3: 0.00\n",
      "Epoch [4274], train_loss: 458.10 with loss1: 402.63, loss2: 55.47 and loss3: 0.00\n",
      "Epoch [4275], train_loss: 459.20 with loss1: 403.69, loss2: 55.51 and loss3: 0.00\n",
      "Epoch [4276], train_loss: 459.72 with loss1: 404.23, loss2: 55.49 and loss3: 0.00\n",
      "Epoch [4277], train_loss: 465.56 with loss1: 410.05, loss2: 55.50 and loss3: 0.00\n",
      "Epoch [4278], train_loss: 464.75 with loss1: 409.28, loss2: 55.47 and loss3: 0.00\n",
      "Epoch [4279], train_loss: 468.29 with loss1: 412.75, loss2: 55.55 and loss3: 0.00\n",
      "Epoch [4280], train_loss: 468.41 with loss1: 412.98, loss2: 55.43 and loss3: 0.00\n",
      "Epoch [4281], train_loss: 473.51 with loss1: 418.02, loss2: 55.48 and loss3: 0.00\n",
      "Epoch [4282], train_loss: 474.21 with loss1: 418.77, loss2: 55.44 and loss3: 0.00\n",
      "Epoch [4283], train_loss: 479.15 with loss1: 423.66, loss2: 55.49 and loss3: 0.00\n",
      "Epoch [4284], train_loss: 477.69 with loss1: 422.26, loss2: 55.43 and loss3: 0.00\n",
      "Epoch [4285], train_loss: 482.52 with loss1: 427.10, loss2: 55.43 and loss3: 0.00\n",
      "Epoch [4286], train_loss: 481.87 with loss1: 426.45, loss2: 55.41 and loss3: 0.00\n",
      "Epoch [4287], train_loss: 485.63 with loss1: 430.22, loss2: 55.41 and loss3: 0.00\n",
      "Epoch [4288], train_loss: 484.14 with loss1: 428.70, loss2: 55.44 and loss3: 0.00\n",
      "Epoch [4289], train_loss: 486.24 with loss1: 430.77, loss2: 55.47 and loss3: 0.00\n",
      "Epoch [4290], train_loss: 484.16 with loss1: 428.78, loss2: 55.38 and loss3: 0.00\n",
      "Epoch [4291], train_loss: 487.37 with loss1: 431.96, loss2: 55.41 and loss3: 0.00\n",
      "Epoch [4292], train_loss: 484.73 with loss1: 429.33, loss2: 55.40 and loss3: 0.00\n",
      "Epoch [4293], train_loss: 486.58 with loss1: 431.16, loss2: 55.42 and loss3: 0.00\n",
      "Epoch [4294], train_loss: 483.71 with loss1: 428.38, loss2: 55.33 and loss3: 0.00\n",
      "Epoch [4295], train_loss: 487.47 with loss1: 432.09, loss2: 55.39 and loss3: 0.00\n",
      "Epoch [4296], train_loss: 484.22 with loss1: 428.86, loss2: 55.36 and loss3: 0.00\n",
      "Epoch [4297], train_loss: 488.30 with loss1: 432.91, loss2: 55.39 and loss3: 0.00\n",
      "Epoch [4298], train_loss: 483.96 with loss1: 428.62, loss2: 55.34 and loss3: 0.00\n",
      "Epoch [4299], train_loss: 484.93 with loss1: 429.57, loss2: 55.36 and loss3: 0.00\n",
      "Epoch [4300], train_loss: 480.14 with loss1: 424.82, loss2: 55.32 and loss3: 0.00\n",
      "Epoch [4301], train_loss: 482.69 with loss1: 427.38, loss2: 55.31 and loss3: 0.00\n",
      "Epoch [4302], train_loss: 478.47 with loss1: 423.10, loss2: 55.37 and loss3: 0.00\n",
      "Epoch [4303], train_loss: 479.69 with loss1: 424.34, loss2: 55.35 and loss3: 0.00\n",
      "Epoch [4304], train_loss: 476.69 with loss1: 421.38, loss2: 55.31 and loss3: 0.00\n",
      "Epoch [4305], train_loss: 476.86 with loss1: 421.57, loss2: 55.29 and loss3: 0.00\n",
      "Epoch [4306], train_loss: 475.09 with loss1: 419.75, loss2: 55.34 and loss3: 0.00\n",
      "Epoch [4307], train_loss: 474.12 with loss1: 418.83, loss2: 55.29 and loss3: 0.00\n",
      "Epoch [4308], train_loss: 470.46 with loss1: 415.14, loss2: 55.32 and loss3: 0.00\n",
      "Epoch [4309], train_loss: 470.87 with loss1: 415.60, loss2: 55.27 and loss3: 0.00\n",
      "Epoch [4310], train_loss: 470.61 with loss1: 415.30, loss2: 55.31 and loss3: 0.00\n",
      "Epoch [4311], train_loss: 468.60 with loss1: 413.34, loss2: 55.26 and loss3: 0.00\n",
      "Epoch [4312], train_loss: 464.77 with loss1: 409.46, loss2: 55.31 and loss3: 0.00\n",
      "Epoch [4313], train_loss: 464.59 with loss1: 409.33, loss2: 55.26 and loss3: 0.00\n",
      "Epoch [4314], train_loss: 462.96 with loss1: 407.69, loss2: 55.27 and loss3: 0.00\n",
      "Epoch [4315], train_loss: 461.56 with loss1: 406.31, loss2: 55.25 and loss3: 0.00\n",
      "Epoch [4316], train_loss: 460.09 with loss1: 404.86, loss2: 55.23 and loss3: 0.00\n",
      "Epoch [4317], train_loss: 460.17 with loss1: 404.95, loss2: 55.21 and loss3: 0.00\n",
      "Epoch [4318], train_loss: 458.17 with loss1: 402.96, loss2: 55.21 and loss3: 0.00\n",
      "Epoch [4319], train_loss: 457.95 with loss1: 402.70, loss2: 55.26 and loss3: 0.00\n",
      "Epoch [4320], train_loss: 455.40 with loss1: 400.24, loss2: 55.16 and loss3: 0.00\n",
      "Epoch [4321], train_loss: 456.25 with loss1: 401.01, loss2: 55.23 and loss3: 0.00\n",
      "Epoch [4322], train_loss: 457.52 with loss1: 402.30, loss2: 55.21 and loss3: 0.00\n",
      "Epoch [4323], train_loss: 455.02 with loss1: 399.84, loss2: 55.18 and loss3: 0.00\n",
      "Epoch [4324], train_loss: 454.69 with loss1: 399.52, loss2: 55.17 and loss3: 0.00\n",
      "Epoch [4325], train_loss: 451.83 with loss1: 396.64, loss2: 55.19 and loss3: 0.00\n",
      "Epoch [4326], train_loss: 450.22 with loss1: 395.01, loss2: 55.21 and loss3: 0.00\n",
      "Epoch [4327], train_loss: 449.23 with loss1: 394.10, loss2: 55.13 and loss3: 0.00\n",
      "Epoch [4328], train_loss: 448.27 with loss1: 393.17, loss2: 55.10 and loss3: 0.00\n",
      "Epoch [4329], train_loss: 448.25 with loss1: 393.09, loss2: 55.16 and loss3: 0.00\n",
      "Epoch [4330], train_loss: 446.48 with loss1: 391.35, loss2: 55.13 and loss3: 0.00\n",
      "Epoch [4331], train_loss: 445.93 with loss1: 390.77, loss2: 55.16 and loss3: 0.00\n",
      "Epoch [4332], train_loss: 445.83 with loss1: 390.74, loss2: 55.09 and loss3: 0.00\n",
      "Epoch [4333], train_loss: 445.08 with loss1: 389.98, loss2: 55.10 and loss3: 0.00\n",
      "Epoch [4334], train_loss: 443.70 with loss1: 388.60, loss2: 55.10 and loss3: 0.00\n",
      "Epoch [4335], train_loss: 445.84 with loss1: 390.79, loss2: 55.05 and loss3: 0.00\n",
      "Epoch [4336], train_loss: 443.01 with loss1: 387.99, loss2: 55.02 and loss3: 0.00\n",
      "Epoch [4337], train_loss: 442.91 with loss1: 387.77, loss2: 55.14 and loss3: 0.00\n",
      "Epoch [4338], train_loss: 441.25 with loss1: 386.18, loss2: 55.07 and loss3: 0.00\n",
      "Epoch [4339], train_loss: 441.51 with loss1: 386.40, loss2: 55.10 and loss3: 0.00\n",
      "Epoch [4340], train_loss: 441.40 with loss1: 386.30, loss2: 55.09 and loss3: 0.00\n",
      "Epoch [4341], train_loss: 442.30 with loss1: 387.23, loss2: 55.08 and loss3: 0.00\n",
      "Epoch [4342], train_loss: 441.17 with loss1: 386.09, loss2: 55.08 and loss3: 0.00\n",
      "Epoch [4343], train_loss: 441.22 with loss1: 386.17, loss2: 55.05 and loss3: 0.00\n",
      "Epoch [4344], train_loss: 441.91 with loss1: 386.87, loss2: 55.04 and loss3: 0.00\n",
      "Epoch [4345], train_loss: 441.51 with loss1: 386.43, loss2: 55.08 and loss3: 0.00\n",
      "Epoch [4346], train_loss: 441.14 with loss1: 386.06, loss2: 55.07 and loss3: 0.00\n",
      "Epoch [4347], train_loss: 441.77 with loss1: 386.76, loss2: 55.00 and loss3: 0.00\n",
      "Epoch [4348], train_loss: 441.78 with loss1: 386.76, loss2: 55.02 and loss3: 0.00\n",
      "Epoch [4349], train_loss: 441.92 with loss1: 386.88, loss2: 55.04 and loss3: 0.00\n",
      "Epoch [4350], train_loss: 441.24 with loss1: 386.23, loss2: 55.02 and loss3: 0.00\n",
      "Epoch [4351], train_loss: 441.94 with loss1: 386.91, loss2: 55.03 and loss3: 0.00\n",
      "Epoch [4352], train_loss: 441.60 with loss1: 386.55, loss2: 55.05 and loss3: 0.00\n",
      "Epoch [4353], train_loss: 443.75 with loss1: 388.74, loss2: 55.01 and loss3: 0.00\n",
      "Epoch [4354], train_loss: 441.52 with loss1: 386.57, loss2: 54.95 and loss3: 0.00\n",
      "Epoch [4355], train_loss: 443.38 with loss1: 388.44, loss2: 54.94 and loss3: 0.00\n",
      "Epoch [4356], train_loss: 442.14 with loss1: 387.16, loss2: 54.98 and loss3: 0.00\n",
      "Epoch [4357], train_loss: 442.35 with loss1: 387.36, loss2: 54.99 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4358], train_loss: 442.96 with loss1: 387.97, loss2: 54.99 and loss3: 0.00\n",
      "Epoch [4359], train_loss: 442.19 with loss1: 387.25, loss2: 54.94 and loss3: 0.00\n",
      "Epoch [4360], train_loss: 441.26 with loss1: 386.27, loss2: 54.99 and loss3: 0.00\n",
      "Epoch [4361], train_loss: 443.36 with loss1: 388.45, loss2: 54.91 and loss3: 0.00\n",
      "Epoch [4362], train_loss: 442.57 with loss1: 387.63, loss2: 54.94 and loss3: 0.00\n",
      "Epoch [4363], train_loss: 443.42 with loss1: 388.57, loss2: 54.85 and loss3: 0.00\n",
      "Epoch [4364], train_loss: 442.59 with loss1: 387.70, loss2: 54.90 and loss3: 0.00\n",
      "Epoch [4365], train_loss: 443.61 with loss1: 388.73, loss2: 54.88 and loss3: 0.00\n",
      "Epoch [4366], train_loss: 445.22 with loss1: 390.30, loss2: 54.92 and loss3: 0.00\n",
      "Epoch [4367], train_loss: 445.01 with loss1: 390.16, loss2: 54.85 and loss3: 0.00\n",
      "Epoch [4368], train_loss: 443.35 with loss1: 388.45, loss2: 54.90 and loss3: 0.00\n",
      "Epoch [4369], train_loss: 446.17 with loss1: 391.30, loss2: 54.87 and loss3: 0.00\n",
      "Epoch [4370], train_loss: 445.60 with loss1: 390.74, loss2: 54.85 and loss3: 0.00\n",
      "Epoch [4371], train_loss: 448.19 with loss1: 393.32, loss2: 54.87 and loss3: 0.00\n",
      "Epoch [4372], train_loss: 445.47 with loss1: 390.61, loss2: 54.86 and loss3: 0.00\n",
      "Epoch [4373], train_loss: 446.90 with loss1: 392.04, loss2: 54.86 and loss3: 0.00\n",
      "Epoch [4374], train_loss: 446.01 with loss1: 391.13, loss2: 54.88 and loss3: 0.00\n",
      "Epoch [4375], train_loss: 447.80 with loss1: 392.93, loss2: 54.86 and loss3: 0.00\n",
      "Epoch [4376], train_loss: 444.94 with loss1: 390.11, loss2: 54.83 and loss3: 0.00\n",
      "Epoch [4377], train_loss: 447.65 with loss1: 392.82, loss2: 54.83 and loss3: 0.00\n",
      "Epoch [4378], train_loss: 445.97 with loss1: 391.14, loss2: 54.83 and loss3: 0.00\n",
      "Epoch [4379], train_loss: 447.53 with loss1: 392.69, loss2: 54.84 and loss3: 0.00\n",
      "Epoch [4380], train_loss: 447.66 with loss1: 392.80, loss2: 54.85 and loss3: 0.00\n",
      "Epoch [4381], train_loss: 448.02 with loss1: 393.22, loss2: 54.80 and loss3: 0.00\n",
      "Epoch [4382], train_loss: 447.84 with loss1: 393.03, loss2: 54.81 and loss3: 0.00\n",
      "Epoch [4383], train_loss: 448.72 with loss1: 393.95, loss2: 54.77 and loss3: 0.00\n",
      "Epoch [4384], train_loss: 448.40 with loss1: 393.57, loss2: 54.83 and loss3: 0.00\n",
      "Epoch [4385], train_loss: 448.30 with loss1: 393.54, loss2: 54.75 and loss3: 0.00\n",
      "Epoch [4386], train_loss: 446.79 with loss1: 391.98, loss2: 54.80 and loss3: 0.00\n",
      "Epoch [4387], train_loss: 448.16 with loss1: 393.46, loss2: 54.70 and loss3: 0.00\n",
      "Epoch [4388], train_loss: 447.12 with loss1: 392.37, loss2: 54.74 and loss3: 0.00\n",
      "Epoch [4389], train_loss: 447.20 with loss1: 392.46, loss2: 54.73 and loss3: 0.00\n",
      "Epoch [4390], train_loss: 446.12 with loss1: 391.34, loss2: 54.78 and loss3: 0.00\n",
      "Epoch [4391], train_loss: 445.96 with loss1: 391.19, loss2: 54.76 and loss3: 0.00\n",
      "Epoch [4392], train_loss: 445.73 with loss1: 390.97, loss2: 54.76 and loss3: 0.00\n",
      "Epoch [4393], train_loss: 445.74 with loss1: 391.07, loss2: 54.66 and loss3: 0.00\n",
      "Epoch [4394], train_loss: 443.81 with loss1: 389.06, loss2: 54.76 and loss3: 0.00\n",
      "Epoch [4395], train_loss: 444.68 with loss1: 389.93, loss2: 54.75 and loss3: 0.00\n",
      "Epoch [4396], train_loss: 445.15 with loss1: 390.45, loss2: 54.69 and loss3: 0.00\n",
      "Epoch [4397], train_loss: 443.66 with loss1: 389.02, loss2: 54.64 and loss3: 0.00\n",
      "Epoch [4398], train_loss: 443.41 with loss1: 388.73, loss2: 54.67 and loss3: 0.00\n",
      "Epoch [4399], train_loss: 443.03 with loss1: 388.35, loss2: 54.67 and loss3: 0.00\n",
      "Epoch [4400], train_loss: 443.93 with loss1: 389.27, loss2: 54.66 and loss3: 0.00\n",
      "Epoch [4401], train_loss: 444.58 with loss1: 389.96, loss2: 54.62 and loss3: 0.00\n",
      "Epoch [4402], train_loss: 443.20 with loss1: 388.59, loss2: 54.61 and loss3: 0.00\n",
      "Epoch [4403], train_loss: 444.76 with loss1: 390.14, loss2: 54.61 and loss3: 0.00\n",
      "Epoch [4404], train_loss: 443.47 with loss1: 388.78, loss2: 54.69 and loss3: 0.00\n",
      "Epoch [4405], train_loss: 444.56 with loss1: 389.90, loss2: 54.66 and loss3: 0.00\n",
      "Epoch [4406], train_loss: 443.34 with loss1: 388.68, loss2: 54.67 and loss3: 0.00\n",
      "Epoch [4407], train_loss: 443.78 with loss1: 389.18, loss2: 54.59 and loss3: 0.00\n",
      "Epoch [4408], train_loss: 442.40 with loss1: 387.75, loss2: 54.64 and loss3: 0.00\n",
      "Epoch [4409], train_loss: 442.87 with loss1: 388.24, loss2: 54.63 and loss3: 0.00\n",
      "Epoch [4410], train_loss: 441.89 with loss1: 387.25, loss2: 54.63 and loss3: 0.00\n",
      "Epoch [4411], train_loss: 441.21 with loss1: 386.65, loss2: 54.56 and loss3: 0.00\n",
      "Epoch [4412], train_loss: 441.03 with loss1: 386.42, loss2: 54.61 and loss3: 0.00\n",
      "Epoch [4413], train_loss: 440.99 with loss1: 386.37, loss2: 54.62 and loss3: 0.00\n",
      "Epoch [4414], train_loss: 440.21 with loss1: 385.65, loss2: 54.55 and loss3: 0.00\n",
      "Epoch [4415], train_loss: 441.86 with loss1: 387.33, loss2: 54.53 and loss3: 0.00\n",
      "Epoch [4416], train_loss: 441.03 with loss1: 386.47, loss2: 54.56 and loss3: 0.00\n",
      "Epoch [4417], train_loss: 440.57 with loss1: 386.03, loss2: 54.54 and loss3: 0.00\n",
      "Epoch [4418], train_loss: 439.52 with loss1: 384.99, loss2: 54.53 and loss3: 0.00\n",
      "Epoch [4419], train_loss: 441.79 with loss1: 387.30, loss2: 54.49 and loss3: 0.00\n",
      "Epoch [4420], train_loss: 440.01 with loss1: 385.47, loss2: 54.54 and loss3: 0.00\n",
      "Epoch [4421], train_loss: 439.04 with loss1: 384.56, loss2: 54.48 and loss3: 0.00\n",
      "Epoch [4422], train_loss: 441.33 with loss1: 386.77, loss2: 54.56 and loss3: 0.00\n",
      "Epoch [4423], train_loss: 440.90 with loss1: 386.39, loss2: 54.51 and loss3: 0.00\n",
      "Epoch [4424], train_loss: 440.85 with loss1: 386.36, loss2: 54.49 and loss3: 0.00\n",
      "Epoch [4425], train_loss: 440.29 with loss1: 385.85, loss2: 54.44 and loss3: 0.00\n",
      "Epoch [4426], train_loss: 438.91 with loss1: 384.39, loss2: 54.52 and loss3: 0.00\n",
      "Epoch [4427], train_loss: 438.25 with loss1: 383.78, loss2: 54.48 and loss3: 0.00\n",
      "Epoch [4428], train_loss: 438.09 with loss1: 383.62, loss2: 54.48 and loss3: 0.00\n",
      "Epoch [4429], train_loss: 439.07 with loss1: 384.60, loss2: 54.47 and loss3: 0.00\n",
      "Epoch [4430], train_loss: 437.91 with loss1: 383.44, loss2: 54.46 and loss3: 0.00\n",
      "Epoch [4431], train_loss: 439.22 with loss1: 384.80, loss2: 54.42 and loss3: 0.00\n",
      "Epoch [4432], train_loss: 438.34 with loss1: 383.85, loss2: 54.49 and loss3: 0.00\n",
      "Epoch [4433], train_loss: 439.12 with loss1: 384.66, loss2: 54.46 and loss3: 0.00\n",
      "Epoch [4434], train_loss: 440.71 with loss1: 386.29, loss2: 54.42 and loss3: 0.00\n",
      "Epoch [4435], train_loss: 440.06 with loss1: 385.64, loss2: 54.43 and loss3: 0.00\n",
      "Epoch [4436], train_loss: 438.56 with loss1: 384.14, loss2: 54.42 and loss3: 0.00\n",
      "Epoch [4437], train_loss: 440.28 with loss1: 385.85, loss2: 54.43 and loss3: 0.00\n",
      "Epoch [4438], train_loss: 438.66 with loss1: 384.27, loss2: 54.39 and loss3: 0.00\n",
      "Epoch [4439], train_loss: 439.52 with loss1: 385.10, loss2: 54.42 and loss3: 0.00\n",
      "Epoch [4440], train_loss: 438.60 with loss1: 384.18, loss2: 54.42 and loss3: 0.00\n",
      "Epoch [4441], train_loss: 440.05 with loss1: 385.66, loss2: 54.38 and loss3: 0.00\n",
      "Epoch [4442], train_loss: 439.85 with loss1: 385.43, loss2: 54.42 and loss3: 0.00\n",
      "Epoch [4443], train_loss: 440.82 with loss1: 386.43, loss2: 54.39 and loss3: 0.00\n",
      "Epoch [4444], train_loss: 439.24 with loss1: 384.84, loss2: 54.40 and loss3: 0.00\n",
      "Epoch [4445], train_loss: 441.69 with loss1: 387.22, loss2: 54.48 and loss3: 0.00\n",
      "Epoch [4446], train_loss: 439.85 with loss1: 385.47, loss2: 54.39 and loss3: 0.00\n",
      "Epoch [4447], train_loss: 441.16 with loss1: 386.77, loss2: 54.38 and loss3: 0.00\n",
      "Epoch [4448], train_loss: 439.90 with loss1: 385.48, loss2: 54.43 and loss3: 0.00\n",
      "Epoch [4449], train_loss: 442.02 with loss1: 387.67, loss2: 54.35 and loss3: 0.00\n",
      "Epoch [4450], train_loss: 441.01 with loss1: 386.62, loss2: 54.39 and loss3: 0.00\n",
      "Epoch [4451], train_loss: 442.54 with loss1: 388.22, loss2: 54.32 and loss3: 0.00\n",
      "Epoch [4452], train_loss: 442.22 with loss1: 387.81, loss2: 54.41 and loss3: 0.00\n",
      "Epoch [4453], train_loss: 443.09 with loss1: 388.77, loss2: 54.32 and loss3: 0.00\n",
      "Epoch [4454], train_loss: 442.77 with loss1: 388.42, loss2: 54.36 and loss3: 0.00\n",
      "Epoch [4455], train_loss: 445.29 with loss1: 391.01, loss2: 54.29 and loss3: 0.00\n",
      "Epoch [4456], train_loss: 443.48 with loss1: 389.12, loss2: 54.36 and loss3: 0.00\n",
      "Epoch [4457], train_loss: 446.76 with loss1: 392.44, loss2: 54.32 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4458], train_loss: 447.17 with loss1: 392.86, loss2: 54.31 and loss3: 0.00\n",
      "Epoch [4459], train_loss: 448.54 with loss1: 394.29, loss2: 54.25 and loss3: 0.00\n",
      "Epoch [4460], train_loss: 447.81 with loss1: 393.46, loss2: 54.35 and loss3: 0.00\n",
      "Epoch [4461], train_loss: 449.97 with loss1: 395.74, loss2: 54.23 and loss3: 0.00\n",
      "Epoch [4462], train_loss: 448.72 with loss1: 394.40, loss2: 54.32 and loss3: 0.00\n",
      "Epoch [4463], train_loss: 452.88 with loss1: 398.63, loss2: 54.26 and loss3: 0.00\n",
      "Epoch [4464], train_loss: 451.98 with loss1: 397.73, loss2: 54.25 and loss3: 0.00\n",
      "Epoch [4465], train_loss: 453.33 with loss1: 399.06, loss2: 54.27 and loss3: 0.00\n",
      "Epoch [4466], train_loss: 452.66 with loss1: 398.38, loss2: 54.28 and loss3: 0.00\n",
      "Epoch [4467], train_loss: 455.11 with loss1: 400.85, loss2: 54.26 and loss3: 0.00\n",
      "Epoch [4468], train_loss: 454.67 with loss1: 400.45, loss2: 54.22 and loss3: 0.00\n",
      "Epoch [4469], train_loss: 459.33 with loss1: 405.05, loss2: 54.28 and loss3: 0.00\n",
      "Epoch [4470], train_loss: 458.26 with loss1: 403.92, loss2: 54.34 and loss3: 0.00\n",
      "Epoch [4471], train_loss: 460.67 with loss1: 406.40, loss2: 54.27 and loss3: 0.00\n",
      "Epoch [4472], train_loss: 458.84 with loss1: 404.62, loss2: 54.21 and loss3: 0.00\n",
      "Epoch [4473], train_loss: 461.38 with loss1: 407.22, loss2: 54.16 and loss3: 0.00\n",
      "Epoch [4474], train_loss: 462.21 with loss1: 408.02, loss2: 54.19 and loss3: 0.00\n",
      "Epoch [4475], train_loss: 464.93 with loss1: 410.72, loss2: 54.21 and loss3: 0.00\n",
      "Epoch [4476], train_loss: 462.13 with loss1: 407.88, loss2: 54.25 and loss3: 0.00\n",
      "Epoch [4477], train_loss: 465.11 with loss1: 410.91, loss2: 54.20 and loss3: 0.00\n",
      "Epoch [4478], train_loss: 463.70 with loss1: 409.51, loss2: 54.19 and loss3: 0.00\n",
      "Epoch [4479], train_loss: 467.14 with loss1: 412.92, loss2: 54.22 and loss3: 0.00\n",
      "Epoch [4480], train_loss: 466.36 with loss1: 412.10, loss2: 54.26 and loss3: 0.00\n",
      "Epoch [4481], train_loss: 467.85 with loss1: 413.62, loss2: 54.23 and loss3: 0.00\n",
      "Epoch [4482], train_loss: 464.35 with loss1: 410.16, loss2: 54.19 and loss3: 0.00\n",
      "Epoch [4483], train_loss: 465.32 with loss1: 411.11, loss2: 54.21 and loss3: 0.00\n",
      "Epoch [4484], train_loss: 463.62 with loss1: 409.43, loss2: 54.18 and loss3: 0.00\n",
      "Epoch [4485], train_loss: 464.22 with loss1: 409.98, loss2: 54.24 and loss3: 0.00\n",
      "Epoch [4486], train_loss: 462.64 with loss1: 408.43, loss2: 54.21 and loss3: 0.00\n",
      "Epoch [4487], train_loss: 463.25 with loss1: 409.09, loss2: 54.15 and loss3: 0.00\n",
      "Epoch [4488], train_loss: 460.66 with loss1: 406.44, loss2: 54.22 and loss3: 0.00\n",
      "Epoch [4489], train_loss: 463.24 with loss1: 409.15, loss2: 54.09 and loss3: 0.00\n",
      "Epoch [4490], train_loss: 459.28 with loss1: 405.11, loss2: 54.17 and loss3: 0.00\n",
      "Epoch [4491], train_loss: 459.30 with loss1: 405.16, loss2: 54.13 and loss3: 0.00\n",
      "Epoch [4492], train_loss: 456.88 with loss1: 402.70, loss2: 54.19 and loss3: 0.00\n",
      "Epoch [4493], train_loss: 458.05 with loss1: 403.88, loss2: 54.17 and loss3: 0.00\n",
      "Epoch [4494], train_loss: 456.10 with loss1: 401.95, loss2: 54.15 and loss3: 0.00\n",
      "Epoch [4495], train_loss: 455.20 with loss1: 401.11, loss2: 54.09 and loss3: 0.00\n",
      "Epoch [4496], train_loss: 453.99 with loss1: 399.85, loss2: 54.13 and loss3: 0.00\n",
      "Epoch [4497], train_loss: 453.02 with loss1: 398.81, loss2: 54.21 and loss3: 0.00\n",
      "Epoch [4498], train_loss: 452.64 with loss1: 398.50, loss2: 54.14 and loss3: 0.00\n",
      "Epoch [4499], train_loss: 450.91 with loss1: 396.75, loss2: 54.16 and loss3: 0.00\n",
      "Epoch [4500], train_loss: 449.12 with loss1: 394.95, loss2: 54.17 and loss3: 0.00\n",
      "Epoch [4501], train_loss: 450.03 with loss1: 395.92, loss2: 54.11 and loss3: 0.00\n",
      "Epoch [4502], train_loss: 448.14 with loss1: 394.04, loss2: 54.11 and loss3: 0.00\n",
      "Epoch [4503], train_loss: 446.82 with loss1: 392.72, loss2: 54.10 and loss3: 0.00\n",
      "Epoch [4504], train_loss: 445.27 with loss1: 391.17, loss2: 54.11 and loss3: 0.00\n",
      "Epoch [4505], train_loss: 444.77 with loss1: 390.67, loss2: 54.10 and loss3: 0.00\n",
      "Epoch [4506], train_loss: 445.02 with loss1: 390.89, loss2: 54.12 and loss3: 0.00\n",
      "Epoch [4507], train_loss: 444.59 with loss1: 390.52, loss2: 54.07 and loss3: 0.00\n",
      "Epoch [4508], train_loss: 442.42 with loss1: 388.35, loss2: 54.07 and loss3: 0.00\n",
      "Epoch [4509], train_loss: 443.89 with loss1: 389.80, loss2: 54.09 and loss3: 0.00\n",
      "Epoch [4510], train_loss: 439.91 with loss1: 385.78, loss2: 54.13 and loss3: 0.00\n",
      "Epoch [4511], train_loss: 440.96 with loss1: 386.80, loss2: 54.16 and loss3: 0.00\n",
      "Epoch [4512], train_loss: 440.44 with loss1: 386.43, loss2: 54.01 and loss3: 0.00\n",
      "Epoch [4513], train_loss: 440.29 with loss1: 386.25, loss2: 54.05 and loss3: 0.00\n",
      "Epoch [4514], train_loss: 441.19 with loss1: 387.19, loss2: 54.01 and loss3: 0.00\n",
      "Epoch [4515], train_loss: 438.18 with loss1: 384.15, loss2: 54.03 and loss3: 0.00\n",
      "Epoch [4516], train_loss: 437.12 with loss1: 383.11, loss2: 54.01 and loss3: 0.00\n",
      "Epoch [4517], train_loss: 437.59 with loss1: 383.58, loss2: 54.02 and loss3: 0.00\n",
      "Epoch [4518], train_loss: 437.95 with loss1: 383.96, loss2: 53.99 and loss3: 0.00\n",
      "Epoch [4519], train_loss: 435.49 with loss1: 381.47, loss2: 54.01 and loss3: 0.00\n",
      "Epoch [4520], train_loss: 434.27 with loss1: 380.26, loss2: 54.01 and loss3: 0.00\n",
      "Epoch [4521], train_loss: 435.10 with loss1: 381.07, loss2: 54.03 and loss3: 0.00\n",
      "Epoch [4522], train_loss: 433.47 with loss1: 379.47, loss2: 54.00 and loss3: 0.00\n",
      "Epoch [4523], train_loss: 433.09 with loss1: 379.11, loss2: 53.98 and loss3: 0.00\n",
      "Epoch [4524], train_loss: 432.49 with loss1: 378.51, loss2: 53.98 and loss3: 0.00\n",
      "Epoch [4525], train_loss: 434.01 with loss1: 379.99, loss2: 54.02 and loss3: 0.00\n",
      "Epoch [4526], train_loss: 431.59 with loss1: 377.60, loss2: 53.98 and loss3: 0.00\n",
      "Epoch [4527], train_loss: 431.95 with loss1: 378.00, loss2: 53.95 and loss3: 0.00\n",
      "Epoch [4528], train_loss: 429.73 with loss1: 375.79, loss2: 53.94 and loss3: 0.00\n",
      "Epoch [4529], train_loss: 430.69 with loss1: 376.71, loss2: 53.98 and loss3: 0.00\n",
      "Epoch [4530], train_loss: 429.95 with loss1: 375.98, loss2: 53.97 and loss3: 0.00\n",
      "Epoch [4531], train_loss: 430.84 with loss1: 376.93, loss2: 53.91 and loss3: 0.00\n",
      "Epoch [4532], train_loss: 430.35 with loss1: 376.41, loss2: 53.93 and loss3: 0.00\n",
      "Epoch [4533], train_loss: 429.45 with loss1: 375.51, loss2: 53.94 and loss3: 0.00\n",
      "Epoch [4534], train_loss: 429.77 with loss1: 375.84, loss2: 53.93 and loss3: 0.00\n",
      "Epoch [4535], train_loss: 430.43 with loss1: 376.51, loss2: 53.92 and loss3: 0.00\n",
      "Epoch [4536], train_loss: 430.05 with loss1: 376.22, loss2: 53.83 and loss3: 0.00\n",
      "Epoch [4537], train_loss: 430.98 with loss1: 377.04, loss2: 53.94 and loss3: 0.00\n",
      "Epoch [4538], train_loss: 428.86 with loss1: 374.92, loss2: 53.95 and loss3: 0.00\n",
      "Epoch [4539], train_loss: 430.04 with loss1: 376.18, loss2: 53.85 and loss3: 0.00\n",
      "Epoch [4540], train_loss: 429.46 with loss1: 375.60, loss2: 53.87 and loss3: 0.00\n",
      "Epoch [4541], train_loss: 429.54 with loss1: 375.65, loss2: 53.90 and loss3: 0.00\n",
      "Epoch [4542], train_loss: 428.36 with loss1: 374.55, loss2: 53.81 and loss3: 0.00\n",
      "Epoch [4543], train_loss: 428.73 with loss1: 374.89, loss2: 53.84 and loss3: 0.00\n",
      "Epoch [4544], train_loss: 429.65 with loss1: 375.81, loss2: 53.84 and loss3: 0.00\n",
      "Epoch [4545], train_loss: 428.70 with loss1: 374.83, loss2: 53.86 and loss3: 0.00\n",
      "Epoch [4546], train_loss: 428.91 with loss1: 375.07, loss2: 53.84 and loss3: 0.00\n",
      "Epoch [4547], train_loss: 429.49 with loss1: 375.66, loss2: 53.84 and loss3: 0.00\n",
      "Epoch [4548], train_loss: 428.99 with loss1: 375.21, loss2: 53.78 and loss3: 0.00\n",
      "Epoch [4549], train_loss: 430.21 with loss1: 376.31, loss2: 53.90 and loss3: 0.00\n",
      "Epoch [4550], train_loss: 428.60 with loss1: 374.80, loss2: 53.80 and loss3: 0.00\n",
      "Epoch [4551], train_loss: 429.88 with loss1: 376.04, loss2: 53.84 and loss3: 0.00\n",
      "Epoch [4552], train_loss: 428.17 with loss1: 374.43, loss2: 53.74 and loss3: 0.00\n",
      "Epoch [4553], train_loss: 429.98 with loss1: 376.19, loss2: 53.79 and loss3: 0.00\n",
      "Epoch [4554], train_loss: 428.81 with loss1: 375.01, loss2: 53.80 and loss3: 0.00\n",
      "Epoch [4555], train_loss: 429.71 with loss1: 375.91, loss2: 53.80 and loss3: 0.00\n",
      "Epoch [4556], train_loss: 430.44 with loss1: 376.68, loss2: 53.75 and loss3: 0.00\n",
      "Epoch [4557], train_loss: 430.43 with loss1: 376.65, loss2: 53.78 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4558], train_loss: 431.36 with loss1: 377.57, loss2: 53.79 and loss3: 0.00\n",
      "Epoch [4559], train_loss: 430.55 with loss1: 376.76, loss2: 53.79 and loss3: 0.00\n",
      "Epoch [4560], train_loss: 429.73 with loss1: 375.95, loss2: 53.79 and loss3: 0.00\n",
      "Epoch [4561], train_loss: 430.82 with loss1: 376.99, loss2: 53.83 and loss3: 0.00\n",
      "Epoch [4562], train_loss: 429.71 with loss1: 376.01, loss2: 53.70 and loss3: 0.00\n",
      "Epoch [4563], train_loss: 431.23 with loss1: 377.45, loss2: 53.78 and loss3: 0.00\n",
      "Epoch [4564], train_loss: 431.47 with loss1: 377.76, loss2: 53.71 and loss3: 0.00\n",
      "Epoch [4565], train_loss: 432.52 with loss1: 378.79, loss2: 53.73 and loss3: 0.00\n",
      "Epoch [4566], train_loss: 432.25 with loss1: 378.57, loss2: 53.68 and loss3: 0.00\n",
      "Epoch [4567], train_loss: 432.80 with loss1: 379.01, loss2: 53.79 and loss3: 0.00\n",
      "Epoch [4568], train_loss: 432.67 with loss1: 378.99, loss2: 53.68 and loss3: 0.00\n",
      "Epoch [4569], train_loss: 434.36 with loss1: 380.66, loss2: 53.71 and loss3: 0.00\n",
      "Epoch [4570], train_loss: 433.41 with loss1: 379.76, loss2: 53.64 and loss3: 0.00\n",
      "Epoch [4571], train_loss: 435.16 with loss1: 381.44, loss2: 53.73 and loss3: 0.00\n",
      "Epoch [4572], train_loss: 434.68 with loss1: 380.99, loss2: 53.70 and loss3: 0.00\n",
      "Epoch [4573], train_loss: 436.16 with loss1: 382.49, loss2: 53.67 and loss3: 0.00\n",
      "Epoch [4574], train_loss: 436.05 with loss1: 382.40, loss2: 53.65 and loss3: 0.00\n",
      "Epoch [4575], train_loss: 437.36 with loss1: 383.70, loss2: 53.66 and loss3: 0.00\n",
      "Epoch [4576], train_loss: 436.61 with loss1: 382.89, loss2: 53.72 and loss3: 0.00\n",
      "Epoch [4577], train_loss: 437.74 with loss1: 384.04, loss2: 53.70 and loss3: 0.00\n",
      "Epoch [4578], train_loss: 437.43 with loss1: 383.78, loss2: 53.65 and loss3: 0.00\n",
      "Epoch [4579], train_loss: 438.81 with loss1: 385.16, loss2: 53.65 and loss3: 0.00\n",
      "Epoch [4580], train_loss: 437.92 with loss1: 384.33, loss2: 53.59 and loss3: 0.00\n",
      "Epoch [4581], train_loss: 439.97 with loss1: 386.32, loss2: 53.65 and loss3: 0.00\n",
      "Epoch [4582], train_loss: 440.03 with loss1: 386.42, loss2: 53.62 and loss3: 0.00\n",
      "Epoch [4583], train_loss: 440.25 with loss1: 386.65, loss2: 53.59 and loss3: 0.00\n",
      "Epoch [4584], train_loss: 440.15 with loss1: 386.58, loss2: 53.57 and loss3: 0.00\n",
      "Epoch [4585], train_loss: 439.59 with loss1: 385.94, loss2: 53.65 and loss3: 0.00\n",
      "Epoch [4586], train_loss: 439.32 with loss1: 385.73, loss2: 53.59 and loss3: 0.00\n",
      "Epoch [4587], train_loss: 441.11 with loss1: 387.45, loss2: 53.66 and loss3: 0.00\n",
      "Epoch [4588], train_loss: 439.81 with loss1: 386.19, loss2: 53.63 and loss3: 0.00\n",
      "Epoch [4589], train_loss: 439.78 with loss1: 386.18, loss2: 53.60 and loss3: 0.00\n",
      "Epoch [4590], train_loss: 439.54 with loss1: 386.00, loss2: 53.54 and loss3: 0.00\n",
      "Epoch [4591], train_loss: 440.77 with loss1: 387.14, loss2: 53.63 and loss3: 0.00\n",
      "Epoch [4592], train_loss: 440.17 with loss1: 386.62, loss2: 53.55 and loss3: 0.00\n",
      "Epoch [4593], train_loss: 442.10 with loss1: 388.50, loss2: 53.60 and loss3: 0.00\n",
      "Epoch [4594], train_loss: 439.36 with loss1: 385.77, loss2: 53.58 and loss3: 0.00\n",
      "Epoch [4595], train_loss: 441.94 with loss1: 388.32, loss2: 53.62 and loss3: 0.00\n",
      "Epoch [4596], train_loss: 441.50 with loss1: 387.95, loss2: 53.55 and loss3: 0.00\n",
      "Epoch [4597], train_loss: 442.20 with loss1: 388.61, loss2: 53.59 and loss3: 0.00\n",
      "Epoch [4598], train_loss: 440.18 with loss1: 386.61, loss2: 53.58 and loss3: 0.00\n",
      "Epoch [4599], train_loss: 443.18 with loss1: 389.63, loss2: 53.55 and loss3: 0.00\n",
      "Epoch [4600], train_loss: 442.30 with loss1: 388.77, loss2: 53.52 and loss3: 0.00\n",
      "Epoch [4601], train_loss: 442.57 with loss1: 389.02, loss2: 53.55 and loss3: 0.00\n",
      "Epoch [4602], train_loss: 441.85 with loss1: 388.31, loss2: 53.53 and loss3: 0.00\n",
      "Epoch [4603], train_loss: 442.44 with loss1: 388.93, loss2: 53.51 and loss3: 0.00\n",
      "Epoch [4604], train_loss: 442.67 with loss1: 389.11, loss2: 53.56 and loss3: 0.00\n",
      "Epoch [4605], train_loss: 444.14 with loss1: 390.65, loss2: 53.50 and loss3: 0.00\n",
      "Epoch [4606], train_loss: 442.71 with loss1: 389.24, loss2: 53.47 and loss3: 0.00\n",
      "Epoch [4607], train_loss: 443.94 with loss1: 390.39, loss2: 53.55 and loss3: 0.00\n",
      "Epoch [4608], train_loss: 442.74 with loss1: 389.23, loss2: 53.51 and loss3: 0.00\n",
      "Epoch [4609], train_loss: 444.88 with loss1: 391.39, loss2: 53.49 and loss3: 0.00\n",
      "Epoch [4610], train_loss: 444.61 with loss1: 391.11, loss2: 53.51 and loss3: 0.00\n",
      "Epoch [4611], train_loss: 444.02 with loss1: 390.51, loss2: 53.51 and loss3: 0.00\n",
      "Epoch [4612], train_loss: 441.72 with loss1: 388.21, loss2: 53.51 and loss3: 0.00\n",
      "Epoch [4613], train_loss: 442.91 with loss1: 389.42, loss2: 53.50 and loss3: 0.00\n",
      "Epoch [4614], train_loss: 441.10 with loss1: 387.61, loss2: 53.48 and loss3: 0.00\n",
      "Epoch [4615], train_loss: 442.67 with loss1: 389.17, loss2: 53.50 and loss3: 0.00\n",
      "Epoch [4616], train_loss: 440.63 with loss1: 387.14, loss2: 53.49 and loss3: 0.00\n",
      "Epoch [4617], train_loss: 442.23 with loss1: 388.70, loss2: 53.54 and loss3: 0.00\n",
      "Epoch [4618], train_loss: 439.34 with loss1: 385.88, loss2: 53.46 and loss3: 0.00\n",
      "Epoch [4619], train_loss: 439.75 with loss1: 386.32, loss2: 53.43 and loss3: 0.00\n",
      "Epoch [4620], train_loss: 438.52 with loss1: 385.03, loss2: 53.49 and loss3: 0.00\n",
      "Epoch [4621], train_loss: 439.20 with loss1: 385.74, loss2: 53.47 and loss3: 0.00\n",
      "Epoch [4622], train_loss: 439.13 with loss1: 385.64, loss2: 53.50 and loss3: 0.00\n",
      "Epoch [4623], train_loss: 438.93 with loss1: 385.47, loss2: 53.46 and loss3: 0.00\n",
      "Epoch [4624], train_loss: 437.71 with loss1: 384.27, loss2: 53.44 and loss3: 0.00\n",
      "Epoch [4625], train_loss: 441.59 with loss1: 388.13, loss2: 53.46 and loss3: 0.00\n",
      "Epoch [4626], train_loss: 437.31 with loss1: 383.86, loss2: 53.45 and loss3: 0.00\n",
      "Epoch [4627], train_loss: 438.61 with loss1: 385.19, loss2: 53.42 and loss3: 0.00\n",
      "Epoch [4628], train_loss: 436.83 with loss1: 383.43, loss2: 53.40 and loss3: 0.00\n",
      "Epoch [4629], train_loss: 439.05 with loss1: 385.63, loss2: 53.42 and loss3: 0.00\n",
      "Epoch [4630], train_loss: 437.17 with loss1: 383.78, loss2: 53.39 and loss3: 0.00\n",
      "Epoch [4631], train_loss: 438.39 with loss1: 384.99, loss2: 53.40 and loss3: 0.00\n",
      "Epoch [4632], train_loss: 436.65 with loss1: 383.29, loss2: 53.35 and loss3: 0.00\n",
      "Epoch [4633], train_loss: 438.26 with loss1: 384.87, loss2: 53.39 and loss3: 0.00\n",
      "Epoch [4634], train_loss: 437.77 with loss1: 384.39, loss2: 53.38 and loss3: 0.00\n",
      "Epoch [4635], train_loss: 437.88 with loss1: 384.46, loss2: 53.43 and loss3: 0.00\n",
      "Epoch [4636], train_loss: 438.11 with loss1: 384.74, loss2: 53.37 and loss3: 0.00\n",
      "Epoch [4637], train_loss: 439.73 with loss1: 386.32, loss2: 53.41 and loss3: 0.00\n",
      "Epoch [4638], train_loss: 436.63 with loss1: 383.23, loss2: 53.41 and loss3: 0.00\n",
      "Epoch [4639], train_loss: 438.60 with loss1: 385.25, loss2: 53.35 and loss3: 0.00\n",
      "Epoch [4640], train_loss: 437.10 with loss1: 383.77, loss2: 53.34 and loss3: 0.00\n",
      "Epoch [4641], train_loss: 439.94 with loss1: 386.55, loss2: 53.39 and loss3: 0.00\n",
      "Epoch [4642], train_loss: 437.34 with loss1: 384.02, loss2: 53.32 and loss3: 0.00\n",
      "Epoch [4643], train_loss: 439.70 with loss1: 386.39, loss2: 53.31 and loss3: 0.00\n",
      "Epoch [4644], train_loss: 437.78 with loss1: 384.51, loss2: 53.28 and loss3: 0.00\n",
      "Epoch [4645], train_loss: 438.25 with loss1: 384.88, loss2: 53.37 and loss3: 0.00\n",
      "Epoch [4646], train_loss: 437.98 with loss1: 384.67, loss2: 53.31 and loss3: 0.00\n",
      "Epoch [4647], train_loss: 439.68 with loss1: 386.31, loss2: 53.37 and loss3: 0.00\n",
      "Epoch [4648], train_loss: 436.86 with loss1: 383.53, loss2: 53.33 and loss3: 0.00\n",
      "Epoch [4649], train_loss: 438.66 with loss1: 385.38, loss2: 53.28 and loss3: 0.00\n",
      "Epoch [4650], train_loss: 438.29 with loss1: 384.99, loss2: 53.30 and loss3: 0.00\n",
      "Epoch [4651], train_loss: 440.08 with loss1: 386.75, loss2: 53.34 and loss3: 0.00\n",
      "Epoch [4652], train_loss: 438.47 with loss1: 385.16, loss2: 53.32 and loss3: 0.00\n",
      "Epoch [4653], train_loss: 438.87 with loss1: 385.54, loss2: 53.32 and loss3: 0.00\n",
      "Epoch [4654], train_loss: 438.97 with loss1: 385.68, loss2: 53.29 and loss3: 0.00\n",
      "Epoch [4655], train_loss: 439.47 with loss1: 386.19, loss2: 53.29 and loss3: 0.00\n",
      "Epoch [4656], train_loss: 438.39 with loss1: 385.15, loss2: 53.24 and loss3: 0.00\n",
      "Epoch [4657], train_loss: 440.21 with loss1: 386.96, loss2: 53.25 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4658], train_loss: 436.72 with loss1: 383.41, loss2: 53.31 and loss3: 0.00\n",
      "Epoch [4659], train_loss: 437.37 with loss1: 384.13, loss2: 53.25 and loss3: 0.00\n",
      "Epoch [4660], train_loss: 435.49 with loss1: 382.22, loss2: 53.27 and loss3: 0.00\n",
      "Epoch [4661], train_loss: 436.81 with loss1: 383.50, loss2: 53.31 and loss3: 0.00\n",
      "Epoch [4662], train_loss: 435.51 with loss1: 382.25, loss2: 53.26 and loss3: 0.00\n",
      "Epoch [4663], train_loss: 435.32 with loss1: 382.11, loss2: 53.21 and loss3: 0.00\n",
      "Epoch [4664], train_loss: 434.00 with loss1: 380.78, loss2: 53.22 and loss3: 0.00\n",
      "Epoch [4665], train_loss: 436.28 with loss1: 383.04, loss2: 53.24 and loss3: 0.00\n",
      "Epoch [4666], train_loss: 434.24 with loss1: 380.98, loss2: 53.26 and loss3: 0.00\n",
      "Epoch [4667], train_loss: 435.78 with loss1: 382.57, loss2: 53.21 and loss3: 0.00\n",
      "Epoch [4668], train_loss: 433.63 with loss1: 380.41, loss2: 53.22 and loss3: 0.00\n",
      "Epoch [4669], train_loss: 434.15 with loss1: 380.99, loss2: 53.16 and loss3: 0.00\n",
      "Epoch [4670], train_loss: 431.66 with loss1: 378.48, loss2: 53.19 and loss3: 0.00\n",
      "Epoch [4671], train_loss: 433.03 with loss1: 379.80, loss2: 53.23 and loss3: 0.00\n",
      "Epoch [4672], train_loss: 431.76 with loss1: 378.60, loss2: 53.16 and loss3: 0.00\n",
      "Epoch [4673], train_loss: 433.15 with loss1: 380.06, loss2: 53.09 and loss3: 0.00\n",
      "Epoch [4674], train_loss: 431.02 with loss1: 377.86, loss2: 53.16 and loss3: 0.00\n",
      "Epoch [4675], train_loss: 432.12 with loss1: 378.97, loss2: 53.16 and loss3: 0.00\n",
      "Epoch [4676], train_loss: 430.19 with loss1: 376.99, loss2: 53.20 and loss3: 0.00\n",
      "Epoch [4677], train_loss: 430.24 with loss1: 377.13, loss2: 53.11 and loss3: 0.00\n",
      "Epoch [4678], train_loss: 428.64 with loss1: 375.50, loss2: 53.13 and loss3: 0.00\n",
      "Epoch [4679], train_loss: 429.50 with loss1: 376.42, loss2: 53.08 and loss3: 0.00\n",
      "Epoch [4680], train_loss: 427.26 with loss1: 374.16, loss2: 53.09 and loss3: 0.00\n",
      "Epoch [4681], train_loss: 427.90 with loss1: 374.78, loss2: 53.13 and loss3: 0.00\n",
      "Epoch [4682], train_loss: 428.12 with loss1: 374.98, loss2: 53.14 and loss3: 0.00\n",
      "Epoch [4683], train_loss: 428.97 with loss1: 375.85, loss2: 53.12 and loss3: 0.00\n",
      "Epoch [4684], train_loss: 426.72 with loss1: 373.62, loss2: 53.11 and loss3: 0.00\n",
      "Epoch [4685], train_loss: 427.80 with loss1: 374.75, loss2: 53.05 and loss3: 0.00\n",
      "Epoch [4686], train_loss: 425.82 with loss1: 372.73, loss2: 53.09 and loss3: 0.00\n",
      "Epoch [4687], train_loss: 427.60 with loss1: 374.54, loss2: 53.06 and loss3: 0.00\n",
      "Epoch [4688], train_loss: 425.41 with loss1: 372.33, loss2: 53.07 and loss3: 0.00\n",
      "Epoch [4689], train_loss: 427.01 with loss1: 373.95, loss2: 53.06 and loss3: 0.00\n",
      "Epoch [4690], train_loss: 426.81 with loss1: 373.75, loss2: 53.05 and loss3: 0.00\n",
      "Epoch [4691], train_loss: 424.64 with loss1: 371.55, loss2: 53.09 and loss3: 0.00\n",
      "Epoch [4692], train_loss: 424.34 with loss1: 371.29, loss2: 53.05 and loss3: 0.00\n",
      "Epoch [4693], train_loss: 424.89 with loss1: 371.89, loss2: 53.01 and loss3: 0.00\n",
      "Epoch [4694], train_loss: 423.58 with loss1: 370.52, loss2: 53.06 and loss3: 0.00\n",
      "Epoch [4695], train_loss: 426.18 with loss1: 373.15, loss2: 53.03 and loss3: 0.00\n",
      "Epoch [4696], train_loss: 423.42 with loss1: 370.38, loss2: 53.04 and loss3: 0.00\n",
      "Epoch [4697], train_loss: 423.55 with loss1: 370.51, loss2: 53.04 and loss3: 0.00\n",
      "Epoch [4698], train_loss: 423.46 with loss1: 370.41, loss2: 53.05 and loss3: 0.00\n",
      "Epoch [4699], train_loss: 423.18 with loss1: 370.18, loss2: 53.00 and loss3: 0.00\n",
      "Epoch [4700], train_loss: 422.46 with loss1: 369.49, loss2: 52.97 and loss3: 0.00\n",
      "Epoch [4701], train_loss: 422.90 with loss1: 369.92, loss2: 52.99 and loss3: 0.00\n",
      "Epoch [4702], train_loss: 422.58 with loss1: 369.62, loss2: 52.95 and loss3: 0.00\n",
      "Epoch [4703], train_loss: 423.00 with loss1: 370.05, loss2: 52.95 and loss3: 0.00\n",
      "Epoch [4704], train_loss: 421.98 with loss1: 369.02, loss2: 52.96 and loss3: 0.00\n",
      "Epoch [4705], train_loss: 422.50 with loss1: 369.54, loss2: 52.96 and loss3: 0.00\n",
      "Epoch [4706], train_loss: 422.00 with loss1: 369.04, loss2: 52.96 and loss3: 0.00\n",
      "Epoch [4707], train_loss: 422.80 with loss1: 369.84, loss2: 52.96 and loss3: 0.00\n",
      "Epoch [4708], train_loss: 422.64 with loss1: 369.63, loss2: 53.01 and loss3: 0.00\n",
      "Epoch [4709], train_loss: 422.96 with loss1: 370.02, loss2: 52.94 and loss3: 0.00\n",
      "Epoch [4710], train_loss: 421.67 with loss1: 368.73, loss2: 52.95 and loss3: 0.00\n",
      "Epoch [4711], train_loss: 423.57 with loss1: 370.66, loss2: 52.90 and loss3: 0.00\n",
      "Epoch [4712], train_loss: 422.80 with loss1: 369.82, loss2: 52.98 and loss3: 0.00\n",
      "Epoch [4713], train_loss: 424.72 with loss1: 371.87, loss2: 52.85 and loss3: 0.00\n",
      "Epoch [4714], train_loss: 423.97 with loss1: 370.99, loss2: 52.97 and loss3: 0.00\n",
      "Epoch [4715], train_loss: 424.47 with loss1: 371.56, loss2: 52.91 and loss3: 0.00\n",
      "Epoch [4716], train_loss: 424.19 with loss1: 371.28, loss2: 52.92 and loss3: 0.00\n",
      "Epoch [4717], train_loss: 425.51 with loss1: 372.63, loss2: 52.88 and loss3: 0.00\n",
      "Epoch [4718], train_loss: 424.52 with loss1: 371.58, loss2: 52.94 and loss3: 0.00\n",
      "Epoch [4719], train_loss: 424.93 with loss1: 372.04, loss2: 52.89 and loss3: 0.00\n",
      "Epoch [4720], train_loss: 424.34 with loss1: 371.42, loss2: 52.92 and loss3: 0.00\n",
      "Epoch [4721], train_loss: 425.87 with loss1: 373.02, loss2: 52.85 and loss3: 0.00\n",
      "Epoch [4722], train_loss: 425.37 with loss1: 372.46, loss2: 52.91 and loss3: 0.00\n",
      "Epoch [4723], train_loss: 426.54 with loss1: 373.70, loss2: 52.85 and loss3: 0.00\n",
      "Epoch [4724], train_loss: 426.57 with loss1: 373.62, loss2: 52.95 and loss3: 0.00\n",
      "Epoch [4725], train_loss: 427.65 with loss1: 374.84, loss2: 52.81 and loss3: 0.00\n",
      "Epoch [4726], train_loss: 426.83 with loss1: 373.96, loss2: 52.87 and loss3: 0.00\n",
      "Epoch [4727], train_loss: 428.83 with loss1: 376.01, loss2: 52.82 and loss3: 0.00\n",
      "Epoch [4728], train_loss: 428.85 with loss1: 375.95, loss2: 52.90 and loss3: 0.00\n",
      "Epoch [4729], train_loss: 429.69 with loss1: 376.90, loss2: 52.79 and loss3: 0.00\n",
      "Epoch [4730], train_loss: 431.09 with loss1: 378.24, loss2: 52.85 and loss3: 0.00\n",
      "Epoch [4731], train_loss: 431.06 with loss1: 378.21, loss2: 52.85 and loss3: 0.00\n",
      "Epoch [4732], train_loss: 432.65 with loss1: 379.82, loss2: 52.83 and loss3: 0.00\n",
      "Epoch [4733], train_loss: 436.57 with loss1: 383.76, loss2: 52.81 and loss3: 0.00\n",
      "Epoch [4734], train_loss: 434.99 with loss1: 382.15, loss2: 52.84 and loss3: 0.00\n",
      "Epoch [4735], train_loss: 438.52 with loss1: 385.76, loss2: 52.76 and loss3: 0.00\n",
      "Epoch [4736], train_loss: 437.59 with loss1: 384.73, loss2: 52.86 and loss3: 0.00\n",
      "Epoch [4737], train_loss: 439.93 with loss1: 387.16, loss2: 52.77 and loss3: 0.00\n",
      "Epoch [4738], train_loss: 440.04 with loss1: 387.20, loss2: 52.84 and loss3: 0.00\n",
      "Epoch [4739], train_loss: 444.77 with loss1: 391.98, loss2: 52.79 and loss3: 0.00\n",
      "Epoch [4740], train_loss: 444.00 with loss1: 391.23, loss2: 52.77 and loss3: 0.00\n",
      "Epoch [4741], train_loss: 446.48 with loss1: 393.78, loss2: 52.70 and loss3: 0.00\n",
      "Epoch [4742], train_loss: 447.11 with loss1: 394.35, loss2: 52.77 and loss3: 0.00\n",
      "Epoch [4743], train_loss: 450.95 with loss1: 398.23, loss2: 52.72 and loss3: 0.00\n",
      "Epoch [4744], train_loss: 450.43 with loss1: 397.66, loss2: 52.77 and loss3: 0.00\n",
      "Epoch [4745], train_loss: 454.13 with loss1: 401.42, loss2: 52.71 and loss3: 0.00\n",
      "Epoch [4746], train_loss: 453.20 with loss1: 400.42, loss2: 52.78 and loss3: 0.00\n",
      "Epoch [4747], train_loss: 458.17 with loss1: 405.44, loss2: 52.73 and loss3: 0.00\n",
      "Epoch [4748], train_loss: 455.07 with loss1: 402.29, loss2: 52.78 and loss3: 0.00\n",
      "Epoch [4749], train_loss: 458.33 with loss1: 405.54, loss2: 52.78 and loss3: 0.00\n",
      "Epoch [4750], train_loss: 457.11 with loss1: 404.34, loss2: 52.78 and loss3: 0.00\n",
      "Epoch [4751], train_loss: 460.56 with loss1: 407.86, loss2: 52.70 and loss3: 0.00\n",
      "Epoch [4752], train_loss: 459.06 with loss1: 406.34, loss2: 52.71 and loss3: 0.00\n",
      "Epoch [4753], train_loss: 462.87 with loss1: 410.17, loss2: 52.70 and loss3: 0.00\n",
      "Epoch [4754], train_loss: 458.11 with loss1: 405.39, loss2: 52.71 and loss3: 0.00\n",
      "Epoch [4755], train_loss: 462.19 with loss1: 409.46, loss2: 52.73 and loss3: 0.00\n",
      "Epoch [4756], train_loss: 458.25 with loss1: 405.49, loss2: 52.77 and loss3: 0.00\n",
      "Epoch [4757], train_loss: 460.20 with loss1: 407.51, loss2: 52.69 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4758], train_loss: 458.91 with loss1: 406.18, loss2: 52.73 and loss3: 0.00\n",
      "Epoch [4759], train_loss: 460.08 with loss1: 407.40, loss2: 52.68 and loss3: 0.00\n",
      "Epoch [4760], train_loss: 460.71 with loss1: 408.02, loss2: 52.69 and loss3: 0.00\n",
      "Epoch [4761], train_loss: 458.83 with loss1: 406.15, loss2: 52.68 and loss3: 0.00\n",
      "Epoch [4762], train_loss: 455.70 with loss1: 402.98, loss2: 52.72 and loss3: 0.00\n",
      "Epoch [4763], train_loss: 457.34 with loss1: 404.62, loss2: 52.72 and loss3: 0.00\n",
      "Epoch [4764], train_loss: 452.65 with loss1: 399.98, loss2: 52.67 and loss3: 0.00\n",
      "Epoch [4765], train_loss: 453.98 with loss1: 401.27, loss2: 52.70 and loss3: 0.00\n",
      "Epoch [4766], train_loss: 449.78 with loss1: 397.12, loss2: 52.67 and loss3: 0.00\n",
      "Epoch [4767], train_loss: 449.40 with loss1: 396.72, loss2: 52.69 and loss3: 0.00\n",
      "Epoch [4768], train_loss: 446.52 with loss1: 393.80, loss2: 52.72 and loss3: 0.00\n",
      "Epoch [4769], train_loss: 447.50 with loss1: 394.86, loss2: 52.63 and loss3: 0.00\n",
      "Epoch [4770], train_loss: 443.54 with loss1: 390.85, loss2: 52.70 and loss3: 0.00\n",
      "Epoch [4771], train_loss: 443.24 with loss1: 390.58, loss2: 52.66 and loss3: 0.00\n",
      "Epoch [4772], train_loss: 439.75 with loss1: 387.08, loss2: 52.67 and loss3: 0.00\n",
      "Epoch [4773], train_loss: 441.08 with loss1: 388.42, loss2: 52.66 and loss3: 0.00\n",
      "Epoch [4774], train_loss: 437.55 with loss1: 384.91, loss2: 52.65 and loss3: 0.00\n",
      "Epoch [4775], train_loss: 437.45 with loss1: 384.82, loss2: 52.63 and loss3: 0.00\n",
      "Epoch [4776], train_loss: 435.55 with loss1: 382.94, loss2: 52.61 and loss3: 0.00\n",
      "Epoch [4777], train_loss: 435.72 with loss1: 383.11, loss2: 52.61 and loss3: 0.00\n",
      "Epoch [4778], train_loss: 433.41 with loss1: 380.76, loss2: 52.65 and loss3: 0.00\n",
      "Epoch [4779], train_loss: 432.35 with loss1: 379.74, loss2: 52.61 and loss3: 0.00\n",
      "Epoch [4780], train_loss: 431.33 with loss1: 378.71, loss2: 52.62 and loss3: 0.00\n",
      "Epoch [4781], train_loss: 431.03 with loss1: 378.50, loss2: 52.53 and loss3: 0.00\n",
      "Epoch [4782], train_loss: 428.66 with loss1: 376.06, loss2: 52.60 and loss3: 0.00\n",
      "Epoch [4783], train_loss: 429.16 with loss1: 376.63, loss2: 52.53 and loss3: 0.00\n",
      "Epoch [4784], train_loss: 426.63 with loss1: 374.01, loss2: 52.62 and loss3: 0.00\n",
      "Epoch [4785], train_loss: 426.39 with loss1: 373.81, loss2: 52.59 and loss3: 0.00\n",
      "Epoch [4786], train_loss: 427.20 with loss1: 374.62, loss2: 52.58 and loss3: 0.00\n",
      "Epoch [4787], train_loss: 426.38 with loss1: 373.82, loss2: 52.56 and loss3: 0.00\n",
      "Epoch [4788], train_loss: 424.24 with loss1: 371.69, loss2: 52.55 and loss3: 0.00\n",
      "Epoch [4789], train_loss: 424.75 with loss1: 372.23, loss2: 52.52 and loss3: 0.00\n",
      "Epoch [4790], train_loss: 422.95 with loss1: 370.41, loss2: 52.54 and loss3: 0.00\n",
      "Epoch [4791], train_loss: 422.96 with loss1: 370.43, loss2: 52.53 and loss3: 0.00\n",
      "Epoch [4792], train_loss: 422.06 with loss1: 369.52, loss2: 52.54 and loss3: 0.00\n",
      "Epoch [4793], train_loss: 422.60 with loss1: 370.10, loss2: 52.50 and loss3: 0.00\n",
      "Epoch [4794], train_loss: 421.08 with loss1: 368.59, loss2: 52.49 and loss3: 0.00\n",
      "Epoch [4795], train_loss: 422.44 with loss1: 369.95, loss2: 52.48 and loss3: 0.00\n",
      "Epoch [4796], train_loss: 422.26 with loss1: 369.75, loss2: 52.51 and loss3: 0.00\n",
      "Epoch [4797], train_loss: 422.56 with loss1: 370.07, loss2: 52.49 and loss3: 0.00\n",
      "Epoch [4798], train_loss: 421.04 with loss1: 368.56, loss2: 52.49 and loss3: 0.00\n",
      "Epoch [4799], train_loss: 422.61 with loss1: 370.16, loss2: 52.45 and loss3: 0.00\n",
      "Epoch [4800], train_loss: 420.83 with loss1: 368.42, loss2: 52.41 and loss3: 0.00\n",
      "Epoch [4801], train_loss: 422.82 with loss1: 370.34, loss2: 52.48 and loss3: 0.00\n",
      "Epoch [4802], train_loss: 422.54 with loss1: 370.08, loss2: 52.46 and loss3: 0.00\n",
      "Epoch [4803], train_loss: 422.95 with loss1: 370.47, loss2: 52.48 and loss3: 0.00\n",
      "Epoch [4804], train_loss: 420.99 with loss1: 368.58, loss2: 52.41 and loss3: 0.00\n",
      "Epoch [4805], train_loss: 421.24 with loss1: 368.74, loss2: 52.50 and loss3: 0.00\n",
      "Epoch [4806], train_loss: 421.25 with loss1: 368.85, loss2: 52.40 and loss3: 0.00\n",
      "Epoch [4807], train_loss: 421.69 with loss1: 369.26, loss2: 52.43 and loss3: 0.00\n",
      "Epoch [4808], train_loss: 421.42 with loss1: 368.96, loss2: 52.46 and loss3: 0.00\n",
      "Epoch [4809], train_loss: 420.64 with loss1: 368.18, loss2: 52.46 and loss3: 0.00\n",
      "Epoch [4810], train_loss: 419.16 with loss1: 366.69, loss2: 52.47 and loss3: 0.00\n",
      "Epoch [4811], train_loss: 420.50 with loss1: 368.08, loss2: 52.42 and loss3: 0.00\n",
      "Epoch [4812], train_loss: 421.08 with loss1: 368.69, loss2: 52.39 and loss3: 0.00\n",
      "Epoch [4813], train_loss: 420.47 with loss1: 368.08, loss2: 52.39 and loss3: 0.00\n",
      "Epoch [4814], train_loss: 419.54 with loss1: 367.15, loss2: 52.38 and loss3: 0.00\n",
      "Epoch [4815], train_loss: 420.56 with loss1: 368.12, loss2: 52.44 and loss3: 0.00\n",
      "Epoch [4816], train_loss: 420.87 with loss1: 368.48, loss2: 52.38 and loss3: 0.00\n",
      "Epoch [4817], train_loss: 420.68 with loss1: 368.33, loss2: 52.35 and loss3: 0.00\n",
      "Epoch [4818], train_loss: 421.12 with loss1: 368.76, loss2: 52.36 and loss3: 0.00\n",
      "Epoch [4819], train_loss: 422.09 with loss1: 369.66, loss2: 52.43 and loss3: 0.00\n",
      "Epoch [4820], train_loss: 420.34 with loss1: 368.00, loss2: 52.33 and loss3: 0.00\n",
      "Epoch [4821], train_loss: 421.60 with loss1: 369.25, loss2: 52.35 and loss3: 0.00\n",
      "Epoch [4822], train_loss: 420.02 with loss1: 367.68, loss2: 52.34 and loss3: 0.00\n",
      "Epoch [4823], train_loss: 421.37 with loss1: 369.04, loss2: 52.33 and loss3: 0.00\n",
      "Epoch [4824], train_loss: 421.08 with loss1: 368.77, loss2: 52.31 and loss3: 0.00\n",
      "Epoch [4825], train_loss: 420.31 with loss1: 367.99, loss2: 52.32 and loss3: 0.00\n",
      "Epoch [4826], train_loss: 420.76 with loss1: 368.45, loss2: 52.31 and loss3: 0.00\n",
      "Epoch [4827], train_loss: 422.95 with loss1: 370.65, loss2: 52.30 and loss3: 0.00\n",
      "Epoch [4828], train_loss: 420.87 with loss1: 368.62, loss2: 52.25 and loss3: 0.00\n",
      "Epoch [4829], train_loss: 422.38 with loss1: 370.08, loss2: 52.29 and loss3: 0.00\n",
      "Epoch [4830], train_loss: 421.61 with loss1: 369.34, loss2: 52.28 and loss3: 0.00\n",
      "Epoch [4831], train_loss: 421.93 with loss1: 369.64, loss2: 52.29 and loss3: 0.00\n",
      "Epoch [4832], train_loss: 421.86 with loss1: 369.58, loss2: 52.28 and loss3: 0.00\n",
      "Epoch [4833], train_loss: 423.02 with loss1: 370.73, loss2: 52.29 and loss3: 0.00\n",
      "Epoch [4834], train_loss: 421.53 with loss1: 369.30, loss2: 52.22 and loss3: 0.00\n",
      "Epoch [4835], train_loss: 423.91 with loss1: 371.61, loss2: 52.31 and loss3: 0.00\n",
      "Epoch [4836], train_loss: 421.59 with loss1: 369.41, loss2: 52.19 and loss3: 0.00\n",
      "Epoch [4837], train_loss: 422.49 with loss1: 370.25, loss2: 52.24 and loss3: 0.00\n",
      "Epoch [4838], train_loss: 423.05 with loss1: 370.85, loss2: 52.21 and loss3: 0.00\n",
      "Epoch [4839], train_loss: 423.94 with loss1: 371.69, loss2: 52.25 and loss3: 0.00\n",
      "Epoch [4840], train_loss: 423.12 with loss1: 370.87, loss2: 52.25 and loss3: 0.00\n",
      "Epoch [4841], train_loss: 424.05 with loss1: 371.79, loss2: 52.26 and loss3: 0.00\n",
      "Epoch [4842], train_loss: 424.93 with loss1: 372.76, loss2: 52.17 and loss3: 0.00\n",
      "Epoch [4843], train_loss: 424.71 with loss1: 372.50, loss2: 52.21 and loss3: 0.00\n",
      "Epoch [4844], train_loss: 425.15 with loss1: 372.96, loss2: 52.19 and loss3: 0.00\n",
      "Epoch [4845], train_loss: 424.98 with loss1: 372.77, loss2: 52.21 and loss3: 0.00\n",
      "Epoch [4846], train_loss: 423.85 with loss1: 371.68, loss2: 52.17 and loss3: 0.00\n",
      "Epoch [4847], train_loss: 426.28 with loss1: 374.14, loss2: 52.15 and loss3: 0.00\n",
      "Epoch [4848], train_loss: 425.37 with loss1: 373.21, loss2: 52.16 and loss3: 0.00\n",
      "Epoch [4849], train_loss: 425.85 with loss1: 373.68, loss2: 52.17 and loss3: 0.00\n",
      "Epoch [4850], train_loss: 424.75 with loss1: 372.57, loss2: 52.18 and loss3: 0.00\n",
      "Epoch [4851], train_loss: 426.07 with loss1: 373.94, loss2: 52.13 and loss3: 0.00\n",
      "Epoch [4852], train_loss: 424.70 with loss1: 372.58, loss2: 52.12 and loss3: 0.00\n",
      "Epoch [4853], train_loss: 427.52 with loss1: 375.35, loss2: 52.17 and loss3: 0.00\n",
      "Epoch [4854], train_loss: 425.75 with loss1: 373.64, loss2: 52.11 and loss3: 0.00\n",
      "Epoch [4855], train_loss: 427.47 with loss1: 375.31, loss2: 52.16 and loss3: 0.00\n",
      "Epoch [4856], train_loss: 427.23 with loss1: 375.13, loss2: 52.10 and loss3: 0.00\n",
      "Epoch [4857], train_loss: 427.95 with loss1: 375.82, loss2: 52.13 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4858], train_loss: 425.68 with loss1: 373.64, loss2: 52.04 and loss3: 0.00\n",
      "Epoch [4859], train_loss: 428.31 with loss1: 376.14, loss2: 52.17 and loss3: 0.00\n",
      "Epoch [4860], train_loss: 427.17 with loss1: 375.08, loss2: 52.09 and loss3: 0.00\n",
      "Epoch [4861], train_loss: 428.50 with loss1: 376.40, loss2: 52.10 and loss3: 0.00\n",
      "Epoch [4862], train_loss: 426.60 with loss1: 374.51, loss2: 52.09 and loss3: 0.00\n",
      "Epoch [4863], train_loss: 428.26 with loss1: 376.17, loss2: 52.08 and loss3: 0.00\n",
      "Epoch [4864], train_loss: 427.26 with loss1: 375.17, loss2: 52.09 and loss3: 0.00\n",
      "Epoch [4865], train_loss: 430.20 with loss1: 378.16, loss2: 52.04 and loss3: 0.00\n",
      "Epoch [4866], train_loss: 428.89 with loss1: 376.78, loss2: 52.11 and loss3: 0.00\n",
      "Epoch [4867], train_loss: 429.00 with loss1: 376.90, loss2: 52.10 and loss3: 0.00\n",
      "Epoch [4868], train_loss: 428.93 with loss1: 376.88, loss2: 52.06 and loss3: 0.00\n",
      "Epoch [4869], train_loss: 428.46 with loss1: 376.40, loss2: 52.07 and loss3: 0.00\n",
      "Epoch [4870], train_loss: 427.39 with loss1: 375.34, loss2: 52.06 and loss3: 0.00\n",
      "Epoch [4871], train_loss: 428.12 with loss1: 376.05, loss2: 52.07 and loss3: 0.00\n",
      "Epoch [4872], train_loss: 427.34 with loss1: 375.32, loss2: 52.02 and loss3: 0.00\n",
      "Epoch [4873], train_loss: 428.25 with loss1: 376.29, loss2: 51.97 and loss3: 0.00\n",
      "Epoch [4874], train_loss: 426.77 with loss1: 374.69, loss2: 52.08 and loss3: 0.00\n",
      "Epoch [4875], train_loss: 426.84 with loss1: 374.82, loss2: 52.01 and loss3: 0.00\n",
      "Epoch [4876], train_loss: 426.87 with loss1: 374.88, loss2: 51.98 and loss3: 0.00\n",
      "Epoch [4877], train_loss: 427.47 with loss1: 375.46, loss2: 52.01 and loss3: 0.00\n",
      "Epoch [4878], train_loss: 426.45 with loss1: 374.42, loss2: 52.03 and loss3: 0.00\n",
      "Epoch [4879], train_loss: 426.64 with loss1: 374.60, loss2: 52.05 and loss3: 0.00\n",
      "Epoch [4880], train_loss: 424.91 with loss1: 372.96, loss2: 51.95 and loss3: 0.00\n",
      "Epoch [4881], train_loss: 425.65 with loss1: 373.64, loss2: 52.01 and loss3: 0.00\n",
      "Epoch [4882], train_loss: 425.43 with loss1: 373.43, loss2: 51.99 and loss3: 0.00\n",
      "Epoch [4883], train_loss: 426.30 with loss1: 374.28, loss2: 52.02 and loss3: 0.00\n",
      "Epoch [4884], train_loss: 425.02 with loss1: 373.03, loss2: 51.99 and loss3: 0.00\n",
      "Epoch [4885], train_loss: 425.65 with loss1: 373.67, loss2: 51.98 and loss3: 0.00\n",
      "Epoch [4886], train_loss: 424.64 with loss1: 372.68, loss2: 51.96 and loss3: 0.00\n",
      "Epoch [4887], train_loss: 424.53 with loss1: 372.59, loss2: 51.94 and loss3: 0.00\n",
      "Epoch [4888], train_loss: 423.18 with loss1: 371.25, loss2: 51.93 and loss3: 0.00\n",
      "Epoch [4889], train_loss: 425.15 with loss1: 373.26, loss2: 51.89 and loss3: 0.00\n",
      "Epoch [4890], train_loss: 423.72 with loss1: 371.76, loss2: 51.96 and loss3: 0.00\n",
      "Epoch [4891], train_loss: 425.37 with loss1: 373.39, loss2: 51.99 and loss3: 0.00\n",
      "Epoch [4892], train_loss: 422.11 with loss1: 370.15, loss2: 51.96 and loss3: 0.00\n",
      "Epoch [4893], train_loss: 423.05 with loss1: 371.12, loss2: 51.93 and loss3: 0.00\n",
      "Epoch [4894], train_loss: 422.82 with loss1: 370.95, loss2: 51.88 and loss3: 0.00\n",
      "Epoch [4895], train_loss: 421.83 with loss1: 369.89, loss2: 51.94 and loss3: 0.00\n",
      "Epoch [4896], train_loss: 421.59 with loss1: 369.70, loss2: 51.89 and loss3: 0.00\n",
      "Epoch [4897], train_loss: 422.50 with loss1: 370.62, loss2: 51.88 and loss3: 0.00\n",
      "Epoch [4898], train_loss: 421.36 with loss1: 369.45, loss2: 51.91 and loss3: 0.00\n",
      "Epoch [4899], train_loss: 422.53 with loss1: 370.65, loss2: 51.88 and loss3: 0.00\n",
      "Epoch [4900], train_loss: 420.66 with loss1: 368.75, loss2: 51.91 and loss3: 0.00\n",
      "Epoch [4901], train_loss: 423.08 with loss1: 371.17, loss2: 51.90 and loss3: 0.00\n",
      "Epoch [4902], train_loss: 421.28 with loss1: 369.37, loss2: 51.92 and loss3: 0.00\n",
      "Epoch [4903], train_loss: 422.33 with loss1: 370.41, loss2: 51.92 and loss3: 0.00\n",
      "Epoch [4904], train_loss: 421.80 with loss1: 369.92, loss2: 51.88 and loss3: 0.00\n",
      "Epoch [4905], train_loss: 421.35 with loss1: 369.53, loss2: 51.82 and loss3: 0.00\n",
      "Epoch [4906], train_loss: 420.04 with loss1: 368.22, loss2: 51.82 and loss3: 0.00\n",
      "Epoch [4907], train_loss: 421.36 with loss1: 369.52, loss2: 51.84 and loss3: 0.00\n",
      "Epoch [4908], train_loss: 419.87 with loss1: 368.01, loss2: 51.86 and loss3: 0.00\n",
      "Epoch [4909], train_loss: 421.52 with loss1: 369.66, loss2: 51.87 and loss3: 0.00\n",
      "Epoch [4910], train_loss: 420.63 with loss1: 368.82, loss2: 51.81 and loss3: 0.00\n",
      "Epoch [4911], train_loss: 421.59 with loss1: 369.73, loss2: 51.87 and loss3: 0.00\n",
      "Epoch [4912], train_loss: 420.70 with loss1: 368.88, loss2: 51.81 and loss3: 0.00\n",
      "Epoch [4913], train_loss: 422.21 with loss1: 370.38, loss2: 51.83 and loss3: 0.00\n",
      "Epoch [4914], train_loss: 420.95 with loss1: 369.15, loss2: 51.80 and loss3: 0.00\n",
      "Epoch [4915], train_loss: 421.38 with loss1: 369.58, loss2: 51.80 and loss3: 0.00\n",
      "Epoch [4916], train_loss: 420.78 with loss1: 368.95, loss2: 51.83 and loss3: 0.00\n",
      "Epoch [4917], train_loss: 421.16 with loss1: 369.39, loss2: 51.77 and loss3: 0.00\n",
      "Epoch [4918], train_loss: 421.93 with loss1: 370.14, loss2: 51.79 and loss3: 0.00\n",
      "Epoch [4919], train_loss: 422.20 with loss1: 370.40, loss2: 51.79 and loss3: 0.00\n",
      "Epoch [4920], train_loss: 419.95 with loss1: 368.16, loss2: 51.79 and loss3: 0.00\n",
      "Epoch [4921], train_loss: 420.56 with loss1: 368.81, loss2: 51.75 and loss3: 0.00\n",
      "Epoch [4922], train_loss: 420.58 with loss1: 368.82, loss2: 51.76 and loss3: 0.00\n",
      "Epoch [4923], train_loss: 419.93 with loss1: 368.16, loss2: 51.78 and loss3: 0.00\n",
      "Epoch [4924], train_loss: 420.26 with loss1: 368.46, loss2: 51.80 and loss3: 0.00\n",
      "Epoch [4925], train_loss: 421.95 with loss1: 370.17, loss2: 51.78 and loss3: 0.00\n",
      "Epoch [4926], train_loss: 421.56 with loss1: 369.77, loss2: 51.79 and loss3: 0.00\n",
      "Epoch [4927], train_loss: 422.07 with loss1: 370.26, loss2: 51.81 and loss3: 0.00\n",
      "Epoch [4928], train_loss: 420.59 with loss1: 368.81, loss2: 51.78 and loss3: 0.00\n",
      "Epoch [4929], train_loss: 421.27 with loss1: 369.57, loss2: 51.70 and loss3: 0.00\n",
      "Epoch [4930], train_loss: 421.12 with loss1: 369.36, loss2: 51.76 and loss3: 0.00\n",
      "Epoch [4931], train_loss: 422.03 with loss1: 370.29, loss2: 51.74 and loss3: 0.00\n",
      "Epoch [4932], train_loss: 421.44 with loss1: 369.71, loss2: 51.73 and loss3: 0.00\n",
      "Epoch [4933], train_loss: 421.97 with loss1: 370.25, loss2: 51.72 and loss3: 0.00\n",
      "Epoch [4934], train_loss: 420.73 with loss1: 369.00, loss2: 51.73 and loss3: 0.00\n",
      "Epoch [4935], train_loss: 423.51 with loss1: 371.79, loss2: 51.72 and loss3: 0.00\n",
      "Epoch [4936], train_loss: 422.10 with loss1: 370.39, loss2: 51.72 and loss3: 0.00\n",
      "Epoch [4937], train_loss: 424.28 with loss1: 372.55, loss2: 51.73 and loss3: 0.00\n",
      "Epoch [4938], train_loss: 422.47 with loss1: 370.78, loss2: 51.68 and loss3: 0.00\n",
      "Epoch [4939], train_loss: 423.01 with loss1: 371.29, loss2: 51.72 and loss3: 0.00\n",
      "Epoch [4940], train_loss: 423.16 with loss1: 371.47, loss2: 51.68 and loss3: 0.00\n",
      "Epoch [4941], train_loss: 423.76 with loss1: 372.09, loss2: 51.67 and loss3: 0.00\n",
      "Epoch [4942], train_loss: 422.80 with loss1: 371.11, loss2: 51.69 and loss3: 0.00\n",
      "Epoch [4943], train_loss: 425.32 with loss1: 373.64, loss2: 51.68 and loss3: 0.00\n",
      "Epoch [4944], train_loss: 424.56 with loss1: 372.86, loss2: 51.70 and loss3: 0.00\n",
      "Epoch [4945], train_loss: 425.17 with loss1: 373.59, loss2: 51.58 and loss3: 0.00\n",
      "Epoch [4946], train_loss: 424.12 with loss1: 372.43, loss2: 51.69 and loss3: 0.00\n",
      "Epoch [4947], train_loss: 425.26 with loss1: 373.60, loss2: 51.66 and loss3: 0.00\n",
      "Epoch [4948], train_loss: 424.19 with loss1: 372.52, loss2: 51.67 and loss3: 0.00\n",
      "Epoch [4949], train_loss: 425.64 with loss1: 374.00, loss2: 51.63 and loss3: 0.00\n",
      "Epoch [4950], train_loss: 425.77 with loss1: 374.07, loss2: 51.70 and loss3: 0.00\n",
      "Epoch [4951], train_loss: 425.98 with loss1: 374.32, loss2: 51.66 and loss3: 0.00\n",
      "Epoch [4952], train_loss: 426.36 with loss1: 374.68, loss2: 51.68 and loss3: 0.00\n",
      "Epoch [4953], train_loss: 426.85 with loss1: 375.21, loss2: 51.64 and loss3: 0.00\n",
      "Epoch [4954], train_loss: 426.11 with loss1: 374.46, loss2: 51.65 and loss3: 0.00\n",
      "Epoch [4955], train_loss: 427.60 with loss1: 375.99, loss2: 51.60 and loss3: 0.00\n",
      "Epoch [4956], train_loss: 425.80 with loss1: 374.21, loss2: 51.59 and loss3: 0.00\n",
      "Epoch [4957], train_loss: 427.19 with loss1: 375.61, loss2: 51.58 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4958], train_loss: 426.95 with loss1: 375.30, loss2: 51.65 and loss3: 0.00\n",
      "Epoch [4959], train_loss: 428.66 with loss1: 377.05, loss2: 51.61 and loss3: 0.00\n",
      "Epoch [4960], train_loss: 427.62 with loss1: 375.96, loss2: 51.65 and loss3: 0.00\n",
      "Epoch [4961], train_loss: 428.28 with loss1: 376.74, loss2: 51.55 and loss3: 0.00\n",
      "Epoch [4962], train_loss: 428.41 with loss1: 376.80, loss2: 51.61 and loss3: 0.00\n",
      "Epoch [4963], train_loss: 428.35 with loss1: 376.85, loss2: 51.50 and loss3: 0.00\n",
      "Epoch [4964], train_loss: 427.64 with loss1: 376.01, loss2: 51.63 and loss3: 0.00\n",
      "Epoch [4965], train_loss: 429.93 with loss1: 378.43, loss2: 51.50 and loss3: 0.00\n",
      "Epoch [4966], train_loss: 429.09 with loss1: 377.51, loss2: 51.58 and loss3: 0.00\n",
      "Epoch [4967], train_loss: 429.70 with loss1: 378.19, loss2: 51.51 and loss3: 0.00\n",
      "Epoch [4968], train_loss: 429.10 with loss1: 377.49, loss2: 51.62 and loss3: 0.00\n",
      "Epoch [4969], train_loss: 431.19 with loss1: 379.63, loss2: 51.56 and loss3: 0.00\n",
      "Epoch [4970], train_loss: 430.03 with loss1: 378.44, loss2: 51.58 and loss3: 0.00\n",
      "Epoch [4971], train_loss: 430.22 with loss1: 378.71, loss2: 51.51 and loss3: 0.00\n",
      "Epoch [4972], train_loss: 427.88 with loss1: 376.34, loss2: 51.54 and loss3: 0.00\n",
      "Epoch [4973], train_loss: 430.12 with loss1: 378.61, loss2: 51.52 and loss3: 0.00\n",
      "Epoch [4974], train_loss: 429.20 with loss1: 377.63, loss2: 51.57 and loss3: 0.00\n",
      "Epoch [4975], train_loss: 430.09 with loss1: 378.53, loss2: 51.56 and loss3: 0.00\n",
      "Epoch [4976], train_loss: 428.54 with loss1: 377.01, loss2: 51.53 and loss3: 0.00\n",
      "Epoch [4977], train_loss: 430.45 with loss1: 378.98, loss2: 51.47 and loss3: 0.00\n",
      "Epoch [4978], train_loss: 428.90 with loss1: 377.37, loss2: 51.53 and loss3: 0.00\n",
      "Epoch [4979], train_loss: 428.84 with loss1: 377.41, loss2: 51.43 and loss3: 0.00\n",
      "Epoch [4980], train_loss: 429.01 with loss1: 377.50, loss2: 51.51 and loss3: 0.00\n",
      "Epoch [4981], train_loss: 427.98 with loss1: 376.54, loss2: 51.43 and loss3: 0.00\n",
      "Epoch [4982], train_loss: 426.44 with loss1: 374.95, loss2: 51.48 and loss3: 0.00\n",
      "Epoch [4983], train_loss: 427.35 with loss1: 375.90, loss2: 51.45 and loss3: 0.00\n",
      "Epoch [4984], train_loss: 426.64 with loss1: 375.12, loss2: 51.52 and loss3: 0.00\n",
      "Epoch [4985], train_loss: 425.52 with loss1: 374.06, loss2: 51.46 and loss3: 0.00\n",
      "Epoch [4986], train_loss: 424.86 with loss1: 373.32, loss2: 51.54 and loss3: 0.00\n",
      "Epoch [4987], train_loss: 424.69 with loss1: 373.22, loss2: 51.47 and loss3: 0.00\n",
      "Epoch [4988], train_loss: 423.46 with loss1: 372.01, loss2: 51.44 and loss3: 0.00\n",
      "Epoch [4989], train_loss: 423.94 with loss1: 372.54, loss2: 51.41 and loss3: 0.00\n",
      "Epoch [4990], train_loss: 422.43 with loss1: 371.01, loss2: 51.43 and loss3: 0.00\n",
      "Epoch [4991], train_loss: 423.56 with loss1: 372.12, loss2: 51.44 and loss3: 0.00\n",
      "Epoch [4992], train_loss: 421.20 with loss1: 369.73, loss2: 51.46 and loss3: 0.00\n",
      "Epoch [4993], train_loss: 421.69 with loss1: 370.31, loss2: 51.38 and loss3: 0.00\n",
      "Epoch [4994], train_loss: 419.02 with loss1: 367.58, loss2: 51.44 and loss3: 0.00\n",
      "Epoch [4995], train_loss: 420.19 with loss1: 368.81, loss2: 51.39 and loss3: 0.00\n",
      "Epoch [4996], train_loss: 419.12 with loss1: 367.68, loss2: 51.43 and loss3: 0.00\n",
      "Epoch [4997], train_loss: 419.54 with loss1: 368.18, loss2: 51.35 and loss3: 0.00\n",
      "Epoch [4998], train_loss: 418.47 with loss1: 367.08, loss2: 51.39 and loss3: 0.00\n",
      "Epoch [4999], train_loss: 417.68 with loss1: 366.35, loss2: 51.33 and loss3: 0.00\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1., lamb=1.\n",
    "# model version 3.5\n",
    "# with loss2 and loss3\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history = fit(epochs=5000, lr=5e-6, h0=h0, model=model, lamb=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b27ee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 329123.41 with loss1: 323455.31, loss2: 5668.11 and loss3: 4974.29\n",
      "Epoch [1], train_loss: 108716.60 with loss1: 104278.55, loss2: 4438.04 and loss3: 4964.21\n",
      "Epoch [2], train_loss: 98304.52 with loss1: 87122.17, loss2: 11182.34 and loss3: 4954.17\n",
      "Epoch [3], train_loss: 102761.45 with loss1: 94071.50, loss2: 8689.95 and loss3: 4943.99\n",
      "Epoch [4], train_loss: 89377.16 with loss1: 78407.27, loss2: 10969.88 and loss3: 4933.88\n",
      "Epoch [5], train_loss: 59220.52 with loss1: 53239.18, loss2: 5981.34 and loss3: 4923.75\n",
      "Epoch [6], train_loss: 46482.69 with loss1: 40215.39, loss2: 6267.30 and loss3: 4913.71\n",
      "Epoch [7], train_loss: 45847.09 with loss1: 40732.93, loss2: 5114.16 and loss3: 4903.68\n",
      "Epoch [8], train_loss: 36060.29 with loss1: 30861.48, loss2: 5198.81 and loss3: 4893.68\n",
      "Epoch [9], train_loss: 35958.02 with loss1: 30857.61, loss2: 5100.41 and loss3: 4883.70\n",
      "Epoch [10], train_loss: 28545.50 with loss1: 22262.40, loss2: 6283.10 and loss3: 4873.73\n",
      "Epoch [11], train_loss: 28578.09 with loss1: 20403.38, loss2: 8174.71 and loss3: 4863.74\n",
      "Epoch [12], train_loss: 29085.36 with loss1: 17270.59, loss2: 11814.76 and loss3: 4853.72\n",
      "Epoch [13], train_loss: 27428.48 with loss1: 17843.24, loss2: 9585.25 and loss3: 4843.63\n",
      "Epoch [14], train_loss: 19253.94 with loss1: 13668.05, loss2: 5585.89 and loss3: 4833.58\n",
      "Epoch [15], train_loss: 17078.55 with loss1: 11984.16, loss2: 5094.39 and loss3: 4823.64\n",
      "Epoch [16], train_loss: 15757.97 with loss1: 10894.68, loss2: 4863.29 and loss3: 4813.72\n",
      "Epoch [17], train_loss: 14583.77 with loss1: 10297.10, loss2: 4286.67 and loss3: 4803.80\n",
      "Epoch [18], train_loss: 13659.53 with loss1: 9541.66, loss2: 4117.87 and loss3: 4793.91\n",
      "Epoch [19], train_loss: 12743.13 with loss1: 9211.42, loss2: 3531.71 and loss3: 4784.03\n",
      "Epoch [20], train_loss: 12064.74 with loss1: 8706.42, loss2: 3358.32 and loss3: 4774.17\n",
      "Epoch [21], train_loss: 11684.35 with loss1: 8450.11, loss2: 3234.24 and loss3: 4764.32\n",
      "Epoch [22], train_loss: 11384.11 with loss1: 8296.86, loss2: 3087.25 and loss3: 4754.49\n",
      "Epoch [23], train_loss: 11112.39 with loss1: 8246.71, loss2: 2865.68 and loss3: 4744.66\n",
      "Epoch [24], train_loss: 11186.47 with loss1: 8320.10, loss2: 2866.38 and loss3: 4734.85\n",
      "Epoch [25], train_loss: 11221.80 with loss1: 8518.02, loss2: 2703.78 and loss3: 4725.05\n",
      "Epoch [26], train_loss: 11679.88 with loss1: 9038.95, loss2: 2640.93 and loss3: 4715.26\n",
      "Epoch [27], train_loss: 11477.06 with loss1: 8893.48, loss2: 2583.58 and loss3: 4705.49\n",
      "Epoch [28], train_loss: 11338.95 with loss1: 8789.05, loss2: 2549.91 and loss3: 4695.72\n",
      "Epoch [29], train_loss: 10932.61 with loss1: 8302.38, loss2: 2630.23 and loss3: 4685.96\n",
      "Epoch [30], train_loss: 10684.98 with loss1: 8011.57, loss2: 2673.41 and loss3: 4676.21\n",
      "Epoch [31], train_loss: 10320.27 with loss1: 7617.62, loss2: 2702.65 and loss3: 4666.48\n",
      "Epoch [32], train_loss: 10234.93 with loss1: 7380.29, loss2: 2854.63 and loss3: 4656.75\n",
      "Epoch [33], train_loss: 10029.54 with loss1: 7163.76, loss2: 2865.78 and loss3: 4647.03\n",
      "Epoch [34], train_loss: 10005.31 with loss1: 7018.96, loss2: 2986.35 and loss3: 4637.31\n",
      "Epoch [35], train_loss: 9734.39 with loss1: 6860.76, loss2: 2873.63 and loss3: 4627.61\n",
      "Epoch [36], train_loss: 9720.27 with loss1: 6753.57, loss2: 2966.70 and loss3: 4617.92\n",
      "Epoch [37], train_loss: 9531.65 with loss1: 6591.16, loss2: 2940.49 and loss3: 4608.23\n",
      "Epoch [38], train_loss: 9197.92 with loss1: 6482.36, loss2: 2715.56 and loss3: 4598.56\n",
      "Epoch [39], train_loss: 8813.22 with loss1: 6324.41, loss2: 2488.81 and loss3: 4588.90\n",
      "Epoch [40], train_loss: 8623.13 with loss1: 6244.22, loss2: 2378.91 and loss3: 4579.26\n",
      "Epoch [41], train_loss: 8277.80 with loss1: 6042.22, loss2: 2235.58 and loss3: 4569.63\n",
      "Epoch [42], train_loss: 8098.83 with loss1: 6025.84, loss2: 2072.99 and loss3: 4560.02\n",
      "Epoch [43], train_loss: 7960.00 with loss1: 5888.05, loss2: 2071.95 and loss3: 4550.41\n",
      "Epoch [44], train_loss: 7878.96 with loss1: 5869.94, loss2: 2009.01 and loss3: 4540.82\n",
      "Epoch [45], train_loss: 7680.48 with loss1: 5746.58, loss2: 1933.91 and loss3: 4531.23\n",
      "Epoch [46], train_loss: 7620.35 with loss1: 5755.29, loss2: 1865.06 and loss3: 4521.65\n",
      "Epoch [47], train_loss: 7511.68 with loss1: 5642.97, loss2: 1868.71 and loss3: 4512.08\n",
      "Epoch [48], train_loss: 7503.35 with loss1: 5660.78, loss2: 1842.57 and loss3: 4502.53\n",
      "Epoch [49], train_loss: 7393.18 with loss1: 5572.97, loss2: 1820.21 and loss3: 4492.98\n",
      "Epoch [50], train_loss: 7401.77 with loss1: 5589.51, loss2: 1812.25 and loss3: 4483.44\n",
      "Epoch [51], train_loss: 7242.71 with loss1: 5492.05, loss2: 1750.66 and loss3: 4473.91\n",
      "Epoch [52], train_loss: 7226.14 with loss1: 5486.17, loss2: 1739.97 and loss3: 4464.39\n",
      "Epoch [53], train_loss: 7211.52 with loss1: 5436.46, loss2: 1775.06 and loss3: 4454.88\n",
      "Epoch [54], train_loss: 7162.47 with loss1: 5454.56, loss2: 1707.91 and loss3: 4445.38\n",
      "Epoch [55], train_loss: 6992.91 with loss1: 5300.33, loss2: 1692.58 and loss3: 4435.89\n",
      "Epoch [56], train_loss: 6989.70 with loss1: 5329.47, loss2: 1660.23 and loss3: 4426.41\n",
      "Epoch [57], train_loss: 6898.01 with loss1: 5212.94, loss2: 1685.07 and loss3: 4416.94\n",
      "Epoch [58], train_loss: 6857.04 with loss1: 5205.56, loss2: 1651.49 and loss3: 4407.48\n",
      "Epoch [59], train_loss: 6707.12 with loss1: 5067.48, loss2: 1639.64 and loss3: 4398.04\n",
      "Epoch [60], train_loss: 6752.52 with loss1: 5135.10, loss2: 1617.43 and loss3: 4388.60\n",
      "Epoch [61], train_loss: 6544.14 with loss1: 4959.27, loss2: 1584.86 and loss3: 4379.17\n",
      "Epoch [62], train_loss: 6545.02 with loss1: 4997.74, loss2: 1547.27 and loss3: 4369.75\n",
      "Epoch [63], train_loss: 6447.62 with loss1: 4878.53, loss2: 1569.08 and loss3: 4360.34\n",
      "Epoch [64], train_loss: 6382.19 with loss1: 4881.16, loss2: 1501.03 and loss3: 4350.95\n",
      "Epoch [65], train_loss: 6338.78 with loss1: 4774.23, loss2: 1564.56 and loss3: 4341.57\n",
      "Epoch [66], train_loss: 6299.83 with loss1: 4791.33, loss2: 1508.51 and loss3: 4332.20\n",
      "Epoch [67], train_loss: 6173.41 with loss1: 4667.53, loss2: 1505.88 and loss3: 4322.84\n",
      "Epoch [68], train_loss: 6256.83 with loss1: 4749.77, loss2: 1507.06 and loss3: 4313.49\n",
      "Epoch [69], train_loss: 6102.19 with loss1: 4603.20, loss2: 1498.99 and loss3: 4304.15\n",
      "Epoch [70], train_loss: 6128.94 with loss1: 4684.50, loss2: 1444.44 and loss3: 4294.82\n",
      "Epoch [71], train_loss: 5999.05 with loss1: 4539.49, loss2: 1459.55 and loss3: 4285.50\n",
      "Epoch [72], train_loss: 5993.03 with loss1: 4577.05, loss2: 1415.97 and loss3: 4276.19\n",
      "Epoch [73], train_loss: 5850.42 with loss1: 4429.84, loss2: 1420.58 and loss3: 4266.89\n",
      "Epoch [74], train_loss: 5829.65 with loss1: 4466.44, loss2: 1363.21 and loss3: 4257.59\n",
      "Epoch [75], train_loss: 5755.98 with loss1: 4330.93, loss2: 1425.05 and loss3: 4248.31\n",
      "Epoch [76], train_loss: 5779.25 with loss1: 4396.60, loss2: 1382.64 and loss3: 4239.03\n",
      "Epoch [77], train_loss: 5681.94 with loss1: 4275.66, loss2: 1406.29 and loss3: 4229.75\n",
      "Epoch [78], train_loss: 5681.85 with loss1: 4307.03, loss2: 1374.81 and loss3: 4220.49\n",
      "Epoch [79], train_loss: 5542.82 with loss1: 4194.77, loss2: 1348.05 and loss3: 4211.24\n",
      "Epoch [80], train_loss: 5533.22 with loss1: 4206.15, loss2: 1327.07 and loss3: 4202.00\n",
      "Epoch [81], train_loss: 5473.51 with loss1: 4120.88, loss2: 1352.63 and loss3: 4192.76\n",
      "Epoch [82], train_loss: 5447.34 with loss1: 4158.62, loss2: 1288.72 and loss3: 4183.54\n",
      "Epoch [83], train_loss: 5385.72 with loss1: 4076.69, loss2: 1309.04 and loss3: 4174.32\n",
      "Epoch [84], train_loss: 5414.50 with loss1: 4134.48, loss2: 1280.02 and loss3: 4165.12\n",
      "Epoch [85], train_loss: 5350.46 with loss1: 4028.70, loss2: 1321.76 and loss3: 4155.93\n",
      "Epoch [86], train_loss: 5333.19 with loss1: 4075.42, loss2: 1257.78 and loss3: 4146.75\n",
      "Epoch [87], train_loss: 5234.25 with loss1: 3953.40, loss2: 1280.85 and loss3: 4137.57\n",
      "Epoch [88], train_loss: 5243.20 with loss1: 3995.04, loss2: 1248.16 and loss3: 4128.40\n",
      "Epoch [89], train_loss: 5149.61 with loss1: 3897.46, loss2: 1252.14 and loss3: 4119.25\n",
      "Epoch [90], train_loss: 5187.53 with loss1: 3948.73, loss2: 1238.80 and loss3: 4110.11\n",
      "Epoch [91], train_loss: 5062.72 with loss1: 3829.73, loss2: 1232.99 and loss3: 4100.98\n",
      "Epoch [92], train_loss: 5112.65 with loss1: 3899.82, loss2: 1212.83 and loss3: 4091.86\n",
      "Epoch [93], train_loss: 5017.90 with loss1: 3782.93, loss2: 1234.97 and loss3: 4082.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94], train_loss: 5055.60 with loss1: 3862.99, loss2: 1192.61 and loss3: 4073.65\n",
      "Epoch [95], train_loss: 4959.99 with loss1: 3761.03, loss2: 1198.96 and loss3: 4064.57\n",
      "Epoch [96], train_loss: 4997.83 with loss1: 3805.47, loss2: 1192.36 and loss3: 4055.49\n",
      "Epoch [97], train_loss: 4892.52 with loss1: 3689.89, loss2: 1202.64 and loss3: 4046.43\n",
      "Epoch [98], train_loss: 4925.75 with loss1: 3768.88, loss2: 1156.87 and loss3: 4037.38\n",
      "Epoch [99], train_loss: 4835.24 with loss1: 3629.34, loss2: 1205.91 and loss3: 4028.34\n",
      "Epoch [100], train_loss: 4802.86 with loss1: 3656.89, loss2: 1145.97 and loss3: 4019.31\n",
      "Epoch [101], train_loss: 4711.14 with loss1: 3547.56, loss2: 1163.57 and loss3: 4010.28\n",
      "Epoch [102], train_loss: 4710.84 with loss1: 3577.26, loss2: 1133.58 and loss3: 4001.28\n",
      "Epoch [103], train_loss: 4647.38 with loss1: 3513.17, loss2: 1134.21 and loss3: 3992.28\n",
      "Epoch [104], train_loss: 4627.93 with loss1: 3528.17, loss2: 1099.76 and loss3: 3983.30\n",
      "Epoch [105], train_loss: 4619.30 with loss1: 3471.76, loss2: 1147.54 and loss3: 3974.32\n",
      "Epoch [106], train_loss: 4595.71 with loss1: 3508.90, loss2: 1086.81 and loss3: 3965.35\n",
      "Epoch [107], train_loss: 4555.05 with loss1: 3444.59, loss2: 1110.46 and loss3: 3956.39\n",
      "Epoch [108], train_loss: 4547.09 with loss1: 3458.26, loss2: 1088.83 and loss3: 3947.44\n",
      "Epoch [109], train_loss: 4511.74 with loss1: 3397.68, loss2: 1114.06 and loss3: 3938.50\n",
      "Epoch [110], train_loss: 4517.00 with loss1: 3431.74, loss2: 1085.26 and loss3: 3929.57\n",
      "Epoch [111], train_loss: 4442.40 with loss1: 3360.10, loss2: 1082.30 and loss3: 3920.66\n",
      "Epoch [112], train_loss: 4444.58 with loss1: 3399.03, loss2: 1045.55 and loss3: 3911.75\n",
      "Epoch [113], train_loss: 4408.36 with loss1: 3324.75, loss2: 1083.60 and loss3: 3902.85\n",
      "Epoch [114], train_loss: 4415.70 with loss1: 3374.43, loss2: 1041.28 and loss3: 3893.97\n",
      "Epoch [115], train_loss: 4383.96 with loss1: 3309.59, loss2: 1074.37 and loss3: 3885.09\n",
      "Epoch [116], train_loss: 4398.25 with loss1: 3354.50, loss2: 1043.75 and loss3: 3876.22\n",
      "Epoch [117], train_loss: 4332.11 with loss1: 3280.21, loss2: 1051.90 and loss3: 3867.36\n",
      "Epoch [118], train_loss: 4348.98 with loss1: 3320.49, loss2: 1028.49 and loss3: 3858.51\n",
      "Epoch [119], train_loss: 4297.83 with loss1: 3248.81, loss2: 1049.03 and loss3: 3849.67\n",
      "Epoch [120], train_loss: 4316.30 with loss1: 3308.19, loss2: 1008.11 and loss3: 3840.84\n",
      "Epoch [121], train_loss: 4237.88 with loss1: 3236.69, loss2: 1001.18 and loss3: 3832.01\n",
      "Epoch [122], train_loss: 4285.17 with loss1: 3286.01, loss2: 999.16 and loss3: 3823.21\n",
      "Epoch [123], train_loss: 4198.85 with loss1: 3191.41, loss2: 1007.45 and loss3: 3814.41\n",
      "Epoch [124], train_loss: 4252.74 with loss1: 3252.17, loss2: 1000.57 and loss3: 3805.62\n",
      "Epoch [125], train_loss: 4164.35 with loss1: 3158.96, loss2: 1005.39 and loss3: 3796.84\n",
      "Epoch [126], train_loss: 4167.35 with loss1: 3195.93, loss2: 971.41 and loss3: 3788.07\n",
      "Epoch [127], train_loss: 4077.52 with loss1: 3106.53, loss2: 970.98 and loss3: 3779.31\n",
      "Epoch [128], train_loss: 4098.84 with loss1: 3118.51, loss2: 980.33 and loss3: 3770.56\n",
      "Epoch [129], train_loss: 4043.42 with loss1: 3083.95, loss2: 959.47 and loss3: 3761.82\n",
      "Epoch [130], train_loss: 4077.61 with loss1: 3095.38, loss2: 982.23 and loss3: 3753.10\n",
      "Epoch [131], train_loss: 4050.35 with loss1: 3123.59, loss2: 926.76 and loss3: 3744.38\n",
      "Epoch [132], train_loss: 4114.60 with loss1: 3127.09, loss2: 987.51 and loss3: 3735.67\n",
      "Epoch [133], train_loss: 4079.95 with loss1: 3142.27, loss2: 937.68 and loss3: 3726.98\n",
      "Epoch [134], train_loss: 4117.03 with loss1: 3140.13, loss2: 976.90 and loss3: 3718.29\n",
      "Epoch [135], train_loss: 4071.99 with loss1: 3154.47, loss2: 917.52 and loss3: 3709.61\n",
      "Epoch [136], train_loss: 4072.54 with loss1: 3100.92, loss2: 971.62 and loss3: 3700.94\n",
      "Epoch [137], train_loss: 3978.54 with loss1: 3070.80, loss2: 907.75 and loss3: 3692.29\n",
      "Epoch [138], train_loss: 3931.61 with loss1: 2984.93, loss2: 946.68 and loss3: 3683.65\n",
      "Epoch [139], train_loss: 3861.30 with loss1: 2956.65, loss2: 904.65 and loss3: 3675.01\n",
      "Epoch [140], train_loss: 3824.36 with loss1: 2884.47, loss2: 939.89 and loss3: 3666.39\n",
      "Epoch [141], train_loss: 3749.30 with loss1: 2860.79, loss2: 888.51 and loss3: 3657.78\n",
      "Epoch [142], train_loss: 3748.46 with loss1: 2833.19, loss2: 915.28 and loss3: 3649.18\n",
      "Epoch [143], train_loss: 3710.09 with loss1: 2820.01, loss2: 890.08 and loss3: 3640.60\n",
      "Epoch [144], train_loss: 3684.31 with loss1: 2769.65, loss2: 914.66 and loss3: 3632.02\n",
      "Epoch [145], train_loss: 3624.74 with loss1: 2760.90, loss2: 863.84 and loss3: 3623.46\n",
      "Epoch [146], train_loss: 3628.01 with loss1: 2728.12, loss2: 899.89 and loss3: 3614.90\n",
      "Epoch [147], train_loss: 3627.61 with loss1: 2766.25, loss2: 861.35 and loss3: 3606.36\n",
      "Epoch [148], train_loss: 3626.50 with loss1: 2744.74, loss2: 881.76 and loss3: 3597.82\n",
      "Epoch [149], train_loss: 3685.46 with loss1: 2832.98, loss2: 852.48 and loss3: 3589.29\n",
      "Epoch [150], train_loss: 3690.84 with loss1: 2812.10, loss2: 878.74 and loss3: 3580.77\n",
      "Epoch [151], train_loss: 3765.13 with loss1: 2933.84, loss2: 831.29 and loss3: 3572.26\n",
      "Epoch [152], train_loss: 3774.57 with loss1: 2907.44, loss2: 867.13 and loss3: 3563.76\n",
      "Epoch [153], train_loss: 3906.47 with loss1: 3065.69, loss2: 840.78 and loss3: 3555.27\n",
      "Epoch [154], train_loss: 3834.14 with loss1: 2966.72, loss2: 867.42 and loss3: 3546.79\n",
      "Epoch [155], train_loss: 3903.94 with loss1: 3076.87, loss2: 827.07 and loss3: 3538.33\n",
      "Epoch [156], train_loss: 3772.75 with loss1: 2927.11, loss2: 845.65 and loss3: 3529.87\n",
      "Epoch [157], train_loss: 3778.66 with loss1: 2956.13, loss2: 822.53 and loss3: 3521.43\n",
      "Epoch [158], train_loss: 3687.15 with loss1: 2841.83, loss2: 845.32 and loss3: 3512.99\n",
      "Epoch [159], train_loss: 3668.04 with loss1: 2848.82, loss2: 819.23 and loss3: 3504.57\n",
      "Epoch [160], train_loss: 3581.21 with loss1: 2758.24, loss2: 822.98 and loss3: 3496.16\n",
      "Epoch [161], train_loss: 3568.63 with loss1: 2764.00, loss2: 804.63 and loss3: 3487.76\n",
      "Epoch [162], train_loss: 3533.96 with loss1: 2716.89, loss2: 817.06 and loss3: 3479.37\n",
      "Epoch [163], train_loss: 3513.23 with loss1: 2715.23, loss2: 798.00 and loss3: 3470.99\n",
      "Epoch [164], train_loss: 3473.27 with loss1: 2663.93, loss2: 809.34 and loss3: 3462.62\n",
      "Epoch [165], train_loss: 3467.47 with loss1: 2680.79, loss2: 786.67 and loss3: 3454.27\n",
      "Epoch [166], train_loss: 3435.21 with loss1: 2635.35, loss2: 799.86 and loss3: 3445.92\n",
      "Epoch [167], train_loss: 3425.62 with loss1: 2645.17, loss2: 780.44 and loss3: 3437.58\n",
      "Epoch [168], train_loss: 3394.82 with loss1: 2599.16, loss2: 795.65 and loss3: 3429.26\n",
      "Epoch [169], train_loss: 3406.03 with loss1: 2628.20, loss2: 777.83 and loss3: 3420.94\n",
      "Epoch [170], train_loss: 3352.79 with loss1: 2578.57, loss2: 774.22 and loss3: 3412.64\n",
      "Epoch [171], train_loss: 3349.24 with loss1: 2578.57, loss2: 770.67 and loss3: 3404.35\n",
      "Epoch [172], train_loss: 3317.79 with loss1: 2548.50, loss2: 769.29 and loss3: 3396.07\n",
      "Epoch [173], train_loss: 3293.62 with loss1: 2546.42, loss2: 747.19 and loss3: 3387.80\n",
      "Epoch [174], train_loss: 3270.23 with loss1: 2510.97, loss2: 759.26 and loss3: 3379.54\n",
      "Epoch [175], train_loss: 3261.55 with loss1: 2515.84, loss2: 745.71 and loss3: 3371.29\n",
      "Epoch [176], train_loss: 3234.55 with loss1: 2479.47, loss2: 755.08 and loss3: 3363.06\n",
      "Epoch [177], train_loss: 3232.10 with loss1: 2501.26, loss2: 730.84 and loss3: 3354.83\n",
      "Epoch [178], train_loss: 3228.76 with loss1: 2474.95, loss2: 753.80 and loss3: 3346.61\n",
      "Epoch [179], train_loss: 3236.76 with loss1: 2498.92, loss2: 737.84 and loss3: 3338.40\n",
      "Epoch [180], train_loss: 3211.96 with loss1: 2474.41, loss2: 737.55 and loss3: 3330.21\n",
      "Epoch [181], train_loss: 3201.56 with loss1: 2476.74, loss2: 724.81 and loss3: 3322.02\n",
      "Epoch [182], train_loss: 3176.74 with loss1: 2443.87, loss2: 732.87 and loss3: 3313.84\n",
      "Epoch [183], train_loss: 3200.59 with loss1: 2473.75, loss2: 726.84 and loss3: 3305.67\n",
      "Epoch [184], train_loss: 3170.29 with loss1: 2443.21, loss2: 727.08 and loss3: 3297.51\n",
      "Epoch [185], train_loss: 3186.28 with loss1: 2474.89, loss2: 711.39 and loss3: 3289.35\n",
      "Epoch [186], train_loss: 3159.94 with loss1: 2437.18, loss2: 722.76 and loss3: 3281.20\n",
      "Epoch [187], train_loss: 3169.73 with loss1: 2474.42, loss2: 695.31 and loss3: 3273.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [188], train_loss: 3140.02 with loss1: 2420.09, loss2: 719.93 and loss3: 3264.93\n",
      "Epoch [189], train_loss: 3169.78 with loss1: 2471.39, loss2: 698.39 and loss3: 3256.81\n",
      "Epoch [190], train_loss: 3143.05 with loss1: 2430.36, loss2: 712.69 and loss3: 3248.70\n",
      "Epoch [191], train_loss: 3186.84 with loss1: 2506.64, loss2: 680.19 and loss3: 3240.60\n",
      "Epoch [192], train_loss: 3184.24 with loss1: 2470.73, loss2: 713.51 and loss3: 3232.50\n",
      "Epoch [193], train_loss: 3219.88 with loss1: 2531.75, loss2: 688.12 and loss3: 3224.42\n",
      "Epoch [194], train_loss: 3154.61 with loss1: 2447.03, loss2: 707.59 and loss3: 3216.35\n",
      "Epoch [195], train_loss: 3179.13 with loss1: 2498.67, loss2: 680.46 and loss3: 3208.28\n",
      "Epoch [196], train_loss: 3124.28 with loss1: 2422.79, loss2: 701.48 and loss3: 3200.23\n",
      "Epoch [197], train_loss: 3102.03 with loss1: 2426.90, loss2: 675.13 and loss3: 3192.18\n",
      "Epoch [198], train_loss: 3048.06 with loss1: 2361.16, loss2: 686.90 and loss3: 3184.14\n",
      "Epoch [199], train_loss: 3027.36 with loss1: 2360.87, loss2: 666.49 and loss3: 3176.11\n",
      "Epoch [200], train_loss: 2980.05 with loss1: 2298.77, loss2: 681.28 and loss3: 3168.09\n",
      "Epoch [201], train_loss: 2980.33 with loss1: 2325.00, loss2: 655.33 and loss3: 3160.08\n",
      "Epoch [202], train_loss: 2936.98 with loss1: 2269.98, loss2: 667.00 and loss3: 3152.09\n",
      "Epoch [203], train_loss: 2933.81 with loss1: 2280.42, loss2: 653.38 and loss3: 3144.10\n",
      "Epoch [204], train_loss: 2916.08 with loss1: 2254.84, loss2: 661.24 and loss3: 3136.12\n",
      "Epoch [205], train_loss: 2915.41 with loss1: 2259.24, loss2: 656.17 and loss3: 3128.16\n",
      "Epoch [206], train_loss: 2892.09 with loss1: 2235.05, loss2: 657.04 and loss3: 3120.20\n",
      "Epoch [207], train_loss: 2895.38 with loss1: 2255.94, loss2: 639.44 and loss3: 3112.26\n",
      "Epoch [208], train_loss: 2898.67 with loss1: 2244.62, loss2: 654.05 and loss3: 3104.33\n",
      "Epoch [209], train_loss: 2914.75 with loss1: 2279.82, loss2: 634.93 and loss3: 3096.41\n",
      "Epoch [210], train_loss: 2896.81 with loss1: 2250.48, loss2: 646.33 and loss3: 3088.50\n",
      "Epoch [211], train_loss: 2893.04 with loss1: 2268.21, loss2: 624.83 and loss3: 3080.61\n",
      "Epoch [212], train_loss: 2887.42 with loss1: 2245.18, loss2: 642.24 and loss3: 3072.72\n",
      "Epoch [213], train_loss: 2884.35 with loss1: 2260.79, loss2: 623.55 and loss3: 3064.84\n",
      "Epoch [214], train_loss: 2877.18 with loss1: 2242.14, loss2: 635.04 and loss3: 3056.97\n",
      "Epoch [215], train_loss: 2880.93 with loss1: 2267.22, loss2: 613.71 and loss3: 3049.11\n",
      "Epoch [216], train_loss: 2866.13 with loss1: 2241.63, loss2: 624.50 and loss3: 3041.26\n",
      "Epoch [217], train_loss: 2875.23 with loss1: 2267.95, loss2: 607.28 and loss3: 3033.42\n",
      "Epoch [218], train_loss: 2860.79 with loss1: 2238.61, loss2: 622.18 and loss3: 3025.58\n",
      "Epoch [219], train_loss: 2885.43 with loss1: 2280.62, loss2: 604.81 and loss3: 3017.76\n",
      "Epoch [220], train_loss: 2848.86 with loss1: 2240.90, loss2: 607.96 and loss3: 3009.94\n",
      "Epoch [221], train_loss: 2874.32 with loss1: 2272.61, loss2: 601.71 and loss3: 3002.14\n",
      "Epoch [222], train_loss: 2842.94 with loss1: 2234.10, loss2: 608.84 and loss3: 2994.34\n",
      "Epoch [223], train_loss: 2860.21 with loss1: 2265.67, loss2: 594.53 and loss3: 2986.56\n",
      "Epoch [224], train_loss: 2822.94 with loss1: 2215.82, loss2: 607.11 and loss3: 2978.78\n",
      "Epoch [225], train_loss: 2847.42 with loss1: 2253.37, loss2: 594.05 and loss3: 2971.02\n",
      "Epoch [226], train_loss: 2796.31 with loss1: 2197.74, loss2: 598.56 and loss3: 2963.26\n",
      "Epoch [227], train_loss: 2819.81 with loss1: 2233.11, loss2: 586.70 and loss3: 2955.51\n",
      "Epoch [228], train_loss: 2782.46 with loss1: 2192.40, loss2: 590.07 and loss3: 2947.77\n",
      "Epoch [229], train_loss: 2805.81 with loss1: 2224.83, loss2: 580.98 and loss3: 2940.04\n",
      "Epoch [230], train_loss: 2763.58 with loss1: 2179.23, loss2: 584.36 and loss3: 2932.32\n",
      "Epoch [231], train_loss: 2781.14 with loss1: 2204.74, loss2: 576.39 and loss3: 2924.61\n",
      "Epoch [232], train_loss: 2738.38 with loss1: 2156.28, loss2: 582.10 and loss3: 2916.91\n",
      "Epoch [233], train_loss: 2757.81 with loss1: 2180.75, loss2: 577.07 and loss3: 2909.22\n",
      "Epoch [234], train_loss: 2720.72 with loss1: 2147.17, loss2: 573.55 and loss3: 2901.54\n",
      "Epoch [235], train_loss: 2738.15 with loss1: 2170.33, loss2: 567.82 and loss3: 2893.86\n",
      "Epoch [236], train_loss: 2713.29 with loss1: 2143.88, loss2: 569.41 and loss3: 2886.20\n",
      "Epoch [237], train_loss: 2731.94 with loss1: 2165.14, loss2: 566.80 and loss3: 2878.56\n",
      "Epoch [238], train_loss: 2693.41 with loss1: 2128.55, loss2: 564.86 and loss3: 2870.93\n",
      "Epoch [239], train_loss: 2704.57 with loss1: 2143.90, loss2: 560.67 and loss3: 2863.31\n",
      "Epoch [240], train_loss: 2679.75 with loss1: 2116.11, loss2: 563.64 and loss3: 2855.71\n",
      "Epoch [241], train_loss: 2680.47 with loss1: 2118.08, loss2: 562.40 and loss3: 2848.10\n",
      "Epoch [242], train_loss: 2636.65 with loss1: 2087.95, loss2: 548.70 and loss3: 2840.51\n",
      "Epoch [243], train_loss: 2649.82 with loss1: 2091.92, loss2: 557.90 and loss3: 2832.93\n",
      "Epoch [244], train_loss: 2610.45 with loss1: 2064.48, loss2: 545.97 and loss3: 2825.35\n",
      "Epoch [245], train_loss: 2615.64 with loss1: 2066.35, loss2: 549.29 and loss3: 2817.78\n",
      "Epoch [246], train_loss: 2586.24 with loss1: 2040.47, loss2: 545.77 and loss3: 2810.22\n",
      "Epoch [247], train_loss: 2581.79 with loss1: 2035.78, loss2: 546.01 and loss3: 2802.67\n",
      "Epoch [248], train_loss: 2552.98 with loss1: 2018.13, loss2: 534.86 and loss3: 2795.13\n",
      "Epoch [249], train_loss: 2556.26 with loss1: 2015.50, loss2: 540.75 and loss3: 2787.59\n",
      "Epoch [250], train_loss: 2530.84 with loss1: 1997.97, loss2: 532.87 and loss3: 2780.07\n",
      "Epoch [251], train_loss: 2529.46 with loss1: 1997.07, loss2: 532.39 and loss3: 2772.55\n",
      "Epoch [252], train_loss: 2515.44 with loss1: 1988.52, loss2: 526.92 and loss3: 2765.04\n",
      "Epoch [253], train_loss: 2527.91 with loss1: 1996.77, loss2: 531.14 and loss3: 2757.55\n",
      "Epoch [254], train_loss: 2519.79 with loss1: 1992.95, loss2: 526.84 and loss3: 2750.07\n",
      "Epoch [255], train_loss: 2515.09 with loss1: 1987.86, loss2: 527.23 and loss3: 2742.60\n",
      "Epoch [256], train_loss: 2490.99 with loss1: 1973.53, loss2: 517.46 and loss3: 2735.14\n",
      "Epoch [257], train_loss: 2501.29 with loss1: 1978.91, loss2: 522.37 and loss3: 2727.70\n",
      "Epoch [258], train_loss: 2484.44 with loss1: 1968.04, loss2: 516.40 and loss3: 2720.27\n",
      "Epoch [259], train_loss: 2491.66 with loss1: 1976.13, loss2: 515.53 and loss3: 2712.85\n",
      "Epoch [260], train_loss: 2482.61 with loss1: 1973.39, loss2: 509.22 and loss3: 2705.44\n",
      "Epoch [261], train_loss: 2499.53 with loss1: 1987.45, loss2: 512.08 and loss3: 2698.04\n",
      "Epoch [262], train_loss: 2489.46 with loss1: 1982.74, loss2: 506.72 and loss3: 2690.65\n",
      "Epoch [263], train_loss: 2508.13 with loss1: 1999.61, loss2: 508.52 and loss3: 2683.27\n",
      "Epoch [264], train_loss: 2500.50 with loss1: 2002.03, loss2: 498.46 and loss3: 2675.90\n",
      "Epoch [265], train_loss: 2525.10 with loss1: 2021.85, loss2: 503.26 and loss3: 2668.54\n",
      "Epoch [266], train_loss: 2527.12 with loss1: 2022.67, loss2: 504.45 and loss3: 2661.19\n",
      "Epoch [267], train_loss: 2546.78 with loss1: 2047.69, loss2: 499.09 and loss3: 2653.86\n",
      "Epoch [268], train_loss: 2530.60 with loss1: 2032.66, loss2: 497.94 and loss3: 2646.53\n",
      "Epoch [269], train_loss: 2530.63 with loss1: 2034.79, loss2: 495.84 and loss3: 2639.22\n",
      "Epoch [270], train_loss: 2501.85 with loss1: 2007.87, loss2: 493.98 and loss3: 2631.91\n",
      "Epoch [271], train_loss: 2518.58 with loss1: 2023.07, loss2: 495.51 and loss3: 2624.62\n",
      "Epoch [272], train_loss: 2468.92 with loss1: 1976.49, loss2: 492.43 and loss3: 2617.33\n",
      "Epoch [273], train_loss: 2467.62 with loss1: 1979.14, loss2: 488.48 and loss3: 2610.06\n",
      "Epoch [274], train_loss: 2436.91 with loss1: 1947.18, loss2: 489.73 and loss3: 2602.80\n",
      "Epoch [275], train_loss: 2443.50 with loss1: 1959.29, loss2: 484.22 and loss3: 2595.55\n",
      "Epoch [276], train_loss: 2413.06 with loss1: 1929.01, loss2: 484.04 and loss3: 2588.31\n",
      "Epoch [277], train_loss: 2419.29 with loss1: 1941.01, loss2: 478.28 and loss3: 2581.08\n",
      "Epoch [278], train_loss: 2396.14 with loss1: 1915.35, loss2: 480.79 and loss3: 2573.86\n",
      "Epoch [279], train_loss: 2400.44 with loss1: 1924.07, loss2: 476.37 and loss3: 2566.65\n",
      "Epoch [280], train_loss: 2385.56 with loss1: 1909.85, loss2: 475.71 and loss3: 2559.46\n",
      "Epoch [281], train_loss: 2405.18 with loss1: 1936.97, loss2: 468.21 and loss3: 2552.27\n",
      "Epoch [282], train_loss: 2383.84 with loss1: 1908.39, loss2: 475.45 and loss3: 2545.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [283], train_loss: 2389.56 with loss1: 1922.43, loss2: 467.13 and loss3: 2537.94\n",
      "Epoch [284], train_loss: 2377.88 with loss1: 1907.74, loss2: 470.14 and loss3: 2530.79\n",
      "Epoch [285], train_loss: 2401.51 with loss1: 1941.21, loss2: 460.30 and loss3: 2523.65\n",
      "Epoch [286], train_loss: 2400.87 with loss1: 1932.23, loss2: 468.64 and loss3: 2516.52\n",
      "Epoch [287], train_loss: 2426.70 with loss1: 1966.11, loss2: 460.59 and loss3: 2509.40\n",
      "Epoch [288], train_loss: 2423.66 with loss1: 1955.74, loss2: 467.91 and loss3: 2502.30\n",
      "Epoch [289], train_loss: 2464.84 with loss1: 2013.54, loss2: 451.30 and loss3: 2495.21\n",
      "Epoch [290], train_loss: 2451.81 with loss1: 1990.67, loss2: 461.13 and loss3: 2488.13\n",
      "Epoch [291], train_loss: 2499.06 with loss1: 2045.46, loss2: 453.60 and loss3: 2481.06\n",
      "Epoch [292], train_loss: 2455.35 with loss1: 2000.24, loss2: 455.11 and loss3: 2474.00\n",
      "Epoch [293], train_loss: 2487.05 with loss1: 2037.09, loss2: 449.96 and loss3: 2466.95\n",
      "Epoch [294], train_loss: 2406.19 with loss1: 1954.35, loss2: 451.84 and loss3: 2459.91\n",
      "Epoch [295], train_loss: 2418.80 with loss1: 1976.02, loss2: 442.79 and loss3: 2452.88\n",
      "Epoch [296], train_loss: 2350.54 with loss1: 1902.81, loss2: 447.73 and loss3: 2445.86\n",
      "Epoch [297], train_loss: 2344.61 with loss1: 1902.53, loss2: 442.07 and loss3: 2438.85\n",
      "Epoch [298], train_loss: 2294.39 with loss1: 1848.14, loss2: 446.24 and loss3: 2431.86\n",
      "Epoch [299], train_loss: 2287.57 with loss1: 1853.02, loss2: 434.56 and loss3: 2424.88\n",
      "Epoch [300], train_loss: 2252.34 with loss1: 1811.23, loss2: 441.11 and loss3: 2417.91\n",
      "Epoch [301], train_loss: 2242.98 with loss1: 1811.25, loss2: 431.73 and loss3: 2410.95\n",
      "Epoch [302], train_loss: 2217.65 with loss1: 1781.63, loss2: 436.02 and loss3: 2404.00\n",
      "Epoch [303], train_loss: 2209.63 with loss1: 1782.64, loss2: 426.99 and loss3: 2397.06\n",
      "Epoch [304], train_loss: 2206.36 with loss1: 1771.88, loss2: 434.49 and loss3: 2390.12\n",
      "Epoch [305], train_loss: 2198.79 with loss1: 1775.99, loss2: 422.81 and loss3: 2383.20\n",
      "Epoch [306], train_loss: 2185.54 with loss1: 1755.30, loss2: 430.24 and loss3: 2376.29\n",
      "Epoch [307], train_loss: 2185.06 with loss1: 1763.85, loss2: 421.21 and loss3: 2369.39\n",
      "Epoch [308], train_loss: 2178.97 with loss1: 1751.41, loss2: 427.56 and loss3: 2362.49\n",
      "Epoch [309], train_loss: 2175.55 with loss1: 1752.72, loss2: 422.84 and loss3: 2355.60\n",
      "Epoch [310], train_loss: 2160.20 with loss1: 1736.69, loss2: 423.51 and loss3: 2348.73\n",
      "Epoch [311], train_loss: 2162.19 with loss1: 1745.24, loss2: 416.95 and loss3: 2341.86\n",
      "Epoch [312], train_loss: 2156.51 with loss1: 1737.77, loss2: 418.74 and loss3: 2335.01\n",
      "Epoch [313], train_loss: 2163.03 with loss1: 1749.34, loss2: 413.69 and loss3: 2328.17\n",
      "Epoch [314], train_loss: 2147.99 with loss1: 1731.77, loss2: 416.22 and loss3: 2321.34\n",
      "Epoch [315], train_loss: 2152.48 with loss1: 1741.57, loss2: 410.90 and loss3: 2314.52\n",
      "Epoch [316], train_loss: 2147.18 with loss1: 1731.88, loss2: 415.29 and loss3: 2307.71\n",
      "Epoch [317], train_loss: 2151.11 with loss1: 1743.68, loss2: 407.43 and loss3: 2300.90\n",
      "Epoch [318], train_loss: 2145.07 with loss1: 1735.73, loss2: 409.33 and loss3: 2294.11\n",
      "Epoch [319], train_loss: 2144.95 with loss1: 1740.15, loss2: 404.80 and loss3: 2287.32\n",
      "Epoch [320], train_loss: 2143.85 with loss1: 1738.67, loss2: 405.17 and loss3: 2280.56\n",
      "Epoch [321], train_loss: 2142.00 with loss1: 1741.67, loss2: 400.33 and loss3: 2273.80\n",
      "Epoch [322], train_loss: 2146.68 with loss1: 1742.57, loss2: 404.11 and loss3: 2267.05\n",
      "Epoch [323], train_loss: 2164.84 with loss1: 1766.66, loss2: 398.18 and loss3: 2260.31\n",
      "Epoch [324], train_loss: 2151.81 with loss1: 1748.91, loss2: 402.90 and loss3: 2253.58\n",
      "Epoch [325], train_loss: 2165.74 with loss1: 1769.08, loss2: 396.66 and loss3: 2246.86\n",
      "Epoch [326], train_loss: 2151.77 with loss1: 1751.70, loss2: 400.07 and loss3: 2240.16\n",
      "Epoch [327], train_loss: 2166.94 with loss1: 1773.66, loss2: 393.28 and loss3: 2233.47\n",
      "Epoch [328], train_loss: 2162.31 with loss1: 1765.55, loss2: 396.76 and loss3: 2226.79\n",
      "Epoch [329], train_loss: 2174.41 with loss1: 1784.80, loss2: 389.61 and loss3: 2220.11\n",
      "Epoch [330], train_loss: 2171.71 with loss1: 1775.55, loss2: 396.16 and loss3: 2213.46\n",
      "Epoch [331], train_loss: 2177.67 with loss1: 1790.63, loss2: 387.05 and loss3: 2206.81\n",
      "Epoch [332], train_loss: 2150.86 with loss1: 1760.29, loss2: 390.57 and loss3: 2200.17\n",
      "Epoch [333], train_loss: 2166.79 with loss1: 1781.48, loss2: 385.31 and loss3: 2193.53\n",
      "Epoch [334], train_loss: 2138.57 with loss1: 1750.27, loss2: 388.30 and loss3: 2186.91\n",
      "Epoch [335], train_loss: 2153.71 with loss1: 1767.30, loss2: 386.41 and loss3: 2180.30\n",
      "Epoch [336], train_loss: 2121.48 with loss1: 1734.48, loss2: 387.00 and loss3: 2173.70\n",
      "Epoch [337], train_loss: 2120.97 with loss1: 1739.29, loss2: 381.68 and loss3: 2167.11\n",
      "Epoch [338], train_loss: 2092.09 with loss1: 1707.50, loss2: 384.58 and loss3: 2160.53\n",
      "Epoch [339], train_loss: 2093.13 with loss1: 1717.18, loss2: 375.95 and loss3: 2153.96\n",
      "Epoch [340], train_loss: 2063.01 with loss1: 1682.47, loss2: 380.54 and loss3: 2147.40\n",
      "Epoch [341], train_loss: 2057.52 with loss1: 1684.78, loss2: 372.74 and loss3: 2140.84\n",
      "Epoch [342], train_loss: 2042.13 with loss1: 1662.51, loss2: 379.62 and loss3: 2134.30\n",
      "Epoch [343], train_loss: 2039.85 with loss1: 1664.99, loss2: 374.86 and loss3: 2127.76\n",
      "Epoch [344], train_loss: 2027.68 with loss1: 1651.00, loss2: 376.67 and loss3: 2121.23\n",
      "Epoch [345], train_loss: 2019.70 with loss1: 1650.02, loss2: 369.68 and loss3: 2114.71\n",
      "Epoch [346], train_loss: 2008.53 with loss1: 1635.53, loss2: 373.00 and loss3: 2108.20\n",
      "Epoch [347], train_loss: 2008.69 with loss1: 1640.26, loss2: 368.43 and loss3: 2101.70\n",
      "Epoch [348], train_loss: 1988.94 with loss1: 1622.01, loss2: 366.93 and loss3: 2095.22\n",
      "Epoch [349], train_loss: 1997.85 with loss1: 1633.30, loss2: 364.55 and loss3: 2088.75\n",
      "Epoch [350], train_loss: 1990.49 with loss1: 1623.61, loss2: 366.88 and loss3: 2082.28\n",
      "Epoch [351], train_loss: 1994.72 with loss1: 1631.54, loss2: 363.18 and loss3: 2075.82\n",
      "Epoch [352], train_loss: 1980.84 with loss1: 1616.68, loss2: 364.16 and loss3: 2069.37\n",
      "Epoch [353], train_loss: 1995.53 with loss1: 1635.75, loss2: 359.77 and loss3: 2062.94\n",
      "Epoch [354], train_loss: 1991.79 with loss1: 1629.29, loss2: 362.50 and loss3: 2056.52\n",
      "Epoch [355], train_loss: 2001.69 with loss1: 1645.94, loss2: 355.75 and loss3: 2050.11\n",
      "Epoch [356], train_loss: 1999.28 with loss1: 1638.81, loss2: 360.47 and loss3: 2043.70\n",
      "Epoch [357], train_loss: 2014.61 with loss1: 1655.90, loss2: 358.71 and loss3: 2037.31\n",
      "Epoch [358], train_loss: 2008.92 with loss1: 1653.41, loss2: 355.50 and loss3: 2030.93\n",
      "Epoch [359], train_loss: 2020.74 with loss1: 1664.51, loss2: 356.23 and loss3: 2024.56\n",
      "Epoch [360], train_loss: 1991.58 with loss1: 1639.08, loss2: 352.51 and loss3: 2018.20\n",
      "Epoch [361], train_loss: 1999.10 with loss1: 1644.93, loss2: 354.17 and loss3: 2011.85\n",
      "Epoch [362], train_loss: 1971.04 with loss1: 1619.48, loss2: 351.56 and loss3: 2005.51\n",
      "Epoch [363], train_loss: 1958.45 with loss1: 1608.91, loss2: 349.54 and loss3: 1999.18\n",
      "Epoch [364], train_loss: 1927.31 with loss1: 1578.46, loss2: 348.85 and loss3: 1992.86\n",
      "Epoch [365], train_loss: 1925.38 with loss1: 1577.36, loss2: 348.01 and loss3: 1986.54\n",
      "Epoch [366], train_loss: 1909.32 with loss1: 1562.96, loss2: 346.35 and loss3: 1980.24\n",
      "Epoch [367], train_loss: 1911.11 with loss1: 1566.73, loss2: 344.38 and loss3: 1973.94\n",
      "Epoch [368], train_loss: 1898.60 with loss1: 1553.05, loss2: 345.55 and loss3: 1967.66\n",
      "Epoch [369], train_loss: 1898.00 with loss1: 1555.94, loss2: 342.06 and loss3: 1961.38\n",
      "Epoch [370], train_loss: 1888.76 with loss1: 1546.32, loss2: 342.44 and loss3: 1955.11\n",
      "Epoch [371], train_loss: 1892.70 with loss1: 1553.39, loss2: 339.31 and loss3: 1948.85\n",
      "Epoch [372], train_loss: 1895.02 with loss1: 1557.29, loss2: 337.74 and loss3: 1942.61\n",
      "Epoch [373], train_loss: 1909.30 with loss1: 1571.76, loss2: 337.54 and loss3: 1936.37\n",
      "Epoch [374], train_loss: 1911.13 with loss1: 1573.45, loss2: 337.68 and loss3: 1930.15\n",
      "Epoch [375], train_loss: 1921.61 with loss1: 1584.28, loss2: 337.33 and loss3: 1923.93\n",
      "Epoch [376], train_loss: 1920.61 with loss1: 1586.93, loss2: 333.68 and loss3: 1917.73\n",
      "Epoch [377], train_loss: 1944.91 with loss1: 1610.96, loss2: 333.95 and loss3: 1911.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [378], train_loss: 1945.51 with loss1: 1612.37, loss2: 333.14 and loss3: 1905.33\n",
      "Epoch [379], train_loss: 1971.79 with loss1: 1638.40, loss2: 333.39 and loss3: 1899.15\n",
      "Epoch [380], train_loss: 1976.44 with loss1: 1646.58, loss2: 329.86 and loss3: 1892.97\n",
      "Epoch [381], train_loss: 2010.77 with loss1: 1678.76, loss2: 332.01 and loss3: 1886.79\n",
      "Epoch [382], train_loss: 2000.52 with loss1: 1670.47, loss2: 330.05 and loss3: 1880.63\n",
      "Epoch [383], train_loss: 2024.00 with loss1: 1693.56, loss2: 330.44 and loss3: 1874.49\n",
      "Epoch [384], train_loss: 2019.30 with loss1: 1692.86, loss2: 326.44 and loss3: 1868.35\n",
      "Epoch [385], train_loss: 2042.70 with loss1: 1712.53, loss2: 330.17 and loss3: 1862.23\n",
      "Epoch [386], train_loss: 2027.71 with loss1: 1700.49, loss2: 327.22 and loss3: 1856.12\n",
      "Epoch [387], train_loss: 2051.88 with loss1: 1725.84, loss2: 326.05 and loss3: 1850.01\n",
      "Epoch [388], train_loss: 2033.22 with loss1: 1708.46, loss2: 324.77 and loss3: 1843.91\n",
      "Epoch [389], train_loss: 2038.81 with loss1: 1713.03, loss2: 325.78 and loss3: 1837.83\n",
      "Epoch [390], train_loss: 2003.64 with loss1: 1683.44, loss2: 320.20 and loss3: 1831.75\n",
      "Epoch [391], train_loss: 2006.89 with loss1: 1682.95, loss2: 323.94 and loss3: 1825.70\n",
      "Epoch [392], train_loss: 1973.67 with loss1: 1654.82, loss2: 318.84 and loss3: 1819.65\n",
      "Epoch [393], train_loss: 1975.18 with loss1: 1654.73, loss2: 320.45 and loss3: 1813.61\n",
      "Epoch [394], train_loss: 1938.14 with loss1: 1620.30, loss2: 317.85 and loss3: 1807.60\n",
      "Epoch [395], train_loss: 1936.16 with loss1: 1617.08, loss2: 319.08 and loss3: 1801.59\n",
      "Epoch [396], train_loss: 1901.04 with loss1: 1585.65, loss2: 315.40 and loss3: 1795.60\n",
      "Epoch [397], train_loss: 1903.17 with loss1: 1587.15, loss2: 316.02 and loss3: 1789.61\n",
      "Epoch [398], train_loss: 1878.80 with loss1: 1565.60, loss2: 313.20 and loss3: 1783.64\n",
      "Epoch [399], train_loss: 1876.70 with loss1: 1561.65, loss2: 315.05 and loss3: 1777.68\n",
      "Epoch [400], train_loss: 1847.89 with loss1: 1537.23, loss2: 310.65 and loss3: 1771.73\n",
      "Epoch [401], train_loss: 1851.59 with loss1: 1537.20, loss2: 314.39 and loss3: 1765.79\n",
      "Epoch [402], train_loss: 1830.12 with loss1: 1521.49, loss2: 308.64 and loss3: 1759.85\n",
      "Epoch [403], train_loss: 1832.43 with loss1: 1521.69, loss2: 310.74 and loss3: 1753.93\n",
      "Epoch [404], train_loss: 1808.59 with loss1: 1501.47, loss2: 307.12 and loss3: 1748.01\n",
      "Epoch [405], train_loss: 1808.55 with loss1: 1497.76, loss2: 310.79 and loss3: 1742.09\n",
      "Epoch [406], train_loss: 1794.34 with loss1: 1486.82, loss2: 307.53 and loss3: 1736.19\n",
      "Epoch [407], train_loss: 1793.54 with loss1: 1483.72, loss2: 309.82 and loss3: 1730.30\n",
      "Epoch [408], train_loss: 1782.10 with loss1: 1475.12, loss2: 306.97 and loss3: 1724.42\n",
      "Epoch [409], train_loss: 1782.38 with loss1: 1477.58, loss2: 304.80 and loss3: 1718.55\n",
      "Epoch [410], train_loss: 1766.58 with loss1: 1464.83, loss2: 301.74 and loss3: 1712.69\n",
      "Epoch [411], train_loss: 1770.35 with loss1: 1468.08, loss2: 302.27 and loss3: 1706.85\n",
      "Epoch [412], train_loss: 1758.74 with loss1: 1457.92, loss2: 300.81 and loss3: 1701.01\n",
      "Epoch [413], train_loss: 1768.52 with loss1: 1466.60, loss2: 301.91 and loss3: 1695.19\n",
      "Epoch [414], train_loss: 1763.68 with loss1: 1464.49, loss2: 299.20 and loss3: 1689.39\n",
      "Epoch [415], train_loss: 1770.15 with loss1: 1471.23, loss2: 298.92 and loss3: 1683.59\n",
      "Epoch [416], train_loss: 1761.85 with loss1: 1465.21, loss2: 296.64 and loss3: 1677.80\n",
      "Epoch [417], train_loss: 1769.82 with loss1: 1473.40, loss2: 296.42 and loss3: 1672.02\n",
      "Epoch [418], train_loss: 1769.41 with loss1: 1471.49, loss2: 297.92 and loss3: 1666.26\n",
      "Epoch [419], train_loss: 1781.15 with loss1: 1486.71, loss2: 294.44 and loss3: 1660.51\n",
      "Epoch [420], train_loss: 1773.35 with loss1: 1479.16, loss2: 294.19 and loss3: 1654.78\n",
      "Epoch [421], train_loss: 1789.42 with loss1: 1496.84, loss2: 292.58 and loss3: 1649.05\n",
      "Epoch [422], train_loss: 1787.79 with loss1: 1494.70, loss2: 293.10 and loss3: 1643.34\n",
      "Epoch [423], train_loss: 1813.17 with loss1: 1520.39, loss2: 292.78 and loss3: 1637.64\n",
      "Epoch [424], train_loss: 1806.16 with loss1: 1515.11, loss2: 291.05 and loss3: 1631.94\n",
      "Epoch [425], train_loss: 1830.60 with loss1: 1540.01, loss2: 290.59 and loss3: 1626.26\n",
      "Epoch [426], train_loss: 1814.72 with loss1: 1522.58, loss2: 292.14 and loss3: 1620.58\n",
      "Epoch [427], train_loss: 1844.74 with loss1: 1556.05, loss2: 288.69 and loss3: 1614.91\n",
      "Epoch [428], train_loss: 1822.94 with loss1: 1533.50, loss2: 289.45 and loss3: 1609.25\n",
      "Epoch [429], train_loss: 1846.38 with loss1: 1560.15, loss2: 286.23 and loss3: 1603.60\n",
      "Epoch [430], train_loss: 1826.48 with loss1: 1538.65, loss2: 287.83 and loss3: 1597.96\n",
      "Epoch [431], train_loss: 1839.17 with loss1: 1554.71, loss2: 284.46 and loss3: 1592.33\n",
      "Epoch [432], train_loss: 1811.68 with loss1: 1524.85, loss2: 286.83 and loss3: 1586.71\n",
      "Epoch [433], train_loss: 1812.94 with loss1: 1527.60, loss2: 285.33 and loss3: 1581.11\n",
      "Epoch [434], train_loss: 1779.31 with loss1: 1495.59, loss2: 283.72 and loss3: 1575.51\n",
      "Epoch [435], train_loss: 1780.09 with loss1: 1497.95, loss2: 282.14 and loss3: 1569.93\n",
      "Epoch [436], train_loss: 1745.72 with loss1: 1463.13, loss2: 282.60 and loss3: 1564.36\n",
      "Epoch [437], train_loss: 1752.64 with loss1: 1470.40, loss2: 282.24 and loss3: 1558.79\n",
      "Epoch [438], train_loss: 1727.62 with loss1: 1446.14, loss2: 281.48 and loss3: 1553.24\n",
      "Epoch [439], train_loss: 1725.11 with loss1: 1446.84, loss2: 278.28 and loss3: 1547.70\n",
      "Epoch [440], train_loss: 1708.72 with loss1: 1429.17, loss2: 279.55 and loss3: 1542.16\n",
      "Epoch [441], train_loss: 1706.44 with loss1: 1431.17, loss2: 275.27 and loss3: 1536.64\n",
      "Epoch [442], train_loss: 1691.57 with loss1: 1414.16, loss2: 277.41 and loss3: 1531.11\n",
      "Epoch [443], train_loss: 1699.27 with loss1: 1425.29, loss2: 273.98 and loss3: 1525.60\n",
      "Epoch [444], train_loss: 1683.07 with loss1: 1406.66, loss2: 276.41 and loss3: 1520.09\n",
      "Epoch [445], train_loss: 1681.27 with loss1: 1407.43, loss2: 273.84 and loss3: 1514.60\n",
      "Epoch [446], train_loss: 1672.40 with loss1: 1397.90, loss2: 274.50 and loss3: 1509.11\n",
      "Epoch [447], train_loss: 1675.95 with loss1: 1405.27, loss2: 270.69 and loss3: 1503.64\n",
      "Epoch [448], train_loss: 1664.99 with loss1: 1392.35, loss2: 272.63 and loss3: 1498.18\n",
      "Epoch [449], train_loss: 1667.42 with loss1: 1396.29, loss2: 271.13 and loss3: 1492.73\n",
      "Epoch [450], train_loss: 1657.43 with loss1: 1386.25, loss2: 271.18 and loss3: 1487.28\n",
      "Epoch [451], train_loss: 1669.24 with loss1: 1400.86, loss2: 268.38 and loss3: 1481.85\n",
      "Epoch [452], train_loss: 1657.07 with loss1: 1386.72, loss2: 270.35 and loss3: 1476.42\n",
      "Epoch [453], train_loss: 1662.38 with loss1: 1395.55, loss2: 266.83 and loss3: 1471.01\n",
      "Epoch [454], train_loss: 1654.72 with loss1: 1387.24, loss2: 267.48 and loss3: 1465.60\n",
      "Epoch [455], train_loss: 1650.44 with loss1: 1386.06, loss2: 264.38 and loss3: 1460.20\n",
      "Epoch [456], train_loss: 1644.78 with loss1: 1379.55, loss2: 265.23 and loss3: 1454.81\n",
      "Epoch [457], train_loss: 1649.37 with loss1: 1383.36, loss2: 266.01 and loss3: 1449.44\n",
      "Epoch [458], train_loss: 1647.66 with loss1: 1382.20, loss2: 265.46 and loss3: 1444.08\n",
      "Epoch [459], train_loss: 1649.47 with loss1: 1387.71, loss2: 261.76 and loss3: 1438.73\n",
      "Epoch [460], train_loss: 1649.00 with loss1: 1385.71, loss2: 263.29 and loss3: 1433.39\n",
      "Epoch [461], train_loss: 1655.56 with loss1: 1395.53, loss2: 260.04 and loss3: 1428.06\n",
      "Epoch [462], train_loss: 1642.09 with loss1: 1380.78, loss2: 261.31 and loss3: 1422.75\n",
      "Epoch [463], train_loss: 1652.93 with loss1: 1392.54, loss2: 260.40 and loss3: 1417.45\n",
      "Epoch [464], train_loss: 1642.80 with loss1: 1381.40, loss2: 261.40 and loss3: 1412.16\n",
      "Epoch [465], train_loss: 1646.44 with loss1: 1387.71, loss2: 258.73 and loss3: 1406.88\n",
      "Epoch [466], train_loss: 1639.29 with loss1: 1378.93, loss2: 260.36 and loss3: 1401.61\n",
      "Epoch [467], train_loss: 1643.99 with loss1: 1386.46, loss2: 257.53 and loss3: 1396.35\n",
      "Epoch [468], train_loss: 1637.27 with loss1: 1379.12, loss2: 258.15 and loss3: 1391.10\n",
      "Epoch [469], train_loss: 1637.64 with loss1: 1382.32, loss2: 255.32 and loss3: 1385.86\n",
      "Epoch [470], train_loss: 1636.08 with loss1: 1380.70, loss2: 255.38 and loss3: 1380.63\n",
      "Epoch [471], train_loss: 1644.84 with loss1: 1389.84, loss2: 254.99 and loss3: 1375.41\n",
      "Epoch [472], train_loss: 1632.78 with loss1: 1378.44, loss2: 254.34 and loss3: 1370.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [473], train_loss: 1635.49 with loss1: 1381.91, loss2: 253.58 and loss3: 1365.00\n",
      "Epoch [474], train_loss: 1629.05 with loss1: 1375.40, loss2: 253.65 and loss3: 1359.82\n",
      "Epoch [475], train_loss: 1636.35 with loss1: 1383.07, loss2: 253.28 and loss3: 1354.64\n",
      "Epoch [476], train_loss: 1633.58 with loss1: 1379.87, loss2: 253.71 and loss3: 1349.49\n",
      "Epoch [477], train_loss: 1641.52 with loss1: 1389.58, loss2: 251.94 and loss3: 1344.34\n",
      "Epoch [478], train_loss: 1625.86 with loss1: 1372.94, loss2: 252.92 and loss3: 1339.19\n",
      "Epoch [479], train_loss: 1635.81 with loss1: 1385.63, loss2: 250.18 and loss3: 1334.06\n",
      "Epoch [480], train_loss: 1632.18 with loss1: 1380.98, loss2: 251.20 and loss3: 1328.94\n",
      "Epoch [481], train_loss: 1645.56 with loss1: 1395.70, loss2: 249.86 and loss3: 1323.84\n",
      "Epoch [482], train_loss: 1629.21 with loss1: 1380.87, loss2: 248.34 and loss3: 1318.74\n",
      "Epoch [483], train_loss: 1648.26 with loss1: 1398.84, loss2: 249.42 and loss3: 1313.65\n",
      "Epoch [484], train_loss: 1635.37 with loss1: 1387.55, loss2: 247.82 and loss3: 1308.58\n",
      "Epoch [485], train_loss: 1656.20 with loss1: 1409.19, loss2: 247.01 and loss3: 1303.51\n",
      "Epoch [486], train_loss: 1635.80 with loss1: 1389.90, loss2: 245.89 and loss3: 1298.45\n",
      "Epoch [487], train_loss: 1648.69 with loss1: 1403.03, loss2: 245.67 and loss3: 1293.40\n",
      "Epoch [488], train_loss: 1636.34 with loss1: 1390.15, loss2: 246.19 and loss3: 1288.36\n",
      "Epoch [489], train_loss: 1650.50 with loss1: 1405.84, loss2: 244.66 and loss3: 1283.34\n",
      "Epoch [490], train_loss: 1633.64 with loss1: 1390.96, loss2: 242.68 and loss3: 1278.32\n",
      "Epoch [491], train_loss: 1648.00 with loss1: 1405.38, loss2: 242.62 and loss3: 1273.31\n",
      "Epoch [492], train_loss: 1637.98 with loss1: 1397.07, loss2: 240.91 and loss3: 1268.31\n",
      "Epoch [493], train_loss: 1650.38 with loss1: 1406.16, loss2: 244.22 and loss3: 1263.32\n",
      "Epoch [494], train_loss: 1630.60 with loss1: 1390.48, loss2: 240.12 and loss3: 1258.34\n",
      "Epoch [495], train_loss: 1650.07 with loss1: 1408.19, loss2: 241.88 and loss3: 1253.39\n",
      "Epoch [496], train_loss: 1622.26 with loss1: 1382.81, loss2: 239.45 and loss3: 1248.44\n",
      "Epoch [497], train_loss: 1637.99 with loss1: 1398.01, loss2: 239.98 and loss3: 1243.50\n",
      "Epoch [498], train_loss: 1619.01 with loss1: 1380.27, loss2: 238.74 and loss3: 1238.57\n",
      "Epoch [499], train_loss: 1625.24 with loss1: 1386.82, loss2: 238.42 and loss3: 1233.65\n",
      "Epoch [500], train_loss: 1605.42 with loss1: 1369.39, loss2: 236.03 and loss3: 1228.74\n",
      "Epoch [501], train_loss: 1611.49 with loss1: 1373.65, loss2: 237.84 and loss3: 1223.83\n",
      "Epoch [502], train_loss: 1590.01 with loss1: 1354.16, loss2: 235.84 and loss3: 1218.94\n",
      "Epoch [503], train_loss: 1603.40 with loss1: 1367.16, loss2: 236.25 and loss3: 1214.05\n",
      "Epoch [504], train_loss: 1585.91 with loss1: 1351.39, loss2: 234.51 and loss3: 1209.17\n",
      "Epoch [505], train_loss: 1592.99 with loss1: 1358.30, loss2: 234.69 and loss3: 1204.30\n",
      "Epoch [506], train_loss: 1574.54 with loss1: 1340.30, loss2: 234.25 and loss3: 1199.44\n",
      "Epoch [507], train_loss: 1577.29 with loss1: 1343.08, loss2: 234.21 and loss3: 1194.59\n",
      "Epoch [508], train_loss: 1561.82 with loss1: 1328.82, loss2: 233.00 and loss3: 1189.76\n",
      "Epoch [509], train_loss: 1564.69 with loss1: 1331.47, loss2: 233.22 and loss3: 1184.93\n",
      "Epoch [510], train_loss: 1546.84 with loss1: 1316.37, loss2: 230.47 and loss3: 1180.11\n",
      "Epoch [511], train_loss: 1549.52 with loss1: 1317.81, loss2: 231.71 and loss3: 1175.30\n",
      "Epoch [512], train_loss: 1538.12 with loss1: 1309.03, loss2: 229.09 and loss3: 1170.49\n",
      "Epoch [513], train_loss: 1541.75 with loss1: 1310.04, loss2: 231.71 and loss3: 1165.70\n",
      "Epoch [514], train_loss: 1531.63 with loss1: 1302.80, loss2: 228.83 and loss3: 1160.92\n",
      "Epoch [515], train_loss: 1532.38 with loss1: 1301.59, loss2: 230.79 and loss3: 1156.15\n",
      "Epoch [516], train_loss: 1518.64 with loss1: 1290.26, loss2: 228.37 and loss3: 1151.39\n",
      "Epoch [517], train_loss: 1522.32 with loss1: 1292.59, loss2: 229.73 and loss3: 1146.63\n",
      "Epoch [518], train_loss: 1513.52 with loss1: 1286.64, loss2: 226.88 and loss3: 1141.89\n",
      "Epoch [519], train_loss: 1515.07 with loss1: 1285.75, loss2: 229.32 and loss3: 1137.15\n",
      "Epoch [520], train_loss: 1509.58 with loss1: 1282.47, loss2: 227.11 and loss3: 1132.43\n",
      "Epoch [521], train_loss: 1514.43 with loss1: 1286.27, loss2: 228.16 and loss3: 1127.71\n",
      "Epoch [522], train_loss: 1502.26 with loss1: 1277.68, loss2: 224.58 and loss3: 1123.01\n",
      "Epoch [523], train_loss: 1504.01 with loss1: 1277.48, loss2: 226.53 and loss3: 1118.32\n",
      "Epoch [524], train_loss: 1494.15 with loss1: 1269.70, loss2: 224.45 and loss3: 1113.63\n",
      "Epoch [525], train_loss: 1494.92 with loss1: 1268.67, loss2: 226.25 and loss3: 1108.95\n",
      "Epoch [526], train_loss: 1493.44 with loss1: 1269.76, loss2: 223.67 and loss3: 1104.29\n",
      "Epoch [527], train_loss: 1498.24 with loss1: 1272.24, loss2: 226.01 and loss3: 1099.64\n",
      "Epoch [528], train_loss: 1497.58 with loss1: 1274.81, loss2: 222.77 and loss3: 1094.99\n",
      "Epoch [529], train_loss: 1493.85 with loss1: 1269.33, loss2: 224.52 and loss3: 1090.37\n",
      "Epoch [530], train_loss: 1485.68 with loss1: 1263.77, loss2: 221.91 and loss3: 1085.75\n",
      "Epoch [531], train_loss: 1488.01 with loss1: 1264.88, loss2: 223.13 and loss3: 1081.15\n",
      "Epoch [532], train_loss: 1483.66 with loss1: 1263.78, loss2: 219.88 and loss3: 1076.56\n",
      "Epoch [533], train_loss: 1489.69 with loss1: 1267.44, loss2: 222.25 and loss3: 1071.98\n",
      "Epoch [534], train_loss: 1482.82 with loss1: 1262.45, loss2: 220.36 and loss3: 1067.41\n",
      "Epoch [535], train_loss: 1491.98 with loss1: 1270.58, loss2: 221.40 and loss3: 1062.84\n",
      "Epoch [536], train_loss: 1479.27 with loss1: 1259.74, loss2: 219.53 and loss3: 1058.28\n",
      "Epoch [537], train_loss: 1492.93 with loss1: 1272.11, loss2: 220.82 and loss3: 1053.73\n",
      "Epoch [538], train_loss: 1483.35 with loss1: 1264.78, loss2: 218.58 and loss3: 1049.19\n",
      "Epoch [539], train_loss: 1490.52 with loss1: 1271.10, loss2: 219.42 and loss3: 1044.65\n",
      "Epoch [540], train_loss: 1480.95 with loss1: 1263.18, loss2: 217.77 and loss3: 1040.13\n",
      "Epoch [541], train_loss: 1493.83 with loss1: 1275.44, loss2: 218.40 and loss3: 1035.61\n",
      "Epoch [542], train_loss: 1487.86 with loss1: 1272.18, loss2: 215.68 and loss3: 1031.10\n",
      "Epoch [543], train_loss: 1503.12 with loss1: 1285.48, loss2: 217.63 and loss3: 1026.60\n",
      "Epoch [544], train_loss: 1492.59 with loss1: 1277.81, loss2: 214.77 and loss3: 1022.11\n",
      "Epoch [545], train_loss: 1508.84 with loss1: 1291.20, loss2: 217.64 and loss3: 1017.63\n",
      "Epoch [546], train_loss: 1494.91 with loss1: 1279.88, loss2: 215.02 and loss3: 1013.17\n",
      "Epoch [547], train_loss: 1506.82 with loss1: 1290.28, loss2: 216.53 and loss3: 1008.71\n",
      "Epoch [548], train_loss: 1498.50 with loss1: 1283.92, loss2: 214.58 and loss3: 1004.27\n",
      "Epoch [549], train_loss: 1508.45 with loss1: 1292.77, loss2: 215.69 and loss3: 999.84\n",
      "Epoch [550], train_loss: 1491.05 with loss1: 1276.42, loss2: 214.63 and loss3: 995.42\n",
      "Epoch [551], train_loss: 1495.26 with loss1: 1280.20, loss2: 215.06 and loss3: 991.00\n",
      "Epoch [552], train_loss: 1479.58 with loss1: 1267.19, loss2: 212.40 and loss3: 986.60\n",
      "Epoch [553], train_loss: 1480.62 with loss1: 1267.05, loss2: 213.57 and loss3: 982.20\n",
      "Epoch [554], train_loss: 1459.84 with loss1: 1246.92, loss2: 212.92 and loss3: 977.81\n",
      "Epoch [555], train_loss: 1458.54 with loss1: 1245.32, loss2: 213.22 and loss3: 973.43\n",
      "Epoch [556], train_loss: 1441.60 with loss1: 1229.97, loss2: 211.64 and loss3: 969.06\n",
      "Epoch [557], train_loss: 1450.25 with loss1: 1236.75, loss2: 213.51 and loss3: 964.71\n",
      "Epoch [558], train_loss: 1432.69 with loss1: 1220.79, loss2: 211.90 and loss3: 960.36\n",
      "Epoch [559], train_loss: 1438.54 with loss1: 1226.71, loss2: 211.82 and loss3: 956.02\n",
      "Epoch [560], train_loss: 1420.23 with loss1: 1210.66, loss2: 209.57 and loss3: 951.69\n",
      "Epoch [561], train_loss: 1424.04 with loss1: 1213.37, loss2: 210.67 and loss3: 947.37\n",
      "Epoch [562], train_loss: 1418.96 with loss1: 1209.71, loss2: 209.25 and loss3: 943.07\n",
      "Epoch [563], train_loss: 1429.43 with loss1: 1219.09, loss2: 210.33 and loss3: 938.77\n",
      "Epoch [564], train_loss: 1423.68 with loss1: 1214.17, loss2: 209.51 and loss3: 934.48\n",
      "Epoch [565], train_loss: 1434.70 with loss1: 1225.02, loss2: 209.69 and loss3: 930.20\n",
      "Epoch [566], train_loss: 1432.73 with loss1: 1224.68, loss2: 208.06 and loss3: 925.92\n",
      "Epoch [567], train_loss: 1455.02 with loss1: 1246.72, loss2: 208.29 and loss3: 921.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [568], train_loss: 1447.79 with loss1: 1240.35, loss2: 207.45 and loss3: 917.39\n",
      "Epoch [569], train_loss: 1470.14 with loss1: 1262.05, loss2: 208.08 and loss3: 913.15\n",
      "Epoch [570], train_loss: 1460.34 with loss1: 1254.09, loss2: 206.25 and loss3: 908.92\n",
      "Epoch [571], train_loss: 1474.94 with loss1: 1268.50, loss2: 206.44 and loss3: 904.70\n",
      "Epoch [572], train_loss: 1453.74 with loss1: 1248.13, loss2: 205.61 and loss3: 900.50\n",
      "Epoch [573], train_loss: 1457.97 with loss1: 1251.67, loss2: 206.31 and loss3: 896.30\n",
      "Epoch [574], train_loss: 1431.22 with loss1: 1226.24, loss2: 204.98 and loss3: 892.12\n",
      "Epoch [575], train_loss: 1428.62 with loss1: 1222.11, loss2: 206.51 and loss3: 887.94\n",
      "Epoch [576], train_loss: 1398.11 with loss1: 1194.10, loss2: 204.01 and loss3: 883.78\n",
      "Epoch [577], train_loss: 1398.59 with loss1: 1193.99, loss2: 204.60 and loss3: 879.62\n",
      "Epoch [578], train_loss: 1379.14 with loss1: 1176.99, loss2: 202.15 and loss3: 875.47\n",
      "Epoch [579], train_loss: 1384.58 with loss1: 1180.40, loss2: 204.19 and loss3: 871.33\n",
      "Epoch [580], train_loss: 1371.64 with loss1: 1169.61, loss2: 202.03 and loss3: 867.21\n",
      "Epoch [581], train_loss: 1370.93 with loss1: 1167.42, loss2: 203.50 and loss3: 863.09\n",
      "Epoch [582], train_loss: 1364.50 with loss1: 1162.25, loss2: 202.25 and loss3: 858.98\n",
      "Epoch [583], train_loss: 1361.34 with loss1: 1158.47, loss2: 202.87 and loss3: 854.89\n",
      "Epoch [584], train_loss: 1359.60 with loss1: 1157.23, loss2: 202.37 and loss3: 850.80\n",
      "Epoch [585], train_loss: 1358.73 with loss1: 1156.41, loss2: 202.32 and loss3: 846.72\n",
      "Epoch [586], train_loss: 1357.38 with loss1: 1155.95, loss2: 201.43 and loss3: 842.66\n",
      "Epoch [587], train_loss: 1359.19 with loss1: 1157.30, loss2: 201.89 and loss3: 838.60\n",
      "Epoch [588], train_loss: 1355.91 with loss1: 1155.61, loss2: 200.30 and loss3: 834.56\n",
      "Epoch [589], train_loss: 1360.82 with loss1: 1160.55, loss2: 200.27 and loss3: 830.53\n",
      "Epoch [590], train_loss: 1357.93 with loss1: 1158.73, loss2: 199.21 and loss3: 826.50\n",
      "Epoch [591], train_loss: 1364.58 with loss1: 1164.57, loss2: 200.01 and loss3: 822.49\n",
      "Epoch [592], train_loss: 1358.35 with loss1: 1159.92, loss2: 198.42 and loss3: 818.48\n",
      "Epoch [593], train_loss: 1365.99 with loss1: 1166.44, loss2: 199.55 and loss3: 814.49\n",
      "Epoch [594], train_loss: 1361.01 with loss1: 1162.88, loss2: 198.14 and loss3: 810.51\n",
      "Epoch [595], train_loss: 1366.84 with loss1: 1167.88, loss2: 198.96 and loss3: 806.54\n",
      "Epoch [596], train_loss: 1364.52 with loss1: 1166.90, loss2: 197.62 and loss3: 802.59\n",
      "Epoch [597], train_loss: 1375.70 with loss1: 1177.18, loss2: 198.53 and loss3: 798.65\n",
      "Epoch [598], train_loss: 1372.14 with loss1: 1174.13, loss2: 198.01 and loss3: 794.73\n",
      "Epoch [599], train_loss: 1376.63 with loss1: 1178.39, loss2: 198.24 and loss3: 790.82\n",
      "Epoch [600], train_loss: 1366.83 with loss1: 1170.57, loss2: 196.27 and loss3: 786.91\n",
      "Epoch [601], train_loss: 1374.90 with loss1: 1177.73, loss2: 197.16 and loss3: 783.01\n",
      "Epoch [602], train_loss: 1371.87 with loss1: 1175.76, loss2: 196.12 and loss3: 779.12\n",
      "Epoch [603], train_loss: 1375.84 with loss1: 1178.53, loss2: 197.31 and loss3: 775.23\n",
      "Epoch [604], train_loss: 1372.56 with loss1: 1177.23, loss2: 195.33 and loss3: 771.36\n",
      "Epoch [605], train_loss: 1379.82 with loss1: 1183.61, loss2: 196.21 and loss3: 767.50\n",
      "Epoch [606], train_loss: 1368.68 with loss1: 1174.18, loss2: 194.51 and loss3: 763.64\n",
      "Epoch [607], train_loss: 1378.70 with loss1: 1182.16, loss2: 196.54 and loss3: 759.80\n",
      "Epoch [608], train_loss: 1365.09 with loss1: 1170.33, loss2: 194.75 and loss3: 755.97\n",
      "Epoch [609], train_loss: 1374.02 with loss1: 1179.38, loss2: 194.64 and loss3: 752.15\n",
      "Epoch [610], train_loss: 1361.37 with loss1: 1167.83, loss2: 193.54 and loss3: 748.34\n",
      "Epoch [611], train_loss: 1372.56 with loss1: 1177.90, loss2: 194.66 and loss3: 744.54\n",
      "Epoch [612], train_loss: 1362.72 with loss1: 1169.44, loss2: 193.28 and loss3: 740.74\n",
      "Epoch [613], train_loss: 1370.24 with loss1: 1175.07, loss2: 195.17 and loss3: 736.96\n",
      "Epoch [614], train_loss: 1365.30 with loss1: 1172.23, loss2: 193.06 and loss3: 733.18\n",
      "Epoch [615], train_loss: 1367.77 with loss1: 1174.05, loss2: 193.72 and loss3: 729.41\n",
      "Epoch [616], train_loss: 1355.82 with loss1: 1162.98, loss2: 192.84 and loss3: 725.65\n",
      "Epoch [617], train_loss: 1359.04 with loss1: 1165.29, loss2: 193.74 and loss3: 721.89\n",
      "Epoch [618], train_loss: 1349.97 with loss1: 1158.06, loss2: 191.91 and loss3: 718.15\n",
      "Epoch [619], train_loss: 1348.41 with loss1: 1154.60, loss2: 193.81 and loss3: 714.41\n",
      "Epoch [620], train_loss: 1344.82 with loss1: 1153.20, loss2: 191.62 and loss3: 710.69\n",
      "Epoch [621], train_loss: 1347.68 with loss1: 1154.10, loss2: 193.58 and loss3: 706.97\n",
      "Epoch [622], train_loss: 1333.68 with loss1: 1143.20, loss2: 190.47 and loss3: 703.27\n",
      "Epoch [623], train_loss: 1341.17 with loss1: 1148.98, loss2: 192.18 and loss3: 699.58\n",
      "Epoch [624], train_loss: 1330.97 with loss1: 1139.85, loss2: 191.12 and loss3: 695.90\n",
      "Epoch [625], train_loss: 1337.17 with loss1: 1145.05, loss2: 192.13 and loss3: 692.22\n",
      "Epoch [626], train_loss: 1328.61 with loss1: 1138.51, loss2: 190.10 and loss3: 688.56\n",
      "Epoch [627], train_loss: 1331.62 with loss1: 1139.51, loss2: 192.11 and loss3: 684.90\n",
      "Epoch [628], train_loss: 1327.57 with loss1: 1136.70, loss2: 190.87 and loss3: 681.26\n",
      "Epoch [629], train_loss: 1330.75 with loss1: 1139.26, loss2: 191.49 and loss3: 677.62\n",
      "Epoch [630], train_loss: 1326.15 with loss1: 1135.64, loss2: 190.51 and loss3: 673.99\n",
      "Epoch [631], train_loss: 1334.67 with loss1: 1143.99, loss2: 190.67 and loss3: 670.36\n",
      "Epoch [632], train_loss: 1330.43 with loss1: 1140.24, loss2: 190.20 and loss3: 666.75\n",
      "Epoch [633], train_loss: 1341.82 with loss1: 1151.72, loss2: 190.10 and loss3: 663.15\n",
      "Epoch [634], train_loss: 1338.00 with loss1: 1148.25, loss2: 189.75 and loss3: 659.55\n",
      "Epoch [635], train_loss: 1345.88 with loss1: 1156.12, loss2: 189.77 and loss3: 655.96\n",
      "Epoch [636], train_loss: 1348.06 with loss1: 1159.39, loss2: 188.66 and loss3: 652.38\n",
      "Epoch [637], train_loss: 1362.05 with loss1: 1172.32, loss2: 189.73 and loss3: 648.81\n",
      "Epoch [638], train_loss: 1358.17 with loss1: 1168.89, loss2: 189.28 and loss3: 645.26\n",
      "Epoch [639], train_loss: 1374.99 with loss1: 1184.88, loss2: 190.11 and loss3: 641.71\n",
      "Epoch [640], train_loss: 1372.33 with loss1: 1182.73, loss2: 189.61 and loss3: 638.18\n",
      "Epoch [641], train_loss: 1396.43 with loss1: 1207.10, loss2: 189.34 and loss3: 634.67\n",
      "Epoch [642], train_loss: 1398.90 with loss1: 1209.71, loss2: 189.19 and loss3: 631.17\n",
      "Epoch [643], train_loss: 1424.41 with loss1: 1235.66, loss2: 188.76 and loss3: 627.68\n",
      "Epoch [644], train_loss: 1415.74 with loss1: 1227.28, loss2: 188.46 and loss3: 624.19\n",
      "Epoch [645], train_loss: 1432.17 with loss1: 1243.62, loss2: 188.55 and loss3: 620.72\n",
      "Epoch [646], train_loss: 1420.35 with loss1: 1231.58, loss2: 188.78 and loss3: 617.26\n",
      "Epoch [647], train_loss: 1428.38 with loss1: 1239.89, loss2: 188.49 and loss3: 613.80\n",
      "Epoch [648], train_loss: 1400.14 with loss1: 1211.50, loss2: 188.64 and loss3: 610.36\n",
      "Epoch [649], train_loss: 1395.95 with loss1: 1207.47, loss2: 188.47 and loss3: 606.92\n",
      "Epoch [650], train_loss: 1368.96 with loss1: 1179.89, loss2: 189.07 and loss3: 603.50\n",
      "Epoch [651], train_loss: 1353.16 with loss1: 1164.81, loss2: 188.35 and loss3: 600.08\n",
      "Epoch [652], train_loss: 1331.75 with loss1: 1143.86, loss2: 187.89 and loss3: 596.67\n",
      "Epoch [653], train_loss: 1320.27 with loss1: 1132.93, loss2: 187.34 and loss3: 593.27\n",
      "Epoch [654], train_loss: 1311.87 with loss1: 1124.63, loss2: 187.24 and loss3: 589.88\n",
      "Epoch [655], train_loss: 1308.45 with loss1: 1122.08, loss2: 186.36 and loss3: 586.50\n",
      "Epoch [656], train_loss: 1292.51 with loss1: 1105.59, loss2: 186.92 and loss3: 583.14\n",
      "Epoch [657], train_loss: 1284.77 with loss1: 1098.09, loss2: 186.68 and loss3: 579.78\n",
      "Epoch [658], train_loss: 1273.59 with loss1: 1086.93, loss2: 186.66 and loss3: 576.44\n",
      "Epoch [659], train_loss: 1266.13 with loss1: 1079.59, loss2: 186.54 and loss3: 573.11\n",
      "Epoch [660], train_loss: 1258.44 with loss1: 1072.16, loss2: 186.27 and loss3: 569.78\n",
      "Epoch [661], train_loss: 1254.45 with loss1: 1067.76, loss2: 186.69 and loss3: 566.47\n",
      "Epoch [662], train_loss: 1251.40 with loss1: 1065.02, loss2: 186.38 and loss3: 563.18\n",
      "Epoch [663], train_loss: 1248.03 with loss1: 1062.00, loss2: 186.03 and loss3: 559.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [664], train_loss: 1244.99 with loss1: 1058.81, loss2: 186.18 and loss3: 556.61\n",
      "Epoch [665], train_loss: 1240.76 with loss1: 1055.63, loss2: 185.13 and loss3: 553.34\n",
      "Epoch [666], train_loss: 1239.82 with loss1: 1054.37, loss2: 185.45 and loss3: 550.08\n",
      "Epoch [667], train_loss: 1235.30 with loss1: 1050.40, loss2: 184.91 and loss3: 546.83\n",
      "Epoch [668], train_loss: 1231.88 with loss1: 1046.69, loss2: 185.19 and loss3: 543.59\n",
      "Epoch [669], train_loss: 1231.85 with loss1: 1046.76, loss2: 185.09 and loss3: 540.36\n",
      "Epoch [670], train_loss: 1228.34 with loss1: 1043.13, loss2: 185.21 and loss3: 537.14\n",
      "Epoch [671], train_loss: 1228.72 with loss1: 1043.92, loss2: 184.80 and loss3: 533.94\n",
      "Epoch [672], train_loss: 1222.74 with loss1: 1038.62, loss2: 184.12 and loss3: 530.74\n",
      "Epoch [673], train_loss: 1230.99 with loss1: 1046.32, loss2: 184.67 and loss3: 527.56\n",
      "Epoch [674], train_loss: 1222.81 with loss1: 1038.72, loss2: 184.09 and loss3: 524.37\n",
      "Epoch [675], train_loss: 1230.14 with loss1: 1045.18, loss2: 184.96 and loss3: 521.18\n",
      "Epoch [676], train_loss: 1225.80 with loss1: 1041.63, loss2: 184.17 and loss3: 518.01\n",
      "Epoch [677], train_loss: 1225.09 with loss1: 1040.82, loss2: 184.28 and loss3: 514.85\n",
      "Epoch [678], train_loss: 1226.18 with loss1: 1042.33, loss2: 183.84 and loss3: 511.69\n",
      "Epoch [679], train_loss: 1225.89 with loss1: 1042.14, loss2: 183.75 and loss3: 508.55\n",
      "Epoch [680], train_loss: 1226.22 with loss1: 1042.53, loss2: 183.69 and loss3: 505.41\n",
      "Epoch [681], train_loss: 1231.14 with loss1: 1047.49, loss2: 183.65 and loss3: 502.28\n",
      "Epoch [682], train_loss: 1223.70 with loss1: 1040.62, loss2: 183.09 and loss3: 499.17\n",
      "Epoch [683], train_loss: 1226.13 with loss1: 1042.78, loss2: 183.35 and loss3: 496.07\n",
      "Epoch [684], train_loss: 1224.82 with loss1: 1041.27, loss2: 183.54 and loss3: 492.98\n",
      "Epoch [685], train_loss: 1228.78 with loss1: 1045.29, loss2: 183.49 and loss3: 489.91\n",
      "Epoch [686], train_loss: 1222.51 with loss1: 1039.04, loss2: 183.46 and loss3: 486.86\n",
      "Epoch [687], train_loss: 1226.74 with loss1: 1044.14, loss2: 182.60 and loss3: 483.81\n",
      "Epoch [688], train_loss: 1224.21 with loss1: 1042.10, loss2: 182.11 and loss3: 480.77\n",
      "Epoch [689], train_loss: 1230.30 with loss1: 1047.37, loss2: 182.93 and loss3: 477.73\n",
      "Epoch [690], train_loss: 1217.89 with loss1: 1035.32, loss2: 182.57 and loss3: 474.71\n",
      "Epoch [691], train_loss: 1225.93 with loss1: 1043.40, loss2: 182.53 and loss3: 471.69\n",
      "Epoch [692], train_loss: 1212.51 with loss1: 1030.28, loss2: 182.23 and loss3: 468.69\n",
      "Epoch [693], train_loss: 1218.41 with loss1: 1036.21, loss2: 182.20 and loss3: 465.70\n",
      "Epoch [694], train_loss: 1208.09 with loss1: 1026.27, loss2: 181.82 and loss3: 462.72\n",
      "Epoch [695], train_loss: 1214.08 with loss1: 1032.03, loss2: 182.06 and loss3: 459.75\n",
      "Epoch [696], train_loss: 1207.14 with loss1: 1025.69, loss2: 181.45 and loss3: 456.79\n",
      "Epoch [697], train_loss: 1214.45 with loss1: 1032.46, loss2: 181.98 and loss3: 453.83\n",
      "Epoch [698], train_loss: 1207.18 with loss1: 1026.50, loss2: 180.68 and loss3: 450.89\n",
      "Epoch [699], train_loss: 1207.21 with loss1: 1025.54, loss2: 181.67 and loss3: 447.95\n",
      "Epoch [700], train_loss: 1204.28 with loss1: 1023.45, loss2: 180.83 and loss3: 445.02\n",
      "Epoch [701], train_loss: 1207.45 with loss1: 1026.06, loss2: 181.38 and loss3: 442.10\n",
      "Epoch [702], train_loss: 1205.46 with loss1: 1024.42, loss2: 181.04 and loss3: 439.19\n",
      "Epoch [703], train_loss: 1213.44 with loss1: 1032.54, loss2: 180.90 and loss3: 436.30\n",
      "Epoch [704], train_loss: 1208.05 with loss1: 1027.97, loss2: 180.09 and loss3: 433.41\n",
      "Epoch [705], train_loss: 1222.26 with loss1: 1040.95, loss2: 181.30 and loss3: 430.54\n",
      "Epoch [706], train_loss: 1218.69 with loss1: 1038.91, loss2: 179.78 and loss3: 427.68\n",
      "Epoch [707], train_loss: 1228.63 with loss1: 1047.04, loss2: 181.59 and loss3: 424.83\n",
      "Epoch [708], train_loss: 1227.46 with loss1: 1047.31, loss2: 180.15 and loss3: 421.98\n",
      "Epoch [709], train_loss: 1240.14 with loss1: 1058.80, loss2: 181.34 and loss3: 419.15\n",
      "Epoch [710], train_loss: 1240.10 with loss1: 1061.15, loss2: 178.95 and loss3: 416.32\n",
      "Epoch [711], train_loss: 1255.91 with loss1: 1074.91, loss2: 181.00 and loss3: 413.50\n",
      "Epoch [712], train_loss: 1248.91 with loss1: 1069.83, loss2: 179.08 and loss3: 410.69\n",
      "Epoch [713], train_loss: 1268.65 with loss1: 1087.83, loss2: 180.82 and loss3: 407.90\n",
      "Epoch [714], train_loss: 1266.31 with loss1: 1087.57, loss2: 178.74 and loss3: 405.11\n",
      "Epoch [715], train_loss: 1277.38 with loss1: 1096.77, loss2: 180.61 and loss3: 402.33\n",
      "Epoch [716], train_loss: 1268.86 with loss1: 1090.20, loss2: 178.67 and loss3: 399.57\n",
      "Epoch [717], train_loss: 1280.08 with loss1: 1099.71, loss2: 180.37 and loss3: 396.82\n",
      "Epoch [718], train_loss: 1273.18 with loss1: 1094.29, loss2: 178.89 and loss3: 394.08\n",
      "Epoch [719], train_loss: 1282.72 with loss1: 1101.93, loss2: 180.79 and loss3: 391.34\n",
      "Epoch [720], train_loss: 1273.04 with loss1: 1095.48, loss2: 177.56 and loss3: 388.62\n",
      "Epoch [721], train_loss: 1279.46 with loss1: 1099.20, loss2: 180.26 and loss3: 385.91\n",
      "Epoch [722], train_loss: 1267.11 with loss1: 1088.40, loss2: 178.71 and loss3: 383.21\n",
      "Epoch [723], train_loss: 1269.02 with loss1: 1088.98, loss2: 180.04 and loss3: 380.52\n",
      "Epoch [724], train_loss: 1257.16 with loss1: 1079.10, loss2: 178.06 and loss3: 377.84\n",
      "Epoch [725], train_loss: 1264.13 with loss1: 1084.26, loss2: 179.87 and loss3: 375.16\n",
      "Epoch [726], train_loss: 1248.14 with loss1: 1069.21, loss2: 178.93 and loss3: 372.50\n",
      "Epoch [727], train_loss: 1254.97 with loss1: 1075.28, loss2: 179.69 and loss3: 369.84\n",
      "Epoch [728], train_loss: 1243.00 with loss1: 1064.91, loss2: 178.08 and loss3: 367.19\n",
      "Epoch [729], train_loss: 1248.18 with loss1: 1069.05, loss2: 179.12 and loss3: 364.55\n",
      "Epoch [730], train_loss: 1239.44 with loss1: 1061.67, loss2: 177.77 and loss3: 361.91\n",
      "Epoch [731], train_loss: 1244.48 with loss1: 1065.36, loss2: 179.12 and loss3: 359.29\n",
      "Epoch [732], train_loss: 1233.04 with loss1: 1054.63, loss2: 178.41 and loss3: 356.68\n",
      "Epoch [733], train_loss: 1233.69 with loss1: 1054.34, loss2: 179.35 and loss3: 354.08\n",
      "Epoch [734], train_loss: 1223.96 with loss1: 1045.46, loss2: 178.50 and loss3: 351.49\n",
      "Epoch [735], train_loss: 1227.13 with loss1: 1047.80, loss2: 179.33 and loss3: 348.91\n",
      "Epoch [736], train_loss: 1216.05 with loss1: 1038.04, loss2: 178.01 and loss3: 346.34\n",
      "Epoch [737], train_loss: 1221.52 with loss1: 1042.95, loss2: 178.57 and loss3: 343.77\n",
      "Epoch [738], train_loss: 1210.43 with loss1: 1032.92, loss2: 177.50 and loss3: 341.22\n",
      "Epoch [739], train_loss: 1212.35 with loss1: 1033.46, loss2: 178.89 and loss3: 338.67\n",
      "Epoch [740], train_loss: 1206.01 with loss1: 1028.55, loss2: 177.46 and loss3: 336.13\n",
      "Epoch [741], train_loss: 1204.76 with loss1: 1026.12, loss2: 178.65 and loss3: 333.60\n",
      "Epoch [742], train_loss: 1197.77 with loss1: 1020.44, loss2: 177.32 and loss3: 331.08\n",
      "Epoch [743], train_loss: 1197.09 with loss1: 1018.98, loss2: 178.11 and loss3: 328.57\n",
      "Epoch [744], train_loss: 1188.80 with loss1: 1011.18, loss2: 177.62 and loss3: 326.06\n",
      "Epoch [745], train_loss: 1193.38 with loss1: 1014.54, loss2: 178.83 and loss3: 323.57\n",
      "Epoch [746], train_loss: 1182.36 with loss1: 1004.59, loss2: 177.77 and loss3: 321.08\n",
      "Epoch [747], train_loss: 1182.48 with loss1: 1004.62, loss2: 177.86 and loss3: 318.60\n",
      "Epoch [748], train_loss: 1176.20 with loss1: 999.38, loss2: 176.82 and loss3: 316.14\n",
      "Epoch [749], train_loss: 1180.50 with loss1: 1002.71, loss2: 177.79 and loss3: 313.67\n",
      "Epoch [750], train_loss: 1170.90 with loss1: 994.19, loss2: 176.71 and loss3: 311.22\n",
      "Epoch [751], train_loss: 1173.63 with loss1: 996.21, loss2: 177.42 and loss3: 308.77\n",
      "Epoch [752], train_loss: 1165.82 with loss1: 988.90, loss2: 176.92 and loss3: 306.32\n",
      "Epoch [753], train_loss: 1166.60 with loss1: 989.15, loss2: 177.45 and loss3: 303.88\n",
      "Epoch [754], train_loss: 1156.60 with loss1: 980.09, loss2: 176.51 and loss3: 301.45\n",
      "Epoch [755], train_loss: 1163.12 with loss1: 985.98, loss2: 177.14 and loss3: 299.03\n",
      "Epoch [756], train_loss: 1157.56 with loss1: 981.08, loss2: 176.48 and loss3: 296.62\n",
      "Epoch [757], train_loss: 1159.12 with loss1: 981.86, loss2: 177.26 and loss3: 294.21\n",
      "Epoch [758], train_loss: 1150.48 with loss1: 973.96, loss2: 176.52 and loss3: 291.82\n",
      "Epoch [759], train_loss: 1152.82 with loss1: 975.28, loss2: 177.54 and loss3: 289.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [760], train_loss: 1149.29 with loss1: 973.39, loss2: 175.90 and loss3: 287.06\n",
      "Epoch [761], train_loss: 1150.77 with loss1: 973.13, loss2: 177.64 and loss3: 284.69\n",
      "Epoch [762], train_loss: 1143.34 with loss1: 966.99, loss2: 176.35 and loss3: 282.34\n",
      "Epoch [763], train_loss: 1146.38 with loss1: 969.22, loss2: 177.16 and loss3: 280.00\n",
      "Epoch [764], train_loss: 1142.98 with loss1: 966.95, loss2: 176.02 and loss3: 277.68\n",
      "Epoch [765], train_loss: 1146.12 with loss1: 969.19, loss2: 176.93 and loss3: 275.36\n",
      "Epoch [766], train_loss: 1143.90 with loss1: 967.27, loss2: 176.63 and loss3: 273.05\n",
      "Epoch [767], train_loss: 1147.13 with loss1: 970.16, loss2: 176.97 and loss3: 270.75\n",
      "Epoch [768], train_loss: 1148.67 with loss1: 972.62, loss2: 176.05 and loss3: 268.46\n",
      "Epoch [769], train_loss: 1151.81 with loss1: 974.92, loss2: 176.89 and loss3: 266.18\n",
      "Epoch [770], train_loss: 1147.34 with loss1: 971.37, loss2: 175.97 and loss3: 263.90\n",
      "Epoch [771], train_loss: 1161.67 with loss1: 984.70, loss2: 176.97 and loss3: 261.63\n",
      "Epoch [772], train_loss: 1154.98 with loss1: 978.53, loss2: 176.45 and loss3: 259.37\n",
      "Epoch [773], train_loss: 1162.64 with loss1: 985.81, loss2: 176.83 and loss3: 257.12\n",
      "Epoch [774], train_loss: 1159.14 with loss1: 983.32, loss2: 175.82 and loss3: 254.88\n",
      "Epoch [775], train_loss: 1166.57 with loss1: 989.56, loss2: 177.01 and loss3: 252.65\n",
      "Epoch [776], train_loss: 1165.99 with loss1: 989.79, loss2: 176.20 and loss3: 250.43\n",
      "Epoch [777], train_loss: 1175.96 with loss1: 999.04, loss2: 176.91 and loss3: 248.23\n",
      "Epoch [778], train_loss: 1175.50 with loss1: 999.26, loss2: 176.24 and loss3: 246.03\n",
      "Epoch [779], train_loss: 1191.16 with loss1: 1014.66, loss2: 176.50 and loss3: 243.84\n",
      "Epoch [780], train_loss: 1186.91 with loss1: 1010.57, loss2: 176.34 and loss3: 241.67\n",
      "Epoch [781], train_loss: 1199.91 with loss1: 1023.15, loss2: 176.76 and loss3: 239.51\n",
      "Epoch [782], train_loss: 1200.83 with loss1: 1024.75, loss2: 176.09 and loss3: 237.36\n",
      "Epoch [783], train_loss: 1218.55 with loss1: 1042.16, loss2: 176.39 and loss3: 235.21\n",
      "Epoch [784], train_loss: 1219.81 with loss1: 1043.38, loss2: 176.43 and loss3: 233.08\n",
      "Epoch [785], train_loss: 1235.85 with loss1: 1059.14, loss2: 176.71 and loss3: 230.95\n",
      "Epoch [786], train_loss: 1231.19 with loss1: 1054.61, loss2: 176.58 and loss3: 228.84\n",
      "Epoch [787], train_loss: 1240.83 with loss1: 1064.48, loss2: 176.34 and loss3: 226.73\n",
      "Epoch [788], train_loss: 1235.47 with loss1: 1058.36, loss2: 177.12 and loss3: 224.63\n",
      "Epoch [789], train_loss: 1248.94 with loss1: 1071.87, loss2: 177.06 and loss3: 222.54\n",
      "Epoch [790], train_loss: 1238.80 with loss1: 1062.23, loss2: 176.58 and loss3: 220.46\n",
      "Epoch [791], train_loss: 1249.37 with loss1: 1072.33, loss2: 177.04 and loss3: 218.39\n",
      "Epoch [792], train_loss: 1241.14 with loss1: 1064.60, loss2: 176.55 and loss3: 216.34\n",
      "Epoch [793], train_loss: 1245.71 with loss1: 1069.12, loss2: 176.59 and loss3: 214.29\n",
      "Epoch [794], train_loss: 1233.26 with loss1: 1056.48, loss2: 176.77 and loss3: 212.26\n",
      "Epoch [795], train_loss: 1233.77 with loss1: 1056.64, loss2: 177.13 and loss3: 210.23\n",
      "Epoch [796], train_loss: 1214.92 with loss1: 1038.13, loss2: 176.80 and loss3: 208.21\n",
      "Epoch [797], train_loss: 1209.23 with loss1: 1031.71, loss2: 177.52 and loss3: 206.20\n",
      "Epoch [798], train_loss: 1195.49 with loss1: 1019.36, loss2: 176.13 and loss3: 204.20\n",
      "Epoch [799], train_loss: 1188.41 with loss1: 1011.77, loss2: 176.64 and loss3: 202.21\n",
      "Epoch [800], train_loss: 1173.32 with loss1: 997.06, loss2: 176.26 and loss3: 200.24\n",
      "Epoch [801], train_loss: 1172.86 with loss1: 996.07, loss2: 176.79 and loss3: 198.27\n",
      "Epoch [802], train_loss: 1166.77 with loss1: 989.73, loss2: 177.04 and loss3: 196.31\n",
      "Epoch [803], train_loss: 1157.79 with loss1: 981.27, loss2: 176.52 and loss3: 194.36\n",
      "Epoch [804], train_loss: 1153.81 with loss1: 977.04, loss2: 176.78 and loss3: 192.42\n",
      "Epoch [805], train_loss: 1148.60 with loss1: 971.98, loss2: 176.62 and loss3: 190.49\n",
      "Epoch [806], train_loss: 1141.60 with loss1: 964.77, loss2: 176.83 and loss3: 188.56\n",
      "Epoch [807], train_loss: 1136.19 with loss1: 959.61, loss2: 176.58 and loss3: 186.65\n",
      "Epoch [808], train_loss: 1130.07 with loss1: 953.72, loss2: 176.35 and loss3: 184.74\n",
      "Epoch [809], train_loss: 1127.39 with loss1: 951.06, loss2: 176.33 and loss3: 182.83\n",
      "Epoch [810], train_loss: 1122.13 with loss1: 945.22, loss2: 176.91 and loss3: 180.93\n",
      "Epoch [811], train_loss: 1118.74 with loss1: 942.15, loss2: 176.58 and loss3: 179.05\n",
      "Epoch [812], train_loss: 1111.02 with loss1: 934.70, loss2: 176.32 and loss3: 177.17\n",
      "Epoch [813], train_loss: 1104.96 with loss1: 928.49, loss2: 176.47 and loss3: 175.30\n",
      "Epoch [814], train_loss: 1099.86 with loss1: 923.99, loss2: 175.87 and loss3: 173.44\n",
      "Epoch [815], train_loss: 1095.81 with loss1: 918.63, loss2: 177.19 and loss3: 171.59\n",
      "Epoch [816], train_loss: 1091.96 with loss1: 915.25, loss2: 176.71 and loss3: 169.75\n",
      "Epoch [817], train_loss: 1087.75 with loss1: 910.99, loss2: 176.76 and loss3: 167.92\n",
      "Epoch [818], train_loss: 1086.53 with loss1: 910.20, loss2: 176.33 and loss3: 166.10\n",
      "Epoch [819], train_loss: 1081.90 with loss1: 905.53, loss2: 176.37 and loss3: 164.28\n",
      "Epoch [820], train_loss: 1077.39 with loss1: 900.82, loss2: 176.57 and loss3: 162.48\n",
      "Epoch [821], train_loss: 1076.30 with loss1: 899.94, loss2: 176.36 and loss3: 160.68\n",
      "Epoch [822], train_loss: 1072.91 with loss1: 897.26, loss2: 175.66 and loss3: 158.90\n",
      "Epoch [823], train_loss: 1078.37 with loss1: 902.15, loss2: 176.22 and loss3: 157.13\n",
      "Epoch [824], train_loss: 1071.45 with loss1: 895.00, loss2: 176.45 and loss3: 155.36\n",
      "Epoch [825], train_loss: 1074.08 with loss1: 897.94, loss2: 176.14 and loss3: 153.60\n",
      "Epoch [826], train_loss: 1072.70 with loss1: 896.49, loss2: 176.22 and loss3: 151.84\n",
      "Epoch [827], train_loss: 1074.81 with loss1: 898.44, loss2: 176.37 and loss3: 150.10\n",
      "Epoch [828], train_loss: 1073.17 with loss1: 897.86, loss2: 175.30 and loss3: 148.37\n",
      "Epoch [829], train_loss: 1072.74 with loss1: 896.82, loss2: 175.92 and loss3: 146.65\n",
      "Epoch [830], train_loss: 1073.59 with loss1: 897.67, loss2: 175.92 and loss3: 144.93\n",
      "Epoch [831], train_loss: 1071.08 with loss1: 895.43, loss2: 175.65 and loss3: 143.23\n",
      "Epoch [832], train_loss: 1068.65 with loss1: 892.97, loss2: 175.68 and loss3: 141.54\n",
      "Epoch [833], train_loss: 1070.54 with loss1: 894.50, loss2: 176.04 and loss3: 139.85\n",
      "Epoch [834], train_loss: 1070.02 with loss1: 894.27, loss2: 175.75 and loss3: 138.17\n",
      "Epoch [835], train_loss: 1071.00 with loss1: 894.80, loss2: 176.20 and loss3: 136.51\n",
      "Epoch [836], train_loss: 1073.96 with loss1: 898.87, loss2: 175.09 and loss3: 134.85\n",
      "Epoch [837], train_loss: 1073.62 with loss1: 897.54, loss2: 176.08 and loss3: 133.21\n",
      "Epoch [838], train_loss: 1073.72 with loss1: 898.38, loss2: 175.34 and loss3: 131.58\n",
      "Epoch [839], train_loss: 1083.04 with loss1: 907.02, loss2: 176.02 and loss3: 129.96\n",
      "Epoch [840], train_loss: 1081.46 with loss1: 906.29, loss2: 175.17 and loss3: 128.35\n",
      "Epoch [841], train_loss: 1085.29 with loss1: 908.89, loss2: 176.40 and loss3: 126.74\n",
      "Epoch [842], train_loss: 1081.89 with loss1: 906.45, loss2: 175.44 and loss3: 125.14\n",
      "Epoch [843], train_loss: 1091.21 with loss1: 915.16, loss2: 176.05 and loss3: 123.56\n",
      "Epoch [844], train_loss: 1091.31 with loss1: 915.80, loss2: 175.51 and loss3: 121.99\n",
      "Epoch [845], train_loss: 1102.19 with loss1: 926.28, loss2: 175.90 and loss3: 120.42\n",
      "Epoch [846], train_loss: 1097.91 with loss1: 922.26, loss2: 175.65 and loss3: 118.87\n",
      "Epoch [847], train_loss: 1108.45 with loss1: 932.33, loss2: 176.12 and loss3: 117.32\n",
      "Epoch [848], train_loss: 1109.66 with loss1: 934.16, loss2: 175.50 and loss3: 115.78\n",
      "Epoch [849], train_loss: 1121.92 with loss1: 945.76, loss2: 176.17 and loss3: 114.26\n",
      "Epoch [850], train_loss: 1119.45 with loss1: 944.03, loss2: 175.42 and loss3: 112.75\n",
      "Epoch [851], train_loss: 1136.98 with loss1: 960.63, loss2: 176.35 and loss3: 111.24\n",
      "Epoch [852], train_loss: 1132.81 with loss1: 957.15, loss2: 175.66 and loss3: 109.74\n",
      "Epoch [853], train_loss: 1146.10 with loss1: 969.69, loss2: 176.41 and loss3: 108.26\n",
      "Epoch [854], train_loss: 1142.49 with loss1: 966.22, loss2: 176.27 and loss3: 106.77\n",
      "Epoch [855], train_loss: 1155.92 with loss1: 979.18, loss2: 176.73 and loss3: 105.30\n",
      "Epoch [856], train_loss: 1152.47 with loss1: 976.67, loss2: 175.80 and loss3: 103.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [857], train_loss: 1168.09 with loss1: 991.61, loss2: 176.48 and loss3: 102.40\n",
      "Epoch [858], train_loss: 1161.48 with loss1: 985.86, loss2: 175.61 and loss3: 100.96\n",
      "Epoch [859], train_loss: 1170.89 with loss1: 993.74, loss2: 177.14 and loss3: 99.54\n",
      "Epoch [860], train_loss: 1161.74 with loss1: 985.69, loss2: 176.05 and loss3: 98.12\n",
      "Epoch [861], train_loss: 1172.59 with loss1: 995.58, loss2: 177.01 and loss3: 96.70\n",
      "Epoch [862], train_loss: 1161.90 with loss1: 986.13, loss2: 175.77 and loss3: 95.31\n",
      "Epoch [863], train_loss: 1170.01 with loss1: 993.06, loss2: 176.94 and loss3: 93.92\n",
      "Epoch [864], train_loss: 1154.01 with loss1: 977.93, loss2: 176.08 and loss3: 92.54\n",
      "Epoch [865], train_loss: 1158.29 with loss1: 981.55, loss2: 176.74 and loss3: 91.17\n",
      "Epoch [866], train_loss: 1147.54 with loss1: 970.87, loss2: 176.67 and loss3: 89.81\n",
      "Epoch [867], train_loss: 1156.95 with loss1: 979.98, loss2: 176.98 and loss3: 88.47\n",
      "Epoch [868], train_loss: 1137.39 with loss1: 961.25, loss2: 176.15 and loss3: 87.13\n",
      "Epoch [869], train_loss: 1139.14 with loss1: 961.36, loss2: 177.78 and loss3: 85.80\n",
      "Epoch [870], train_loss: 1129.68 with loss1: 953.32, loss2: 176.36 and loss3: 84.48\n",
      "Epoch [871], train_loss: 1131.30 with loss1: 953.75, loss2: 177.54 and loss3: 83.17\n",
      "Epoch [872], train_loss: 1121.96 with loss1: 945.20, loss2: 176.77 and loss3: 81.86\n",
      "Epoch [873], train_loss: 1126.02 with loss1: 948.14, loss2: 177.88 and loss3: 80.57\n",
      "Epoch [874], train_loss: 1115.59 with loss1: 938.59, loss2: 176.99 and loss3: 79.28\n",
      "Epoch [875], train_loss: 1120.14 with loss1: 942.26, loss2: 177.88 and loss3: 78.00\n",
      "Epoch [876], train_loss: 1111.06 with loss1: 933.87, loss2: 177.19 and loss3: 76.73\n",
      "Epoch [877], train_loss: 1109.05 with loss1: 931.01, loss2: 178.04 and loss3: 75.47\n",
      "Epoch [878], train_loss: 1102.61 with loss1: 925.20, loss2: 177.41 and loss3: 74.23\n",
      "Epoch [879], train_loss: 1105.58 with loss1: 927.47, loss2: 178.11 and loss3: 72.98\n",
      "Epoch [880], train_loss: 1100.08 with loss1: 922.60, loss2: 177.48 and loss3: 71.75\n",
      "Epoch [881], train_loss: 1101.80 with loss1: 923.69, loss2: 178.11 and loss3: 70.54\n",
      "Epoch [882], train_loss: 1094.38 with loss1: 916.92, loss2: 177.46 and loss3: 69.33\n",
      "Epoch [883], train_loss: 1093.94 with loss1: 915.95, loss2: 178.00 and loss3: 68.14\n",
      "Epoch [884], train_loss: 1086.01 with loss1: 908.78, loss2: 177.23 and loss3: 66.97\n",
      "Epoch [885], train_loss: 1090.15 with loss1: 911.89, loss2: 178.25 and loss3: 65.80\n",
      "Epoch [886], train_loss: 1086.11 with loss1: 908.78, loss2: 177.33 and loss3: 64.65\n",
      "Epoch [887], train_loss: 1087.62 with loss1: 909.43, loss2: 178.19 and loss3: 63.51\n",
      "Epoch [888], train_loss: 1082.47 with loss1: 904.70, loss2: 177.77 and loss3: 62.38\n",
      "Epoch [889], train_loss: 1083.01 with loss1: 904.96, loss2: 178.05 and loss3: 61.26\n",
      "Epoch [890], train_loss: 1080.07 with loss1: 902.51, loss2: 177.55 and loss3: 60.14\n",
      "Epoch [891], train_loss: 1082.42 with loss1: 903.96, loss2: 178.46 and loss3: 59.03\n",
      "Epoch [892], train_loss: 1078.43 with loss1: 900.76, loss2: 177.67 and loss3: 57.94\n",
      "Epoch [893], train_loss: 1085.11 with loss1: 906.40, loss2: 178.71 and loss3: 56.85\n",
      "Epoch [894], train_loss: 1073.04 with loss1: 895.34, loss2: 177.70 and loss3: 55.78\n",
      "Epoch [895], train_loss: 1078.11 with loss1: 899.81, loss2: 178.29 and loss3: 54.72\n",
      "Epoch [896], train_loss: 1069.00 with loss1: 891.23, loss2: 177.77 and loss3: 53.67\n",
      "Epoch [897], train_loss: 1073.50 with loss1: 895.23, loss2: 178.28 and loss3: 52.63\n",
      "Epoch [898], train_loss: 1066.54 with loss1: 888.71, loss2: 177.83 and loss3: 51.59\n",
      "Epoch [899], train_loss: 1067.22 with loss1: 888.73, loss2: 178.49 and loss3: 50.57\n",
      "Epoch [900], train_loss: 1060.25 with loss1: 882.21, loss2: 178.04 and loss3: 49.55\n",
      "Epoch [901], train_loss: 1064.54 with loss1: 885.75, loss2: 178.79 and loss3: 48.54\n",
      "Epoch [902], train_loss: 1058.33 with loss1: 879.88, loss2: 178.45 and loss3: 47.54\n",
      "Epoch [903], train_loss: 1059.65 with loss1: 880.99, loss2: 178.66 and loss3: 46.55\n",
      "Epoch [904], train_loss: 1056.80 with loss1: 878.44, loss2: 178.36 and loss3: 45.57\n",
      "Epoch [905], train_loss: 1060.29 with loss1: 881.72, loss2: 178.56 and loss3: 44.59\n",
      "Epoch [906], train_loss: 1054.35 with loss1: 876.04, loss2: 178.32 and loss3: 43.63\n",
      "Epoch [907], train_loss: 1054.23 with loss1: 875.74, loss2: 178.49 and loss3: 42.68\n",
      "Epoch [908], train_loss: 1051.07 with loss1: 872.12, loss2: 178.95 and loss3: 41.74\n",
      "Epoch [909], train_loss: 1055.92 with loss1: 876.95, loss2: 178.97 and loss3: 40.81\n",
      "Epoch [910], train_loss: 1051.64 with loss1: 872.92, loss2: 178.72 and loss3: 39.89\n",
      "Epoch [911], train_loss: 1053.51 with loss1: 874.52, loss2: 178.99 and loss3: 38.98\n",
      "Epoch [912], train_loss: 1052.01 with loss1: 873.64, loss2: 178.37 and loss3: 38.08\n",
      "Epoch [913], train_loss: 1056.87 with loss1: 877.99, loss2: 178.89 and loss3: 37.20\n",
      "Epoch [914], train_loss: 1051.70 with loss1: 872.86, loss2: 178.84 and loss3: 36.33\n",
      "Epoch [915], train_loss: 1057.32 with loss1: 878.06, loss2: 179.26 and loss3: 35.47\n",
      "Epoch [916], train_loss: 1054.12 with loss1: 875.03, loss2: 179.09 and loss3: 34.62\n",
      "Epoch [917], train_loss: 1059.51 with loss1: 880.06, loss2: 179.45 and loss3: 33.78\n",
      "Epoch [918], train_loss: 1054.51 with loss1: 875.91, loss2: 178.60 and loss3: 32.95\n",
      "Epoch [919], train_loss: 1056.95 with loss1: 877.41, loss2: 179.54 and loss3: 32.14\n",
      "Epoch [920], train_loss: 1057.12 with loss1: 878.59, loss2: 178.53 and loss3: 31.33\n",
      "Epoch [921], train_loss: 1060.09 with loss1: 880.17, loss2: 179.92 and loss3: 30.53\n",
      "Epoch [922], train_loss: 1055.26 with loss1: 876.25, loss2: 179.01 and loss3: 29.75\n",
      "Epoch [923], train_loss: 1064.01 with loss1: 884.46, loss2: 179.55 and loss3: 28.97\n",
      "Epoch [924], train_loss: 1055.88 with loss1: 877.10, loss2: 178.78 and loss3: 28.21\n",
      "Epoch [925], train_loss: 1063.29 with loss1: 883.56, loss2: 179.72 and loss3: 27.46\n",
      "Epoch [926], train_loss: 1057.12 with loss1: 877.84, loss2: 179.28 and loss3: 26.72\n",
      "Epoch [927], train_loss: 1058.76 with loss1: 878.93, loss2: 179.83 and loss3: 25.99\n",
      "Epoch [928], train_loss: 1056.86 with loss1: 877.54, loss2: 179.32 and loss3: 25.27\n",
      "Epoch [929], train_loss: 1064.73 with loss1: 884.66, loss2: 180.06 and loss3: 24.57\n",
      "Epoch [930], train_loss: 1059.39 with loss1: 880.35, loss2: 179.04 and loss3: 23.89\n",
      "Epoch [931], train_loss: 1063.18 with loss1: 883.56, loss2: 179.62 and loss3: 23.21\n",
      "Epoch [932], train_loss: 1056.00 with loss1: 877.09, loss2: 178.91 and loss3: 22.55\n",
      "Epoch [933], train_loss: 1063.70 with loss1: 883.55, loss2: 180.15 and loss3: 21.89\n",
      "Epoch [934], train_loss: 1058.65 with loss1: 879.51, loss2: 179.14 and loss3: 21.24\n",
      "Epoch [935], train_loss: 1062.24 with loss1: 882.62, loss2: 179.62 and loss3: 20.60\n",
      "Epoch [936], train_loss: 1055.16 with loss1: 876.11, loss2: 179.06 and loss3: 19.97\n",
      "Epoch [937], train_loss: 1057.91 with loss1: 878.10, loss2: 179.80 and loss3: 19.35\n",
      "Epoch [938], train_loss: 1057.01 with loss1: 877.52, loss2: 179.50 and loss3: 18.74\n",
      "Epoch [939], train_loss: 1055.25 with loss1: 875.05, loss2: 180.20 and loss3: 18.14\n",
      "Epoch [940], train_loss: 1052.05 with loss1: 872.41, loss2: 179.65 and loss3: 17.54\n",
      "Epoch [941], train_loss: 1056.55 with loss1: 876.38, loss2: 180.17 and loss3: 16.95\n",
      "Epoch [942], train_loss: 1053.56 with loss1: 874.45, loss2: 179.11 and loss3: 16.37\n",
      "Epoch [943], train_loss: 1066.19 with loss1: 886.09, loss2: 180.10 and loss3: 15.81\n",
      "Epoch [944], train_loss: 1058.80 with loss1: 879.29, loss2: 179.51 and loss3: 15.25\n",
      "Epoch [945], train_loss: 1066.90 with loss1: 887.08, loss2: 179.82 and loss3: 14.71\n",
      "Epoch [946], train_loss: 1065.91 with loss1: 885.69, loss2: 180.21 and loss3: 14.18\n",
      "Epoch [947], train_loss: 1069.57 with loss1: 889.47, loss2: 180.10 and loss3: 13.66\n",
      "Epoch [948], train_loss: 1067.55 with loss1: 887.56, loss2: 180.00 and loss3: 13.15\n",
      "Epoch [949], train_loss: 1078.74 with loss1: 897.96, loss2: 180.78 and loss3: 12.65\n",
      "Epoch [950], train_loss: 1068.69 with loss1: 888.49, loss2: 180.20 and loss3: 12.16\n",
      "Epoch [951], train_loss: 1073.91 with loss1: 893.61, loss2: 180.29 and loss3: 11.68\n",
      "Epoch [952], train_loss: 1075.50 with loss1: 895.22, loss2: 180.29 and loss3: 11.20\n",
      "Epoch [953], train_loss: 1086.31 with loss1: 905.36, loss2: 180.95 and loss3: 10.74\n",
      "Epoch [954], train_loss: 1086.51 with loss1: 906.47, loss2: 180.05 and loss3: 10.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [955], train_loss: 1098.17 with loss1: 917.17, loss2: 181.00 and loss3: 9.85\n",
      "Epoch [956], train_loss: 1091.80 with loss1: 911.26, loss2: 180.54 and loss3: 9.42\n",
      "Epoch [957], train_loss: 1107.40 with loss1: 926.45, loss2: 180.95 and loss3: 9.00\n",
      "Epoch [958], train_loss: 1105.90 with loss1: 925.59, loss2: 180.31 and loss3: 8.60\n",
      "Epoch [959], train_loss: 1114.17 with loss1: 933.33, loss2: 180.84 and loss3: 8.20\n",
      "Epoch [960], train_loss: 1111.62 with loss1: 931.26, loss2: 180.36 and loss3: 7.82\n",
      "Epoch [961], train_loss: 1122.31 with loss1: 941.37, loss2: 180.94 and loss3: 7.45\n",
      "Epoch [962], train_loss: 1117.35 with loss1: 936.44, loss2: 180.91 and loss3: 7.09\n",
      "Epoch [963], train_loss: 1128.32 with loss1: 947.04, loss2: 181.28 and loss3: 6.74\n",
      "Epoch [964], train_loss: 1120.29 with loss1: 939.42, loss2: 180.86 and loss3: 6.39\n",
      "Epoch [965], train_loss: 1124.49 with loss1: 943.54, loss2: 180.96 and loss3: 6.05\n",
      "Epoch [966], train_loss: 1116.44 with loss1: 935.62, loss2: 180.82 and loss3: 5.72\n",
      "Epoch [967], train_loss: 1117.76 with loss1: 936.95, loss2: 180.81 and loss3: 5.41\n",
      "Epoch [968], train_loss: 1105.66 with loss1: 924.64, loss2: 181.02 and loss3: 5.11\n",
      "Epoch [969], train_loss: 1103.88 with loss1: 922.51, loss2: 181.37 and loss3: 4.81\n",
      "Epoch [970], train_loss: 1096.56 with loss1: 915.44, loss2: 181.12 and loss3: 4.53\n",
      "Epoch [971], train_loss: 1096.38 with loss1: 915.00, loss2: 181.38 and loss3: 4.26\n",
      "Epoch [972], train_loss: 1088.07 with loss1: 906.90, loss2: 181.17 and loss3: 3.99\n",
      "Epoch [973], train_loss: 1083.62 with loss1: 902.68, loss2: 180.94 and loss3: 3.74\n",
      "Epoch [974], train_loss: 1076.22 with loss1: 894.58, loss2: 181.64 and loss3: 3.49\n",
      "Epoch [975], train_loss: 1075.67 with loss1: 894.14, loss2: 181.53 and loss3: 3.25\n",
      "Epoch [976], train_loss: 1063.94 with loss1: 882.60, loss2: 181.34 and loss3: 3.03\n",
      "Epoch [977], train_loss: 1056.75 with loss1: 875.74, loss2: 181.01 and loss3: 2.81\n",
      "Epoch [978], train_loss: 1045.31 with loss1: 864.04, loss2: 181.27 and loss3: 2.60\n",
      "Epoch [979], train_loss: 1042.18 with loss1: 861.08, loss2: 181.10 and loss3: 2.40\n",
      "Epoch [980], train_loss: 1034.64 with loss1: 853.43, loss2: 181.20 and loss3: 2.22\n",
      "Epoch [981], train_loss: 1031.24 with loss1: 849.60, loss2: 181.64 and loss3: 2.04\n",
      "Epoch [982], train_loss: 1022.96 with loss1: 842.14, loss2: 180.82 and loss3: 1.87\n",
      "Epoch [983], train_loss: 1021.71 with loss1: 841.11, loss2: 180.60 and loss3: 1.71\n",
      "Epoch [984], train_loss: 1018.14 with loss1: 836.93, loss2: 181.21 and loss3: 1.55\n",
      "Epoch [985], train_loss: 1015.40 with loss1: 834.02, loss2: 181.38 and loss3: 1.40\n",
      "Epoch [986], train_loss: 1010.35 with loss1: 829.72, loss2: 180.63 and loss3: 1.27\n",
      "Epoch [987], train_loss: 1007.75 with loss1: 826.79, loss2: 180.96 and loss3: 1.14\n",
      "Epoch [988], train_loss: 1008.02 with loss1: 827.35, loss2: 180.67 and loss3: 1.02\n",
      "Epoch [989], train_loss: 1004.25 with loss1: 823.63, loss2: 180.62 and loss3: 0.91\n",
      "Epoch [990], train_loss: 1002.62 with loss1: 821.98, loss2: 180.64 and loss3: 0.81\n",
      "Epoch [991], train_loss: 998.04 with loss1: 818.16, loss2: 179.88 and loss3: 0.72\n",
      "Epoch [992], train_loss: 996.38 with loss1: 815.73, loss2: 180.65 and loss3: 0.64\n",
      "Epoch [993], train_loss: 996.47 with loss1: 816.01, loss2: 180.46 and loss3: 0.57\n",
      "Epoch [994], train_loss: 994.02 with loss1: 813.62, loss2: 180.40 and loss3: 0.50\n",
      "Epoch [995], train_loss: 996.48 with loss1: 816.23, loss2: 180.24 and loss3: 0.45\n",
      "Epoch [996], train_loss: 991.02 with loss1: 810.62, loss2: 180.39 and loss3: 0.40\n",
      "Epoch [997], train_loss: 992.72 with loss1: 812.97, loss2: 179.76 and loss3: 0.36\n",
      "Epoch [998], train_loss: 991.88 with loss1: 812.02, loss2: 179.86 and loss3: 0.32\n",
      "Epoch [999], train_loss: 992.60 with loss1: 813.14, loss2: 179.45 and loss3: 0.29\n",
      "Epoch [1000], train_loss: 996.89 with loss1: 817.90, loss2: 178.99 and loss3: 0.25\n",
      "Epoch [1001], train_loss: 994.43 with loss1: 815.00, loss2: 179.44 and loss3: 0.22\n",
      "Epoch [1002], train_loss: 1000.23 with loss1: 821.16, loss2: 179.08 and loss3: 0.20\n",
      "Epoch [1003], train_loss: 1000.19 with loss1: 821.00, loss2: 179.19 and loss3: 0.17\n",
      "Epoch [1004], train_loss: 1002.26 with loss1: 823.20, loss2: 179.07 and loss3: 0.16\n",
      "Epoch [1005], train_loss: 1001.97 with loss1: 822.94, loss2: 179.03 and loss3: 0.14\n",
      "Epoch [1006], train_loss: 1003.29 with loss1: 824.67, loss2: 178.62 and loss3: 0.12\n",
      "Epoch [1007], train_loss: 1002.14 with loss1: 823.63, loss2: 178.51 and loss3: 0.11\n",
      "Epoch [1008], train_loss: 1003.70 with loss1: 825.01, loss2: 178.69 and loss3: 0.10\n",
      "Epoch [1009], train_loss: 1001.97 with loss1: 823.90, loss2: 178.08 and loss3: 0.08\n",
      "Epoch [1010], train_loss: 1003.51 with loss1: 825.41, loss2: 178.10 and loss3: 0.07\n",
      "Epoch [1011], train_loss: 1004.40 with loss1: 826.07, loss2: 178.33 and loss3: 0.06\n",
      "Epoch [1012], train_loss: 1004.54 with loss1: 826.67, loss2: 177.87 and loss3: 0.06\n",
      "Epoch [1013], train_loss: 1008.57 with loss1: 831.06, loss2: 177.51 and loss3: 0.05\n",
      "Epoch [1014], train_loss: 1004.14 with loss1: 826.45, loss2: 177.69 and loss3: 0.04\n",
      "Epoch [1015], train_loss: 1003.45 with loss1: 826.04, loss2: 177.41 and loss3: 0.04\n",
      "Epoch [1016], train_loss: 997.63 with loss1: 820.14, loss2: 177.48 and loss3: 0.03\n",
      "Epoch [1017], train_loss: 1001.62 with loss1: 824.52, loss2: 177.11 and loss3: 0.03\n",
      "Epoch [1018], train_loss: 996.39 with loss1: 819.27, loss2: 177.12 and loss3: 0.03\n",
      "Epoch [1019], train_loss: 1000.43 with loss1: 823.60, loss2: 176.83 and loss3: 0.03\n",
      "Epoch [1020], train_loss: 995.55 with loss1: 818.37, loss2: 177.17 and loss3: 0.02\n",
      "Epoch [1021], train_loss: 991.06 with loss1: 814.13, loss2: 176.93 and loss3: 0.02\n",
      "Epoch [1022], train_loss: 990.38 with loss1: 813.80, loss2: 176.57 and loss3: 0.02\n",
      "Epoch [1023], train_loss: 992.37 with loss1: 815.91, loss2: 176.46 and loss3: 0.01\n",
      "Epoch [1024], train_loss: 990.15 with loss1: 813.82, loss2: 176.33 and loss3: 0.01\n",
      "Epoch [1025], train_loss: 991.13 with loss1: 814.86, loss2: 176.26 and loss3: 0.01\n",
      "Epoch [1026], train_loss: 987.31 with loss1: 811.32, loss2: 175.98 and loss3: 0.01\n",
      "Epoch [1027], train_loss: 990.92 with loss1: 814.83, loss2: 176.09 and loss3: 0.01\n",
      "Epoch [1028], train_loss: 991.50 with loss1: 815.57, loss2: 175.94 and loss3: 0.01\n",
      "Epoch [1029], train_loss: 993.96 with loss1: 818.65, loss2: 175.31 and loss3: 0.01\n",
      "Epoch [1030], train_loss: 995.58 with loss1: 820.15, loss2: 175.43 and loss3: 0.00\n",
      "Epoch [1031], train_loss: 999.62 with loss1: 823.76, loss2: 175.87 and loss3: 0.00\n",
      "Epoch [1032], train_loss: 996.44 with loss1: 820.58, loss2: 175.85 and loss3: 0.00\n",
      "Epoch [1033], train_loss: 1001.58 with loss1: 826.00, loss2: 175.59 and loss3: 0.00\n",
      "Epoch [1034], train_loss: 1000.93 with loss1: 826.50, loss2: 174.42 and loss3: 0.00\n",
      "Epoch [1035], train_loss: 1006.58 with loss1: 831.17, loss2: 175.41 and loss3: 0.00\n",
      "Epoch [1036], train_loss: 1006.46 with loss1: 832.00, loss2: 174.46 and loss3: 0.00\n",
      "Epoch [1037], train_loss: 1009.80 with loss1: 834.42, loss2: 175.37 and loss3: 0.00\n",
      "Epoch [1038], train_loss: 1007.33 with loss1: 832.81, loss2: 174.52 and loss3: 0.00\n",
      "Epoch [1039], train_loss: 1019.11 with loss1: 844.58, loss2: 174.53 and loss3: 0.00\n",
      "Epoch [1040], train_loss: 1019.63 with loss1: 845.56, loss2: 174.07 and loss3: 0.00\n",
      "Epoch [1041], train_loss: 1030.86 with loss1: 856.23, loss2: 174.62 and loss3: 0.00\n",
      "Epoch [1042], train_loss: 1031.11 with loss1: 857.39, loss2: 173.72 and loss3: 0.00\n",
      "Epoch [1043], train_loss: 1037.65 with loss1: 863.13, loss2: 174.52 and loss3: 0.00\n",
      "Epoch [1044], train_loss: 1035.34 with loss1: 862.14, loss2: 173.21 and loss3: 0.00\n",
      "Epoch [1045], train_loss: 1040.83 with loss1: 866.46, loss2: 174.37 and loss3: 0.00\n",
      "Epoch [1046], train_loss: 1040.49 with loss1: 867.22, loss2: 173.27 and loss3: 0.00\n",
      "Epoch [1047], train_loss: 1045.85 with loss1: 871.74, loss2: 174.11 and loss3: 0.00\n",
      "Epoch [1048], train_loss: 1045.50 with loss1: 872.47, loss2: 173.02 and loss3: 0.00\n",
      "Epoch [1049], train_loss: 1047.22 with loss1: 873.48, loss2: 173.74 and loss3: 0.00\n",
      "Epoch [1050], train_loss: 1046.68 with loss1: 873.94, loss2: 172.74 and loss3: 0.00\n",
      "Epoch [1051], train_loss: 1053.56 with loss1: 880.15, loss2: 173.41 and loss3: 0.00\n",
      "Epoch [1052], train_loss: 1043.16 with loss1: 870.48, loss2: 172.68 and loss3: 0.00\n",
      "Epoch [1053], train_loss: 1049.47 with loss1: 876.40, loss2: 173.07 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1054], train_loss: 1041.97 with loss1: 869.80, loss2: 172.17 and loss3: 0.00\n",
      "Epoch [1055], train_loss: 1042.08 with loss1: 869.57, loss2: 172.52 and loss3: 0.00\n",
      "Epoch [1056], train_loss: 1036.63 with loss1: 864.61, loss2: 172.02 and loss3: 0.00\n",
      "Epoch [1057], train_loss: 1040.98 with loss1: 868.47, loss2: 172.51 and loss3: 0.00\n",
      "Epoch [1058], train_loss: 1030.12 with loss1: 858.35, loss2: 171.77 and loss3: 0.00\n",
      "Epoch [1059], train_loss: 1039.59 with loss1: 867.17, loss2: 172.41 and loss3: 0.00\n",
      "Epoch [1060], train_loss: 1029.15 with loss1: 858.08, loss2: 171.07 and loss3: 0.00\n",
      "Epoch [1061], train_loss: 1032.82 with loss1: 860.80, loss2: 172.01 and loss3: 0.00\n",
      "Epoch [1062], train_loss: 1020.27 with loss1: 849.03, loss2: 171.24 and loss3: 0.00\n",
      "Epoch [1063], train_loss: 1022.22 with loss1: 850.61, loss2: 171.61 and loss3: 0.00\n",
      "Epoch [1064], train_loss: 1020.90 with loss1: 849.81, loss2: 171.09 and loss3: 0.00\n",
      "Epoch [1065], train_loss: 1020.98 with loss1: 849.08, loss2: 171.90 and loss3: 0.00\n",
      "Epoch [1066], train_loss: 1015.69 with loss1: 844.96, loss2: 170.73 and loss3: 0.00\n",
      "Epoch [1067], train_loss: 1018.06 with loss1: 846.27, loss2: 171.78 and loss3: 0.00\n",
      "Epoch [1068], train_loss: 1014.58 with loss1: 843.75, loss2: 170.83 and loss3: 0.00\n",
      "Epoch [1069], train_loss: 1017.86 with loss1: 846.61, loss2: 171.25 and loss3: 0.00\n",
      "Epoch [1070], train_loss: 1010.34 with loss1: 840.05, loss2: 170.29 and loss3: 0.00\n",
      "Epoch [1071], train_loss: 1011.67 with loss1: 840.39, loss2: 171.28 and loss3: 0.00\n",
      "Epoch [1072], train_loss: 1004.94 with loss1: 834.65, loss2: 170.30 and loss3: 0.00\n",
      "Epoch [1073], train_loss: 1006.19 with loss1: 835.34, loss2: 170.85 and loss3: 0.00\n",
      "Epoch [1074], train_loss: 1001.40 with loss1: 831.35, loss2: 170.04 and loss3: 0.00\n",
      "Epoch [1075], train_loss: 1002.43 with loss1: 831.79, loss2: 170.64 and loss3: 0.00\n",
      "Epoch [1076], train_loss: 996.72 with loss1: 826.51, loss2: 170.21 and loss3: 0.00\n",
      "Epoch [1077], train_loss: 1002.25 with loss1: 831.45, loss2: 170.81 and loss3: 0.00\n",
      "Epoch [1078], train_loss: 997.41 with loss1: 827.39, loss2: 170.03 and loss3: 0.00\n",
      "Epoch [1079], train_loss: 998.46 with loss1: 828.00, loss2: 170.46 and loss3: 0.00\n",
      "Epoch [1080], train_loss: 991.48 with loss1: 822.04, loss2: 169.44 and loss3: 0.00\n",
      "Epoch [1081], train_loss: 996.13 with loss1: 826.12, loss2: 170.01 and loss3: 0.00\n",
      "Epoch [1082], train_loss: 993.09 with loss1: 823.78, loss2: 169.30 and loss3: 0.00\n",
      "Epoch [1083], train_loss: 996.18 with loss1: 826.34, loss2: 169.85 and loss3: 0.00\n",
      "Epoch [1084], train_loss: 993.78 with loss1: 824.03, loss2: 169.75 and loss3: 0.00\n",
      "Epoch [1085], train_loss: 997.09 with loss1: 827.10, loss2: 169.99 and loss3: 0.00\n",
      "Epoch [1086], train_loss: 994.14 with loss1: 824.87, loss2: 169.27 and loss3: 0.00\n",
      "Epoch [1087], train_loss: 998.37 with loss1: 828.73, loss2: 169.65 and loss3: 0.00\n",
      "Epoch [1088], train_loss: 991.82 with loss1: 822.63, loss2: 169.18 and loss3: 0.00\n",
      "Epoch [1089], train_loss: 993.25 with loss1: 823.09, loss2: 170.15 and loss3: 0.00\n",
      "Epoch [1090], train_loss: 990.92 with loss1: 822.21, loss2: 168.72 and loss3: 0.00\n",
      "Epoch [1091], train_loss: 996.88 with loss1: 827.50, loss2: 169.38 and loss3: 0.00\n",
      "Epoch [1092], train_loss: 991.23 with loss1: 822.32, loss2: 168.91 and loss3: 0.00\n",
      "Epoch [1093], train_loss: 994.46 with loss1: 825.18, loss2: 169.28 and loss3: 0.00\n",
      "Epoch [1094], train_loss: 988.51 with loss1: 819.54, loss2: 168.97 and loss3: 0.00\n",
      "Epoch [1095], train_loss: 993.00 with loss1: 823.84, loss2: 169.16 and loss3: 0.00\n",
      "Epoch [1096], train_loss: 992.10 with loss1: 823.44, loss2: 168.66 and loss3: 0.00\n",
      "Epoch [1097], train_loss: 996.81 with loss1: 827.84, loss2: 168.96 and loss3: 0.00\n",
      "Epoch [1098], train_loss: 992.38 with loss1: 824.10, loss2: 168.28 and loss3: 0.00\n",
      "Epoch [1099], train_loss: 997.48 with loss1: 828.79, loss2: 168.70 and loss3: 0.00\n",
      "Epoch [1100], train_loss: 992.76 with loss1: 824.74, loss2: 168.01 and loss3: 0.00\n",
      "Epoch [1101], train_loss: 996.12 with loss1: 827.72, loss2: 168.40 and loss3: 0.00\n",
      "Epoch [1102], train_loss: 988.67 with loss1: 820.35, loss2: 168.32 and loss3: 0.00\n",
      "Epoch [1103], train_loss: 990.43 with loss1: 821.82, loss2: 168.61 and loss3: 0.00\n",
      "Epoch [1104], train_loss: 984.30 with loss1: 816.43, loss2: 167.87 and loss3: 0.00\n",
      "Epoch [1105], train_loss: 982.17 with loss1: 813.97, loss2: 168.20 and loss3: 0.00\n",
      "Epoch [1106], train_loss: 978.21 with loss1: 811.11, loss2: 167.10 and loss3: 0.00\n",
      "Epoch [1107], train_loss: 974.50 with loss1: 806.25, loss2: 168.25 and loss3: 0.00\n",
      "Epoch [1108], train_loss: 971.06 with loss1: 803.56, loss2: 167.50 and loss3: 0.00\n",
      "Epoch [1109], train_loss: 970.15 with loss1: 802.64, loss2: 167.52 and loss3: 0.00\n",
      "Epoch [1110], train_loss: 963.31 with loss1: 796.06, loss2: 167.25 and loss3: 0.00\n",
      "Epoch [1111], train_loss: 958.36 with loss1: 790.86, loss2: 167.51 and loss3: 0.00\n",
      "Epoch [1112], train_loss: 952.99 with loss1: 785.74, loss2: 167.25 and loss3: 0.00\n",
      "Epoch [1113], train_loss: 953.24 with loss1: 786.09, loss2: 167.15 and loss3: 0.00\n",
      "Epoch [1114], train_loss: 948.74 with loss1: 781.30, loss2: 167.44 and loss3: 0.00\n",
      "Epoch [1115], train_loss: 947.19 with loss1: 780.19, loss2: 167.00 and loss3: 0.00\n",
      "Epoch [1116], train_loss: 939.06 with loss1: 772.64, loss2: 166.42 and loss3: 0.00\n",
      "Epoch [1117], train_loss: 939.40 with loss1: 772.81, loss2: 166.59 and loss3: 0.00\n",
      "Epoch [1118], train_loss: 936.58 with loss1: 770.00, loss2: 166.58 and loss3: 0.00\n",
      "Epoch [1119], train_loss: 936.34 with loss1: 769.82, loss2: 166.52 and loss3: 0.00\n",
      "Epoch [1120], train_loss: 929.70 with loss1: 762.79, loss2: 166.91 and loss3: 0.00\n",
      "Epoch [1121], train_loss: 930.59 with loss1: 764.55, loss2: 166.04 and loss3: 0.00\n",
      "Epoch [1122], train_loss: 925.94 with loss1: 759.68, loss2: 166.26 and loss3: 0.00\n",
      "Epoch [1123], train_loss: 923.71 with loss1: 757.47, loss2: 166.24 and loss3: 0.00\n",
      "Epoch [1124], train_loss: 921.20 with loss1: 755.42, loss2: 165.78 and loss3: 0.00\n",
      "Epoch [1125], train_loss: 918.95 with loss1: 753.17, loss2: 165.78 and loss3: 0.00\n",
      "Epoch [1126], train_loss: 916.47 with loss1: 750.90, loss2: 165.57 and loss3: 0.00\n",
      "Epoch [1127], train_loss: 912.99 with loss1: 747.18, loss2: 165.81 and loss3: 0.00\n",
      "Epoch [1128], train_loss: 911.75 with loss1: 746.48, loss2: 165.26 and loss3: 0.00\n",
      "Epoch [1129], train_loss: 911.91 with loss1: 746.16, loss2: 165.75 and loss3: 0.00\n",
      "Epoch [1130], train_loss: 910.37 with loss1: 745.60, loss2: 164.77 and loss3: 0.00\n",
      "Epoch [1131], train_loss: 912.48 with loss1: 747.28, loss2: 165.20 and loss3: 0.00\n",
      "Epoch [1132], train_loss: 910.55 with loss1: 745.65, loss2: 164.90 and loss3: 0.00\n",
      "Epoch [1133], train_loss: 911.51 with loss1: 746.46, loss2: 165.05 and loss3: 0.00\n",
      "Epoch [1134], train_loss: 907.48 with loss1: 742.79, loss2: 164.68 and loss3: 0.00\n",
      "Epoch [1135], train_loss: 909.71 with loss1: 745.07, loss2: 164.64 and loss3: 0.00\n",
      "Epoch [1136], train_loss: 910.65 with loss1: 745.95, loss2: 164.70 and loss3: 0.00\n",
      "Epoch [1137], train_loss: 907.70 with loss1: 743.70, loss2: 164.01 and loss3: 0.00\n",
      "Epoch [1138], train_loss: 904.57 with loss1: 740.33, loss2: 164.24 and loss3: 0.00\n",
      "Epoch [1139], train_loss: 905.48 with loss1: 741.66, loss2: 163.82 and loss3: 0.00\n",
      "Epoch [1140], train_loss: 906.75 with loss1: 743.07, loss2: 163.69 and loss3: 0.00\n",
      "Epoch [1141], train_loss: 908.87 with loss1: 744.85, loss2: 164.01 and loss3: 0.00\n",
      "Epoch [1142], train_loss: 909.20 with loss1: 745.54, loss2: 163.66 and loss3: 0.00\n",
      "Epoch [1143], train_loss: 908.35 with loss1: 744.37, loss2: 163.98 and loss3: 0.00\n",
      "Epoch [1144], train_loss: 911.62 with loss1: 747.78, loss2: 163.84 and loss3: 0.00\n",
      "Epoch [1145], train_loss: 909.84 with loss1: 745.97, loss2: 163.87 and loss3: 0.00\n",
      "Epoch [1146], train_loss: 911.38 with loss1: 747.80, loss2: 163.58 and loss3: 0.00\n",
      "Epoch [1147], train_loss: 909.29 with loss1: 745.90, loss2: 163.39 and loss3: 0.00\n",
      "Epoch [1148], train_loss: 909.80 with loss1: 746.95, loss2: 162.85 and loss3: 0.00\n",
      "Epoch [1149], train_loss: 907.15 with loss1: 743.46, loss2: 163.69 and loss3: 0.00\n",
      "Epoch [1150], train_loss: 908.22 with loss1: 745.01, loss2: 163.21 and loss3: 0.00\n",
      "Epoch [1151], train_loss: 909.31 with loss1: 745.96, loss2: 163.35 and loss3: 0.00\n",
      "Epoch [1152], train_loss: 909.33 with loss1: 745.89, loss2: 163.44 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1153], train_loss: 906.78 with loss1: 743.99, loss2: 162.79 and loss3: 0.00\n",
      "Epoch [1154], train_loss: 909.63 with loss1: 746.47, loss2: 163.16 and loss3: 0.00\n",
      "Epoch [1155], train_loss: 908.64 with loss1: 746.39, loss2: 162.25 and loss3: 0.00\n",
      "Epoch [1156], train_loss: 907.09 with loss1: 744.81, loss2: 162.28 and loss3: 0.00\n",
      "Epoch [1157], train_loss: 909.66 with loss1: 747.52, loss2: 162.14 and loss3: 0.00\n",
      "Epoch [1158], train_loss: 908.69 with loss1: 746.49, loss2: 162.20 and loss3: 0.00\n",
      "Epoch [1159], train_loss: 908.75 with loss1: 746.12, loss2: 162.63 and loss3: 0.00\n",
      "Epoch [1160], train_loss: 906.52 with loss1: 744.06, loss2: 162.46 and loss3: 0.00\n",
      "Epoch [1161], train_loss: 906.78 with loss1: 744.51, loss2: 162.27 and loss3: 0.00\n",
      "Epoch [1162], train_loss: 905.31 with loss1: 743.29, loss2: 162.01 and loss3: 0.00\n",
      "Epoch [1163], train_loss: 903.79 with loss1: 741.54, loss2: 162.25 and loss3: 0.00\n",
      "Epoch [1164], train_loss: 901.04 with loss1: 739.41, loss2: 161.63 and loss3: 0.00\n",
      "Epoch [1165], train_loss: 904.66 with loss1: 742.66, loss2: 162.00 and loss3: 0.00\n",
      "Epoch [1166], train_loss: 899.97 with loss1: 738.55, loss2: 161.43 and loss3: 0.00\n",
      "Epoch [1167], train_loss: 899.96 with loss1: 738.85, loss2: 161.12 and loss3: 0.00\n",
      "Epoch [1168], train_loss: 902.81 with loss1: 741.52, loss2: 161.29 and loss3: 0.00\n",
      "Epoch [1169], train_loss: 904.15 with loss1: 743.05, loss2: 161.11 and loss3: 0.00\n",
      "Epoch [1170], train_loss: 905.24 with loss1: 744.16, loss2: 161.08 and loss3: 0.00\n",
      "Epoch [1171], train_loss: 905.89 with loss1: 745.07, loss2: 160.82 and loss3: 0.00\n",
      "Epoch [1172], train_loss: 909.40 with loss1: 747.90, loss2: 161.50 and loss3: 0.00\n",
      "Epoch [1173], train_loss: 909.47 with loss1: 748.26, loss2: 161.21 and loss3: 0.00\n",
      "Epoch [1174], train_loss: 912.71 with loss1: 751.53, loss2: 161.18 and loss3: 0.00\n",
      "Epoch [1175], train_loss: 918.70 with loss1: 757.87, loss2: 160.84 and loss3: 0.00\n",
      "Epoch [1176], train_loss: 922.12 with loss1: 761.12, loss2: 161.00 and loss3: 0.00\n",
      "Epoch [1177], train_loss: 927.07 with loss1: 766.78, loss2: 160.29 and loss3: 0.00\n",
      "Epoch [1178], train_loss: 939.25 with loss1: 778.48, loss2: 160.77 and loss3: 0.00\n",
      "Epoch [1179], train_loss: 946.30 with loss1: 785.50, loss2: 160.80 and loss3: 0.00\n",
      "Epoch [1180], train_loss: 960.57 with loss1: 799.33, loss2: 161.23 and loss3: 0.00\n",
      "Epoch [1181], train_loss: 966.61 with loss1: 806.64, loss2: 159.97 and loss3: 0.00\n",
      "Epoch [1182], train_loss: 982.13 with loss1: 821.06, loss2: 161.07 and loss3: 0.00\n",
      "Epoch [1183], train_loss: 991.78 with loss1: 831.57, loss2: 160.20 and loss3: 0.00\n",
      "Epoch [1184], train_loss: 1011.83 with loss1: 851.03, loss2: 160.80 and loss3: 0.00\n",
      "Epoch [1185], train_loss: 1021.48 with loss1: 861.34, loss2: 160.14 and loss3: 0.00\n",
      "Epoch [1186], train_loss: 1038.53 with loss1: 877.65, loss2: 160.88 and loss3: 0.00\n",
      "Epoch [1187], train_loss: 1044.67 with loss1: 884.93, loss2: 159.74 and loss3: 0.00\n",
      "Epoch [1188], train_loss: 1069.68 with loss1: 908.99, loss2: 160.69 and loss3: 0.00\n",
      "Epoch [1189], train_loss: 1066.69 with loss1: 906.70, loss2: 159.99 and loss3: 0.00\n",
      "Epoch [1190], train_loss: 1082.53 with loss1: 922.08, loss2: 160.45 and loss3: 0.00\n",
      "Epoch [1191], train_loss: 1078.36 with loss1: 918.61, loss2: 159.75 and loss3: 0.00\n",
      "Epoch [1192], train_loss: 1092.05 with loss1: 931.57, loss2: 160.47 and loss3: 0.00\n",
      "Epoch [1193], train_loss: 1071.85 with loss1: 912.35, loss2: 159.51 and loss3: 0.00\n",
      "Epoch [1194], train_loss: 1079.00 with loss1: 918.45, loss2: 160.54 and loss3: 0.00\n",
      "Epoch [1195], train_loss: 1057.15 with loss1: 897.90, loss2: 159.25 and loss3: 0.00\n",
      "Epoch [1196], train_loss: 1057.47 with loss1: 897.19, loss2: 160.28 and loss3: 0.00\n",
      "Epoch [1197], train_loss: 1032.81 with loss1: 873.40, loss2: 159.41 and loss3: 0.00\n",
      "Epoch [1198], train_loss: 1029.27 with loss1: 868.69, loss2: 160.58 and loss3: 0.00\n",
      "Epoch [1199], train_loss: 1006.50 with loss1: 847.42, loss2: 159.07 and loss3: 0.00\n",
      "Epoch [1200], train_loss: 1002.33 with loss1: 842.61, loss2: 159.72 and loss3: 0.00\n",
      "Epoch [1201], train_loss: 983.77 with loss1: 825.06, loss2: 158.72 and loss3: 0.00\n",
      "Epoch [1202], train_loss: 978.19 with loss1: 818.46, loss2: 159.73 and loss3: 0.00\n",
      "Epoch [1203], train_loss: 961.69 with loss1: 802.72, loss2: 158.97 and loss3: 0.00\n",
      "Epoch [1204], train_loss: 962.07 with loss1: 802.72, loss2: 159.35 and loss3: 0.00\n",
      "Epoch [1205], train_loss: 943.86 with loss1: 785.40, loss2: 158.46 and loss3: 0.00\n",
      "Epoch [1206], train_loss: 944.18 with loss1: 785.05, loss2: 159.13 and loss3: 0.00\n",
      "Epoch [1207], train_loss: 932.82 with loss1: 774.36, loss2: 158.46 and loss3: 0.00\n",
      "Epoch [1208], train_loss: 931.23 with loss1: 772.43, loss2: 158.80 and loss3: 0.00\n",
      "Epoch [1209], train_loss: 923.08 with loss1: 765.01, loss2: 158.07 and loss3: 0.00\n",
      "Epoch [1210], train_loss: 919.81 with loss1: 761.25, loss2: 158.56 and loss3: 0.00\n",
      "Epoch [1211], train_loss: 913.27 with loss1: 755.23, loss2: 158.04 and loss3: 0.00\n",
      "Epoch [1212], train_loss: 914.01 with loss1: 755.62, loss2: 158.39 and loss3: 0.00\n",
      "Epoch [1213], train_loss: 906.22 with loss1: 748.50, loss2: 157.72 and loss3: 0.00\n",
      "Epoch [1214], train_loss: 905.58 with loss1: 748.01, loss2: 157.57 and loss3: 0.00\n",
      "Epoch [1215], train_loss: 904.87 with loss1: 747.46, loss2: 157.42 and loss3: 0.00\n",
      "Epoch [1216], train_loss: 903.46 with loss1: 745.96, loss2: 157.50 and loss3: 0.00\n",
      "Epoch [1217], train_loss: 895.88 with loss1: 738.57, loss2: 157.31 and loss3: 0.00\n",
      "Epoch [1218], train_loss: 899.35 with loss1: 741.63, loss2: 157.73 and loss3: 0.00\n",
      "Epoch [1219], train_loss: 892.57 with loss1: 735.65, loss2: 156.93 and loss3: 0.00\n",
      "Epoch [1220], train_loss: 895.16 with loss1: 737.94, loss2: 157.22 and loss3: 0.00\n",
      "Epoch [1221], train_loss: 891.18 with loss1: 734.56, loss2: 156.62 and loss3: 0.00\n",
      "Epoch [1222], train_loss: 892.25 with loss1: 735.25, loss2: 157.00 and loss3: 0.00\n",
      "Epoch [1223], train_loss: 888.22 with loss1: 731.47, loss2: 156.75 and loss3: 0.00\n",
      "Epoch [1224], train_loss: 890.15 with loss1: 733.53, loss2: 156.62 and loss3: 0.00\n",
      "Epoch [1225], train_loss: 889.69 with loss1: 733.20, loss2: 156.49 and loss3: 0.00\n",
      "Epoch [1226], train_loss: 891.83 with loss1: 735.37, loss2: 156.47 and loss3: 0.00\n",
      "Epoch [1227], train_loss: 889.33 with loss1: 733.15, loss2: 156.18 and loss3: 0.00\n",
      "Epoch [1228], train_loss: 892.17 with loss1: 735.97, loss2: 156.20 and loss3: 0.00\n",
      "Epoch [1229], train_loss: 890.21 with loss1: 734.73, loss2: 155.49 and loss3: 0.00\n",
      "Epoch [1230], train_loss: 892.13 with loss1: 735.51, loss2: 156.62 and loss3: 0.00\n",
      "Epoch [1231], train_loss: 887.47 with loss1: 731.70, loss2: 155.77 and loss3: 0.00\n",
      "Epoch [1232], train_loss: 892.08 with loss1: 736.13, loss2: 155.95 and loss3: 0.00\n",
      "Epoch [1233], train_loss: 886.94 with loss1: 731.59, loss2: 155.35 and loss3: 0.00\n",
      "Epoch [1234], train_loss: 888.57 with loss1: 732.60, loss2: 155.97 and loss3: 0.00\n",
      "Epoch [1235], train_loss: 885.89 with loss1: 730.64, loss2: 155.25 and loss3: 0.00\n",
      "Epoch [1236], train_loss: 887.18 with loss1: 731.55, loss2: 155.63 and loss3: 0.00\n",
      "Epoch [1237], train_loss: 888.83 with loss1: 733.48, loss2: 155.35 and loss3: 0.00\n",
      "Epoch [1238], train_loss: 891.44 with loss1: 735.97, loss2: 155.46 and loss3: 0.00\n",
      "Epoch [1239], train_loss: 889.93 with loss1: 734.54, loss2: 155.39 and loss3: 0.00\n",
      "Epoch [1240], train_loss: 891.12 with loss1: 735.92, loss2: 155.21 and loss3: 0.00\n",
      "Epoch [1241], train_loss: 890.93 with loss1: 736.39, loss2: 154.54 and loss3: 0.00\n",
      "Epoch [1242], train_loss: 896.39 with loss1: 741.10, loss2: 155.29 and loss3: 0.00\n",
      "Epoch [1243], train_loss: 886.42 with loss1: 731.82, loss2: 154.61 and loss3: 0.00\n",
      "Epoch [1244], train_loss: 892.36 with loss1: 737.48, loss2: 154.88 and loss3: 0.00\n",
      "Epoch [1245], train_loss: 891.03 with loss1: 736.43, loss2: 154.60 and loss3: 0.00\n",
      "Epoch [1246], train_loss: 890.39 with loss1: 735.40, loss2: 154.99 and loss3: 0.00\n",
      "Epoch [1247], train_loss: 888.79 with loss1: 734.57, loss2: 154.22 and loss3: 0.00\n",
      "Epoch [1248], train_loss: 891.50 with loss1: 737.26, loss2: 154.25 and loss3: 0.00\n",
      "Epoch [1249], train_loss: 890.31 with loss1: 735.96, loss2: 154.35 and loss3: 0.00\n",
      "Epoch [1250], train_loss: 893.28 with loss1: 738.74, loss2: 154.54 and loss3: 0.00\n",
      "Epoch [1251], train_loss: 889.50 with loss1: 735.26, loss2: 154.24 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1252], train_loss: 894.02 with loss1: 739.98, loss2: 154.04 and loss3: 0.00\n",
      "Epoch [1253], train_loss: 889.95 with loss1: 735.85, loss2: 154.10 and loss3: 0.00\n",
      "Epoch [1254], train_loss: 895.19 with loss1: 741.11, loss2: 154.08 and loss3: 0.00\n",
      "Epoch [1255], train_loss: 894.38 with loss1: 740.64, loss2: 153.74 and loss3: 0.00\n",
      "Epoch [1256], train_loss: 896.27 with loss1: 742.41, loss2: 153.86 and loss3: 0.00\n",
      "Epoch [1257], train_loss: 893.27 with loss1: 739.78, loss2: 153.48 and loss3: 0.00\n",
      "Epoch [1258], train_loss: 900.76 with loss1: 747.29, loss2: 153.47 and loss3: 0.00\n",
      "Epoch [1259], train_loss: 898.86 with loss1: 745.59, loss2: 153.27 and loss3: 0.00\n",
      "Epoch [1260], train_loss: 902.12 with loss1: 748.41, loss2: 153.71 and loss3: 0.00\n",
      "Epoch [1261], train_loss: 896.40 with loss1: 742.81, loss2: 153.59 and loss3: 0.00\n",
      "Epoch [1262], train_loss: 900.95 with loss1: 747.30, loss2: 153.65 and loss3: 0.00\n",
      "Epoch [1263], train_loss: 898.20 with loss1: 745.06, loss2: 153.13 and loss3: 0.00\n",
      "Epoch [1264], train_loss: 905.15 with loss1: 751.78, loss2: 153.37 and loss3: 0.00\n",
      "Epoch [1265], train_loss: 898.83 with loss1: 745.70, loss2: 153.13 and loss3: 0.00\n",
      "Epoch [1266], train_loss: 900.50 with loss1: 747.36, loss2: 153.13 and loss3: 0.00\n",
      "Epoch [1267], train_loss: 897.33 with loss1: 744.36, loss2: 152.97 and loss3: 0.00\n",
      "Epoch [1268], train_loss: 900.79 with loss1: 747.39, loss2: 153.39 and loss3: 0.00\n",
      "Epoch [1269], train_loss: 897.15 with loss1: 744.66, loss2: 152.48 and loss3: 0.00\n",
      "Epoch [1270], train_loss: 900.93 with loss1: 747.52, loss2: 153.41 and loss3: 0.00\n",
      "Epoch [1271], train_loss: 895.04 with loss1: 742.38, loss2: 152.66 and loss3: 0.00\n",
      "Epoch [1272], train_loss: 896.09 with loss1: 743.10, loss2: 152.99 and loss3: 0.00\n",
      "Epoch [1273], train_loss: 892.86 with loss1: 740.35, loss2: 152.50 and loss3: 0.00\n",
      "Epoch [1274], train_loss: 892.97 with loss1: 740.23, loss2: 152.74 and loss3: 0.00\n",
      "Epoch [1275], train_loss: 886.05 with loss1: 733.37, loss2: 152.67 and loss3: 0.00\n",
      "Epoch [1276], train_loss: 888.96 with loss1: 736.22, loss2: 152.73 and loss3: 0.00\n",
      "Epoch [1277], train_loss: 883.16 with loss1: 730.88, loss2: 152.27 and loss3: 0.00\n",
      "Epoch [1278], train_loss: 887.75 with loss1: 735.29, loss2: 152.46 and loss3: 0.00\n",
      "Epoch [1279], train_loss: 884.62 with loss1: 732.67, loss2: 151.95 and loss3: 0.00\n",
      "Epoch [1280], train_loss: 887.61 with loss1: 735.10, loss2: 152.51 and loss3: 0.00\n",
      "Epoch [1281], train_loss: 882.41 with loss1: 730.54, loss2: 151.87 and loss3: 0.00\n",
      "Epoch [1282], train_loss: 885.21 with loss1: 732.79, loss2: 152.42 and loss3: 0.00\n",
      "Epoch [1283], train_loss: 882.72 with loss1: 731.10, loss2: 151.62 and loss3: 0.00\n",
      "Epoch [1284], train_loss: 882.54 with loss1: 730.52, loss2: 152.03 and loss3: 0.00\n",
      "Epoch [1285], train_loss: 879.51 with loss1: 727.69, loss2: 151.82 and loss3: 0.00\n",
      "Epoch [1286], train_loss: 883.54 with loss1: 731.33, loss2: 152.21 and loss3: 0.00\n",
      "Epoch [1287], train_loss: 877.84 with loss1: 726.57, loss2: 151.27 and loss3: 0.00\n",
      "Epoch [1288], train_loss: 880.19 with loss1: 728.36, loss2: 151.83 and loss3: 0.00\n",
      "Epoch [1289], train_loss: 878.89 with loss1: 727.69, loss2: 151.20 and loss3: 0.00\n",
      "Epoch [1290], train_loss: 881.34 with loss1: 729.44, loss2: 151.90 and loss3: 0.00\n",
      "Epoch [1291], train_loss: 876.08 with loss1: 725.10, loss2: 150.98 and loss3: 0.00\n",
      "Epoch [1292], train_loss: 875.82 with loss1: 724.37, loss2: 151.45 and loss3: 0.00\n",
      "Epoch [1293], train_loss: 876.18 with loss1: 725.29, loss2: 150.90 and loss3: 0.00\n",
      "Epoch [1294], train_loss: 876.71 with loss1: 725.39, loss2: 151.31 and loss3: 0.00\n",
      "Epoch [1295], train_loss: 874.35 with loss1: 723.57, loss2: 150.78 and loss3: 0.00\n",
      "Epoch [1296], train_loss: 873.90 with loss1: 722.80, loss2: 151.11 and loss3: 0.00\n",
      "Epoch [1297], train_loss: 872.00 with loss1: 721.63, loss2: 150.37 and loss3: 0.00\n",
      "Epoch [1298], train_loss: 872.47 with loss1: 721.61, loss2: 150.86 and loss3: 0.00\n",
      "Epoch [1299], train_loss: 872.46 with loss1: 721.73, loss2: 150.73 and loss3: 0.00\n",
      "Epoch [1300], train_loss: 874.42 with loss1: 723.39, loss2: 151.04 and loss3: 0.00\n",
      "Epoch [1301], train_loss: 871.81 with loss1: 721.45, loss2: 150.37 and loss3: 0.00\n",
      "Epoch [1302], train_loss: 873.18 with loss1: 722.67, loss2: 150.52 and loss3: 0.00\n",
      "Epoch [1303], train_loss: 876.36 with loss1: 726.47, loss2: 149.89 and loss3: 0.00\n",
      "Epoch [1304], train_loss: 876.21 with loss1: 725.48, loss2: 150.73 and loss3: 0.00\n",
      "Epoch [1305], train_loss: 871.26 with loss1: 721.08, loss2: 150.18 and loss3: 0.00\n",
      "Epoch [1306], train_loss: 874.80 with loss1: 724.64, loss2: 150.16 and loss3: 0.00\n",
      "Epoch [1307], train_loss: 871.88 with loss1: 721.56, loss2: 150.32 and loss3: 0.00\n",
      "Epoch [1308], train_loss: 878.22 with loss1: 728.25, loss2: 149.97 and loss3: 0.00\n",
      "Epoch [1309], train_loss: 874.78 with loss1: 724.91, loss2: 149.87 and loss3: 0.00\n",
      "Epoch [1310], train_loss: 878.88 with loss1: 728.83, loss2: 150.05 and loss3: 0.00\n",
      "Epoch [1311], train_loss: 876.24 with loss1: 726.81, loss2: 149.43 and loss3: 0.00\n",
      "Epoch [1312], train_loss: 879.14 with loss1: 729.35, loss2: 149.79 and loss3: 0.00\n",
      "Epoch [1313], train_loss: 879.16 with loss1: 729.74, loss2: 149.41 and loss3: 0.00\n",
      "Epoch [1314], train_loss: 880.89 with loss1: 731.30, loss2: 149.59 and loss3: 0.00\n",
      "Epoch [1315], train_loss: 880.71 with loss1: 731.56, loss2: 149.15 and loss3: 0.00\n",
      "Epoch [1316], train_loss: 886.00 with loss1: 736.79, loss2: 149.21 and loss3: 0.00\n",
      "Epoch [1317], train_loss: 883.70 with loss1: 733.89, loss2: 149.81 and loss3: 0.00\n",
      "Epoch [1318], train_loss: 887.02 with loss1: 737.62, loss2: 149.40 and loss3: 0.00\n",
      "Epoch [1319], train_loss: 884.97 with loss1: 735.70, loss2: 149.26 and loss3: 0.00\n",
      "Epoch [1320], train_loss: 889.18 with loss1: 739.75, loss2: 149.43 and loss3: 0.00\n",
      "Epoch [1321], train_loss: 886.09 with loss1: 737.20, loss2: 148.89 and loss3: 0.00\n",
      "Epoch [1322], train_loss: 893.48 with loss1: 744.06, loss2: 149.42 and loss3: 0.00\n",
      "Epoch [1323], train_loss: 892.21 with loss1: 743.22, loss2: 148.99 and loss3: 0.00\n",
      "Epoch [1324], train_loss: 897.95 with loss1: 748.16, loss2: 149.79 and loss3: 0.00\n",
      "Epoch [1325], train_loss: 894.03 with loss1: 745.41, loss2: 148.62 and loss3: 0.00\n",
      "Epoch [1326], train_loss: 899.32 with loss1: 749.72, loss2: 149.61 and loss3: 0.00\n",
      "Epoch [1327], train_loss: 896.66 with loss1: 747.89, loss2: 148.76 and loss3: 0.00\n",
      "Epoch [1328], train_loss: 901.81 with loss1: 752.27, loss2: 149.53 and loss3: 0.00\n",
      "Epoch [1329], train_loss: 901.28 with loss1: 752.45, loss2: 148.83 and loss3: 0.00\n",
      "Epoch [1330], train_loss: 902.70 with loss1: 753.54, loss2: 149.16 and loss3: 0.00\n",
      "Epoch [1331], train_loss: 900.69 with loss1: 751.47, loss2: 149.22 and loss3: 0.00\n",
      "Epoch [1332], train_loss: 902.67 with loss1: 753.60, loss2: 149.07 and loss3: 0.00\n",
      "Epoch [1333], train_loss: 899.14 with loss1: 750.66, loss2: 148.48 and loss3: 0.00\n",
      "Epoch [1334], train_loss: 899.15 with loss1: 749.79, loss2: 149.36 and loss3: 0.00\n",
      "Epoch [1335], train_loss: 890.36 with loss1: 741.31, loss2: 149.05 and loss3: 0.00\n",
      "Epoch [1336], train_loss: 889.10 with loss1: 739.99, loss2: 149.11 and loss3: 0.00\n",
      "Epoch [1337], train_loss: 885.89 with loss1: 737.16, loss2: 148.73 and loss3: 0.00\n",
      "Epoch [1338], train_loss: 881.57 with loss1: 732.74, loss2: 148.83 and loss3: 0.00\n",
      "Epoch [1339], train_loss: 871.81 with loss1: 722.87, loss2: 148.94 and loss3: 0.00\n",
      "Epoch [1340], train_loss: 868.84 with loss1: 720.12, loss2: 148.71 and loss3: 0.00\n",
      "Epoch [1341], train_loss: 862.55 with loss1: 714.10, loss2: 148.45 and loss3: 0.00\n",
      "Epoch [1342], train_loss: 862.05 with loss1: 713.27, loss2: 148.77 and loss3: 0.00\n",
      "Epoch [1343], train_loss: 857.21 with loss1: 708.87, loss2: 148.34 and loss3: 0.00\n",
      "Epoch [1344], train_loss: 852.25 with loss1: 703.81, loss2: 148.44 and loss3: 0.00\n",
      "Epoch [1345], train_loss: 850.23 with loss1: 701.61, loss2: 148.62 and loss3: 0.00\n",
      "Epoch [1346], train_loss: 847.02 with loss1: 698.35, loss2: 148.67 and loss3: 0.00\n",
      "Epoch [1347], train_loss: 844.79 with loss1: 696.39, loss2: 148.39 and loss3: 0.00\n",
      "Epoch [1348], train_loss: 843.48 with loss1: 695.07, loss2: 148.41 and loss3: 0.00\n",
      "Epoch [1349], train_loss: 840.58 with loss1: 692.51, loss2: 148.07 and loss3: 0.00\n",
      "Epoch [1350], train_loss: 841.21 with loss1: 692.22, loss2: 148.99 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1351], train_loss: 839.51 with loss1: 691.55, loss2: 147.96 and loss3: 0.00\n",
      "Epoch [1352], train_loss: 835.77 with loss1: 687.67, loss2: 148.10 and loss3: 0.00\n",
      "Epoch [1353], train_loss: 831.40 with loss1: 683.74, loss2: 147.66 and loss3: 0.00\n",
      "Epoch [1354], train_loss: 834.43 with loss1: 686.44, loss2: 147.98 and loss3: 0.00\n",
      "Epoch [1355], train_loss: 832.00 with loss1: 684.30, loss2: 147.70 and loss3: 0.00\n",
      "Epoch [1356], train_loss: 832.93 with loss1: 685.05, loss2: 147.87 and loss3: 0.00\n",
      "Epoch [1357], train_loss: 833.24 with loss1: 685.30, loss2: 147.94 and loss3: 0.00\n",
      "Epoch [1358], train_loss: 832.21 with loss1: 684.35, loss2: 147.86 and loss3: 0.00\n",
      "Epoch [1359], train_loss: 828.54 with loss1: 680.81, loss2: 147.73 and loss3: 0.00\n",
      "Epoch [1360], train_loss: 828.80 with loss1: 681.23, loss2: 147.58 and loss3: 0.00\n",
      "Epoch [1361], train_loss: 830.41 with loss1: 682.78, loss2: 147.63 and loss3: 0.00\n",
      "Epoch [1362], train_loss: 827.44 with loss1: 679.80, loss2: 147.64 and loss3: 0.00\n",
      "Epoch [1363], train_loss: 830.52 with loss1: 683.50, loss2: 147.02 and loss3: 0.00\n",
      "Epoch [1364], train_loss: 829.75 with loss1: 682.31, loss2: 147.44 and loss3: 0.00\n",
      "Epoch [1365], train_loss: 832.06 with loss1: 684.73, loss2: 147.33 and loss3: 0.00\n",
      "Epoch [1366], train_loss: 832.37 with loss1: 685.43, loss2: 146.94 and loss3: 0.00\n",
      "Epoch [1367], train_loss: 835.78 with loss1: 688.67, loss2: 147.11 and loss3: 0.00\n",
      "Epoch [1368], train_loss: 836.67 with loss1: 689.68, loss2: 146.99 and loss3: 0.00\n",
      "Epoch [1369], train_loss: 838.88 with loss1: 691.88, loss2: 147.01 and loss3: 0.00\n",
      "Epoch [1370], train_loss: 837.66 with loss1: 691.00, loss2: 146.66 and loss3: 0.00\n",
      "Epoch [1371], train_loss: 838.68 with loss1: 692.15, loss2: 146.53 and loss3: 0.00\n",
      "Epoch [1372], train_loss: 838.85 with loss1: 692.46, loss2: 146.39 and loss3: 0.00\n",
      "Epoch [1373], train_loss: 838.57 with loss1: 692.17, loss2: 146.41 and loss3: 0.00\n",
      "Epoch [1374], train_loss: 837.09 with loss1: 690.47, loss2: 146.62 and loss3: 0.00\n",
      "Epoch [1375], train_loss: 837.79 with loss1: 691.48, loss2: 146.31 and loss3: 0.00\n",
      "Epoch [1376], train_loss: 835.28 with loss1: 688.73, loss2: 146.55 and loss3: 0.00\n",
      "Epoch [1377], train_loss: 832.96 with loss1: 686.53, loss2: 146.43 and loss3: 0.00\n",
      "Epoch [1378], train_loss: 828.30 with loss1: 681.79, loss2: 146.51 and loss3: 0.00\n",
      "Epoch [1379], train_loss: 827.47 with loss1: 681.30, loss2: 146.17 and loss3: 0.00\n",
      "Epoch [1380], train_loss: 823.14 with loss1: 676.93, loss2: 146.21 and loss3: 0.00\n",
      "Epoch [1381], train_loss: 822.64 with loss1: 676.57, loss2: 146.07 and loss3: 0.00\n",
      "Epoch [1382], train_loss: 817.10 with loss1: 670.67, loss2: 146.43 and loss3: 0.00\n",
      "Epoch [1383], train_loss: 814.63 with loss1: 668.95, loss2: 145.68 and loss3: 0.00\n",
      "Epoch [1384], train_loss: 812.04 with loss1: 666.30, loss2: 145.74 and loss3: 0.00\n",
      "Epoch [1385], train_loss: 814.57 with loss1: 668.73, loss2: 145.84 and loss3: 0.00\n",
      "Epoch [1386], train_loss: 811.17 with loss1: 665.45, loss2: 145.72 and loss3: 0.00\n",
      "Epoch [1387], train_loss: 814.40 with loss1: 669.13, loss2: 145.26 and loss3: 0.00\n",
      "Epoch [1388], train_loss: 808.00 with loss1: 662.19, loss2: 145.81 and loss3: 0.00\n",
      "Epoch [1389], train_loss: 809.20 with loss1: 664.10, loss2: 145.10 and loss3: 0.00\n",
      "Epoch [1390], train_loss: 806.31 with loss1: 661.12, loss2: 145.19 and loss3: 0.00\n",
      "Epoch [1391], train_loss: 804.47 with loss1: 659.47, loss2: 145.00 and loss3: 0.00\n",
      "Epoch [1392], train_loss: 804.02 with loss1: 658.92, loss2: 145.10 and loss3: 0.00\n",
      "Epoch [1393], train_loss: 803.86 with loss1: 658.87, loss2: 145.00 and loss3: 0.00\n",
      "Epoch [1394], train_loss: 805.85 with loss1: 660.95, loss2: 144.90 and loss3: 0.00\n",
      "Epoch [1395], train_loss: 805.33 with loss1: 660.26, loss2: 145.07 and loss3: 0.00\n",
      "Epoch [1396], train_loss: 800.66 with loss1: 656.20, loss2: 144.46 and loss3: 0.00\n",
      "Epoch [1397], train_loss: 800.92 with loss1: 655.92, loss2: 144.99 and loss3: 0.00\n",
      "Epoch [1398], train_loss: 802.70 with loss1: 658.19, loss2: 144.51 and loss3: 0.00\n",
      "Epoch [1399], train_loss: 801.83 with loss1: 657.25, loss2: 144.58 and loss3: 0.00\n",
      "Epoch [1400], train_loss: 802.12 with loss1: 657.56, loss2: 144.56 and loss3: 0.00\n",
      "Epoch [1401], train_loss: 805.57 with loss1: 661.18, loss2: 144.39 and loss3: 0.00\n",
      "Epoch [1402], train_loss: 803.15 with loss1: 658.99, loss2: 144.16 and loss3: 0.00\n",
      "Epoch [1403], train_loss: 805.66 with loss1: 661.62, loss2: 144.04 and loss3: 0.00\n",
      "Epoch [1404], train_loss: 807.31 with loss1: 663.59, loss2: 143.72 and loss3: 0.00\n",
      "Epoch [1405], train_loss: 807.92 with loss1: 664.21, loss2: 143.72 and loss3: 0.00\n",
      "Epoch [1406], train_loss: 809.96 with loss1: 665.98, loss2: 143.97 and loss3: 0.00\n",
      "Epoch [1407], train_loss: 812.39 with loss1: 668.45, loss2: 143.93 and loss3: 0.00\n",
      "Epoch [1408], train_loss: 810.71 with loss1: 667.22, loss2: 143.49 and loss3: 0.00\n",
      "Epoch [1409], train_loss: 815.11 with loss1: 671.59, loss2: 143.51 and loss3: 0.00\n",
      "Epoch [1410], train_loss: 813.89 with loss1: 670.41, loss2: 143.49 and loss3: 0.00\n",
      "Epoch [1411], train_loss: 821.96 with loss1: 678.01, loss2: 143.94 and loss3: 0.00\n",
      "Epoch [1412], train_loss: 818.66 with loss1: 675.18, loss2: 143.49 and loss3: 0.00\n",
      "Epoch [1413], train_loss: 820.18 with loss1: 676.54, loss2: 143.64 and loss3: 0.00\n",
      "Epoch [1414], train_loss: 820.23 with loss1: 677.39, loss2: 142.84 and loss3: 0.00\n",
      "Epoch [1415], train_loss: 826.62 with loss1: 683.14, loss2: 143.48 and loss3: 0.00\n",
      "Epoch [1416], train_loss: 823.05 with loss1: 679.70, loss2: 143.35 and loss3: 0.00\n",
      "Epoch [1417], train_loss: 832.26 with loss1: 688.91, loss2: 143.35 and loss3: 0.00\n",
      "Epoch [1418], train_loss: 830.10 with loss1: 687.44, loss2: 142.66 and loss3: 0.00\n",
      "Epoch [1419], train_loss: 839.09 with loss1: 696.09, loss2: 143.00 and loss3: 0.00\n",
      "Epoch [1420], train_loss: 837.55 with loss1: 695.16, loss2: 142.39 and loss3: 0.00\n",
      "Epoch [1421], train_loss: 848.44 with loss1: 705.35, loss2: 143.09 and loss3: 0.00\n",
      "Epoch [1422], train_loss: 845.72 with loss1: 703.25, loss2: 142.48 and loss3: 0.00\n",
      "Epoch [1423], train_loss: 858.02 with loss1: 714.91, loss2: 143.12 and loss3: 0.00\n",
      "Epoch [1424], train_loss: 859.94 with loss1: 717.64, loss2: 142.30 and loss3: 0.00\n",
      "Epoch [1425], train_loss: 874.28 with loss1: 731.45, loss2: 142.82 and loss3: 0.00\n",
      "Epoch [1426], train_loss: 878.48 with loss1: 736.60, loss2: 141.88 and loss3: 0.00\n",
      "Epoch [1427], train_loss: 896.91 with loss1: 754.52, loss2: 142.39 and loss3: 0.00\n",
      "Epoch [1428], train_loss: 898.21 with loss1: 756.24, loss2: 141.97 and loss3: 0.00\n",
      "Epoch [1429], train_loss: 915.89 with loss1: 773.10, loss2: 142.78 and loss3: 0.00\n",
      "Epoch [1430], train_loss: 921.00 with loss1: 779.15, loss2: 141.85 and loss3: 0.00\n",
      "Epoch [1431], train_loss: 946.03 with loss1: 803.56, loss2: 142.48 and loss3: 0.00\n",
      "Epoch [1432], train_loss: 942.45 with loss1: 800.47, loss2: 141.97 and loss3: 0.00\n",
      "Epoch [1433], train_loss: 962.92 with loss1: 820.43, loss2: 142.48 and loss3: 0.00\n",
      "Epoch [1434], train_loss: 956.05 with loss1: 814.04, loss2: 142.01 and loss3: 0.00\n",
      "Epoch [1435], train_loss: 968.72 with loss1: 826.34, loss2: 142.38 and loss3: 0.00\n",
      "Epoch [1436], train_loss: 960.45 with loss1: 818.62, loss2: 141.83 and loss3: 0.00\n",
      "Epoch [1437], train_loss: 964.77 with loss1: 822.30, loss2: 142.47 and loss3: 0.00\n",
      "Epoch [1438], train_loss: 950.23 with loss1: 808.22, loss2: 142.01 and loss3: 0.00\n",
      "Epoch [1439], train_loss: 953.68 with loss1: 810.97, loss2: 142.71 and loss3: 0.00\n",
      "Epoch [1440], train_loss: 939.54 with loss1: 797.45, loss2: 142.10 and loss3: 0.00\n",
      "Epoch [1441], train_loss: 938.25 with loss1: 795.82, loss2: 142.43 and loss3: 0.00\n",
      "Epoch [1442], train_loss: 918.06 with loss1: 776.11, loss2: 141.95 and loss3: 0.00\n",
      "Epoch [1443], train_loss: 909.45 with loss1: 766.99, loss2: 142.47 and loss3: 0.00\n",
      "Epoch [1444], train_loss: 890.99 with loss1: 749.48, loss2: 141.51 and loss3: 0.00\n",
      "Epoch [1445], train_loss: 884.89 with loss1: 742.66, loss2: 142.23 and loss3: 0.00\n",
      "Epoch [1446], train_loss: 872.97 with loss1: 731.60, loss2: 141.36 and loss3: 0.00\n",
      "Epoch [1447], train_loss: 867.69 with loss1: 725.59, loss2: 142.10 and loss3: 0.00\n",
      "Epoch [1448], train_loss: 859.67 with loss1: 718.21, loss2: 141.46 and loss3: 0.00\n",
      "Epoch [1449], train_loss: 851.16 with loss1: 709.24, loss2: 141.91 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1450], train_loss: 841.07 with loss1: 699.90, loss2: 141.17 and loss3: 0.00\n",
      "Epoch [1451], train_loss: 837.77 with loss1: 695.98, loss2: 141.78 and loss3: 0.00\n",
      "Epoch [1452], train_loss: 828.34 with loss1: 687.45, loss2: 140.89 and loss3: 0.00\n",
      "Epoch [1453], train_loss: 827.93 with loss1: 686.49, loss2: 141.44 and loss3: 0.00\n",
      "Epoch [1454], train_loss: 821.25 with loss1: 680.28, loss2: 140.98 and loss3: 0.00\n",
      "Epoch [1455], train_loss: 819.30 with loss1: 678.51, loss2: 140.79 and loss3: 0.00\n",
      "Epoch [1456], train_loss: 812.92 with loss1: 672.24, loss2: 140.68 and loss3: 0.00\n",
      "Epoch [1457], train_loss: 812.51 with loss1: 671.77, loss2: 140.74 and loss3: 0.00\n",
      "Epoch [1458], train_loss: 808.81 with loss1: 668.08, loss2: 140.73 and loss3: 0.00\n",
      "Epoch [1459], train_loss: 809.09 with loss1: 668.44, loss2: 140.66 and loss3: 0.00\n",
      "Epoch [1460], train_loss: 807.16 with loss1: 666.49, loss2: 140.67 and loss3: 0.00\n",
      "Epoch [1461], train_loss: 803.84 with loss1: 663.11, loss2: 140.73 and loss3: 0.00\n",
      "Epoch [1462], train_loss: 801.63 with loss1: 661.44, loss2: 140.19 and loss3: 0.00\n",
      "Epoch [1463], train_loss: 801.76 with loss1: 660.83, loss2: 140.93 and loss3: 0.00\n",
      "Epoch [1464], train_loss: 798.34 with loss1: 657.85, loss2: 140.49 and loss3: 0.00\n",
      "Epoch [1465], train_loss: 799.01 with loss1: 658.68, loss2: 140.33 and loss3: 0.00\n",
      "Epoch [1466], train_loss: 796.06 with loss1: 656.35, loss2: 139.71 and loss3: 0.00\n",
      "Epoch [1467], train_loss: 795.96 with loss1: 655.91, loss2: 140.06 and loss3: 0.00\n",
      "Epoch [1468], train_loss: 794.92 with loss1: 655.42, loss2: 139.50 and loss3: 0.00\n",
      "Epoch [1469], train_loss: 795.68 with loss1: 655.99, loss2: 139.70 and loss3: 0.00\n",
      "Epoch [1470], train_loss: 792.28 with loss1: 652.71, loss2: 139.57 and loss3: 0.00\n",
      "Epoch [1471], train_loss: 793.42 with loss1: 653.71, loss2: 139.71 and loss3: 0.00\n",
      "Epoch [1472], train_loss: 793.49 with loss1: 653.82, loss2: 139.67 and loss3: 0.00\n",
      "Epoch [1473], train_loss: 793.82 with loss1: 654.22, loss2: 139.60 and loss3: 0.00\n",
      "Epoch [1474], train_loss: 791.48 with loss1: 651.95, loss2: 139.53 and loss3: 0.00\n",
      "Epoch [1475], train_loss: 796.53 with loss1: 656.83, loss2: 139.70 and loss3: 0.00\n",
      "Epoch [1476], train_loss: 793.61 with loss1: 654.26, loss2: 139.35 and loss3: 0.00\n",
      "Epoch [1477], train_loss: 791.96 with loss1: 652.70, loss2: 139.26 and loss3: 0.00\n",
      "Epoch [1478], train_loss: 795.16 with loss1: 656.02, loss2: 139.14 and loss3: 0.00\n",
      "Epoch [1479], train_loss: 797.28 with loss1: 658.18, loss2: 139.10 and loss3: 0.00\n",
      "Epoch [1480], train_loss: 796.55 with loss1: 657.89, loss2: 138.66 and loss3: 0.00\n",
      "Epoch [1481], train_loss: 798.68 with loss1: 659.48, loss2: 139.20 and loss3: 0.00\n",
      "Epoch [1482], train_loss: 798.08 with loss1: 659.11, loss2: 138.97 and loss3: 0.00\n",
      "Epoch [1483], train_loss: 803.37 with loss1: 664.38, loss2: 138.99 and loss3: 0.00\n",
      "Epoch [1484], train_loss: 802.02 with loss1: 662.72, loss2: 139.30 and loss3: 0.00\n",
      "Epoch [1485], train_loss: 804.82 with loss1: 665.72, loss2: 139.10 and loss3: 0.00\n",
      "Epoch [1486], train_loss: 801.16 with loss1: 662.76, loss2: 138.40 and loss3: 0.00\n",
      "Epoch [1487], train_loss: 804.48 with loss1: 665.58, loss2: 138.89 and loss3: 0.00\n",
      "Epoch [1488], train_loss: 802.90 with loss1: 664.30, loss2: 138.60 and loss3: 0.00\n",
      "Epoch [1489], train_loss: 809.97 with loss1: 671.10, loss2: 138.87 and loss3: 0.00\n",
      "Epoch [1490], train_loss: 805.59 with loss1: 666.86, loss2: 138.74 and loss3: 0.00\n",
      "Epoch [1491], train_loss: 810.83 with loss1: 672.38, loss2: 138.46 and loss3: 0.00\n",
      "Epoch [1492], train_loss: 807.28 with loss1: 668.97, loss2: 138.30 and loss3: 0.00\n",
      "Epoch [1493], train_loss: 815.61 with loss1: 677.42, loss2: 138.19 and loss3: 0.00\n",
      "Epoch [1494], train_loss: 813.13 with loss1: 675.01, loss2: 138.13 and loss3: 0.00\n",
      "Epoch [1495], train_loss: 816.91 with loss1: 678.42, loss2: 138.48 and loss3: 0.00\n",
      "Epoch [1496], train_loss: 815.03 with loss1: 677.07, loss2: 137.96 and loss3: 0.00\n",
      "Epoch [1497], train_loss: 820.11 with loss1: 681.78, loss2: 138.33 and loss3: 0.00\n",
      "Epoch [1498], train_loss: 814.83 with loss1: 676.82, loss2: 138.01 and loss3: 0.00\n",
      "Epoch [1499], train_loss: 819.06 with loss1: 680.83, loss2: 138.23 and loss3: 0.00\n",
      "Epoch [1500], train_loss: 817.96 with loss1: 680.08, loss2: 137.89 and loss3: 0.00\n",
      "Epoch [1501], train_loss: 825.63 with loss1: 687.21, loss2: 138.42 and loss3: 0.00\n",
      "Epoch [1502], train_loss: 820.52 with loss1: 682.24, loss2: 138.28 and loss3: 0.00\n",
      "Epoch [1503], train_loss: 824.58 with loss1: 686.12, loss2: 138.46 and loss3: 0.00\n",
      "Epoch [1504], train_loss: 818.20 with loss1: 680.25, loss2: 137.95 and loss3: 0.00\n",
      "Epoch [1505], train_loss: 817.92 with loss1: 679.38, loss2: 138.54 and loss3: 0.00\n",
      "Epoch [1506], train_loss: 816.98 with loss1: 679.08, loss2: 137.90 and loss3: 0.00\n",
      "Epoch [1507], train_loss: 819.47 with loss1: 681.07, loss2: 138.40 and loss3: 0.00\n",
      "Epoch [1508], train_loss: 816.29 with loss1: 678.54, loss2: 137.75 and loss3: 0.00\n",
      "Epoch [1509], train_loss: 817.05 with loss1: 678.64, loss2: 138.41 and loss3: 0.00\n",
      "Epoch [1510], train_loss: 815.64 with loss1: 677.66, loss2: 137.98 and loss3: 0.00\n",
      "Epoch [1511], train_loss: 819.75 with loss1: 681.55, loss2: 138.20 and loss3: 0.00\n",
      "Epoch [1512], train_loss: 816.90 with loss1: 679.13, loss2: 137.77 and loss3: 0.00\n",
      "Epoch [1513], train_loss: 816.70 with loss1: 678.50, loss2: 138.20 and loss3: 0.00\n",
      "Epoch [1514], train_loss: 815.76 with loss1: 677.97, loss2: 137.79 and loss3: 0.00\n",
      "Epoch [1515], train_loss: 816.52 with loss1: 678.30, loss2: 138.23 and loss3: 0.00\n",
      "Epoch [1516], train_loss: 814.26 with loss1: 676.57, loss2: 137.70 and loss3: 0.00\n",
      "Epoch [1517], train_loss: 817.64 with loss1: 679.25, loss2: 138.39 and loss3: 0.00\n",
      "Epoch [1518], train_loss: 818.06 with loss1: 680.10, loss2: 137.96 and loss3: 0.00\n",
      "Epoch [1519], train_loss: 817.24 with loss1: 679.09, loss2: 138.15 and loss3: 0.00\n",
      "Epoch [1520], train_loss: 814.63 with loss1: 676.51, loss2: 138.12 and loss3: 0.00\n",
      "Epoch [1521], train_loss: 817.44 with loss1: 679.21, loss2: 138.22 and loss3: 0.00\n",
      "Epoch [1522], train_loss: 811.85 with loss1: 674.12, loss2: 137.73 and loss3: 0.00\n",
      "Epoch [1523], train_loss: 814.93 with loss1: 677.07, loss2: 137.87 and loss3: 0.00\n",
      "Epoch [1524], train_loss: 814.08 with loss1: 675.94, loss2: 138.15 and loss3: 0.00\n",
      "Epoch [1525], train_loss: 816.39 with loss1: 678.05, loss2: 138.35 and loss3: 0.00\n",
      "Epoch [1526], train_loss: 811.00 with loss1: 673.12, loss2: 137.88 and loss3: 0.00\n",
      "Epoch [1527], train_loss: 813.04 with loss1: 675.20, loss2: 137.84 and loss3: 0.00\n",
      "Epoch [1528], train_loss: 809.27 with loss1: 671.25, loss2: 138.01 and loss3: 0.00\n",
      "Epoch [1529], train_loss: 813.38 with loss1: 675.35, loss2: 138.02 and loss3: 0.00\n",
      "Epoch [1530], train_loss: 812.42 with loss1: 673.88, loss2: 138.54 and loss3: 0.00\n",
      "Epoch [1531], train_loss: 817.10 with loss1: 679.19, loss2: 137.91 and loss3: 0.00\n",
      "Epoch [1532], train_loss: 811.52 with loss1: 673.27, loss2: 138.24 and loss3: 0.00\n",
      "Epoch [1533], train_loss: 811.55 with loss1: 673.49, loss2: 138.07 and loss3: 0.00\n",
      "Epoch [1534], train_loss: 809.67 with loss1: 671.85, loss2: 137.82 and loss3: 0.00\n",
      "Epoch [1535], train_loss: 810.93 with loss1: 673.30, loss2: 137.63 and loss3: 0.00\n",
      "Epoch [1536], train_loss: 806.06 with loss1: 667.78, loss2: 138.28 and loss3: 0.00\n",
      "Epoch [1537], train_loss: 807.76 with loss1: 669.92, loss2: 137.84 and loss3: 0.00\n",
      "Epoch [1538], train_loss: 806.31 with loss1: 668.10, loss2: 138.21 and loss3: 0.00\n",
      "Epoch [1539], train_loss: 805.81 with loss1: 667.95, loss2: 137.87 and loss3: 0.00\n",
      "Epoch [1540], train_loss: 800.36 with loss1: 662.30, loss2: 138.07 and loss3: 0.00\n",
      "Epoch [1541], train_loss: 798.38 with loss1: 660.80, loss2: 137.58 and loss3: 0.00\n",
      "Epoch [1542], train_loss: 796.47 with loss1: 658.35, loss2: 138.12 and loss3: 0.00\n",
      "Epoch [1543], train_loss: 795.55 with loss1: 657.78, loss2: 137.77 and loss3: 0.00\n",
      "Epoch [1544], train_loss: 792.62 with loss1: 654.41, loss2: 138.21 and loss3: 0.00\n",
      "Epoch [1545], train_loss: 790.57 with loss1: 652.82, loss2: 137.75 and loss3: 0.00\n",
      "Epoch [1546], train_loss: 788.04 with loss1: 650.35, loss2: 137.68 and loss3: 0.00\n",
      "Epoch [1547], train_loss: 786.82 with loss1: 649.26, loss2: 137.56 and loss3: 0.00\n",
      "Epoch [1548], train_loss: 786.46 with loss1: 648.85, loss2: 137.61 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1549], train_loss: 787.38 with loss1: 650.12, loss2: 137.26 and loss3: 0.00\n",
      "Epoch [1550], train_loss: 785.35 with loss1: 647.68, loss2: 137.68 and loss3: 0.00\n",
      "Epoch [1551], train_loss: 787.79 with loss1: 650.64, loss2: 137.14 and loss3: 0.00\n",
      "Epoch [1552], train_loss: 789.60 with loss1: 651.83, loss2: 137.77 and loss3: 0.00\n",
      "Epoch [1553], train_loss: 788.24 with loss1: 651.41, loss2: 136.84 and loss3: 0.00\n",
      "Epoch [1554], train_loss: 787.14 with loss1: 649.81, loss2: 137.32 and loss3: 0.00\n",
      "Epoch [1555], train_loss: 783.94 with loss1: 646.86, loss2: 137.08 and loss3: 0.00\n",
      "Epoch [1556], train_loss: 783.43 with loss1: 646.61, loss2: 136.83 and loss3: 0.00\n",
      "Epoch [1557], train_loss: 783.67 with loss1: 647.03, loss2: 136.64 and loss3: 0.00\n",
      "Epoch [1558], train_loss: 785.91 with loss1: 649.15, loss2: 136.76 and loss3: 0.00\n",
      "Epoch [1559], train_loss: 787.40 with loss1: 651.03, loss2: 136.37 and loss3: 0.00\n",
      "Epoch [1560], train_loss: 789.76 with loss1: 653.39, loss2: 136.38 and loss3: 0.00\n",
      "Epoch [1561], train_loss: 790.52 with loss1: 654.63, loss2: 135.89 and loss3: 0.00\n",
      "Epoch [1562], train_loss: 794.96 with loss1: 658.42, loss2: 136.53 and loss3: 0.00\n",
      "Epoch [1563], train_loss: 794.69 with loss1: 658.99, loss2: 135.70 and loss3: 0.00\n",
      "Epoch [1564], train_loss: 798.95 with loss1: 662.34, loss2: 136.61 and loss3: 0.00\n",
      "Epoch [1565], train_loss: 799.06 with loss1: 663.34, loss2: 135.72 and loss3: 0.00\n",
      "Epoch [1566], train_loss: 801.48 with loss1: 665.29, loss2: 136.19 and loss3: 0.00\n",
      "Epoch [1567], train_loss: 805.32 with loss1: 669.96, loss2: 135.36 and loss3: 0.00\n",
      "Epoch [1568], train_loss: 806.25 with loss1: 670.08, loss2: 136.18 and loss3: 0.00\n",
      "Epoch [1569], train_loss: 806.45 with loss1: 671.12, loss2: 135.33 and loss3: 0.00\n",
      "Epoch [1570], train_loss: 809.86 with loss1: 674.11, loss2: 135.74 and loss3: 0.00\n",
      "Epoch [1571], train_loss: 810.11 with loss1: 675.24, loss2: 134.88 and loss3: 0.00\n",
      "Epoch [1572], train_loss: 812.09 with loss1: 676.92, loss2: 135.17 and loss3: 0.00\n",
      "Epoch [1573], train_loss: 810.86 with loss1: 676.17, loss2: 134.69 and loss3: 0.00\n",
      "Epoch [1574], train_loss: 813.59 with loss1: 678.32, loss2: 135.28 and loss3: 0.00\n",
      "Epoch [1575], train_loss: 812.75 with loss1: 678.15, loss2: 134.60 and loss3: 0.00\n",
      "Epoch [1576], train_loss: 817.01 with loss1: 681.69, loss2: 135.32 and loss3: 0.00\n",
      "Epoch [1577], train_loss: 813.32 with loss1: 678.77, loss2: 134.55 and loss3: 0.00\n",
      "Epoch [1578], train_loss: 816.94 with loss1: 681.81, loss2: 135.13 and loss3: 0.00\n",
      "Epoch [1579], train_loss: 813.31 with loss1: 679.13, loss2: 134.17 and loss3: 0.00\n",
      "Epoch [1580], train_loss: 814.08 with loss1: 679.06, loss2: 135.02 and loss3: 0.00\n",
      "Epoch [1581], train_loss: 811.91 with loss1: 677.56, loss2: 134.34 and loss3: 0.00\n",
      "Epoch [1582], train_loss: 812.35 with loss1: 677.42, loss2: 134.93 and loss3: 0.00\n",
      "Epoch [1583], train_loss: 807.07 with loss1: 672.92, loss2: 134.15 and loss3: 0.00\n",
      "Epoch [1584], train_loss: 807.59 with loss1: 673.01, loss2: 134.58 and loss3: 0.00\n",
      "Epoch [1585], train_loss: 802.02 with loss1: 668.40, loss2: 133.61 and loss3: 0.00\n",
      "Epoch [1586], train_loss: 808.62 with loss1: 674.51, loss2: 134.11 and loss3: 0.00\n",
      "Epoch [1587], train_loss: 802.16 with loss1: 668.61, loss2: 133.56 and loss3: 0.00\n",
      "Epoch [1588], train_loss: 803.13 with loss1: 669.04, loss2: 134.09 and loss3: 0.00\n",
      "Epoch [1589], train_loss: 799.52 with loss1: 665.98, loss2: 133.54 and loss3: 0.00\n",
      "Epoch [1590], train_loss: 799.23 with loss1: 665.36, loss2: 133.87 and loss3: 0.00\n",
      "Epoch [1591], train_loss: 793.92 with loss1: 660.44, loss2: 133.47 and loss3: 0.00\n",
      "Epoch [1592], train_loss: 794.65 with loss1: 661.04, loss2: 133.61 and loss3: 0.00\n",
      "Epoch [1593], train_loss: 792.62 with loss1: 659.30, loss2: 133.32 and loss3: 0.00\n",
      "Epoch [1594], train_loss: 792.83 with loss1: 659.40, loss2: 133.43 and loss3: 0.00\n",
      "Epoch [1595], train_loss: 788.76 with loss1: 655.50, loss2: 133.26 and loss3: 0.00\n",
      "Epoch [1596], train_loss: 787.17 with loss1: 653.63, loss2: 133.54 and loss3: 0.00\n",
      "Epoch [1597], train_loss: 784.30 with loss1: 651.07, loss2: 133.23 and loss3: 0.00\n",
      "Epoch [1598], train_loss: 786.56 with loss1: 652.86, loss2: 133.71 and loss3: 0.00\n",
      "Epoch [1599], train_loss: 781.07 with loss1: 648.21, loss2: 132.85 and loss3: 0.00\n",
      "Epoch [1600], train_loss: 778.25 with loss1: 644.93, loss2: 133.31 and loss3: 0.00\n",
      "Epoch [1601], train_loss: 775.07 with loss1: 642.31, loss2: 132.76 and loss3: 0.00\n",
      "Epoch [1602], train_loss: 774.87 with loss1: 641.49, loss2: 133.38 and loss3: 0.00\n",
      "Epoch [1603], train_loss: 770.60 with loss1: 638.01, loss2: 132.58 and loss3: 0.00\n",
      "Epoch [1604], train_loss: 771.91 with loss1: 638.99, loss2: 132.92 and loss3: 0.00\n",
      "Epoch [1605], train_loss: 771.21 with loss1: 638.89, loss2: 132.32 and loss3: 0.00\n",
      "Epoch [1606], train_loss: 768.11 with loss1: 635.46, loss2: 132.65 and loss3: 0.00\n",
      "Epoch [1607], train_loss: 766.60 with loss1: 634.31, loss2: 132.29 and loss3: 0.00\n",
      "Epoch [1608], train_loss: 768.28 with loss1: 635.50, loss2: 132.78 and loss3: 0.00\n",
      "Epoch [1609], train_loss: 765.06 with loss1: 632.96, loss2: 132.10 and loss3: 0.00\n",
      "Epoch [1610], train_loss: 767.70 with loss1: 635.15, loss2: 132.55 and loss3: 0.00\n",
      "Epoch [1611], train_loss: 764.46 with loss1: 632.52, loss2: 131.95 and loss3: 0.00\n",
      "Epoch [1612], train_loss: 764.78 with loss1: 632.49, loss2: 132.28 and loss3: 0.00\n",
      "Epoch [1613], train_loss: 763.35 with loss1: 631.40, loss2: 131.95 and loss3: 0.00\n",
      "Epoch [1614], train_loss: 763.10 with loss1: 631.00, loss2: 132.09 and loss3: 0.00\n",
      "Epoch [1615], train_loss: 761.16 with loss1: 629.45, loss2: 131.71 and loss3: 0.00\n",
      "Epoch [1616], train_loss: 760.89 with loss1: 628.99, loss2: 131.90 and loss3: 0.00\n",
      "Epoch [1617], train_loss: 757.79 with loss1: 626.03, loss2: 131.76 and loss3: 0.00\n",
      "Epoch [1618], train_loss: 759.78 with loss1: 627.70, loss2: 132.07 and loss3: 0.00\n",
      "Epoch [1619], train_loss: 754.91 with loss1: 623.33, loss2: 131.58 and loss3: 0.00\n",
      "Epoch [1620], train_loss: 755.89 with loss1: 624.22, loss2: 131.67 and loss3: 0.00\n",
      "Epoch [1621], train_loss: 753.84 with loss1: 622.47, loss2: 131.37 and loss3: 0.00\n",
      "Epoch [1622], train_loss: 754.64 with loss1: 623.25, loss2: 131.39 and loss3: 0.00\n",
      "Epoch [1623], train_loss: 755.48 with loss1: 624.20, loss2: 131.28 and loss3: 0.00\n",
      "Epoch [1624], train_loss: 753.62 with loss1: 621.79, loss2: 131.83 and loss3: 0.00\n",
      "Epoch [1625], train_loss: 756.04 with loss1: 624.78, loss2: 131.26 and loss3: 0.00\n",
      "Epoch [1626], train_loss: 757.75 with loss1: 626.30, loss2: 131.44 and loss3: 0.00\n",
      "Epoch [1627], train_loss: 754.74 with loss1: 623.52, loss2: 131.23 and loss3: 0.00\n",
      "Epoch [1628], train_loss: 753.52 with loss1: 622.11, loss2: 131.41 and loss3: 0.00\n",
      "Epoch [1629], train_loss: 755.16 with loss1: 623.95, loss2: 131.21 and loss3: 0.00\n",
      "Epoch [1630], train_loss: 757.75 with loss1: 626.83, loss2: 130.92 and loss3: 0.00\n",
      "Epoch [1631], train_loss: 754.17 with loss1: 622.76, loss2: 131.41 and loss3: 0.00\n",
      "Epoch [1632], train_loss: 754.21 with loss1: 622.70, loss2: 131.51 and loss3: 0.00\n",
      "Epoch [1633], train_loss: 753.80 with loss1: 622.10, loss2: 131.70 and loss3: 0.00\n",
      "Epoch [1634], train_loss: 754.42 with loss1: 623.30, loss2: 131.12 and loss3: 0.00\n",
      "Epoch [1635], train_loss: 755.89 with loss1: 624.90, loss2: 130.99 and loss3: 0.00\n",
      "Epoch [1636], train_loss: 758.86 with loss1: 628.01, loss2: 130.85 and loss3: 0.00\n",
      "Epoch [1637], train_loss: 755.30 with loss1: 624.28, loss2: 131.02 and loss3: 0.00\n",
      "Epoch [1638], train_loss: 761.27 with loss1: 630.45, loss2: 130.82 and loss3: 0.00\n",
      "Epoch [1639], train_loss: 758.22 with loss1: 627.64, loss2: 130.58 and loss3: 0.00\n",
      "Epoch [1640], train_loss: 761.50 with loss1: 630.39, loss2: 131.11 and loss3: 0.00\n",
      "Epoch [1641], train_loss: 761.18 with loss1: 630.84, loss2: 130.34 and loss3: 0.00\n",
      "Epoch [1642], train_loss: 765.36 with loss1: 634.61, loss2: 130.75 and loss3: 0.00\n",
      "Epoch [1643], train_loss: 761.46 with loss1: 630.54, loss2: 130.92 and loss3: 0.00\n",
      "Epoch [1644], train_loss: 765.94 with loss1: 634.97, loss2: 130.97 and loss3: 0.00\n",
      "Epoch [1645], train_loss: 763.08 with loss1: 632.74, loss2: 130.34 and loss3: 0.00\n",
      "Epoch [1646], train_loss: 768.93 with loss1: 638.42, loss2: 130.51 and loss3: 0.00\n",
      "Epoch [1647], train_loss: 769.96 with loss1: 639.37, loss2: 130.59 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1648], train_loss: 768.62 with loss1: 637.96, loss2: 130.67 and loss3: 0.00\n",
      "Epoch [1649], train_loss: 767.77 with loss1: 637.44, loss2: 130.33 and loss3: 0.00\n",
      "Epoch [1650], train_loss: 770.58 with loss1: 640.03, loss2: 130.55 and loss3: 0.00\n",
      "Epoch [1651], train_loss: 767.71 with loss1: 637.50, loss2: 130.22 and loss3: 0.00\n",
      "Epoch [1652], train_loss: 767.51 with loss1: 637.08, loss2: 130.42 and loss3: 0.00\n",
      "Epoch [1653], train_loss: 765.91 with loss1: 635.88, loss2: 130.03 and loss3: 0.00\n",
      "Epoch [1654], train_loss: 768.15 with loss1: 638.02, loss2: 130.12 and loss3: 0.00\n",
      "Epoch [1655], train_loss: 768.05 with loss1: 637.91, loss2: 130.13 and loss3: 0.00\n",
      "Epoch [1656], train_loss: 766.00 with loss1: 635.54, loss2: 130.46 and loss3: 0.00\n",
      "Epoch [1657], train_loss: 762.89 with loss1: 633.44, loss2: 129.45 and loss3: 0.00\n",
      "Epoch [1658], train_loss: 767.90 with loss1: 637.56, loss2: 130.34 and loss3: 0.00\n",
      "Epoch [1659], train_loss: 761.85 with loss1: 632.10, loss2: 129.75 and loss3: 0.00\n",
      "Epoch [1660], train_loss: 764.29 with loss1: 634.37, loss2: 129.92 and loss3: 0.00\n",
      "Epoch [1661], train_loss: 760.54 with loss1: 631.06, loss2: 129.48 and loss3: 0.00\n",
      "Epoch [1662], train_loss: 760.45 with loss1: 630.51, loss2: 129.93 and loss3: 0.00\n",
      "Epoch [1663], train_loss: 757.61 with loss1: 628.19, loss2: 129.43 and loss3: 0.00\n",
      "Epoch [1664], train_loss: 756.92 with loss1: 627.02, loss2: 129.91 and loss3: 0.00\n",
      "Epoch [1665], train_loss: 756.00 with loss1: 626.50, loss2: 129.50 and loss3: 0.00\n",
      "Epoch [1666], train_loss: 760.47 with loss1: 630.91, loss2: 129.56 and loss3: 0.00\n",
      "Epoch [1667], train_loss: 754.96 with loss1: 625.66, loss2: 129.31 and loss3: 0.00\n",
      "Epoch [1668], train_loss: 756.61 with loss1: 627.06, loss2: 129.56 and loss3: 0.00\n",
      "Epoch [1669], train_loss: 754.35 with loss1: 624.73, loss2: 129.62 and loss3: 0.00\n",
      "Epoch [1670], train_loss: 754.18 with loss1: 624.58, loss2: 129.59 and loss3: 0.00\n",
      "Epoch [1671], train_loss: 751.41 with loss1: 622.22, loss2: 129.19 and loss3: 0.00\n",
      "Epoch [1672], train_loss: 753.77 with loss1: 624.40, loss2: 129.37 and loss3: 0.00\n",
      "Epoch [1673], train_loss: 751.06 with loss1: 621.71, loss2: 129.35 and loss3: 0.00\n",
      "Epoch [1674], train_loss: 753.20 with loss1: 623.91, loss2: 129.28 and loss3: 0.00\n",
      "Epoch [1675], train_loss: 753.40 with loss1: 624.35, loss2: 129.05 and loss3: 0.00\n",
      "Epoch [1676], train_loss: 752.68 with loss1: 623.59, loss2: 129.09 and loss3: 0.00\n",
      "Epoch [1677], train_loss: 750.39 with loss1: 621.49, loss2: 128.90 and loss3: 0.00\n",
      "Epoch [1678], train_loss: 753.79 with loss1: 624.77, loss2: 129.02 and loss3: 0.00\n",
      "Epoch [1679], train_loss: 752.12 with loss1: 623.53, loss2: 128.59 and loss3: 0.00\n",
      "Epoch [1680], train_loss: 758.16 with loss1: 629.06, loss2: 129.10 and loss3: 0.00\n",
      "Epoch [1681], train_loss: 753.24 with loss1: 624.65, loss2: 128.59 and loss3: 0.00\n",
      "Epoch [1682], train_loss: 756.38 with loss1: 627.26, loss2: 129.12 and loss3: 0.00\n",
      "Epoch [1683], train_loss: 753.34 with loss1: 624.97, loss2: 128.37 and loss3: 0.00\n",
      "Epoch [1684], train_loss: 757.81 with loss1: 628.88, loss2: 128.93 and loss3: 0.00\n",
      "Epoch [1685], train_loss: 756.16 with loss1: 627.73, loss2: 128.43 and loss3: 0.00\n",
      "Epoch [1686], train_loss: 760.54 with loss1: 631.69, loss2: 128.85 and loss3: 0.00\n",
      "Epoch [1687], train_loss: 759.48 with loss1: 630.70, loss2: 128.78 and loss3: 0.00\n",
      "Epoch [1688], train_loss: 762.94 with loss1: 634.01, loss2: 128.93 and loss3: 0.00\n",
      "Epoch [1689], train_loss: 762.11 with loss1: 633.86, loss2: 128.26 and loss3: 0.00\n",
      "Epoch [1690], train_loss: 768.81 with loss1: 640.25, loss2: 128.56 and loss3: 0.00\n",
      "Epoch [1691], train_loss: 767.00 with loss1: 638.82, loss2: 128.18 and loss3: 0.00\n",
      "Epoch [1692], train_loss: 772.82 with loss1: 644.11, loss2: 128.71 and loss3: 0.00\n",
      "Epoch [1693], train_loss: 771.51 with loss1: 643.27, loss2: 128.24 and loss3: 0.00\n",
      "Epoch [1694], train_loss: 779.23 with loss1: 650.62, loss2: 128.61 and loss3: 0.00\n",
      "Epoch [1695], train_loss: 778.48 with loss1: 650.36, loss2: 128.12 and loss3: 0.00\n",
      "Epoch [1696], train_loss: 786.17 with loss1: 657.67, loss2: 128.50 and loss3: 0.00\n",
      "Epoch [1697], train_loss: 786.41 with loss1: 658.03, loss2: 128.38 and loss3: 0.00\n",
      "Epoch [1698], train_loss: 792.24 with loss1: 663.93, loss2: 128.31 and loss3: 0.00\n",
      "Epoch [1699], train_loss: 794.25 with loss1: 665.95, loss2: 128.30 and loss3: 0.00\n",
      "Epoch [1700], train_loss: 799.36 with loss1: 670.46, loss2: 128.89 and loss3: 0.00\n",
      "Epoch [1701], train_loss: 802.79 with loss1: 674.59, loss2: 128.20 and loss3: 0.00\n",
      "Epoch [1702], train_loss: 808.95 with loss1: 680.47, loss2: 128.49 and loss3: 0.00\n",
      "Epoch [1703], train_loss: 812.65 with loss1: 684.25, loss2: 128.40 and loss3: 0.00\n",
      "Epoch [1704], train_loss: 817.37 with loss1: 688.53, loss2: 128.84 and loss3: 0.00\n",
      "Epoch [1705], train_loss: 821.58 with loss1: 693.28, loss2: 128.30 and loss3: 0.00\n",
      "Epoch [1706], train_loss: 822.59 with loss1: 693.28, loss2: 129.30 and loss3: 0.00\n",
      "Epoch [1707], train_loss: 818.22 with loss1: 689.96, loss2: 128.26 and loss3: 0.00\n",
      "Epoch [1708], train_loss: 813.54 with loss1: 684.27, loss2: 129.27 and loss3: 0.00\n",
      "Epoch [1709], train_loss: 811.18 with loss1: 682.59, loss2: 128.59 and loss3: 0.00\n",
      "Epoch [1710], train_loss: 810.24 with loss1: 681.23, loss2: 129.02 and loss3: 0.00\n",
      "Epoch [1711], train_loss: 804.07 with loss1: 675.02, loss2: 129.05 and loss3: 0.00\n",
      "Epoch [1712], train_loss: 795.40 with loss1: 666.30, loss2: 129.10 and loss3: 0.00\n",
      "Epoch [1713], train_loss: 787.79 with loss1: 659.07, loss2: 128.73 and loss3: 0.00\n",
      "Epoch [1714], train_loss: 783.79 with loss1: 654.96, loss2: 128.83 and loss3: 0.00\n",
      "Epoch [1715], train_loss: 775.17 with loss1: 646.32, loss2: 128.85 and loss3: 0.00\n",
      "Epoch [1716], train_loss: 770.34 with loss1: 641.38, loss2: 128.96 and loss3: 0.00\n",
      "Epoch [1717], train_loss: 762.98 with loss1: 634.40, loss2: 128.58 and loss3: 0.00\n",
      "Epoch [1718], train_loss: 758.16 with loss1: 628.96, loss2: 129.20 and loss3: 0.00\n",
      "Epoch [1719], train_loss: 751.50 with loss1: 622.64, loss2: 128.86 and loss3: 0.00\n",
      "Epoch [1720], train_loss: 743.89 with loss1: 615.14, loss2: 128.75 and loss3: 0.00\n",
      "Epoch [1721], train_loss: 738.02 with loss1: 609.77, loss2: 128.25 and loss3: 0.00\n",
      "Epoch [1722], train_loss: 736.13 with loss1: 607.46, loss2: 128.67 and loss3: 0.00\n",
      "Epoch [1723], train_loss: 728.27 with loss1: 600.29, loss2: 127.98 and loss3: 0.00\n",
      "Epoch [1724], train_loss: 726.24 with loss1: 598.14, loss2: 128.10 and loss3: 0.00\n",
      "Epoch [1725], train_loss: 724.42 with loss1: 596.44, loss2: 127.98 and loss3: 0.00\n",
      "Epoch [1726], train_loss: 723.17 with loss1: 594.95, loss2: 128.22 and loss3: 0.00\n",
      "Epoch [1727], train_loss: 719.32 with loss1: 591.10, loss2: 128.22 and loss3: 0.00\n",
      "Epoch [1728], train_loss: 715.43 with loss1: 587.56, loss2: 127.88 and loss3: 0.00\n",
      "Epoch [1729], train_loss: 716.90 with loss1: 589.13, loss2: 127.77 and loss3: 0.00\n",
      "Epoch [1730], train_loss: 713.91 with loss1: 586.19, loss2: 127.72 and loss3: 0.00\n",
      "Epoch [1731], train_loss: 711.05 with loss1: 583.29, loss2: 127.76 and loss3: 0.00\n",
      "Epoch [1732], train_loss: 709.08 with loss1: 581.33, loss2: 127.74 and loss3: 0.00\n",
      "Epoch [1733], train_loss: 707.74 with loss1: 580.30, loss2: 127.44 and loss3: 0.00\n",
      "Epoch [1734], train_loss: 706.41 with loss1: 579.43, loss2: 126.98 and loss3: 0.00\n",
      "Epoch [1735], train_loss: 703.45 with loss1: 576.34, loss2: 127.12 and loss3: 0.00\n",
      "Epoch [1736], train_loss: 703.93 with loss1: 576.81, loss2: 127.11 and loss3: 0.00\n",
      "Epoch [1737], train_loss: 707.05 with loss1: 580.02, loss2: 127.03 and loss3: 0.00\n",
      "Epoch [1738], train_loss: 702.42 with loss1: 575.21, loss2: 127.22 and loss3: 0.00\n",
      "Epoch [1739], train_loss: 703.01 with loss1: 576.09, loss2: 126.92 and loss3: 0.00\n",
      "Epoch [1740], train_loss: 700.87 with loss1: 574.19, loss2: 126.68 and loss3: 0.00\n",
      "Epoch [1741], train_loss: 703.23 with loss1: 576.76, loss2: 126.47 and loss3: 0.00\n",
      "Epoch [1742], train_loss: 700.93 with loss1: 574.22, loss2: 126.71 and loss3: 0.00\n",
      "Epoch [1743], train_loss: 699.21 with loss1: 572.99, loss2: 126.22 and loss3: 0.00\n",
      "Epoch [1744], train_loss: 699.89 with loss1: 573.41, loss2: 126.48 and loss3: 0.00\n",
      "Epoch [1745], train_loss: 700.64 with loss1: 574.49, loss2: 126.15 and loss3: 0.00\n",
      "Epoch [1746], train_loss: 702.15 with loss1: 575.67, loss2: 126.47 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1747], train_loss: 703.14 with loss1: 577.18, loss2: 125.96 and loss3: 0.00\n",
      "Epoch [1748], train_loss: 702.13 with loss1: 576.11, loss2: 126.02 and loss3: 0.00\n",
      "Epoch [1749], train_loss: 701.97 with loss1: 575.99, loss2: 125.98 and loss3: 0.00\n",
      "Epoch [1750], train_loss: 701.95 with loss1: 575.76, loss2: 126.19 and loss3: 0.00\n",
      "Epoch [1751], train_loss: 700.91 with loss1: 575.22, loss2: 125.69 and loss3: 0.00\n",
      "Epoch [1752], train_loss: 700.02 with loss1: 574.21, loss2: 125.81 and loss3: 0.00\n",
      "Epoch [1753], train_loss: 699.52 with loss1: 573.89, loss2: 125.63 and loss3: 0.00\n",
      "Epoch [1754], train_loss: 701.87 with loss1: 576.30, loss2: 125.57 and loss3: 0.00\n",
      "Epoch [1755], train_loss: 701.52 with loss1: 575.80, loss2: 125.71 and loss3: 0.00\n",
      "Epoch [1756], train_loss: 703.03 with loss1: 577.62, loss2: 125.41 and loss3: 0.00\n",
      "Epoch [1757], train_loss: 702.67 with loss1: 577.24, loss2: 125.43 and loss3: 0.00\n",
      "Epoch [1758], train_loss: 704.61 with loss1: 579.07, loss2: 125.55 and loss3: 0.00\n",
      "Epoch [1759], train_loss: 707.35 with loss1: 582.07, loss2: 125.29 and loss3: 0.00\n",
      "Epoch [1760], train_loss: 708.03 with loss1: 582.85, loss2: 125.18 and loss3: 0.00\n",
      "Epoch [1761], train_loss: 709.78 with loss1: 584.65, loss2: 125.13 and loss3: 0.00\n",
      "Epoch [1762], train_loss: 709.51 with loss1: 584.46, loss2: 125.05 and loss3: 0.00\n",
      "Epoch [1763], train_loss: 709.30 with loss1: 584.13, loss2: 125.17 and loss3: 0.00\n",
      "Epoch [1764], train_loss: 710.03 with loss1: 584.99, loss2: 125.04 and loss3: 0.00\n",
      "Epoch [1765], train_loss: 711.53 with loss1: 586.49, loss2: 125.05 and loss3: 0.00\n",
      "Epoch [1766], train_loss: 712.75 with loss1: 587.60, loss2: 125.15 and loss3: 0.00\n",
      "Epoch [1767], train_loss: 715.18 with loss1: 590.50, loss2: 124.68 and loss3: 0.00\n",
      "Epoch [1768], train_loss: 715.90 with loss1: 591.07, loss2: 124.82 and loss3: 0.00\n",
      "Epoch [1769], train_loss: 717.54 with loss1: 592.90, loss2: 124.64 and loss3: 0.00\n",
      "Epoch [1770], train_loss: 717.79 with loss1: 593.02, loss2: 124.76 and loss3: 0.00\n",
      "Epoch [1771], train_loss: 720.87 with loss1: 596.19, loss2: 124.69 and loss3: 0.00\n",
      "Epoch [1772], train_loss: 723.08 with loss1: 598.47, loss2: 124.60 and loss3: 0.00\n",
      "Epoch [1773], train_loss: 719.92 with loss1: 595.31, loss2: 124.61 and loss3: 0.00\n",
      "Epoch [1774], train_loss: 720.91 with loss1: 596.32, loss2: 124.60 and loss3: 0.00\n",
      "Epoch [1775], train_loss: 721.61 with loss1: 597.56, loss2: 124.05 and loss3: 0.00\n",
      "Epoch [1776], train_loss: 724.57 with loss1: 599.85, loss2: 124.72 and loss3: 0.00\n",
      "Epoch [1777], train_loss: 722.63 with loss1: 598.49, loss2: 124.14 and loss3: 0.00\n",
      "Epoch [1778], train_loss: 726.47 with loss1: 602.14, loss2: 124.33 and loss3: 0.00\n",
      "Epoch [1779], train_loss: 727.87 with loss1: 603.79, loss2: 124.08 and loss3: 0.00\n",
      "Epoch [1780], train_loss: 733.70 with loss1: 609.39, loss2: 124.32 and loss3: 0.00\n",
      "Epoch [1781], train_loss: 732.87 with loss1: 609.07, loss2: 123.80 and loss3: 0.00\n",
      "Epoch [1782], train_loss: 738.87 with loss1: 614.46, loss2: 124.41 and loss3: 0.00\n",
      "Epoch [1783], train_loss: 739.21 with loss1: 615.30, loss2: 123.91 and loss3: 0.00\n",
      "Epoch [1784], train_loss: 744.56 with loss1: 620.23, loss2: 124.33 and loss3: 0.00\n",
      "Epoch [1785], train_loss: 743.08 with loss1: 619.44, loss2: 123.64 and loss3: 0.00\n",
      "Epoch [1786], train_loss: 751.03 with loss1: 626.64, loss2: 124.38 and loss3: 0.00\n",
      "Epoch [1787], train_loss: 750.32 with loss1: 626.07, loss2: 124.25 and loss3: 0.00\n",
      "Epoch [1788], train_loss: 757.00 with loss1: 632.86, loss2: 124.14 and loss3: 0.00\n",
      "Epoch [1789], train_loss: 757.20 with loss1: 633.41, loss2: 123.78 and loss3: 0.00\n",
      "Epoch [1790], train_loss: 766.06 with loss1: 641.95, loss2: 124.10 and loss3: 0.00\n",
      "Epoch [1791], train_loss: 763.43 with loss1: 639.75, loss2: 123.69 and loss3: 0.00\n",
      "Epoch [1792], train_loss: 765.21 with loss1: 640.92, loss2: 124.29 and loss3: 0.00\n",
      "Epoch [1793], train_loss: 764.32 with loss1: 640.92, loss2: 123.41 and loss3: 0.00\n",
      "Epoch [1794], train_loss: 773.47 with loss1: 649.44, loss2: 124.03 and loss3: 0.00\n",
      "Epoch [1795], train_loss: 773.09 with loss1: 649.57, loss2: 123.52 and loss3: 0.00\n",
      "Epoch [1796], train_loss: 779.52 with loss1: 655.65, loss2: 123.87 and loss3: 0.00\n",
      "Epoch [1797], train_loss: 778.10 with loss1: 654.60, loss2: 123.51 and loss3: 0.00\n",
      "Epoch [1798], train_loss: 783.80 with loss1: 659.71, loss2: 124.09 and loss3: 0.00\n",
      "Epoch [1799], train_loss: 781.46 with loss1: 657.60, loss2: 123.86 and loss3: 0.00\n",
      "Epoch [1800], train_loss: 788.62 with loss1: 664.77, loss2: 123.85 and loss3: 0.00\n",
      "Epoch [1801], train_loss: 781.95 with loss1: 657.82, loss2: 124.13 and loss3: 0.00\n",
      "Epoch [1802], train_loss: 784.04 with loss1: 659.69, loss2: 124.35 and loss3: 0.00\n",
      "Epoch [1803], train_loss: 776.99 with loss1: 653.01, loss2: 123.98 and loss3: 0.00\n",
      "Epoch [1804], train_loss: 778.87 with loss1: 654.70, loss2: 124.18 and loss3: 0.00\n",
      "Epoch [1805], train_loss: 772.06 with loss1: 647.74, loss2: 124.32 and loss3: 0.00\n",
      "Epoch [1806], train_loss: 770.91 with loss1: 646.61, loss2: 124.29 and loss3: 0.00\n",
      "Epoch [1807], train_loss: 766.07 with loss1: 641.97, loss2: 124.10 and loss3: 0.00\n",
      "Epoch [1808], train_loss: 765.63 with loss1: 641.49, loss2: 124.14 and loss3: 0.00\n",
      "Epoch [1809], train_loss: 759.74 with loss1: 635.82, loss2: 123.92 and loss3: 0.00\n",
      "Epoch [1810], train_loss: 757.34 with loss1: 633.16, loss2: 124.18 and loss3: 0.00\n",
      "Epoch [1811], train_loss: 751.54 with loss1: 627.45, loss2: 124.10 and loss3: 0.00\n",
      "Epoch [1812], train_loss: 751.01 with loss1: 626.79, loss2: 124.22 and loss3: 0.00\n",
      "Epoch [1813], train_loss: 744.07 with loss1: 620.40, loss2: 123.67 and loss3: 0.00\n",
      "Epoch [1814], train_loss: 746.03 with loss1: 621.99, loss2: 124.03 and loss3: 0.00\n",
      "Epoch [1815], train_loss: 741.49 with loss1: 617.67, loss2: 123.82 and loss3: 0.00\n",
      "Epoch [1816], train_loss: 740.20 with loss1: 616.24, loss2: 123.96 and loss3: 0.00\n",
      "Epoch [1817], train_loss: 735.54 with loss1: 611.73, loss2: 123.81 and loss3: 0.00\n",
      "Epoch [1818], train_loss: 737.97 with loss1: 614.29, loss2: 123.68 and loss3: 0.00\n",
      "Epoch [1819], train_loss: 733.59 with loss1: 610.45, loss2: 123.13 and loss3: 0.00\n",
      "Epoch [1820], train_loss: 735.70 with loss1: 612.06, loss2: 123.65 and loss3: 0.00\n",
      "Epoch [1821], train_loss: 729.53 with loss1: 606.43, loss2: 123.10 and loss3: 0.00\n",
      "Epoch [1822], train_loss: 736.09 with loss1: 612.69, loss2: 123.40 and loss3: 0.00\n",
      "Epoch [1823], train_loss: 731.86 with loss1: 608.56, loss2: 123.30 and loss3: 0.00\n",
      "Epoch [1824], train_loss: 736.16 with loss1: 612.79, loss2: 123.37 and loss3: 0.00\n",
      "Epoch [1825], train_loss: 731.78 with loss1: 608.51, loss2: 123.26 and loss3: 0.00\n",
      "Epoch [1826], train_loss: 736.38 with loss1: 613.07, loss2: 123.31 and loss3: 0.00\n",
      "Epoch [1827], train_loss: 732.35 with loss1: 609.61, loss2: 122.74 and loss3: 0.00\n",
      "Epoch [1828], train_loss: 736.41 with loss1: 613.17, loss2: 123.23 and loss3: 0.00\n",
      "Epoch [1829], train_loss: 729.93 with loss1: 607.16, loss2: 122.76 and loss3: 0.00\n",
      "Epoch [1830], train_loss: 733.25 with loss1: 610.35, loss2: 122.89 and loss3: 0.00\n",
      "Epoch [1831], train_loss: 729.43 with loss1: 606.61, loss2: 122.83 and loss3: 0.00\n",
      "Epoch [1832], train_loss: 731.35 with loss1: 608.64, loss2: 122.71 and loss3: 0.00\n",
      "Epoch [1833], train_loss: 727.26 with loss1: 604.89, loss2: 122.37 and loss3: 0.00\n",
      "Epoch [1834], train_loss: 730.08 with loss1: 607.57, loss2: 122.52 and loss3: 0.00\n",
      "Epoch [1835], train_loss: 725.43 with loss1: 603.49, loss2: 121.94 and loss3: 0.00\n",
      "Epoch [1836], train_loss: 728.24 with loss1: 605.93, loss2: 122.30 and loss3: 0.00\n",
      "Epoch [1837], train_loss: 722.41 with loss1: 600.25, loss2: 122.15 and loss3: 0.00\n",
      "Epoch [1838], train_loss: 724.85 with loss1: 602.54, loss2: 122.31 and loss3: 0.00\n",
      "Epoch [1839], train_loss: 721.49 with loss1: 599.71, loss2: 121.78 and loss3: 0.00\n",
      "Epoch [1840], train_loss: 722.96 with loss1: 601.05, loss2: 121.91 and loss3: 0.00\n",
      "Epoch [1841], train_loss: 721.68 with loss1: 600.21, loss2: 121.47 and loss3: 0.00\n",
      "Epoch [1842], train_loss: 724.71 with loss1: 602.64, loss2: 122.07 and loss3: 0.00\n",
      "Epoch [1843], train_loss: 720.07 with loss1: 598.89, loss2: 121.19 and loss3: 0.00\n",
      "Epoch [1844], train_loss: 719.71 with loss1: 597.80, loss2: 121.91 and loss3: 0.00\n",
      "Epoch [1845], train_loss: 716.01 with loss1: 594.59, loss2: 121.42 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1846], train_loss: 717.35 with loss1: 595.58, loss2: 121.78 and loss3: 0.00\n",
      "Epoch [1847], train_loss: 713.50 with loss1: 592.19, loss2: 121.31 and loss3: 0.00\n",
      "Epoch [1848], train_loss: 716.87 with loss1: 595.26, loss2: 121.60 and loss3: 0.00\n",
      "Epoch [1849], train_loss: 714.51 with loss1: 593.54, loss2: 120.97 and loss3: 0.00\n",
      "Epoch [1850], train_loss: 717.59 with loss1: 596.12, loss2: 121.47 and loss3: 0.00\n",
      "Epoch [1851], train_loss: 717.59 with loss1: 596.83, loss2: 120.76 and loss3: 0.00\n",
      "Epoch [1852], train_loss: 722.02 with loss1: 600.54, loss2: 121.48 and loss3: 0.00\n",
      "Epoch [1853], train_loss: 720.86 with loss1: 599.93, loss2: 120.93 and loss3: 0.00\n",
      "Epoch [1854], train_loss: 721.43 with loss1: 599.74, loss2: 121.70 and loss3: 0.00\n",
      "Epoch [1855], train_loss: 720.52 with loss1: 599.67, loss2: 120.85 and loss3: 0.00\n",
      "Epoch [1856], train_loss: 722.62 with loss1: 601.16, loss2: 121.47 and loss3: 0.00\n",
      "Epoch [1857], train_loss: 725.54 with loss1: 604.61, loss2: 120.92 and loss3: 0.00\n",
      "Epoch [1858], train_loss: 729.48 with loss1: 607.78, loss2: 121.70 and loss3: 0.00\n",
      "Epoch [1859], train_loss: 732.53 with loss1: 611.58, loss2: 120.95 and loss3: 0.00\n",
      "Epoch [1860], train_loss: 733.05 with loss1: 611.14, loss2: 121.91 and loss3: 0.00\n",
      "Epoch [1861], train_loss: 737.29 with loss1: 616.34, loss2: 120.94 and loss3: 0.00\n",
      "Epoch [1862], train_loss: 739.50 with loss1: 617.81, loss2: 121.69 and loss3: 0.00\n",
      "Epoch [1863], train_loss: 739.53 with loss1: 617.96, loss2: 121.58 and loss3: 0.00\n",
      "Epoch [1864], train_loss: 738.78 with loss1: 616.78, loss2: 122.00 and loss3: 0.00\n",
      "Epoch [1865], train_loss: 739.25 with loss1: 617.74, loss2: 121.51 and loss3: 0.00\n",
      "Epoch [1866], train_loss: 739.93 with loss1: 617.94, loss2: 121.99 and loss3: 0.00\n",
      "Epoch [1867], train_loss: 742.54 with loss1: 620.95, loss2: 121.59 and loss3: 0.00\n",
      "Epoch [1868], train_loss: 740.83 with loss1: 618.67, loss2: 122.16 and loss3: 0.00\n",
      "Epoch [1869], train_loss: 739.72 with loss1: 618.00, loss2: 121.72 and loss3: 0.00\n",
      "Epoch [1870], train_loss: 731.13 with loss1: 608.83, loss2: 122.30 and loss3: 0.00\n",
      "Epoch [1871], train_loss: 727.64 with loss1: 605.56, loss2: 122.08 and loss3: 0.00\n",
      "Epoch [1872], train_loss: 719.38 with loss1: 596.89, loss2: 122.49 and loss3: 0.00\n",
      "Epoch [1873], train_loss: 717.29 with loss1: 595.53, loss2: 121.76 and loss3: 0.00\n",
      "Epoch [1874], train_loss: 714.22 with loss1: 592.07, loss2: 122.16 and loss3: 0.00\n",
      "Epoch [1875], train_loss: 710.90 with loss1: 589.30, loss2: 121.60 and loss3: 0.00\n",
      "Epoch [1876], train_loss: 705.22 with loss1: 582.95, loss2: 122.27 and loss3: 0.00\n",
      "Epoch [1877], train_loss: 705.59 with loss1: 583.78, loss2: 121.81 and loss3: 0.00\n",
      "Epoch [1878], train_loss: 701.12 with loss1: 579.07, loss2: 122.05 and loss3: 0.00\n",
      "Epoch [1879], train_loss: 698.12 with loss1: 576.30, loss2: 121.82 and loss3: 0.00\n",
      "Epoch [1880], train_loss: 696.71 with loss1: 574.77, loss2: 121.94 and loss3: 0.00\n",
      "Epoch [1881], train_loss: 695.78 with loss1: 573.96, loss2: 121.82 and loss3: 0.00\n",
      "Epoch [1882], train_loss: 692.69 with loss1: 570.37, loss2: 122.32 and loss3: 0.00\n",
      "Epoch [1883], train_loss: 691.07 with loss1: 569.79, loss2: 121.28 and loss3: 0.00\n",
      "Epoch [1884], train_loss: 691.03 with loss1: 569.05, loss2: 121.98 and loss3: 0.00\n",
      "Epoch [1885], train_loss: 687.58 with loss1: 566.14, loss2: 121.44 and loss3: 0.00\n",
      "Epoch [1886], train_loss: 684.91 with loss1: 563.22, loss2: 121.69 and loss3: 0.00\n",
      "Epoch [1887], train_loss: 682.63 with loss1: 561.16, loss2: 121.47 and loss3: 0.00\n",
      "Epoch [1888], train_loss: 677.77 with loss1: 556.08, loss2: 121.69 and loss3: 0.00\n",
      "Epoch [1889], train_loss: 677.39 with loss1: 555.96, loss2: 121.43 and loss3: 0.00\n",
      "Epoch [1890], train_loss: 676.29 with loss1: 554.95, loss2: 121.34 and loss3: 0.00\n",
      "Epoch [1891], train_loss: 676.21 with loss1: 554.69, loss2: 121.52 and loss3: 0.00\n",
      "Epoch [1892], train_loss: 673.63 with loss1: 552.48, loss2: 121.15 and loss3: 0.00\n",
      "Epoch [1893], train_loss: 671.16 with loss1: 550.16, loss2: 121.00 and loss3: 0.00\n",
      "Epoch [1894], train_loss: 668.89 with loss1: 547.98, loss2: 120.91 and loss3: 0.00\n",
      "Epoch [1895], train_loss: 667.08 with loss1: 546.10, loss2: 120.98 and loss3: 0.00\n",
      "Epoch [1896], train_loss: 667.48 with loss1: 546.50, loss2: 120.98 and loss3: 0.00\n",
      "Epoch [1897], train_loss: 670.57 with loss1: 549.99, loss2: 120.57 and loss3: 0.00\n",
      "Epoch [1898], train_loss: 668.20 with loss1: 547.60, loss2: 120.60 and loss3: 0.00\n",
      "Epoch [1899], train_loss: 667.30 with loss1: 546.71, loss2: 120.59 and loss3: 0.00\n",
      "Epoch [1900], train_loss: 666.02 with loss1: 545.31, loss2: 120.71 and loss3: 0.00\n",
      "Epoch [1901], train_loss: 665.20 with loss1: 544.83, loss2: 120.37 and loss3: 0.00\n",
      "Epoch [1902], train_loss: 667.96 with loss1: 547.63, loss2: 120.33 and loss3: 0.00\n",
      "Epoch [1903], train_loss: 667.92 with loss1: 547.89, loss2: 120.03 and loss3: 0.00\n",
      "Epoch [1904], train_loss: 667.68 with loss1: 547.53, loss2: 120.15 and loss3: 0.00\n",
      "Epoch [1905], train_loss: 670.11 with loss1: 550.26, loss2: 119.85 and loss3: 0.00\n",
      "Epoch [1906], train_loss: 669.44 with loss1: 549.41, loss2: 120.03 and loss3: 0.00\n",
      "Epoch [1907], train_loss: 671.26 with loss1: 551.44, loss2: 119.82 and loss3: 0.00\n",
      "Epoch [1908], train_loss: 671.49 with loss1: 551.55, loss2: 119.94 and loss3: 0.00\n",
      "Epoch [1909], train_loss: 672.93 with loss1: 553.27, loss2: 119.66 and loss3: 0.00\n",
      "Epoch [1910], train_loss: 673.95 with loss1: 553.96, loss2: 119.98 and loss3: 0.00\n",
      "Epoch [1911], train_loss: 674.77 with loss1: 555.13, loss2: 119.65 and loss3: 0.00\n",
      "Epoch [1912], train_loss: 676.66 with loss1: 556.79, loss2: 119.87 and loss3: 0.00\n",
      "Epoch [1913], train_loss: 678.03 with loss1: 558.39, loss2: 119.64 and loss3: 0.00\n",
      "Epoch [1914], train_loss: 680.70 with loss1: 560.91, loss2: 119.80 and loss3: 0.00\n",
      "Epoch [1915], train_loss: 682.20 with loss1: 562.78, loss2: 119.42 and loss3: 0.00\n",
      "Epoch [1916], train_loss: 683.27 with loss1: 563.63, loss2: 119.64 and loss3: 0.00\n",
      "Epoch [1917], train_loss: 680.90 with loss1: 561.31, loss2: 119.59 and loss3: 0.00\n",
      "Epoch [1918], train_loss: 682.90 with loss1: 563.22, loss2: 119.68 and loss3: 0.00\n",
      "Epoch [1919], train_loss: 683.39 with loss1: 564.29, loss2: 119.10 and loss3: 0.00\n",
      "Epoch [1920], train_loss: 684.21 with loss1: 564.79, loss2: 119.42 and loss3: 0.00\n",
      "Epoch [1921], train_loss: 686.26 with loss1: 567.22, loss2: 119.04 and loss3: 0.00\n",
      "Epoch [1922], train_loss: 687.64 with loss1: 568.17, loss2: 119.47 and loss3: 0.00\n",
      "Epoch [1923], train_loss: 687.71 with loss1: 568.82, loss2: 118.89 and loss3: 0.00\n",
      "Epoch [1924], train_loss: 689.46 with loss1: 570.47, loss2: 119.00 and loss3: 0.00\n",
      "Epoch [1925], train_loss: 691.16 with loss1: 572.42, loss2: 118.74 and loss3: 0.00\n",
      "Epoch [1926], train_loss: 694.33 with loss1: 575.21, loss2: 119.12 and loss3: 0.00\n",
      "Epoch [1927], train_loss: 695.02 with loss1: 576.28, loss2: 118.74 and loss3: 0.00\n",
      "Epoch [1928], train_loss: 700.01 with loss1: 581.07, loss2: 118.93 and loss3: 0.00\n",
      "Epoch [1929], train_loss: 698.37 with loss1: 579.60, loss2: 118.78 and loss3: 0.00\n",
      "Epoch [1930], train_loss: 703.25 with loss1: 584.67, loss2: 118.57 and loss3: 0.00\n",
      "Epoch [1931], train_loss: 703.24 with loss1: 584.83, loss2: 118.41 and loss3: 0.00\n",
      "Epoch [1932], train_loss: 709.07 with loss1: 590.21, loss2: 118.87 and loss3: 0.00\n",
      "Epoch [1933], train_loss: 710.74 with loss1: 592.64, loss2: 118.10 and loss3: 0.00\n",
      "Epoch [1934], train_loss: 715.79 with loss1: 597.09, loss2: 118.70 and loss3: 0.00\n",
      "Epoch [1935], train_loss: 714.05 with loss1: 595.58, loss2: 118.48 and loss3: 0.00\n",
      "Epoch [1936], train_loss: 721.91 with loss1: 603.28, loss2: 118.63 and loss3: 0.00\n",
      "Epoch [1937], train_loss: 721.96 with loss1: 604.05, loss2: 117.91 and loss3: 0.00\n",
      "Epoch [1938], train_loss: 726.08 with loss1: 607.80, loss2: 118.28 and loss3: 0.00\n",
      "Epoch [1939], train_loss: 724.36 with loss1: 606.49, loss2: 117.86 and loss3: 0.00\n",
      "Epoch [1940], train_loss: 732.06 with loss1: 613.65, loss2: 118.41 and loss3: 0.00\n",
      "Epoch [1941], train_loss: 733.32 with loss1: 615.30, loss2: 118.02 and loss3: 0.00\n",
      "Epoch [1942], train_loss: 737.92 with loss1: 619.51, loss2: 118.41 and loss3: 0.00\n",
      "Epoch [1943], train_loss: 737.14 with loss1: 619.24, loss2: 117.90 and loss3: 0.00\n",
      "Epoch [1944], train_loss: 746.52 with loss1: 627.95, loss2: 118.57 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1945], train_loss: 743.15 with loss1: 625.22, loss2: 117.93 and loss3: 0.00\n",
      "Epoch [1946], train_loss: 745.89 with loss1: 627.66, loss2: 118.23 and loss3: 0.00\n",
      "Epoch [1947], train_loss: 742.16 with loss1: 624.21, loss2: 117.94 and loss3: 0.00\n",
      "Epoch [1948], train_loss: 746.83 with loss1: 628.31, loss2: 118.52 and loss3: 0.00\n",
      "Epoch [1949], train_loss: 740.89 with loss1: 623.28, loss2: 117.62 and loss3: 0.00\n",
      "Epoch [1950], train_loss: 741.37 with loss1: 623.21, loss2: 118.16 and loss3: 0.00\n",
      "Epoch [1951], train_loss: 738.41 with loss1: 620.64, loss2: 117.77 and loss3: 0.00\n",
      "Epoch [1952], train_loss: 741.90 with loss1: 623.68, loss2: 118.22 and loss3: 0.00\n",
      "Epoch [1953], train_loss: 738.14 with loss1: 620.73, loss2: 117.41 and loss3: 0.00\n",
      "Epoch [1954], train_loss: 738.16 with loss1: 620.24, loss2: 117.93 and loss3: 0.00\n",
      "Epoch [1955], train_loss: 736.73 with loss1: 618.84, loss2: 117.90 and loss3: 0.00\n",
      "Epoch [1956], train_loss: 736.97 with loss1: 618.58, loss2: 118.39 and loss3: 0.00\n",
      "Epoch [1957], train_loss: 730.08 with loss1: 612.53, loss2: 117.55 and loss3: 0.00\n",
      "Epoch [1958], train_loss: 732.24 with loss1: 613.94, loss2: 118.29 and loss3: 0.00\n",
      "Epoch [1959], train_loss: 725.29 with loss1: 607.69, loss2: 117.60 and loss3: 0.00\n",
      "Epoch [1960], train_loss: 726.88 with loss1: 609.02, loss2: 117.86 and loss3: 0.00\n",
      "Epoch [1961], train_loss: 718.84 with loss1: 601.34, loss2: 117.50 and loss3: 0.00\n",
      "Epoch [1962], train_loss: 721.60 with loss1: 603.79, loss2: 117.81 and loss3: 0.00\n",
      "Epoch [1963], train_loss: 716.51 with loss1: 599.17, loss2: 117.34 and loss3: 0.00\n",
      "Epoch [1964], train_loss: 714.65 with loss1: 596.82, loss2: 117.83 and loss3: 0.00\n",
      "Epoch [1965], train_loss: 710.01 with loss1: 592.50, loss2: 117.52 and loss3: 0.00\n",
      "Epoch [1966], train_loss: 708.97 with loss1: 591.22, loss2: 117.74 and loss3: 0.00\n",
      "Epoch [1967], train_loss: 707.13 with loss1: 589.74, loss2: 117.39 and loss3: 0.00\n",
      "Epoch [1968], train_loss: 709.38 with loss1: 591.72, loss2: 117.66 and loss3: 0.00\n",
      "Epoch [1969], train_loss: 703.22 with loss1: 585.75, loss2: 117.47 and loss3: 0.00\n",
      "Epoch [1970], train_loss: 701.53 with loss1: 584.18, loss2: 117.35 and loss3: 0.00\n",
      "Epoch [1971], train_loss: 698.86 with loss1: 581.72, loss2: 117.13 and loss3: 0.00\n",
      "Epoch [1972], train_loss: 698.21 with loss1: 580.76, loss2: 117.45 and loss3: 0.00\n",
      "Epoch [1973], train_loss: 693.97 with loss1: 576.94, loss2: 117.03 and loss3: 0.00\n",
      "Epoch [1974], train_loss: 697.14 with loss1: 579.74, loss2: 117.40 and loss3: 0.00\n",
      "Epoch [1975], train_loss: 695.08 with loss1: 577.97, loss2: 117.11 and loss3: 0.00\n",
      "Epoch [1976], train_loss: 693.62 with loss1: 576.16, loss2: 117.47 and loss3: 0.00\n",
      "Epoch [1977], train_loss: 691.12 with loss1: 574.28, loss2: 116.84 and loss3: 0.00\n",
      "Epoch [1978], train_loss: 691.90 with loss1: 574.55, loss2: 117.35 and loss3: 0.00\n",
      "Epoch [1979], train_loss: 688.98 with loss1: 571.83, loss2: 117.15 and loss3: 0.00\n",
      "Epoch [1980], train_loss: 690.28 with loss1: 573.09, loss2: 117.19 and loss3: 0.00\n",
      "Epoch [1981], train_loss: 688.15 with loss1: 571.06, loss2: 117.08 and loss3: 0.00\n",
      "Epoch [1982], train_loss: 687.72 with loss1: 570.43, loss2: 117.30 and loss3: 0.00\n",
      "Epoch [1983], train_loss: 684.38 with loss1: 567.70, loss2: 116.67 and loss3: 0.00\n",
      "Epoch [1984], train_loss: 684.08 with loss1: 567.19, loss2: 116.89 and loss3: 0.00\n",
      "Epoch [1985], train_loss: 686.38 with loss1: 569.85, loss2: 116.52 and loss3: 0.00\n",
      "Epoch [1986], train_loss: 688.33 with loss1: 571.39, loss2: 116.94 and loss3: 0.00\n",
      "Epoch [1987], train_loss: 684.13 with loss1: 567.49, loss2: 116.64 and loss3: 0.00\n",
      "Epoch [1988], train_loss: 685.79 with loss1: 568.90, loss2: 116.89 and loss3: 0.00\n",
      "Epoch [1989], train_loss: 683.87 with loss1: 567.26, loss2: 116.62 and loss3: 0.00\n",
      "Epoch [1990], train_loss: 687.31 with loss1: 570.27, loss2: 117.05 and loss3: 0.00\n",
      "Epoch [1991], train_loss: 683.01 with loss1: 566.40, loss2: 116.61 and loss3: 0.00\n",
      "Epoch [1992], train_loss: 687.18 with loss1: 570.49, loss2: 116.68 and loss3: 0.00\n",
      "Epoch [1993], train_loss: 684.22 with loss1: 567.88, loss2: 116.34 and loss3: 0.00\n",
      "Epoch [1994], train_loss: 686.53 with loss1: 570.00, loss2: 116.53 and loss3: 0.00\n",
      "Epoch [1995], train_loss: 683.90 with loss1: 567.63, loss2: 116.27 and loss3: 0.00\n",
      "Epoch [1996], train_loss: 685.54 with loss1: 569.14, loss2: 116.39 and loss3: 0.00\n",
      "Epoch [1997], train_loss: 684.71 with loss1: 568.54, loss2: 116.16 and loss3: 0.00\n",
      "Epoch [1998], train_loss: 688.50 with loss1: 572.29, loss2: 116.21 and loss3: 0.00\n",
      "Epoch [1999], train_loss: 684.45 with loss1: 568.26, loss2: 116.20 and loss3: 0.00\n",
      "Epoch [2000], train_loss: 689.45 with loss1: 573.06, loss2: 116.39 and loss3: 0.00\n",
      "Epoch [2001], train_loss: 689.47 with loss1: 573.22, loss2: 116.24 and loss3: 0.00\n",
      "Epoch [2002], train_loss: 692.54 with loss1: 576.17, loss2: 116.37 and loss3: 0.00\n",
      "Epoch [2003], train_loss: 687.99 with loss1: 571.85, loss2: 116.14 and loss3: 0.00\n",
      "Epoch [2004], train_loss: 689.68 with loss1: 573.15, loss2: 116.53 and loss3: 0.00\n",
      "Epoch [2005], train_loss: 686.00 with loss1: 569.85, loss2: 116.16 and loss3: 0.00\n",
      "Epoch [2006], train_loss: 688.38 with loss1: 572.09, loss2: 116.29 and loss3: 0.00\n",
      "Epoch [2007], train_loss: 685.24 with loss1: 568.90, loss2: 116.34 and loss3: 0.00\n",
      "Epoch [2008], train_loss: 691.32 with loss1: 575.14, loss2: 116.18 and loss3: 0.00\n",
      "Epoch [2009], train_loss: 688.05 with loss1: 571.74, loss2: 116.32 and loss3: 0.00\n",
      "Epoch [2010], train_loss: 688.21 with loss1: 571.80, loss2: 116.41 and loss3: 0.00\n",
      "Epoch [2011], train_loss: 685.57 with loss1: 569.37, loss2: 116.20 and loss3: 0.00\n",
      "Epoch [2012], train_loss: 685.38 with loss1: 569.07, loss2: 116.31 and loss3: 0.00\n",
      "Epoch [2013], train_loss: 680.42 with loss1: 564.41, loss2: 116.01 and loss3: 0.00\n",
      "Epoch [2014], train_loss: 681.01 with loss1: 564.91, loss2: 116.11 and loss3: 0.00\n",
      "Epoch [2015], train_loss: 679.86 with loss1: 563.95, loss2: 115.91 and loss3: 0.00\n",
      "Epoch [2016], train_loss: 681.60 with loss1: 565.41, loss2: 116.19 and loss3: 0.00\n",
      "Epoch [2017], train_loss: 676.42 with loss1: 560.50, loss2: 115.92 and loss3: 0.00\n",
      "Epoch [2018], train_loss: 680.52 with loss1: 564.62, loss2: 115.90 and loss3: 0.00\n",
      "Epoch [2019], train_loss: 676.98 with loss1: 560.99, loss2: 115.98 and loss3: 0.00\n",
      "Epoch [2020], train_loss: 676.07 with loss1: 560.15, loss2: 115.92 and loss3: 0.00\n",
      "Epoch [2021], train_loss: 675.97 with loss1: 560.07, loss2: 115.90 and loss3: 0.00\n",
      "Epoch [2022], train_loss: 677.42 with loss1: 561.76, loss2: 115.66 and loss3: 0.00\n",
      "Epoch [2023], train_loss: 673.16 with loss1: 557.47, loss2: 115.69 and loss3: 0.00\n",
      "Epoch [2024], train_loss: 677.07 with loss1: 561.03, loss2: 116.04 and loss3: 0.00\n",
      "Epoch [2025], train_loss: 672.03 with loss1: 556.43, loss2: 115.60 and loss3: 0.00\n",
      "Epoch [2026], train_loss: 674.43 with loss1: 558.49, loss2: 115.94 and loss3: 0.00\n",
      "Epoch [2027], train_loss: 668.47 with loss1: 552.96, loss2: 115.50 and loss3: 0.00\n",
      "Epoch [2028], train_loss: 671.99 with loss1: 556.41, loss2: 115.58 and loss3: 0.00\n",
      "Epoch [2029], train_loss: 667.61 with loss1: 552.05, loss2: 115.56 and loss3: 0.00\n",
      "Epoch [2030], train_loss: 667.11 with loss1: 551.10, loss2: 116.01 and loss3: 0.00\n",
      "Epoch [2031], train_loss: 664.53 with loss1: 548.79, loss2: 115.74 and loss3: 0.00\n",
      "Epoch [2032], train_loss: 665.08 with loss1: 549.41, loss2: 115.67 and loss3: 0.00\n",
      "Epoch [2033], train_loss: 663.56 with loss1: 548.13, loss2: 115.44 and loss3: 0.00\n",
      "Epoch [2034], train_loss: 661.78 with loss1: 545.94, loss2: 115.84 and loss3: 0.00\n",
      "Epoch [2035], train_loss: 659.11 with loss1: 543.81, loss2: 115.30 and loss3: 0.00\n",
      "Epoch [2036], train_loss: 657.14 with loss1: 541.70, loss2: 115.43 and loss3: 0.00\n",
      "Epoch [2037], train_loss: 655.12 with loss1: 539.82, loss2: 115.30 and loss3: 0.00\n",
      "Epoch [2038], train_loss: 655.84 with loss1: 540.51, loss2: 115.33 and loss3: 0.00\n",
      "Epoch [2039], train_loss: 655.22 with loss1: 540.06, loss2: 115.16 and loss3: 0.00\n",
      "Epoch [2040], train_loss: 654.64 with loss1: 539.45, loss2: 115.19 and loss3: 0.00\n",
      "Epoch [2041], train_loss: 652.00 with loss1: 537.14, loss2: 114.86 and loss3: 0.00\n",
      "Epoch [2042], train_loss: 653.07 with loss1: 538.19, loss2: 114.88 and loss3: 0.00\n",
      "Epoch [2043], train_loss: 652.29 with loss1: 537.27, loss2: 115.02 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2044], train_loss: 651.81 with loss1: 536.94, loss2: 114.87 and loss3: 0.00\n",
      "Epoch [2045], train_loss: 650.17 with loss1: 535.35, loss2: 114.82 and loss3: 0.00\n",
      "Epoch [2046], train_loss: 650.58 with loss1: 535.81, loss2: 114.77 and loss3: 0.00\n",
      "Epoch [2047], train_loss: 648.48 with loss1: 533.91, loss2: 114.57 and loss3: 0.00\n",
      "Epoch [2048], train_loss: 649.87 with loss1: 535.24, loss2: 114.63 and loss3: 0.00\n",
      "Epoch [2049], train_loss: 649.15 with loss1: 534.55, loss2: 114.60 and loss3: 0.00\n",
      "Epoch [2050], train_loss: 650.29 with loss1: 536.01, loss2: 114.28 and loss3: 0.00\n",
      "Epoch [2051], train_loss: 649.70 with loss1: 535.35, loss2: 114.35 and loss3: 0.00\n",
      "Epoch [2052], train_loss: 653.14 with loss1: 538.88, loss2: 114.26 and loss3: 0.00\n",
      "Epoch [2053], train_loss: 648.69 with loss1: 534.68, loss2: 114.01 and loss3: 0.00\n",
      "Epoch [2054], train_loss: 649.31 with loss1: 534.97, loss2: 114.34 and loss3: 0.00\n",
      "Epoch [2055], train_loss: 647.27 with loss1: 533.06, loss2: 114.22 and loss3: 0.00\n",
      "Epoch [2056], train_loss: 648.31 with loss1: 534.17, loss2: 114.13 and loss3: 0.00\n",
      "Epoch [2057], train_loss: 646.90 with loss1: 533.15, loss2: 113.75 and loss3: 0.00\n",
      "Epoch [2058], train_loss: 648.80 with loss1: 535.03, loss2: 113.77 and loss3: 0.00\n",
      "Epoch [2059], train_loss: 647.78 with loss1: 534.16, loss2: 113.62 and loss3: 0.00\n",
      "Epoch [2060], train_loss: 649.84 with loss1: 536.01, loss2: 113.83 and loss3: 0.00\n",
      "Epoch [2061], train_loss: 650.18 with loss1: 536.68, loss2: 113.50 and loss3: 0.00\n",
      "Epoch [2062], train_loss: 652.11 with loss1: 538.59, loss2: 113.51 and loss3: 0.00\n",
      "Epoch [2063], train_loss: 652.07 with loss1: 538.31, loss2: 113.76 and loss3: 0.00\n",
      "Epoch [2064], train_loss: 655.34 with loss1: 542.03, loss2: 113.31 and loss3: 0.00\n",
      "Epoch [2065], train_loss: 652.10 with loss1: 538.55, loss2: 113.54 and loss3: 0.00\n",
      "Epoch [2066], train_loss: 655.43 with loss1: 541.90, loss2: 113.53 and loss3: 0.00\n",
      "Epoch [2067], train_loss: 653.49 with loss1: 540.13, loss2: 113.35 and loss3: 0.00\n",
      "Epoch [2068], train_loss: 655.80 with loss1: 542.12, loss2: 113.68 and loss3: 0.00\n",
      "Epoch [2069], train_loss: 655.42 with loss1: 542.40, loss2: 113.02 and loss3: 0.00\n",
      "Epoch [2070], train_loss: 658.67 with loss1: 545.44, loss2: 113.22 and loss3: 0.00\n",
      "Epoch [2071], train_loss: 661.05 with loss1: 548.20, loss2: 112.85 and loss3: 0.00\n",
      "Epoch [2072], train_loss: 664.06 with loss1: 550.94, loss2: 113.11 and loss3: 0.00\n",
      "Epoch [2073], train_loss: 664.54 with loss1: 551.39, loss2: 113.15 and loss3: 0.00\n",
      "Epoch [2074], train_loss: 670.50 with loss1: 557.44, loss2: 113.06 and loss3: 0.00\n",
      "Epoch [2075], train_loss: 671.35 with loss1: 558.78, loss2: 112.57 and loss3: 0.00\n",
      "Epoch [2076], train_loss: 676.85 with loss1: 563.70, loss2: 113.15 and loss3: 0.00\n",
      "Epoch [2077], train_loss: 679.86 with loss1: 567.16, loss2: 112.70 and loss3: 0.00\n",
      "Epoch [2078], train_loss: 686.58 with loss1: 573.66, loss2: 112.92 and loss3: 0.00\n",
      "Epoch [2079], train_loss: 691.71 with loss1: 578.90, loss2: 112.80 and loss3: 0.00\n",
      "Epoch [2080], train_loss: 700.70 with loss1: 587.95, loss2: 112.75 and loss3: 0.00\n",
      "Epoch [2081], train_loss: 705.66 with loss1: 593.06, loss2: 112.60 and loss3: 0.00\n",
      "Epoch [2082], train_loss: 716.33 with loss1: 603.66, loss2: 112.68 and loss3: 0.00\n",
      "Epoch [2083], train_loss: 723.10 with loss1: 610.63, loss2: 112.47 and loss3: 0.00\n",
      "Epoch [2084], train_loss: 736.44 with loss1: 624.05, loss2: 112.39 and loss3: 0.00\n",
      "Epoch [2085], train_loss: 744.07 with loss1: 631.87, loss2: 112.21 and loss3: 0.00\n",
      "Epoch [2086], train_loss: 756.79 with loss1: 644.14, loss2: 112.65 and loss3: 0.00\n",
      "Epoch [2087], train_loss: 759.82 with loss1: 647.54, loss2: 112.28 and loss3: 0.00\n",
      "Epoch [2088], train_loss: 771.12 with loss1: 658.32, loss2: 112.79 and loss3: 0.00\n",
      "Epoch [2089], train_loss: 770.25 with loss1: 658.08, loss2: 112.17 and loss3: 0.00\n",
      "Epoch [2090], train_loss: 777.01 with loss1: 663.97, loss2: 113.04 and loss3: 0.00\n",
      "Epoch [2091], train_loss: 773.95 with loss1: 661.39, loss2: 112.55 and loss3: 0.00\n",
      "Epoch [2092], train_loss: 776.19 with loss1: 663.52, loss2: 112.67 and loss3: 0.00\n",
      "Epoch [2093], train_loss: 767.03 with loss1: 654.71, loss2: 112.33 and loss3: 0.00\n",
      "Epoch [2094], train_loss: 763.50 with loss1: 650.66, loss2: 112.84 and loss3: 0.00\n",
      "Epoch [2095], train_loss: 755.59 with loss1: 643.22, loss2: 112.37 and loss3: 0.00\n",
      "Epoch [2096], train_loss: 751.24 with loss1: 638.33, loss2: 112.91 and loss3: 0.00\n",
      "Epoch [2097], train_loss: 742.72 with loss1: 630.38, loss2: 112.34 and loss3: 0.00\n",
      "Epoch [2098], train_loss: 735.00 with loss1: 622.43, loss2: 112.57 and loss3: 0.00\n",
      "Epoch [2099], train_loss: 722.45 with loss1: 610.05, loss2: 112.40 and loss3: 0.00\n",
      "Epoch [2100], train_loss: 719.33 with loss1: 606.72, loss2: 112.61 and loss3: 0.00\n",
      "Epoch [2101], train_loss: 708.48 with loss1: 596.08, loss2: 112.40 and loss3: 0.00\n",
      "Epoch [2102], train_loss: 702.52 with loss1: 589.76, loss2: 112.76 and loss3: 0.00\n",
      "Epoch [2103], train_loss: 694.51 with loss1: 582.16, loss2: 112.35 and loss3: 0.00\n",
      "Epoch [2104], train_loss: 689.04 with loss1: 576.43, loss2: 112.61 and loss3: 0.00\n",
      "Epoch [2105], train_loss: 681.61 with loss1: 569.44, loss2: 112.17 and loss3: 0.00\n",
      "Epoch [2106], train_loss: 676.89 with loss1: 564.25, loss2: 112.64 and loss3: 0.00\n",
      "Epoch [2107], train_loss: 668.77 with loss1: 556.65, loss2: 112.12 and loss3: 0.00\n",
      "Epoch [2108], train_loss: 666.45 with loss1: 553.93, loss2: 112.53 and loss3: 0.00\n",
      "Epoch [2109], train_loss: 659.02 with loss1: 546.81, loss2: 112.20 and loss3: 0.00\n",
      "Epoch [2110], train_loss: 655.31 with loss1: 542.75, loss2: 112.56 and loss3: 0.00\n",
      "Epoch [2111], train_loss: 653.90 with loss1: 541.80, loss2: 112.10 and loss3: 0.00\n",
      "Epoch [2112], train_loss: 649.52 with loss1: 536.75, loss2: 112.77 and loss3: 0.00\n",
      "Epoch [2113], train_loss: 645.31 with loss1: 533.02, loss2: 112.29 and loss3: 0.00\n",
      "Epoch [2114], train_loss: 641.79 with loss1: 529.64, loss2: 112.16 and loss3: 0.00\n",
      "Epoch [2115], train_loss: 640.36 with loss1: 528.52, loss2: 111.84 and loss3: 0.00\n",
      "Epoch [2116], train_loss: 638.69 with loss1: 526.65, loss2: 112.05 and loss3: 0.00\n",
      "Epoch [2117], train_loss: 638.41 with loss1: 526.75, loss2: 111.65 and loss3: 0.00\n",
      "Epoch [2118], train_loss: 637.16 with loss1: 525.06, loss2: 112.10 and loss3: 0.00\n",
      "Epoch [2119], train_loss: 636.82 with loss1: 525.10, loss2: 111.72 and loss3: 0.00\n",
      "Epoch [2120], train_loss: 633.99 with loss1: 522.02, loss2: 111.97 and loss3: 0.00\n",
      "Epoch [2121], train_loss: 632.88 with loss1: 521.22, loss2: 111.66 and loss3: 0.00\n",
      "Epoch [2122], train_loss: 631.05 with loss1: 519.19, loss2: 111.86 and loss3: 0.00\n",
      "Epoch [2123], train_loss: 628.95 with loss1: 517.23, loss2: 111.72 and loss3: 0.00\n",
      "Epoch [2124], train_loss: 630.96 with loss1: 519.41, loss2: 111.54 and loss3: 0.00\n",
      "Epoch [2125], train_loss: 627.75 with loss1: 516.46, loss2: 111.29 and loss3: 0.00\n",
      "Epoch [2126], train_loss: 630.34 with loss1: 518.88, loss2: 111.47 and loss3: 0.00\n",
      "Epoch [2127], train_loss: 630.61 with loss1: 519.44, loss2: 111.17 and loss3: 0.00\n",
      "Epoch [2128], train_loss: 630.29 with loss1: 518.91, loss2: 111.39 and loss3: 0.00\n",
      "Epoch [2129], train_loss: 630.68 with loss1: 519.36, loss2: 111.33 and loss3: 0.00\n",
      "Epoch [2130], train_loss: 632.08 with loss1: 520.89, loss2: 111.20 and loss3: 0.00\n",
      "Epoch [2131], train_loss: 630.78 with loss1: 519.67, loss2: 111.10 and loss3: 0.00\n",
      "Epoch [2132], train_loss: 631.77 with loss1: 520.43, loss2: 111.33 and loss3: 0.00\n",
      "Epoch [2133], train_loss: 631.27 with loss1: 520.53, loss2: 110.73 and loss3: 0.00\n",
      "Epoch [2134], train_loss: 632.13 with loss1: 520.94, loss2: 111.19 and loss3: 0.00\n",
      "Epoch [2135], train_loss: 633.11 with loss1: 522.23, loss2: 110.88 and loss3: 0.00\n",
      "Epoch [2136], train_loss: 633.83 with loss1: 522.80, loss2: 111.02 and loss3: 0.00\n",
      "Epoch [2137], train_loss: 635.61 with loss1: 524.64, loss2: 110.98 and loss3: 0.00\n",
      "Epoch [2138], train_loss: 635.89 with loss1: 524.86, loss2: 111.02 and loss3: 0.00\n",
      "Epoch [2139], train_loss: 635.84 with loss1: 524.90, loss2: 110.94 and loss3: 0.00\n",
      "Epoch [2140], train_loss: 640.59 with loss1: 529.64, loss2: 110.95 and loss3: 0.00\n",
      "Epoch [2141], train_loss: 641.68 with loss1: 530.92, loss2: 110.77 and loss3: 0.00\n",
      "Epoch [2142], train_loss: 641.42 with loss1: 530.63, loss2: 110.79 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2143], train_loss: 640.14 with loss1: 529.28, loss2: 110.86 and loss3: 0.00\n",
      "Epoch [2144], train_loss: 642.23 with loss1: 531.38, loss2: 110.85 and loss3: 0.00\n",
      "Epoch [2145], train_loss: 642.53 with loss1: 531.96, loss2: 110.56 and loss3: 0.00\n",
      "Epoch [2146], train_loss: 643.38 with loss1: 532.63, loss2: 110.75 and loss3: 0.00\n",
      "Epoch [2147], train_loss: 641.98 with loss1: 531.51, loss2: 110.47 and loss3: 0.00\n",
      "Epoch [2148], train_loss: 645.21 with loss1: 534.66, loss2: 110.55 and loss3: 0.00\n",
      "Epoch [2149], train_loss: 646.01 with loss1: 535.33, loss2: 110.68 and loss3: 0.00\n",
      "Epoch [2150], train_loss: 648.21 with loss1: 537.72, loss2: 110.48 and loss3: 0.00\n",
      "Epoch [2151], train_loss: 649.11 with loss1: 538.64, loss2: 110.47 and loss3: 0.00\n",
      "Epoch [2152], train_loss: 649.68 with loss1: 539.14, loss2: 110.54 and loss3: 0.00\n",
      "Epoch [2153], train_loss: 649.32 with loss1: 538.75, loss2: 110.57 and loss3: 0.00\n",
      "Epoch [2154], train_loss: 648.23 with loss1: 537.70, loss2: 110.52 and loss3: 0.00\n",
      "Epoch [2155], train_loss: 650.17 with loss1: 539.80, loss2: 110.37 and loss3: 0.00\n",
      "Epoch [2156], train_loss: 649.75 with loss1: 539.06, loss2: 110.69 and loss3: 0.00\n",
      "Epoch [2157], train_loss: 650.26 with loss1: 540.07, loss2: 110.18 and loss3: 0.00\n",
      "Epoch [2158], train_loss: 647.34 with loss1: 536.87, loss2: 110.47 and loss3: 0.00\n",
      "Epoch [2159], train_loss: 647.58 with loss1: 537.34, loss2: 110.24 and loss3: 0.00\n",
      "Epoch [2160], train_loss: 645.84 with loss1: 535.32, loss2: 110.52 and loss3: 0.00\n",
      "Epoch [2161], train_loss: 646.67 with loss1: 536.34, loss2: 110.33 and loss3: 0.00\n",
      "Epoch [2162], train_loss: 642.99 with loss1: 532.22, loss2: 110.77 and loss3: 0.00\n",
      "Epoch [2163], train_loss: 641.84 with loss1: 531.45, loss2: 110.40 and loss3: 0.00\n",
      "Epoch [2164], train_loss: 639.80 with loss1: 529.33, loss2: 110.46 and loss3: 0.00\n",
      "Epoch [2165], train_loss: 640.85 with loss1: 530.54, loss2: 110.31 and loss3: 0.00\n",
      "Epoch [2166], train_loss: 639.18 with loss1: 528.76, loss2: 110.42 and loss3: 0.00\n",
      "Epoch [2167], train_loss: 636.95 with loss1: 526.88, loss2: 110.08 and loss3: 0.00\n",
      "Epoch [2168], train_loss: 634.79 with loss1: 524.46, loss2: 110.33 and loss3: 0.00\n",
      "Epoch [2169], train_loss: 635.00 with loss1: 525.18, loss2: 109.82 and loss3: 0.00\n",
      "Epoch [2170], train_loss: 634.46 with loss1: 524.27, loss2: 110.18 and loss3: 0.00\n",
      "Epoch [2171], train_loss: 634.04 with loss1: 524.12, loss2: 109.92 and loss3: 0.00\n",
      "Epoch [2172], train_loss: 634.34 with loss1: 523.95, loss2: 110.39 and loss3: 0.00\n",
      "Epoch [2173], train_loss: 633.74 with loss1: 523.65, loss2: 110.09 and loss3: 0.00\n",
      "Epoch [2174], train_loss: 632.70 with loss1: 522.56, loss2: 110.15 and loss3: 0.00\n",
      "Epoch [2175], train_loss: 631.84 with loss1: 521.97, loss2: 109.87 and loss3: 0.00\n",
      "Epoch [2176], train_loss: 631.76 with loss1: 521.51, loss2: 110.25 and loss3: 0.00\n",
      "Epoch [2177], train_loss: 631.43 with loss1: 521.46, loss2: 109.97 and loss3: 0.00\n",
      "Epoch [2178], train_loss: 630.63 with loss1: 520.66, loss2: 109.96 and loss3: 0.00\n",
      "Epoch [2179], train_loss: 634.02 with loss1: 524.30, loss2: 109.73 and loss3: 0.00\n",
      "Epoch [2180], train_loss: 633.49 with loss1: 523.58, loss2: 109.91 and loss3: 0.00\n",
      "Epoch [2181], train_loss: 633.47 with loss1: 523.68, loss2: 109.79 and loss3: 0.00\n",
      "Epoch [2182], train_loss: 633.78 with loss1: 523.77, loss2: 110.00 and loss3: 0.00\n",
      "Epoch [2183], train_loss: 634.65 with loss1: 525.02, loss2: 109.63 and loss3: 0.00\n",
      "Epoch [2184], train_loss: 636.09 with loss1: 526.32, loss2: 109.77 and loss3: 0.00\n",
      "Epoch [2185], train_loss: 636.29 with loss1: 526.62, loss2: 109.67 and loss3: 0.00\n",
      "Epoch [2186], train_loss: 637.11 with loss1: 527.24, loss2: 109.87 and loss3: 0.00\n",
      "Epoch [2187], train_loss: 639.36 with loss1: 529.66, loss2: 109.70 and loss3: 0.00\n",
      "Epoch [2188], train_loss: 640.99 with loss1: 531.57, loss2: 109.42 and loss3: 0.00\n",
      "Epoch [2189], train_loss: 641.48 with loss1: 531.70, loss2: 109.78 and loss3: 0.00\n",
      "Epoch [2190], train_loss: 644.36 with loss1: 534.58, loss2: 109.78 and loss3: 0.00\n",
      "Epoch [2191], train_loss: 646.49 with loss1: 536.94, loss2: 109.55 and loss3: 0.00\n",
      "Epoch [2192], train_loss: 646.81 with loss1: 537.03, loss2: 109.78 and loss3: 0.00\n",
      "Epoch [2193], train_loss: 651.00 with loss1: 541.30, loss2: 109.71 and loss3: 0.00\n",
      "Epoch [2194], train_loss: 648.31 with loss1: 538.54, loss2: 109.77 and loss3: 0.00\n",
      "Epoch [2195], train_loss: 651.03 with loss1: 541.09, loss2: 109.94 and loss3: 0.00\n",
      "Epoch [2196], train_loss: 653.43 with loss1: 543.71, loss2: 109.72 and loss3: 0.00\n",
      "Epoch [2197], train_loss: 656.74 with loss1: 547.27, loss2: 109.47 and loss3: 0.00\n",
      "Epoch [2198], train_loss: 655.15 with loss1: 545.51, loss2: 109.64 and loss3: 0.00\n",
      "Epoch [2199], train_loss: 657.22 with loss1: 547.64, loss2: 109.58 and loss3: 0.00\n",
      "Epoch [2200], train_loss: 655.95 with loss1: 546.43, loss2: 109.52 and loss3: 0.00\n",
      "Epoch [2201], train_loss: 658.33 with loss1: 548.56, loss2: 109.77 and loss3: 0.00\n",
      "Epoch [2202], train_loss: 655.79 with loss1: 546.17, loss2: 109.62 and loss3: 0.00\n",
      "Epoch [2203], train_loss: 654.51 with loss1: 544.77, loss2: 109.74 and loss3: 0.00\n",
      "Epoch [2204], train_loss: 652.71 with loss1: 542.87, loss2: 109.84 and loss3: 0.00\n",
      "Epoch [2205], train_loss: 655.43 with loss1: 545.80, loss2: 109.63 and loss3: 0.00\n",
      "Epoch [2206], train_loss: 650.39 with loss1: 540.95, loss2: 109.44 and loss3: 0.00\n",
      "Epoch [2207], train_loss: 654.83 with loss1: 545.22, loss2: 109.60 and loss3: 0.00\n",
      "Epoch [2208], train_loss: 651.71 with loss1: 542.41, loss2: 109.30 and loss3: 0.00\n",
      "Epoch [2209], train_loss: 654.53 with loss1: 544.74, loss2: 109.80 and loss3: 0.00\n",
      "Epoch [2210], train_loss: 653.25 with loss1: 543.87, loss2: 109.37 and loss3: 0.00\n",
      "Epoch [2211], train_loss: 658.35 with loss1: 548.77, loss2: 109.58 and loss3: 0.00\n",
      "Epoch [2212], train_loss: 658.96 with loss1: 549.71, loss2: 109.25 and loss3: 0.00\n",
      "Epoch [2213], train_loss: 661.66 with loss1: 552.39, loss2: 109.28 and loss3: 0.00\n",
      "Epoch [2214], train_loss: 664.75 with loss1: 555.53, loss2: 109.22 and loss3: 0.00\n",
      "Epoch [2215], train_loss: 672.90 with loss1: 563.70, loss2: 109.20 and loss3: 0.00\n",
      "Epoch [2216], train_loss: 676.01 with loss1: 566.98, loss2: 109.03 and loss3: 0.00\n",
      "Epoch [2217], train_loss: 681.35 with loss1: 571.72, loss2: 109.63 and loss3: 0.00\n",
      "Epoch [2218], train_loss: 684.39 with loss1: 575.27, loss2: 109.12 and loss3: 0.00\n",
      "Epoch [2219], train_loss: 692.27 with loss1: 582.71, loss2: 109.56 and loss3: 0.00\n",
      "Epoch [2220], train_loss: 694.66 with loss1: 585.45, loss2: 109.21 and loss3: 0.00\n",
      "Epoch [2221], train_loss: 706.23 with loss1: 596.88, loss2: 109.34 and loss3: 0.00\n",
      "Epoch [2222], train_loss: 703.43 with loss1: 594.32, loss2: 109.12 and loss3: 0.00\n",
      "Epoch [2223], train_loss: 715.11 with loss1: 605.97, loss2: 109.14 and loss3: 0.00\n",
      "Epoch [2224], train_loss: 714.88 with loss1: 605.78, loss2: 109.11 and loss3: 0.00\n",
      "Epoch [2225], train_loss: 722.58 with loss1: 613.40, loss2: 109.18 and loss3: 0.00\n",
      "Epoch [2226], train_loss: 720.06 with loss1: 611.04, loss2: 109.02 and loss3: 0.00\n",
      "Epoch [2227], train_loss: 731.20 with loss1: 621.97, loss2: 109.24 and loss3: 0.00\n",
      "Epoch [2228], train_loss: 725.37 with loss1: 616.66, loss2: 108.71 and loss3: 0.00\n",
      "Epoch [2229], train_loss: 727.70 with loss1: 618.38, loss2: 109.32 and loss3: 0.00\n",
      "Epoch [2230], train_loss: 721.69 with loss1: 613.06, loss2: 108.63 and loss3: 0.00\n",
      "Epoch [2231], train_loss: 725.24 with loss1: 616.20, loss2: 109.04 and loss3: 0.00\n",
      "Epoch [2232], train_loss: 717.63 with loss1: 609.22, loss2: 108.41 and loss3: 0.00\n",
      "Epoch [2233], train_loss: 718.92 with loss1: 610.11, loss2: 108.80 and loss3: 0.00\n",
      "Epoch [2234], train_loss: 710.12 with loss1: 601.75, loss2: 108.37 and loss3: 0.00\n",
      "Epoch [2235], train_loss: 711.52 with loss1: 602.55, loss2: 108.97 and loss3: 0.00\n",
      "Epoch [2236], train_loss: 703.61 with loss1: 595.11, loss2: 108.50 and loss3: 0.00\n",
      "Epoch [2237], train_loss: 702.31 with loss1: 593.56, loss2: 108.75 and loss3: 0.00\n",
      "Epoch [2238], train_loss: 695.50 with loss1: 587.06, loss2: 108.44 and loss3: 0.00\n",
      "Epoch [2239], train_loss: 691.25 with loss1: 582.48, loss2: 108.77 and loss3: 0.00\n",
      "Epoch [2240], train_loss: 683.95 with loss1: 575.57, loss2: 108.39 and loss3: 0.00\n",
      "Epoch [2241], train_loss: 677.95 with loss1: 569.14, loss2: 108.80 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2242], train_loss: 668.70 with loss1: 560.66, loss2: 108.04 and loss3: 0.00\n",
      "Epoch [2243], train_loss: 666.04 with loss1: 557.20, loss2: 108.85 and loss3: 0.00\n",
      "Epoch [2244], train_loss: 658.24 with loss1: 550.25, loss2: 107.99 and loss3: 0.00\n",
      "Epoch [2245], train_loss: 653.19 with loss1: 544.73, loss2: 108.46 and loss3: 0.00\n",
      "Epoch [2246], train_loss: 646.26 with loss1: 538.26, loss2: 107.99 and loss3: 0.00\n",
      "Epoch [2247], train_loss: 643.13 with loss1: 534.97, loss2: 108.16 and loss3: 0.00\n",
      "Epoch [2248], train_loss: 637.70 with loss1: 529.81, loss2: 107.89 and loss3: 0.00\n",
      "Epoch [2249], train_loss: 634.83 with loss1: 526.92, loss2: 107.90 and loss3: 0.00\n",
      "Epoch [2250], train_loss: 632.43 with loss1: 524.91, loss2: 107.52 and loss3: 0.00\n",
      "Epoch [2251], train_loss: 628.70 with loss1: 520.78, loss2: 107.92 and loss3: 0.00\n",
      "Epoch [2252], train_loss: 627.50 with loss1: 520.29, loss2: 107.21 and loss3: 0.00\n",
      "Epoch [2253], train_loss: 625.64 with loss1: 517.93, loss2: 107.70 and loss3: 0.00\n",
      "Epoch [2254], train_loss: 625.24 with loss1: 517.95, loss2: 107.29 and loss3: 0.00\n",
      "Epoch [2255], train_loss: 625.69 with loss1: 518.08, loss2: 107.60 and loss3: 0.00\n",
      "Epoch [2256], train_loss: 619.92 with loss1: 512.60, loss2: 107.32 and loss3: 0.00\n",
      "Epoch [2257], train_loss: 619.64 with loss1: 512.19, loss2: 107.46 and loss3: 0.00\n",
      "Epoch [2258], train_loss: 617.90 with loss1: 510.83, loss2: 107.08 and loss3: 0.00\n",
      "Epoch [2259], train_loss: 617.60 with loss1: 510.42, loss2: 107.17 and loss3: 0.00\n",
      "Epoch [2260], train_loss: 614.98 with loss1: 507.92, loss2: 107.06 and loss3: 0.00\n",
      "Epoch [2261], train_loss: 615.03 with loss1: 508.07, loss2: 106.96 and loss3: 0.00\n",
      "Epoch [2262], train_loss: 614.46 with loss1: 507.97, loss2: 106.49 and loss3: 0.00\n",
      "Epoch [2263], train_loss: 615.51 with loss1: 508.42, loss2: 107.09 and loss3: 0.00\n",
      "Epoch [2264], train_loss: 614.25 with loss1: 507.80, loss2: 106.45 and loss3: 0.00\n",
      "Epoch [2265], train_loss: 614.41 with loss1: 507.51, loss2: 106.89 and loss3: 0.00\n",
      "Epoch [2266], train_loss: 613.27 with loss1: 506.85, loss2: 106.42 and loss3: 0.00\n",
      "Epoch [2267], train_loss: 614.79 with loss1: 508.19, loss2: 106.60 and loss3: 0.00\n",
      "Epoch [2268], train_loss: 614.94 with loss1: 508.94, loss2: 106.00 and loss3: 0.00\n",
      "Epoch [2269], train_loss: 614.60 with loss1: 508.07, loss2: 106.53 and loss3: 0.00\n",
      "Epoch [2270], train_loss: 615.30 with loss1: 509.27, loss2: 106.03 and loss3: 0.00\n",
      "Epoch [2271], train_loss: 617.37 with loss1: 510.93, loss2: 106.44 and loss3: 0.00\n",
      "Epoch [2272], train_loss: 616.26 with loss1: 510.35, loss2: 105.91 and loss3: 0.00\n",
      "Epoch [2273], train_loss: 619.53 with loss1: 513.23, loss2: 106.30 and loss3: 0.00\n",
      "Epoch [2274], train_loss: 621.31 with loss1: 515.57, loss2: 105.74 and loss3: 0.00\n",
      "Epoch [2275], train_loss: 623.42 with loss1: 516.99, loss2: 106.43 and loss3: 0.00\n",
      "Epoch [2276], train_loss: 621.81 with loss1: 515.85, loss2: 105.96 and loss3: 0.00\n",
      "Epoch [2277], train_loss: 625.01 with loss1: 518.78, loss2: 106.23 and loss3: 0.00\n",
      "Epoch [2278], train_loss: 627.93 with loss1: 521.96, loss2: 105.96 and loss3: 0.00\n",
      "Epoch [2279], train_loss: 627.49 with loss1: 521.26, loss2: 106.24 and loss3: 0.00\n",
      "Epoch [2280], train_loss: 626.61 with loss1: 520.71, loss2: 105.90 and loss3: 0.00\n",
      "Epoch [2281], train_loss: 630.37 with loss1: 524.29, loss2: 106.08 and loss3: 0.00\n",
      "Epoch [2282], train_loss: 630.93 with loss1: 525.20, loss2: 105.74 and loss3: 0.00\n",
      "Epoch [2283], train_loss: 630.00 with loss1: 523.83, loss2: 106.18 and loss3: 0.00\n",
      "Epoch [2284], train_loss: 631.14 with loss1: 525.40, loss2: 105.74 and loss3: 0.00\n",
      "Epoch [2285], train_loss: 630.75 with loss1: 524.65, loss2: 106.09 and loss3: 0.00\n",
      "Epoch [2286], train_loss: 633.96 with loss1: 528.35, loss2: 105.61 and loss3: 0.00\n",
      "Epoch [2287], train_loss: 634.93 with loss1: 528.80, loss2: 106.13 and loss3: 0.00\n",
      "Epoch [2288], train_loss: 635.52 with loss1: 529.91, loss2: 105.60 and loss3: 0.00\n",
      "Epoch [2289], train_loss: 636.71 with loss1: 530.53, loss2: 106.18 and loss3: 0.00\n",
      "Epoch [2290], train_loss: 637.47 with loss1: 531.98, loss2: 105.49 and loss3: 0.00\n",
      "Epoch [2291], train_loss: 634.94 with loss1: 528.91, loss2: 106.03 and loss3: 0.00\n",
      "Epoch [2292], train_loss: 633.58 with loss1: 528.01, loss2: 105.57 and loss3: 0.00\n",
      "Epoch [2293], train_loss: 634.00 with loss1: 528.24, loss2: 105.76 and loss3: 0.00\n",
      "Epoch [2294], train_loss: 631.89 with loss1: 526.41, loss2: 105.48 and loss3: 0.00\n",
      "Epoch [2295], train_loss: 634.19 with loss1: 528.09, loss2: 106.11 and loss3: 0.00\n",
      "Epoch [2296], train_loss: 630.77 with loss1: 525.41, loss2: 105.35 and loss3: 0.00\n",
      "Epoch [2297], train_loss: 632.31 with loss1: 526.42, loss2: 105.89 and loss3: 0.00\n",
      "Epoch [2298], train_loss: 631.32 with loss1: 526.06, loss2: 105.26 and loss3: 0.00\n",
      "Epoch [2299], train_loss: 631.45 with loss1: 525.40, loss2: 106.04 and loss3: 0.00\n",
      "Epoch [2300], train_loss: 630.56 with loss1: 525.36, loss2: 105.20 and loss3: 0.00\n",
      "Epoch [2301], train_loss: 629.66 with loss1: 523.80, loss2: 105.85 and loss3: 0.00\n",
      "Epoch [2302], train_loss: 629.19 with loss1: 523.88, loss2: 105.30 and loss3: 0.00\n",
      "Epoch [2303], train_loss: 628.70 with loss1: 522.73, loss2: 105.97 and loss3: 0.00\n",
      "Epoch [2304], train_loss: 627.11 with loss1: 521.76, loss2: 105.34 and loss3: 0.00\n",
      "Epoch [2305], train_loss: 628.11 with loss1: 522.50, loss2: 105.62 and loss3: 0.00\n",
      "Epoch [2306], train_loss: 627.89 with loss1: 522.76, loss2: 105.13 and loss3: 0.00\n",
      "Epoch [2307], train_loss: 627.47 with loss1: 521.74, loss2: 105.73 and loss3: 0.00\n",
      "Epoch [2308], train_loss: 625.06 with loss1: 519.76, loss2: 105.30 and loss3: 0.00\n",
      "Epoch [2309], train_loss: 622.92 with loss1: 517.23, loss2: 105.69 and loss3: 0.00\n",
      "Epoch [2310], train_loss: 623.13 with loss1: 517.97, loss2: 105.16 and loss3: 0.00\n",
      "Epoch [2311], train_loss: 624.06 with loss1: 518.72, loss2: 105.34 and loss3: 0.00\n",
      "Epoch [2312], train_loss: 620.08 with loss1: 515.13, loss2: 104.95 and loss3: 0.00\n",
      "Epoch [2313], train_loss: 622.05 with loss1: 516.69, loss2: 105.36 and loss3: 0.00\n",
      "Epoch [2314], train_loss: 619.18 with loss1: 514.01, loss2: 105.17 and loss3: 0.00\n",
      "Epoch [2315], train_loss: 617.59 with loss1: 511.91, loss2: 105.68 and loss3: 0.00\n",
      "Epoch [2316], train_loss: 617.19 with loss1: 512.19, loss2: 105.00 and loss3: 0.00\n",
      "Epoch [2317], train_loss: 615.13 with loss1: 509.92, loss2: 105.22 and loss3: 0.00\n",
      "Epoch [2318], train_loss: 613.95 with loss1: 508.84, loss2: 105.11 and loss3: 0.00\n",
      "Epoch [2319], train_loss: 615.88 with loss1: 510.79, loss2: 105.10 and loss3: 0.00\n",
      "Epoch [2320], train_loss: 614.77 with loss1: 509.99, loss2: 104.78 and loss3: 0.00\n",
      "Epoch [2321], train_loss: 613.41 with loss1: 508.45, loss2: 104.96 and loss3: 0.00\n",
      "Epoch [2322], train_loss: 613.02 with loss1: 508.48, loss2: 104.54 and loss3: 0.00\n",
      "Epoch [2323], train_loss: 613.88 with loss1: 509.15, loss2: 104.73 and loss3: 0.00\n",
      "Epoch [2324], train_loss: 612.25 with loss1: 507.69, loss2: 104.57 and loss3: 0.00\n",
      "Epoch [2325], train_loss: 615.05 with loss1: 510.28, loss2: 104.76 and loss3: 0.00\n",
      "Epoch [2326], train_loss: 613.31 with loss1: 508.68, loss2: 104.63 and loss3: 0.00\n",
      "Epoch [2327], train_loss: 613.84 with loss1: 509.04, loss2: 104.80 and loss3: 0.00\n",
      "Epoch [2328], train_loss: 613.21 with loss1: 509.00, loss2: 104.20 and loss3: 0.00\n",
      "Epoch [2329], train_loss: 616.50 with loss1: 512.03, loss2: 104.48 and loss3: 0.00\n",
      "Epoch [2330], train_loss: 618.28 with loss1: 513.84, loss2: 104.45 and loss3: 0.00\n",
      "Epoch [2331], train_loss: 620.32 with loss1: 515.75, loss2: 104.57 and loss3: 0.00\n",
      "Epoch [2332], train_loss: 623.50 with loss1: 519.18, loss2: 104.31 and loss3: 0.00\n",
      "Epoch [2333], train_loss: 627.61 with loss1: 523.08, loss2: 104.53 and loss3: 0.00\n",
      "Epoch [2334], train_loss: 630.52 with loss1: 526.20, loss2: 104.32 and loss3: 0.00\n",
      "Epoch [2335], train_loss: 632.29 with loss1: 527.83, loss2: 104.47 and loss3: 0.00\n",
      "Epoch [2336], train_loss: 634.94 with loss1: 530.49, loss2: 104.46 and loss3: 0.00\n",
      "Epoch [2337], train_loss: 640.06 with loss1: 535.33, loss2: 104.72 and loss3: 0.00\n",
      "Epoch [2338], train_loss: 639.18 with loss1: 534.88, loss2: 104.31 and loss3: 0.00\n",
      "Epoch [2339], train_loss: 642.09 with loss1: 537.25, loss2: 104.84 and loss3: 0.00\n",
      "Epoch [2340], train_loss: 643.56 with loss1: 538.90, loss2: 104.66 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2341], train_loss: 643.41 with loss1: 538.54, loss2: 104.87 and loss3: 0.00\n",
      "Epoch [2342], train_loss: 642.94 with loss1: 538.27, loss2: 104.67 and loss3: 0.00\n",
      "Epoch [2343], train_loss: 645.63 with loss1: 540.72, loss2: 104.91 and loss3: 0.00\n",
      "Epoch [2344], train_loss: 645.89 with loss1: 541.26, loss2: 104.64 and loss3: 0.00\n",
      "Epoch [2345], train_loss: 647.08 with loss1: 542.24, loss2: 104.84 and loss3: 0.00\n",
      "Epoch [2346], train_loss: 643.31 with loss1: 538.63, loss2: 104.68 and loss3: 0.00\n",
      "Epoch [2347], train_loss: 644.09 with loss1: 539.15, loss2: 104.93 and loss3: 0.00\n",
      "Epoch [2348], train_loss: 642.97 with loss1: 538.21, loss2: 104.76 and loss3: 0.00\n",
      "Epoch [2349], train_loss: 644.22 with loss1: 539.31, loss2: 104.91 and loss3: 0.00\n",
      "Epoch [2350], train_loss: 641.38 with loss1: 536.52, loss2: 104.85 and loss3: 0.00\n",
      "Epoch [2351], train_loss: 640.11 with loss1: 535.09, loss2: 105.02 and loss3: 0.00\n",
      "Epoch [2352], train_loss: 634.73 with loss1: 529.97, loss2: 104.76 and loss3: 0.00\n",
      "Epoch [2353], train_loss: 633.56 with loss1: 528.76, loss2: 104.80 and loss3: 0.00\n",
      "Epoch [2354], train_loss: 631.05 with loss1: 526.37, loss2: 104.68 and loss3: 0.00\n",
      "Epoch [2355], train_loss: 630.93 with loss1: 526.06, loss2: 104.87 and loss3: 0.00\n",
      "Epoch [2356], train_loss: 627.41 with loss1: 522.56, loss2: 104.85 and loss3: 0.00\n",
      "Epoch [2357], train_loss: 627.72 with loss1: 522.85, loss2: 104.87 and loss3: 0.00\n",
      "Epoch [2358], train_loss: 623.11 with loss1: 518.40, loss2: 104.71 and loss3: 0.00\n",
      "Epoch [2359], train_loss: 622.49 with loss1: 517.72, loss2: 104.77 and loss3: 0.00\n",
      "Epoch [2360], train_loss: 622.00 with loss1: 517.48, loss2: 104.52 and loss3: 0.00\n",
      "Epoch [2361], train_loss: 619.49 with loss1: 514.92, loss2: 104.57 and loss3: 0.00\n",
      "Epoch [2362], train_loss: 615.87 with loss1: 511.28, loss2: 104.60 and loss3: 0.00\n",
      "Epoch [2363], train_loss: 613.92 with loss1: 509.41, loss2: 104.52 and loss3: 0.00\n",
      "Epoch [2364], train_loss: 611.27 with loss1: 506.91, loss2: 104.36 and loss3: 0.00\n",
      "Epoch [2365], train_loss: 613.73 with loss1: 509.28, loss2: 104.45 and loss3: 0.00\n",
      "Epoch [2366], train_loss: 610.07 with loss1: 505.95, loss2: 104.12 and loss3: 0.00\n",
      "Epoch [2367], train_loss: 612.26 with loss1: 507.87, loss2: 104.38 and loss3: 0.00\n",
      "Epoch [2368], train_loss: 608.76 with loss1: 504.64, loss2: 104.12 and loss3: 0.00\n",
      "Epoch [2369], train_loss: 609.05 with loss1: 504.82, loss2: 104.23 and loss3: 0.00\n",
      "Epoch [2370], train_loss: 606.86 with loss1: 502.96, loss2: 103.90 and loss3: 0.00\n",
      "Epoch [2371], train_loss: 608.51 with loss1: 504.36, loss2: 104.14 and loss3: 0.00\n",
      "Epoch [2372], train_loss: 607.26 with loss1: 503.22, loss2: 104.04 and loss3: 0.00\n",
      "Epoch [2373], train_loss: 607.09 with loss1: 503.39, loss2: 103.70 and loss3: 0.00\n",
      "Epoch [2374], train_loss: 609.49 with loss1: 505.79, loss2: 103.70 and loss3: 0.00\n",
      "Epoch [2375], train_loss: 610.17 with loss1: 506.34, loss2: 103.83 and loss3: 0.00\n",
      "Epoch [2376], train_loss: 608.71 with loss1: 505.12, loss2: 103.59 and loss3: 0.00\n",
      "Epoch [2377], train_loss: 610.56 with loss1: 506.76, loss2: 103.80 and loss3: 0.00\n",
      "Epoch [2378], train_loss: 610.70 with loss1: 507.05, loss2: 103.65 and loss3: 0.00\n",
      "Epoch [2379], train_loss: 611.12 with loss1: 507.29, loss2: 103.84 and loss3: 0.00\n",
      "Epoch [2380], train_loss: 613.47 with loss1: 509.90, loss2: 103.57 and loss3: 0.00\n",
      "Epoch [2381], train_loss: 615.45 with loss1: 511.75, loss2: 103.70 and loss3: 0.00\n",
      "Epoch [2382], train_loss: 618.68 with loss1: 514.93, loss2: 103.74 and loss3: 0.00\n",
      "Epoch [2383], train_loss: 619.13 with loss1: 515.50, loss2: 103.63 and loss3: 0.00\n",
      "Epoch [2384], train_loss: 619.92 with loss1: 516.55, loss2: 103.37 and loss3: 0.00\n",
      "Epoch [2385], train_loss: 622.94 with loss1: 519.49, loss2: 103.45 and loss3: 0.00\n",
      "Epoch [2386], train_loss: 624.36 with loss1: 520.71, loss2: 103.65 and loss3: 0.00\n",
      "Epoch [2387], train_loss: 626.56 with loss1: 522.88, loss2: 103.68 and loss3: 0.00\n",
      "Epoch [2388], train_loss: 629.03 with loss1: 525.29, loss2: 103.74 and loss3: 0.00\n",
      "Epoch [2389], train_loss: 633.89 with loss1: 530.40, loss2: 103.50 and loss3: 0.00\n",
      "Epoch [2390], train_loss: 636.87 with loss1: 533.42, loss2: 103.44 and loss3: 0.00\n",
      "Epoch [2391], train_loss: 641.01 with loss1: 537.43, loss2: 103.58 and loss3: 0.00\n",
      "Epoch [2392], train_loss: 640.20 with loss1: 536.69, loss2: 103.51 and loss3: 0.00\n",
      "Epoch [2393], train_loss: 645.03 with loss1: 540.99, loss2: 104.03 and loss3: 0.00\n",
      "Epoch [2394], train_loss: 645.91 with loss1: 542.11, loss2: 103.80 and loss3: 0.00\n",
      "Epoch [2395], train_loss: 649.62 with loss1: 545.68, loss2: 103.94 and loss3: 0.00\n",
      "Epoch [2396], train_loss: 647.88 with loss1: 544.12, loss2: 103.75 and loss3: 0.00\n",
      "Epoch [2397], train_loss: 650.69 with loss1: 546.76, loss2: 103.93 and loss3: 0.00\n",
      "Epoch [2398], train_loss: 651.37 with loss1: 547.46, loss2: 103.91 and loss3: 0.00\n",
      "Epoch [2399], train_loss: 654.58 with loss1: 550.44, loss2: 104.14 and loss3: 0.00\n",
      "Epoch [2400], train_loss: 653.96 with loss1: 549.90, loss2: 104.05 and loss3: 0.00\n",
      "Epoch [2401], train_loss: 658.82 with loss1: 554.61, loss2: 104.21 and loss3: 0.00\n",
      "Epoch [2402], train_loss: 656.05 with loss1: 551.92, loss2: 104.12 and loss3: 0.00\n",
      "Epoch [2403], train_loss: 659.70 with loss1: 555.39, loss2: 104.31 and loss3: 0.00\n",
      "Epoch [2404], train_loss: 653.81 with loss1: 549.61, loss2: 104.21 and loss3: 0.00\n",
      "Epoch [2405], train_loss: 655.29 with loss1: 551.02, loss2: 104.27 and loss3: 0.00\n",
      "Epoch [2406], train_loss: 649.25 with loss1: 545.03, loss2: 104.21 and loss3: 0.00\n",
      "Epoch [2407], train_loss: 649.29 with loss1: 545.12, loss2: 104.17 and loss3: 0.00\n",
      "Epoch [2408], train_loss: 643.78 with loss1: 539.80, loss2: 103.98 and loss3: 0.00\n",
      "Epoch [2409], train_loss: 643.36 with loss1: 539.00, loss2: 104.36 and loss3: 0.00\n",
      "Epoch [2410], train_loss: 636.70 with loss1: 532.75, loss2: 103.95 and loss3: 0.00\n",
      "Epoch [2411], train_loss: 636.75 with loss1: 532.54, loss2: 104.21 and loss3: 0.00\n",
      "Epoch [2412], train_loss: 632.20 with loss1: 528.06, loss2: 104.14 and loss3: 0.00\n",
      "Epoch [2413], train_loss: 632.38 with loss1: 528.19, loss2: 104.19 and loss3: 0.00\n",
      "Epoch [2414], train_loss: 625.85 with loss1: 522.21, loss2: 103.64 and loss3: 0.00\n",
      "Epoch [2415], train_loss: 628.26 with loss1: 524.21, loss2: 104.06 and loss3: 0.00\n",
      "Epoch [2416], train_loss: 624.80 with loss1: 521.21, loss2: 103.59 and loss3: 0.00\n",
      "Epoch [2417], train_loss: 625.61 with loss1: 521.71, loss2: 103.90 and loss3: 0.00\n",
      "Epoch [2418], train_loss: 622.20 with loss1: 518.72, loss2: 103.48 and loss3: 0.00\n",
      "Epoch [2419], train_loss: 621.99 with loss1: 518.27, loss2: 103.72 and loss3: 0.00\n",
      "Epoch [2420], train_loss: 622.18 with loss1: 518.82, loss2: 103.36 and loss3: 0.00\n",
      "Epoch [2421], train_loss: 624.11 with loss1: 520.65, loss2: 103.47 and loss3: 0.00\n",
      "Epoch [2422], train_loss: 623.62 with loss1: 520.39, loss2: 103.23 and loss3: 0.00\n",
      "Epoch [2423], train_loss: 626.46 with loss1: 522.79, loss2: 103.68 and loss3: 0.00\n",
      "Epoch [2424], train_loss: 623.69 with loss1: 520.58, loss2: 103.12 and loss3: 0.00\n",
      "Epoch [2425], train_loss: 627.45 with loss1: 523.95, loss2: 103.49 and loss3: 0.00\n",
      "Epoch [2426], train_loss: 625.21 with loss1: 522.14, loss2: 103.08 and loss3: 0.00\n",
      "Epoch [2427], train_loss: 630.83 with loss1: 527.37, loss2: 103.46 and loss3: 0.00\n",
      "Epoch [2428], train_loss: 625.67 with loss1: 522.58, loss2: 103.09 and loss3: 0.00\n",
      "Epoch [2429], train_loss: 630.96 with loss1: 527.63, loss2: 103.32 and loss3: 0.00\n",
      "Epoch [2430], train_loss: 626.77 with loss1: 523.81, loss2: 102.96 and loss3: 0.00\n",
      "Epoch [2431], train_loss: 627.68 with loss1: 524.38, loss2: 103.30 and loss3: 0.00\n",
      "Epoch [2432], train_loss: 624.36 with loss1: 521.46, loss2: 102.90 and loss3: 0.00\n",
      "Epoch [2433], train_loss: 626.14 with loss1: 523.07, loss2: 103.08 and loss3: 0.00\n",
      "Epoch [2434], train_loss: 621.66 with loss1: 518.79, loss2: 102.87 and loss3: 0.00\n",
      "Epoch [2435], train_loss: 621.46 with loss1: 518.55, loss2: 102.91 and loss3: 0.00\n",
      "Epoch [2436], train_loss: 618.44 with loss1: 515.78, loss2: 102.66 and loss3: 0.00\n",
      "Epoch [2437], train_loss: 620.03 with loss1: 517.00, loss2: 103.03 and loss3: 0.00\n",
      "Epoch [2438], train_loss: 614.41 with loss1: 511.84, loss2: 102.57 and loss3: 0.00\n",
      "Epoch [2439], train_loss: 613.21 with loss1: 510.49, loss2: 102.72 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2440], train_loss: 609.60 with loss1: 507.11, loss2: 102.49 and loss3: 0.00\n",
      "Epoch [2441], train_loss: 609.41 with loss1: 506.88, loss2: 102.53 and loss3: 0.00\n",
      "Epoch [2442], train_loss: 605.87 with loss1: 503.64, loss2: 102.23 and loss3: 0.00\n",
      "Epoch [2443], train_loss: 606.54 with loss1: 504.20, loss2: 102.35 and loss3: 0.00\n",
      "Epoch [2444], train_loss: 603.60 with loss1: 501.46, loss2: 102.13 and loss3: 0.00\n",
      "Epoch [2445], train_loss: 605.37 with loss1: 503.09, loss2: 102.27 and loss3: 0.00\n",
      "Epoch [2446], train_loss: 600.41 with loss1: 498.10, loss2: 102.31 and loss3: 0.00\n",
      "Epoch [2447], train_loss: 601.54 with loss1: 499.13, loss2: 102.41 and loss3: 0.00\n",
      "Epoch [2448], train_loss: 597.26 with loss1: 495.07, loss2: 102.19 and loss3: 0.00\n",
      "Epoch [2449], train_loss: 599.71 with loss1: 497.51, loss2: 102.21 and loss3: 0.00\n",
      "Epoch [2450], train_loss: 597.72 with loss1: 495.56, loss2: 102.16 and loss3: 0.00\n",
      "Epoch [2451], train_loss: 597.69 with loss1: 495.50, loss2: 102.19 and loss3: 0.00\n",
      "Epoch [2452], train_loss: 593.37 with loss1: 491.45, loss2: 101.92 and loss3: 0.00\n",
      "Epoch [2453], train_loss: 594.65 with loss1: 492.56, loss2: 102.10 and loss3: 0.00\n",
      "Epoch [2454], train_loss: 592.45 with loss1: 490.67, loss2: 101.78 and loss3: 0.00\n",
      "Epoch [2455], train_loss: 594.81 with loss1: 493.02, loss2: 101.79 and loss3: 0.00\n",
      "Epoch [2456], train_loss: 592.03 with loss1: 490.24, loss2: 101.79 and loss3: 0.00\n",
      "Epoch [2457], train_loss: 592.10 with loss1: 490.25, loss2: 101.85 and loss3: 0.00\n",
      "Epoch [2458], train_loss: 590.30 with loss1: 488.61, loss2: 101.69 and loss3: 0.00\n",
      "Epoch [2459], train_loss: 590.83 with loss1: 489.30, loss2: 101.54 and loss3: 0.00\n",
      "Epoch [2460], train_loss: 590.61 with loss1: 489.36, loss2: 101.25 and loss3: 0.00\n",
      "Epoch [2461], train_loss: 590.17 with loss1: 488.47, loss2: 101.69 and loss3: 0.00\n",
      "Epoch [2462], train_loss: 591.07 with loss1: 489.37, loss2: 101.70 and loss3: 0.00\n",
      "Epoch [2463], train_loss: 592.89 with loss1: 491.46, loss2: 101.44 and loss3: 0.00\n",
      "Epoch [2464], train_loss: 595.45 with loss1: 494.07, loss2: 101.38 and loss3: 0.00\n",
      "Epoch [2465], train_loss: 594.74 with loss1: 493.03, loss2: 101.71 and loss3: 0.00\n",
      "Epoch [2466], train_loss: 595.55 with loss1: 493.94, loss2: 101.61 and loss3: 0.00\n",
      "Epoch [2467], train_loss: 595.21 with loss1: 493.84, loss2: 101.37 and loss3: 0.00\n",
      "Epoch [2468], train_loss: 597.03 with loss1: 495.53, loss2: 101.51 and loss3: 0.00\n",
      "Epoch [2469], train_loss: 598.14 with loss1: 496.79, loss2: 101.35 and loss3: 0.00\n",
      "Epoch [2470], train_loss: 600.83 with loss1: 499.18, loss2: 101.64 and loss3: 0.00\n",
      "Epoch [2471], train_loss: 602.28 with loss1: 500.86, loss2: 101.42 and loss3: 0.00\n",
      "Epoch [2472], train_loss: 602.52 with loss1: 500.93, loss2: 101.58 and loss3: 0.00\n",
      "Epoch [2473], train_loss: 606.68 with loss1: 505.18, loss2: 101.50 and loss3: 0.00\n",
      "Epoch [2474], train_loss: 610.85 with loss1: 509.36, loss2: 101.50 and loss3: 0.00\n",
      "Epoch [2475], train_loss: 609.53 with loss1: 507.93, loss2: 101.60 and loss3: 0.00\n",
      "Epoch [2476], train_loss: 609.62 with loss1: 508.10, loss2: 101.52 and loss3: 0.00\n",
      "Epoch [2477], train_loss: 613.87 with loss1: 512.39, loss2: 101.48 and loss3: 0.00\n",
      "Epoch [2478], train_loss: 615.50 with loss1: 513.87, loss2: 101.63 and loss3: 0.00\n",
      "Epoch [2479], train_loss: 617.38 with loss1: 516.02, loss2: 101.37 and loss3: 0.00\n",
      "Epoch [2480], train_loss: 618.57 with loss1: 516.93, loss2: 101.64 and loss3: 0.00\n",
      "Epoch [2481], train_loss: 618.72 with loss1: 517.39, loss2: 101.33 and loss3: 0.00\n",
      "Epoch [2482], train_loss: 623.64 with loss1: 522.10, loss2: 101.54 and loss3: 0.00\n",
      "Epoch [2483], train_loss: 626.39 with loss1: 524.90, loss2: 101.49 and loss3: 0.00\n",
      "Epoch [2484], train_loss: 630.44 with loss1: 528.82, loss2: 101.62 and loss3: 0.00\n",
      "Epoch [2485], train_loss: 631.13 with loss1: 529.96, loss2: 101.17 and loss3: 0.00\n",
      "Epoch [2486], train_loss: 632.83 with loss1: 531.09, loss2: 101.74 and loss3: 0.00\n",
      "Epoch [2487], train_loss: 636.37 with loss1: 534.85, loss2: 101.52 and loss3: 0.00\n",
      "Epoch [2488], train_loss: 639.34 with loss1: 537.47, loss2: 101.87 and loss3: 0.00\n",
      "Epoch [2489], train_loss: 639.45 with loss1: 537.96, loss2: 101.49 and loss3: 0.00\n",
      "Epoch [2490], train_loss: 641.97 with loss1: 540.19, loss2: 101.79 and loss3: 0.00\n",
      "Epoch [2491], train_loss: 640.21 with loss1: 538.69, loss2: 101.52 and loss3: 0.00\n",
      "Epoch [2492], train_loss: 641.40 with loss1: 539.68, loss2: 101.72 and loss3: 0.00\n",
      "Epoch [2493], train_loss: 640.22 with loss1: 539.02, loss2: 101.20 and loss3: 0.00\n",
      "Epoch [2494], train_loss: 643.73 with loss1: 541.96, loss2: 101.77 and loss3: 0.00\n",
      "Epoch [2495], train_loss: 641.07 with loss1: 539.87, loss2: 101.20 and loss3: 0.00\n",
      "Epoch [2496], train_loss: 643.80 with loss1: 542.27, loss2: 101.52 and loss3: 0.00\n",
      "Epoch [2497], train_loss: 639.28 with loss1: 538.18, loss2: 101.10 and loss3: 0.00\n",
      "Epoch [2498], train_loss: 642.05 with loss1: 540.64, loss2: 101.41 and loss3: 0.00\n",
      "Epoch [2499], train_loss: 639.35 with loss1: 538.31, loss2: 101.04 and loss3: 0.00\n",
      "Epoch [2500], train_loss: 641.38 with loss1: 539.95, loss2: 101.43 and loss3: 0.00\n",
      "Epoch [2501], train_loss: 638.00 with loss1: 537.12, loss2: 100.88 and loss3: 0.00\n",
      "Epoch [2502], train_loss: 637.21 with loss1: 535.65, loss2: 101.55 and loss3: 0.00\n",
      "Epoch [2503], train_loss: 631.93 with loss1: 531.07, loss2: 100.87 and loss3: 0.00\n",
      "Epoch [2504], train_loss: 633.55 with loss1: 532.29, loss2: 101.26 and loss3: 0.00\n",
      "Epoch [2505], train_loss: 630.63 with loss1: 529.85, loss2: 100.78 and loss3: 0.00\n",
      "Epoch [2506], train_loss: 633.41 with loss1: 532.22, loss2: 101.18 and loss3: 0.00\n",
      "Epoch [2507], train_loss: 627.94 with loss1: 527.33, loss2: 100.61 and loss3: 0.00\n",
      "Epoch [2508], train_loss: 629.20 with loss1: 528.17, loss2: 101.03 and loss3: 0.00\n",
      "Epoch [2509], train_loss: 626.13 with loss1: 525.59, loss2: 100.55 and loss3: 0.00\n",
      "Epoch [2510], train_loss: 629.53 with loss1: 528.53, loss2: 101.00 and loss3: 0.00\n",
      "Epoch [2511], train_loss: 624.17 with loss1: 523.63, loss2: 100.54 and loss3: 0.00\n",
      "Epoch [2512], train_loss: 627.13 with loss1: 526.38, loss2: 100.75 and loss3: 0.00\n",
      "Epoch [2513], train_loss: 624.10 with loss1: 523.72, loss2: 100.38 and loss3: 0.00\n",
      "Epoch [2514], train_loss: 625.44 with loss1: 524.77, loss2: 100.68 and loss3: 0.00\n",
      "Epoch [2515], train_loss: 621.53 with loss1: 521.50, loss2: 100.03 and loss3: 0.00\n",
      "Epoch [2516], train_loss: 622.85 with loss1: 522.36, loss2: 100.49 and loss3: 0.00\n",
      "Epoch [2517], train_loss: 620.93 with loss1: 520.61, loss2: 100.32 and loss3: 0.00\n",
      "Epoch [2518], train_loss: 621.23 with loss1: 521.16, loss2: 100.06 and loss3: 0.00\n",
      "Epoch [2519], train_loss: 619.26 with loss1: 519.20, loss2: 100.05 and loss3: 0.00\n",
      "Epoch [2520], train_loss: 621.20 with loss1: 520.94, loss2: 100.25 and loss3: 0.00\n",
      "Epoch [2521], train_loss: 617.44 with loss1: 517.37, loss2: 100.07 and loss3: 0.00\n",
      "Epoch [2522], train_loss: 618.88 with loss1: 518.47, loss2: 100.41 and loss3: 0.00\n",
      "Epoch [2523], train_loss: 616.34 with loss1: 516.32, loss2: 100.01 and loss3: 0.00\n",
      "Epoch [2524], train_loss: 614.90 with loss1: 514.96, loss2: 99.94 and loss3: 0.00\n",
      "Epoch [2525], train_loss: 615.55 with loss1: 515.83, loss2: 99.72 and loss3: 0.00\n",
      "Epoch [2526], train_loss: 614.04 with loss1: 514.16, loss2: 99.88 and loss3: 0.00\n",
      "Epoch [2527], train_loss: 609.50 with loss1: 509.78, loss2: 99.72 and loss3: 0.00\n",
      "Epoch [2528], train_loss: 611.11 with loss1: 510.82, loss2: 100.29 and loss3: 0.00\n",
      "Epoch [2529], train_loss: 608.62 with loss1: 508.96, loss2: 99.66 and loss3: 0.00\n",
      "Epoch [2530], train_loss: 608.98 with loss1: 509.08, loss2: 99.90 and loss3: 0.00\n",
      "Epoch [2531], train_loss: 607.49 with loss1: 507.88, loss2: 99.61 and loss3: 0.00\n",
      "Epoch [2532], train_loss: 606.97 with loss1: 507.13, loss2: 99.84 and loss3: 0.00\n",
      "Epoch [2533], train_loss: 606.23 with loss1: 506.97, loss2: 99.26 and loss3: 0.00\n",
      "Epoch [2534], train_loss: 607.07 with loss1: 507.39, loss2: 99.68 and loss3: 0.00\n",
      "Epoch [2535], train_loss: 604.27 with loss1: 504.84, loss2: 99.43 and loss3: 0.00\n",
      "Epoch [2536], train_loss: 605.85 with loss1: 506.15, loss2: 99.71 and loss3: 0.00\n",
      "Epoch [2537], train_loss: 605.55 with loss1: 506.32, loss2: 99.23 and loss3: 0.00\n",
      "Epoch [2538], train_loss: 605.35 with loss1: 505.92, loss2: 99.43 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2539], train_loss: 605.13 with loss1: 505.72, loss2: 99.41 and loss3: 0.00\n",
      "Epoch [2540], train_loss: 606.05 with loss1: 506.48, loss2: 99.56 and loss3: 0.00\n",
      "Epoch [2541], train_loss: 605.20 with loss1: 506.01, loss2: 99.20 and loss3: 0.00\n",
      "Epoch [2542], train_loss: 604.68 with loss1: 505.20, loss2: 99.47 and loss3: 0.00\n",
      "Epoch [2543], train_loss: 603.58 with loss1: 504.32, loss2: 99.26 and loss3: 0.00\n",
      "Epoch [2544], train_loss: 606.42 with loss1: 506.92, loss2: 99.50 and loss3: 0.00\n",
      "Epoch [2545], train_loss: 603.87 with loss1: 504.54, loss2: 99.34 and loss3: 0.00\n",
      "Epoch [2546], train_loss: 605.73 with loss1: 506.51, loss2: 99.22 and loss3: 0.00\n",
      "Epoch [2547], train_loss: 602.08 with loss1: 503.23, loss2: 98.85 and loss3: 0.00\n",
      "Epoch [2548], train_loss: 605.12 with loss1: 505.95, loss2: 99.17 and loss3: 0.00\n",
      "Epoch [2549], train_loss: 601.33 with loss1: 502.04, loss2: 99.29 and loss3: 0.00\n",
      "Epoch [2550], train_loss: 602.63 with loss1: 503.70, loss2: 98.93 and loss3: 0.00\n",
      "Epoch [2551], train_loss: 600.98 with loss1: 501.95, loss2: 99.04 and loss3: 0.00\n",
      "Epoch [2552], train_loss: 603.53 with loss1: 504.49, loss2: 99.04 and loss3: 0.00\n",
      "Epoch [2553], train_loss: 600.92 with loss1: 501.99, loss2: 98.93 and loss3: 0.00\n",
      "Epoch [2554], train_loss: 602.18 with loss1: 503.20, loss2: 98.98 and loss3: 0.00\n",
      "Epoch [2555], train_loss: 598.67 with loss1: 499.77, loss2: 98.90 and loss3: 0.00\n",
      "Epoch [2556], train_loss: 597.14 with loss1: 498.20, loss2: 98.94 and loss3: 0.00\n",
      "Epoch [2557], train_loss: 595.15 with loss1: 496.20, loss2: 98.95 and loss3: 0.00\n",
      "Epoch [2558], train_loss: 595.62 with loss1: 496.72, loss2: 98.91 and loss3: 0.00\n",
      "Epoch [2559], train_loss: 594.95 with loss1: 496.03, loss2: 98.92 and loss3: 0.00\n",
      "Epoch [2560], train_loss: 593.58 with loss1: 494.96, loss2: 98.63 and loss3: 0.00\n",
      "Epoch [2561], train_loss: 592.40 with loss1: 493.66, loss2: 98.74 and loss3: 0.00\n",
      "Epoch [2562], train_loss: 591.24 with loss1: 492.46, loss2: 98.78 and loss3: 0.00\n",
      "Epoch [2563], train_loss: 591.84 with loss1: 493.09, loss2: 98.75 and loss3: 0.00\n",
      "Epoch [2564], train_loss: 591.48 with loss1: 492.72, loss2: 98.76 and loss3: 0.00\n",
      "Epoch [2565], train_loss: 587.61 with loss1: 489.07, loss2: 98.54 and loss3: 0.00\n",
      "Epoch [2566], train_loss: 592.68 with loss1: 494.43, loss2: 98.25 and loss3: 0.00\n",
      "Epoch [2567], train_loss: 589.97 with loss1: 491.25, loss2: 98.72 and loss3: 0.00\n",
      "Epoch [2568], train_loss: 589.06 with loss1: 490.40, loss2: 98.66 and loss3: 0.00\n",
      "Epoch [2569], train_loss: 587.57 with loss1: 489.25, loss2: 98.32 and loss3: 0.00\n",
      "Epoch [2570], train_loss: 588.19 with loss1: 489.71, loss2: 98.49 and loss3: 0.00\n",
      "Epoch [2571], train_loss: 584.71 with loss1: 486.37, loss2: 98.34 and loss3: 0.00\n",
      "Epoch [2572], train_loss: 586.51 with loss1: 488.07, loss2: 98.44 and loss3: 0.00\n",
      "Epoch [2573], train_loss: 587.32 with loss1: 489.18, loss2: 98.15 and loss3: 0.00\n",
      "Epoch [2574], train_loss: 589.00 with loss1: 490.85, loss2: 98.15 and loss3: 0.00\n",
      "Epoch [2575], train_loss: 586.63 with loss1: 488.41, loss2: 98.22 and loss3: 0.00\n",
      "Epoch [2576], train_loss: 587.43 with loss1: 489.15, loss2: 98.28 and loss3: 0.00\n",
      "Epoch [2577], train_loss: 586.21 with loss1: 487.88, loss2: 98.34 and loss3: 0.00\n",
      "Epoch [2578], train_loss: 585.41 with loss1: 487.14, loss2: 98.27 and loss3: 0.00\n",
      "Epoch [2579], train_loss: 584.67 with loss1: 486.46, loss2: 98.21 and loss3: 0.00\n",
      "Epoch [2580], train_loss: 585.73 with loss1: 487.58, loss2: 98.15 and loss3: 0.00\n",
      "Epoch [2581], train_loss: 585.19 with loss1: 487.44, loss2: 97.75 and loss3: 0.00\n",
      "Epoch [2582], train_loss: 585.07 with loss1: 487.16, loss2: 97.91 and loss3: 0.00\n",
      "Epoch [2583], train_loss: 585.11 with loss1: 487.10, loss2: 98.02 and loss3: 0.00\n",
      "Epoch [2584], train_loss: 587.16 with loss1: 489.23, loss2: 97.93 and loss3: 0.00\n",
      "Epoch [2585], train_loss: 585.15 with loss1: 487.42, loss2: 97.74 and loss3: 0.00\n",
      "Epoch [2586], train_loss: 587.05 with loss1: 489.26, loss2: 97.78 and loss3: 0.00\n",
      "Epoch [2587], train_loss: 587.58 with loss1: 489.75, loss2: 97.83 and loss3: 0.00\n",
      "Epoch [2588], train_loss: 588.51 with loss1: 490.72, loss2: 97.79 and loss3: 0.00\n",
      "Epoch [2589], train_loss: 586.79 with loss1: 488.93, loss2: 97.85 and loss3: 0.00\n",
      "Epoch [2590], train_loss: 588.43 with loss1: 490.67, loss2: 97.75 and loss3: 0.00\n",
      "Epoch [2591], train_loss: 585.47 with loss1: 487.75, loss2: 97.72 and loss3: 0.00\n",
      "Epoch [2592], train_loss: 584.85 with loss1: 487.15, loss2: 97.71 and loss3: 0.00\n",
      "Epoch [2593], train_loss: 585.46 with loss1: 487.70, loss2: 97.76 and loss3: 0.00\n",
      "Epoch [2594], train_loss: 585.30 with loss1: 487.52, loss2: 97.79 and loss3: 0.00\n",
      "Epoch [2595], train_loss: 584.46 with loss1: 486.72, loss2: 97.75 and loss3: 0.00\n",
      "Epoch [2596], train_loss: 585.10 with loss1: 487.41, loss2: 97.70 and loss3: 0.00\n",
      "Epoch [2597], train_loss: 583.92 with loss1: 486.17, loss2: 97.75 and loss3: 0.00\n",
      "Epoch [2598], train_loss: 587.12 with loss1: 489.52, loss2: 97.61 and loss3: 0.00\n",
      "Epoch [2599], train_loss: 586.53 with loss1: 488.87, loss2: 97.66 and loss3: 0.00\n",
      "Epoch [2600], train_loss: 589.43 with loss1: 491.85, loss2: 97.58 and loss3: 0.00\n",
      "Epoch [2601], train_loss: 586.46 with loss1: 488.68, loss2: 97.78 and loss3: 0.00\n",
      "Epoch [2602], train_loss: 586.03 with loss1: 488.16, loss2: 97.87 and loss3: 0.00\n",
      "Epoch [2603], train_loss: 583.86 with loss1: 486.20, loss2: 97.66 and loss3: 0.00\n",
      "Epoch [2604], train_loss: 584.30 with loss1: 486.62, loss2: 97.68 and loss3: 0.00\n",
      "Epoch [2605], train_loss: 583.51 with loss1: 485.70, loss2: 97.81 and loss3: 0.00\n",
      "Epoch [2606], train_loss: 584.22 with loss1: 486.61, loss2: 97.61 and loss3: 0.00\n",
      "Epoch [2607], train_loss: 581.00 with loss1: 483.31, loss2: 97.69 and loss3: 0.00\n",
      "Epoch [2608], train_loss: 579.44 with loss1: 481.60, loss2: 97.84 and loss3: 0.00\n",
      "Epoch [2609], train_loss: 577.34 with loss1: 479.69, loss2: 97.66 and loss3: 0.00\n",
      "Epoch [2610], train_loss: 577.65 with loss1: 480.04, loss2: 97.61 and loss3: 0.00\n",
      "Epoch [2611], train_loss: 574.49 with loss1: 477.20, loss2: 97.29 and loss3: 0.00\n",
      "Epoch [2612], train_loss: 575.82 with loss1: 478.23, loss2: 97.58 and loss3: 0.00\n",
      "Epoch [2613], train_loss: 574.79 with loss1: 477.66, loss2: 97.13 and loss3: 0.00\n",
      "Epoch [2614], train_loss: 575.26 with loss1: 477.69, loss2: 97.57 and loss3: 0.00\n",
      "Epoch [2615], train_loss: 572.22 with loss1: 475.08, loss2: 97.15 and loss3: 0.00\n",
      "Epoch [2616], train_loss: 574.38 with loss1: 477.12, loss2: 97.26 and loss3: 0.00\n",
      "Epoch [2617], train_loss: 573.09 with loss1: 475.98, loss2: 97.11 and loss3: 0.00\n",
      "Epoch [2618], train_loss: 575.10 with loss1: 477.81, loss2: 97.29 and loss3: 0.00\n",
      "Epoch [2619], train_loss: 572.30 with loss1: 475.25, loss2: 97.04 and loss3: 0.00\n",
      "Epoch [2620], train_loss: 573.32 with loss1: 476.18, loss2: 97.14 and loss3: 0.00\n",
      "Epoch [2621], train_loss: 570.83 with loss1: 473.86, loss2: 96.97 and loss3: 0.00\n",
      "Epoch [2622], train_loss: 569.69 with loss1: 472.65, loss2: 97.04 and loss3: 0.00\n",
      "Epoch [2623], train_loss: 570.15 with loss1: 473.31, loss2: 96.84 and loss3: 0.00\n",
      "Epoch [2624], train_loss: 568.96 with loss1: 472.10, loss2: 96.86 and loss3: 0.00\n",
      "Epoch [2625], train_loss: 568.13 with loss1: 471.52, loss2: 96.62 and loss3: 0.00\n",
      "Epoch [2626], train_loss: 568.11 with loss1: 471.31, loss2: 96.81 and loss3: 0.00\n",
      "Epoch [2627], train_loss: 566.15 with loss1: 469.60, loss2: 96.56 and loss3: 0.00\n",
      "Epoch [2628], train_loss: 567.91 with loss1: 471.25, loss2: 96.66 and loss3: 0.00\n",
      "Epoch [2629], train_loss: 567.10 with loss1: 470.58, loss2: 96.52 and loss3: 0.00\n",
      "Epoch [2630], train_loss: 568.78 with loss1: 472.08, loss2: 96.70 and loss3: 0.00\n",
      "Epoch [2631], train_loss: 566.78 with loss1: 470.32, loss2: 96.46 and loss3: 0.00\n",
      "Epoch [2632], train_loss: 569.49 with loss1: 472.96, loss2: 96.53 and loss3: 0.00\n",
      "Epoch [2633], train_loss: 569.80 with loss1: 473.33, loss2: 96.47 and loss3: 0.00\n",
      "Epoch [2634], train_loss: 567.63 with loss1: 471.01, loss2: 96.63 and loss3: 0.00\n",
      "Epoch [2635], train_loss: 568.35 with loss1: 472.02, loss2: 96.33 and loss3: 0.00\n",
      "Epoch [2636], train_loss: 568.71 with loss1: 472.40, loss2: 96.31 and loss3: 0.00\n",
      "Epoch [2637], train_loss: 567.20 with loss1: 471.11, loss2: 96.09 and loss3: 0.00\n",
      "Epoch [2638], train_loss: 569.47 with loss1: 473.29, loss2: 96.18 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2639], train_loss: 568.99 with loss1: 472.61, loss2: 96.39 and loss3: 0.00\n",
      "Epoch [2640], train_loss: 570.00 with loss1: 473.79, loss2: 96.21 and loss3: 0.00\n",
      "Epoch [2641], train_loss: 568.25 with loss1: 472.32, loss2: 95.93 and loss3: 0.00\n",
      "Epoch [2642], train_loss: 571.66 with loss1: 475.67, loss2: 95.99 and loss3: 0.00\n",
      "Epoch [2643], train_loss: 570.83 with loss1: 474.84, loss2: 95.99 and loss3: 0.00\n",
      "Epoch [2644], train_loss: 572.50 with loss1: 476.18, loss2: 96.31 and loss3: 0.00\n",
      "Epoch [2645], train_loss: 571.39 with loss1: 475.33, loss2: 96.06 and loss3: 0.00\n",
      "Epoch [2646], train_loss: 572.16 with loss1: 475.87, loss2: 96.29 and loss3: 0.00\n",
      "Epoch [2647], train_loss: 573.68 with loss1: 477.97, loss2: 95.71 and loss3: 0.00\n",
      "Epoch [2648], train_loss: 572.80 with loss1: 476.77, loss2: 96.03 and loss3: 0.00\n",
      "Epoch [2649], train_loss: 574.34 with loss1: 478.55, loss2: 95.80 and loss3: 0.00\n",
      "Epoch [2650], train_loss: 576.29 with loss1: 480.21, loss2: 96.08 and loss3: 0.00\n",
      "Epoch [2651], train_loss: 577.16 with loss1: 481.60, loss2: 95.56 and loss3: 0.00\n",
      "Epoch [2652], train_loss: 579.05 with loss1: 482.94, loss2: 96.12 and loss3: 0.00\n",
      "Epoch [2653], train_loss: 578.17 with loss1: 482.39, loss2: 95.78 and loss3: 0.00\n",
      "Epoch [2654], train_loss: 580.64 with loss1: 484.47, loss2: 96.17 and loss3: 0.00\n",
      "Epoch [2655], train_loss: 580.17 with loss1: 484.50, loss2: 95.66 and loss3: 0.00\n",
      "Epoch [2656], train_loss: 581.85 with loss1: 485.73, loss2: 96.12 and loss3: 0.00\n",
      "Epoch [2657], train_loss: 580.51 with loss1: 484.55, loss2: 95.96 and loss3: 0.00\n",
      "Epoch [2658], train_loss: 580.26 with loss1: 484.26, loss2: 96.00 and loss3: 0.00\n",
      "Epoch [2659], train_loss: 578.93 with loss1: 483.02, loss2: 95.91 and loss3: 0.00\n",
      "Epoch [2660], train_loss: 582.20 with loss1: 486.02, loss2: 96.18 and loss3: 0.00\n",
      "Epoch [2661], train_loss: 579.93 with loss1: 484.19, loss2: 95.75 and loss3: 0.00\n",
      "Epoch [2662], train_loss: 581.92 with loss1: 485.49, loss2: 96.43 and loss3: 0.00\n",
      "Epoch [2663], train_loss: 582.47 with loss1: 486.55, loss2: 95.92 and loss3: 0.00\n",
      "Epoch [2664], train_loss: 585.19 with loss1: 488.85, loss2: 96.33 and loss3: 0.00\n",
      "Epoch [2665], train_loss: 586.28 with loss1: 490.31, loss2: 95.97 and loss3: 0.00\n",
      "Epoch [2666], train_loss: 591.03 with loss1: 494.75, loss2: 96.28 and loss3: 0.00\n",
      "Epoch [2667], train_loss: 590.26 with loss1: 494.16, loss2: 96.10 and loss3: 0.00\n",
      "Epoch [2668], train_loss: 591.36 with loss1: 494.62, loss2: 96.73 and loss3: 0.00\n",
      "Epoch [2669], train_loss: 594.08 with loss1: 498.14, loss2: 95.94 and loss3: 0.00\n",
      "Epoch [2670], train_loss: 598.03 with loss1: 501.31, loss2: 96.72 and loss3: 0.00\n",
      "Epoch [2671], train_loss: 596.04 with loss1: 499.71, loss2: 96.34 and loss3: 0.00\n",
      "Epoch [2672], train_loss: 599.32 with loss1: 502.65, loss2: 96.67 and loss3: 0.00\n",
      "Epoch [2673], train_loss: 599.94 with loss1: 503.60, loss2: 96.34 and loss3: 0.00\n",
      "Epoch [2674], train_loss: 601.46 with loss1: 504.67, loss2: 96.80 and loss3: 0.00\n",
      "Epoch [2675], train_loss: 600.19 with loss1: 503.64, loss2: 96.54 and loss3: 0.00\n",
      "Epoch [2676], train_loss: 604.70 with loss1: 507.85, loss2: 96.86 and loss3: 0.00\n",
      "Epoch [2677], train_loss: 607.70 with loss1: 511.33, loss2: 96.38 and loss3: 0.00\n",
      "Epoch [2678], train_loss: 614.54 with loss1: 517.48, loss2: 97.06 and loss3: 0.00\n",
      "Epoch [2679], train_loss: 614.93 with loss1: 518.62, loss2: 96.31 and loss3: 0.00\n",
      "Epoch [2680], train_loss: 625.23 with loss1: 528.20, loss2: 97.03 and loss3: 0.00\n",
      "Epoch [2681], train_loss: 628.53 with loss1: 532.15, loss2: 96.37 and loss3: 0.00\n",
      "Epoch [2682], train_loss: 636.75 with loss1: 539.78, loss2: 96.97 and loss3: 0.00\n",
      "Epoch [2683], train_loss: 641.88 with loss1: 545.52, loss2: 96.36 and loss3: 0.00\n",
      "Epoch [2684], train_loss: 651.71 with loss1: 554.84, loss2: 96.88 and loss3: 0.00\n",
      "Epoch [2685], train_loss: 656.29 with loss1: 560.00, loss2: 96.29 and loss3: 0.00\n",
      "Epoch [2686], train_loss: 663.15 with loss1: 566.23, loss2: 96.93 and loss3: 0.00\n",
      "Epoch [2687], train_loss: 667.31 with loss1: 571.00, loss2: 96.31 and loss3: 0.00\n",
      "Epoch [2688], train_loss: 671.92 with loss1: 574.97, loss2: 96.95 and loss3: 0.00\n",
      "Epoch [2689], train_loss: 672.22 with loss1: 575.92, loss2: 96.30 and loss3: 0.00\n",
      "Epoch [2690], train_loss: 674.09 with loss1: 577.36, loss2: 96.73 and loss3: 0.00\n",
      "Epoch [2691], train_loss: 667.62 with loss1: 571.21, loss2: 96.40 and loss3: 0.00\n",
      "Epoch [2692], train_loss: 663.15 with loss1: 566.33, loss2: 96.82 and loss3: 0.00\n",
      "Epoch [2693], train_loss: 656.02 with loss1: 559.74, loss2: 96.28 and loss3: 0.00\n",
      "Epoch [2694], train_loss: 651.82 with loss1: 554.91, loss2: 96.91 and loss3: 0.00\n",
      "Epoch [2695], train_loss: 643.41 with loss1: 547.08, loss2: 96.32 and loss3: 0.00\n",
      "Epoch [2696], train_loss: 636.49 with loss1: 539.82, loss2: 96.67 and loss3: 0.00\n",
      "Epoch [2697], train_loss: 628.35 with loss1: 532.15, loss2: 96.21 and loss3: 0.00\n",
      "Epoch [2698], train_loss: 623.41 with loss1: 526.90, loss2: 96.52 and loss3: 0.00\n",
      "Epoch [2699], train_loss: 614.14 with loss1: 518.13, loss2: 96.01 and loss3: 0.00\n",
      "Epoch [2700], train_loss: 608.79 with loss1: 512.46, loss2: 96.33 and loss3: 0.00\n",
      "Epoch [2701], train_loss: 604.01 with loss1: 508.04, loss2: 95.97 and loss3: 0.00\n",
      "Epoch [2702], train_loss: 599.09 with loss1: 502.40, loss2: 96.69 and loss3: 0.00\n",
      "Epoch [2703], train_loss: 592.15 with loss1: 496.39, loss2: 95.75 and loss3: 0.00\n",
      "Epoch [2704], train_loss: 587.93 with loss1: 491.58, loss2: 96.35 and loss3: 0.00\n",
      "Epoch [2705], train_loss: 583.09 with loss1: 487.46, loss2: 95.63 and loss3: 0.00\n",
      "Epoch [2706], train_loss: 579.87 with loss1: 483.94, loss2: 95.94 and loss3: 0.00\n",
      "Epoch [2707], train_loss: 576.48 with loss1: 480.98, loss2: 95.51 and loss3: 0.00\n",
      "Epoch [2708], train_loss: 575.11 with loss1: 479.21, loss2: 95.90 and loss3: 0.00\n",
      "Epoch [2709], train_loss: 571.62 with loss1: 476.10, loss2: 95.52 and loss3: 0.00\n",
      "Epoch [2710], train_loss: 569.17 with loss1: 473.54, loss2: 95.63 and loss3: 0.00\n",
      "Epoch [2711], train_loss: 567.65 with loss1: 472.52, loss2: 95.14 and loss3: 0.00\n",
      "Epoch [2712], train_loss: 566.51 with loss1: 470.93, loss2: 95.58 and loss3: 0.00\n",
      "Epoch [2713], train_loss: 564.41 with loss1: 469.31, loss2: 95.10 and loss3: 0.00\n",
      "Epoch [2714], train_loss: 564.29 with loss1: 469.10, loss2: 95.19 and loss3: 0.00\n",
      "Epoch [2715], train_loss: 561.02 with loss1: 465.93, loss2: 95.10 and loss3: 0.00\n",
      "Epoch [2716], train_loss: 562.90 with loss1: 467.84, loss2: 95.07 and loss3: 0.00\n",
      "Epoch [2717], train_loss: 562.37 with loss1: 467.66, loss2: 94.71 and loss3: 0.00\n",
      "Epoch [2718], train_loss: 561.62 with loss1: 466.53, loss2: 95.09 and loss3: 0.00\n",
      "Epoch [2719], train_loss: 562.77 with loss1: 467.94, loss2: 94.83 and loss3: 0.00\n",
      "Epoch [2720], train_loss: 562.10 with loss1: 467.18, loss2: 94.92 and loss3: 0.00\n",
      "Epoch [2721], train_loss: 560.06 with loss1: 465.52, loss2: 94.54 and loss3: 0.00\n",
      "Epoch [2722], train_loss: 561.54 with loss1: 466.61, loss2: 94.93 and loss3: 0.00\n",
      "Epoch [2723], train_loss: 559.72 with loss1: 465.13, loss2: 94.59 and loss3: 0.00\n",
      "Epoch [2724], train_loss: 559.32 with loss1: 464.78, loss2: 94.54 and loss3: 0.00\n",
      "Epoch [2725], train_loss: 557.77 with loss1: 463.34, loss2: 94.43 and loss3: 0.00\n",
      "Epoch [2726], train_loss: 558.59 with loss1: 464.22, loss2: 94.37 and loss3: 0.00\n",
      "Epoch [2727], train_loss: 557.96 with loss1: 463.71, loss2: 94.25 and loss3: 0.00\n",
      "Epoch [2728], train_loss: 558.01 with loss1: 463.50, loss2: 94.51 and loss3: 0.00\n",
      "Epoch [2729], train_loss: 558.35 with loss1: 464.00, loss2: 94.35 and loss3: 0.00\n",
      "Epoch [2730], train_loss: 558.31 with loss1: 463.86, loss2: 94.45 and loss3: 0.00\n",
      "Epoch [2731], train_loss: 556.81 with loss1: 462.82, loss2: 93.99 and loss3: 0.00\n",
      "Epoch [2732], train_loss: 557.94 with loss1: 463.69, loss2: 94.25 and loss3: 0.00\n",
      "Epoch [2733], train_loss: 560.14 with loss1: 466.30, loss2: 93.84 and loss3: 0.00\n",
      "Epoch [2734], train_loss: 561.20 with loss1: 467.09, loss2: 94.11 and loss3: 0.00\n",
      "Epoch [2735], train_loss: 563.33 with loss1: 469.40, loss2: 93.93 and loss3: 0.00\n",
      "Epoch [2736], train_loss: 565.92 with loss1: 471.85, loss2: 94.07 and loss3: 0.00\n",
      "Epoch [2737], train_loss: 564.90 with loss1: 471.16, loss2: 93.74 and loss3: 0.00\n",
      "Epoch [2738], train_loss: 565.86 with loss1: 472.01, loss2: 93.85 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2739], train_loss: 567.11 with loss1: 473.32, loss2: 93.79 and loss3: 0.00\n",
      "Epoch [2740], train_loss: 568.84 with loss1: 474.81, loss2: 94.02 and loss3: 0.00\n",
      "Epoch [2741], train_loss: 567.85 with loss1: 474.24, loss2: 93.62 and loss3: 0.00\n",
      "Epoch [2742], train_loss: 570.87 with loss1: 477.23, loss2: 93.64 and loss3: 0.00\n",
      "Epoch [2743], train_loss: 569.43 with loss1: 475.74, loss2: 93.69 and loss3: 0.00\n",
      "Epoch [2744], train_loss: 573.10 with loss1: 479.43, loss2: 93.67 and loss3: 0.00\n",
      "Epoch [2745], train_loss: 572.68 with loss1: 479.02, loss2: 93.67 and loss3: 0.00\n",
      "Epoch [2746], train_loss: 576.67 with loss1: 483.06, loss2: 93.60 and loss3: 0.00\n",
      "Epoch [2747], train_loss: 574.99 with loss1: 481.55, loss2: 93.44 and loss3: 0.00\n",
      "Epoch [2748], train_loss: 577.22 with loss1: 483.42, loss2: 93.81 and loss3: 0.00\n",
      "Epoch [2749], train_loss: 579.15 with loss1: 485.46, loss2: 93.69 and loss3: 0.00\n",
      "Epoch [2750], train_loss: 581.45 with loss1: 487.72, loss2: 93.72 and loss3: 0.00\n",
      "Epoch [2751], train_loss: 578.55 with loss1: 485.09, loss2: 93.46 and loss3: 0.00\n",
      "Epoch [2752], train_loss: 580.80 with loss1: 486.97, loss2: 93.83 and loss3: 0.00\n",
      "Epoch [2753], train_loss: 579.82 with loss1: 486.35, loss2: 93.47 and loss3: 0.00\n",
      "Epoch [2754], train_loss: 583.54 with loss1: 490.03, loss2: 93.52 and loss3: 0.00\n",
      "Epoch [2755], train_loss: 580.12 with loss1: 486.53, loss2: 93.59 and loss3: 0.00\n",
      "Epoch [2756], train_loss: 583.04 with loss1: 489.20, loss2: 93.83 and loss3: 0.00\n",
      "Epoch [2757], train_loss: 579.62 with loss1: 486.11, loss2: 93.51 and loss3: 0.00\n",
      "Epoch [2758], train_loss: 582.34 with loss1: 488.53, loss2: 93.82 and loss3: 0.00\n",
      "Epoch [2759], train_loss: 580.56 with loss1: 487.30, loss2: 93.26 and loss3: 0.00\n",
      "Epoch [2760], train_loss: 582.17 with loss1: 488.46, loss2: 93.71 and loss3: 0.00\n",
      "Epoch [2761], train_loss: 581.21 with loss1: 487.81, loss2: 93.40 and loss3: 0.00\n",
      "Epoch [2762], train_loss: 581.98 with loss1: 488.47, loss2: 93.51 and loss3: 0.00\n",
      "Epoch [2763], train_loss: 581.59 with loss1: 488.26, loss2: 93.32 and loss3: 0.00\n",
      "Epoch [2764], train_loss: 584.96 with loss1: 491.39, loss2: 93.57 and loss3: 0.00\n",
      "Epoch [2765], train_loss: 582.65 with loss1: 489.24, loss2: 93.41 and loss3: 0.00\n",
      "Epoch [2766], train_loss: 583.86 with loss1: 490.25, loss2: 93.60 and loss3: 0.00\n",
      "Epoch [2767], train_loss: 580.05 with loss1: 486.74, loss2: 93.31 and loss3: 0.00\n",
      "Epoch [2768], train_loss: 584.82 with loss1: 491.30, loss2: 93.52 and loss3: 0.00\n",
      "Epoch [2769], train_loss: 581.86 with loss1: 488.51, loss2: 93.35 and loss3: 0.00\n",
      "Epoch [2770], train_loss: 583.09 with loss1: 489.72, loss2: 93.37 and loss3: 0.00\n",
      "Epoch [2771], train_loss: 579.10 with loss1: 485.89, loss2: 93.20 and loss3: 0.00\n",
      "Epoch [2772], train_loss: 581.20 with loss1: 487.81, loss2: 93.39 and loss3: 0.00\n",
      "Epoch [2773], train_loss: 578.44 with loss1: 485.29, loss2: 93.15 and loss3: 0.00\n",
      "Epoch [2774], train_loss: 579.46 with loss1: 486.09, loss2: 93.37 and loss3: 0.00\n",
      "Epoch [2775], train_loss: 577.34 with loss1: 484.26, loss2: 93.08 and loss3: 0.00\n",
      "Epoch [2776], train_loss: 578.38 with loss1: 485.09, loss2: 93.28 and loss3: 0.00\n",
      "Epoch [2777], train_loss: 577.92 with loss1: 484.84, loss2: 93.08 and loss3: 0.00\n",
      "Epoch [2778], train_loss: 577.02 with loss1: 483.74, loss2: 93.27 and loss3: 0.00\n",
      "Epoch [2779], train_loss: 574.60 with loss1: 481.75, loss2: 92.85 and loss3: 0.00\n",
      "Epoch [2780], train_loss: 575.64 with loss1: 482.68, loss2: 92.96 and loss3: 0.00\n",
      "Epoch [2781], train_loss: 574.33 with loss1: 481.61, loss2: 92.72 and loss3: 0.00\n",
      "Epoch [2782], train_loss: 574.31 with loss1: 481.22, loss2: 93.09 and loss3: 0.00\n",
      "Epoch [2783], train_loss: 573.24 with loss1: 480.28, loss2: 92.96 and loss3: 0.00\n",
      "Epoch [2784], train_loss: 575.01 with loss1: 482.13, loss2: 92.88 and loss3: 0.00\n",
      "Epoch [2785], train_loss: 572.00 with loss1: 479.28, loss2: 92.72 and loss3: 0.00\n",
      "Epoch [2786], train_loss: 572.54 with loss1: 479.68, loss2: 92.86 and loss3: 0.00\n",
      "Epoch [2787], train_loss: 571.84 with loss1: 479.27, loss2: 92.57 and loss3: 0.00\n",
      "Epoch [2788], train_loss: 573.85 with loss1: 480.93, loss2: 92.93 and loss3: 0.00\n",
      "Epoch [2789], train_loss: 571.79 with loss1: 479.17, loss2: 92.61 and loss3: 0.00\n",
      "Epoch [2790], train_loss: 573.54 with loss1: 480.95, loss2: 92.59 and loss3: 0.00\n",
      "Epoch [2791], train_loss: 571.31 with loss1: 478.69, loss2: 92.62 and loss3: 0.00\n",
      "Epoch [2792], train_loss: 572.45 with loss1: 479.70, loss2: 92.74 and loss3: 0.00\n",
      "Epoch [2793], train_loss: 569.76 with loss1: 477.27, loss2: 92.49 and loss3: 0.00\n",
      "Epoch [2794], train_loss: 568.83 with loss1: 476.17, loss2: 92.66 and loss3: 0.00\n",
      "Epoch [2795], train_loss: 566.83 with loss1: 474.55, loss2: 92.28 and loss3: 0.00\n",
      "Epoch [2796], train_loss: 568.40 with loss1: 476.02, loss2: 92.38 and loss3: 0.00\n",
      "Epoch [2797], train_loss: 565.77 with loss1: 473.61, loss2: 92.16 and loss3: 0.00\n",
      "Epoch [2798], train_loss: 567.44 with loss1: 475.07, loss2: 92.38 and loss3: 0.00\n",
      "Epoch [2799], train_loss: 564.36 with loss1: 472.13, loss2: 92.23 and loss3: 0.00\n",
      "Epoch [2800], train_loss: 567.85 with loss1: 475.42, loss2: 92.42 and loss3: 0.00\n",
      "Epoch [2801], train_loss: 564.67 with loss1: 472.37, loss2: 92.30 and loss3: 0.00\n",
      "Epoch [2802], train_loss: 564.84 with loss1: 472.57, loss2: 92.27 and loss3: 0.00\n",
      "Epoch [2803], train_loss: 561.60 with loss1: 469.51, loss2: 92.08 and loss3: 0.00\n",
      "Epoch [2804], train_loss: 562.68 with loss1: 470.52, loss2: 92.16 and loss3: 0.00\n",
      "Epoch [2805], train_loss: 560.33 with loss1: 468.37, loss2: 91.97 and loss3: 0.00\n",
      "Epoch [2806], train_loss: 562.33 with loss1: 470.15, loss2: 92.17 and loss3: 0.00\n",
      "Epoch [2807], train_loss: 559.12 with loss1: 467.22, loss2: 91.90 and loss3: 0.00\n",
      "Epoch [2808], train_loss: 558.00 with loss1: 465.98, loss2: 92.01 and loss3: 0.00\n",
      "Epoch [2809], train_loss: 556.33 with loss1: 464.42, loss2: 91.91 and loss3: 0.00\n",
      "Epoch [2810], train_loss: 558.32 with loss1: 466.41, loss2: 91.91 and loss3: 0.00\n",
      "Epoch [2811], train_loss: 557.71 with loss1: 465.84, loss2: 91.87 and loss3: 0.00\n",
      "Epoch [2812], train_loss: 556.81 with loss1: 464.80, loss2: 92.01 and loss3: 0.00\n",
      "Epoch [2813], train_loss: 554.72 with loss1: 462.99, loss2: 91.73 and loss3: 0.00\n",
      "Epoch [2814], train_loss: 556.73 with loss1: 464.88, loss2: 91.85 and loss3: 0.00\n",
      "Epoch [2815], train_loss: 554.12 with loss1: 462.48, loss2: 91.64 and loss3: 0.00\n",
      "Epoch [2816], train_loss: 554.69 with loss1: 462.85, loss2: 91.84 and loss3: 0.00\n",
      "Epoch [2817], train_loss: 553.16 with loss1: 461.77, loss2: 91.40 and loss3: 0.00\n",
      "Epoch [2818], train_loss: 553.42 with loss1: 461.59, loss2: 91.83 and loss3: 0.00\n",
      "Epoch [2819], train_loss: 550.53 with loss1: 458.90, loss2: 91.63 and loss3: 0.00\n",
      "Epoch [2820], train_loss: 552.13 with loss1: 460.34, loss2: 91.79 and loss3: 0.00\n",
      "Epoch [2821], train_loss: 550.43 with loss1: 458.88, loss2: 91.55 and loss3: 0.00\n",
      "Epoch [2822], train_loss: 549.24 with loss1: 457.82, loss2: 91.42 and loss3: 0.00\n",
      "Epoch [2823], train_loss: 550.29 with loss1: 458.90, loss2: 91.39 and loss3: 0.00\n",
      "Epoch [2824], train_loss: 549.98 with loss1: 458.61, loss2: 91.37 and loss3: 0.00\n",
      "Epoch [2825], train_loss: 546.98 with loss1: 455.81, loss2: 91.17 and loss3: 0.00\n",
      "Epoch [2826], train_loss: 548.41 with loss1: 456.98, loss2: 91.43 and loss3: 0.00\n",
      "Epoch [2827], train_loss: 547.13 with loss1: 455.91, loss2: 91.22 and loss3: 0.00\n",
      "Epoch [2828], train_loss: 548.92 with loss1: 457.64, loss2: 91.28 and loss3: 0.00\n",
      "Epoch [2829], train_loss: 547.12 with loss1: 455.93, loss2: 91.19 and loss3: 0.00\n",
      "Epoch [2830], train_loss: 546.79 with loss1: 455.35, loss2: 91.43 and loss3: 0.00\n",
      "Epoch [2831], train_loss: 547.24 with loss1: 456.38, loss2: 90.87 and loss3: 0.00\n",
      "Epoch [2832], train_loss: 548.30 with loss1: 457.01, loss2: 91.29 and loss3: 0.00\n",
      "Epoch [2833], train_loss: 548.90 with loss1: 457.98, loss2: 90.92 and loss3: 0.00\n",
      "Epoch [2834], train_loss: 548.26 with loss1: 457.13, loss2: 91.13 and loss3: 0.00\n",
      "Epoch [2835], train_loss: 548.98 with loss1: 458.15, loss2: 90.83 and loss3: 0.00\n",
      "Epoch [2836], train_loss: 550.74 with loss1: 459.82, loss2: 90.92 and loss3: 0.00\n",
      "Epoch [2837], train_loss: 548.94 with loss1: 458.23, loss2: 90.71 and loss3: 0.00\n",
      "Epoch [2838], train_loss: 548.75 with loss1: 457.95, loss2: 90.80 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2839], train_loss: 548.11 with loss1: 457.41, loss2: 90.69 and loss3: 0.00\n",
      "Epoch [2840], train_loss: 552.28 with loss1: 461.37, loss2: 90.91 and loss3: 0.00\n",
      "Epoch [2841], train_loss: 550.35 with loss1: 459.54, loss2: 90.81 and loss3: 0.00\n",
      "Epoch [2842], train_loss: 551.71 with loss1: 460.85, loss2: 90.86 and loss3: 0.00\n",
      "Epoch [2843], train_loss: 551.36 with loss1: 460.76, loss2: 90.60 and loss3: 0.00\n",
      "Epoch [2844], train_loss: 554.03 with loss1: 463.28, loss2: 90.74 and loss3: 0.00\n",
      "Epoch [2845], train_loss: 553.85 with loss1: 463.31, loss2: 90.55 and loss3: 0.00\n",
      "Epoch [2846], train_loss: 554.46 with loss1: 463.65, loss2: 90.81 and loss3: 0.00\n",
      "Epoch [2847], train_loss: 556.11 with loss1: 465.25, loss2: 90.87 and loss3: 0.00\n",
      "Epoch [2848], train_loss: 556.61 with loss1: 465.91, loss2: 90.70 and loss3: 0.00\n",
      "Epoch [2849], train_loss: 557.90 with loss1: 467.46, loss2: 90.44 and loss3: 0.00\n",
      "Epoch [2850], train_loss: 560.11 with loss1: 469.30, loss2: 90.80 and loss3: 0.00\n",
      "Epoch [2851], train_loss: 561.13 with loss1: 470.79, loss2: 90.34 and loss3: 0.00\n",
      "Epoch [2852], train_loss: 564.34 with loss1: 473.60, loss2: 90.75 and loss3: 0.00\n",
      "Epoch [2853], train_loss: 564.57 with loss1: 473.92, loss2: 90.65 and loss3: 0.00\n",
      "Epoch [2854], train_loss: 568.25 with loss1: 477.41, loss2: 90.85 and loss3: 0.00\n",
      "Epoch [2855], train_loss: 569.69 with loss1: 479.22, loss2: 90.47 and loss3: 0.00\n",
      "Epoch [2856], train_loss: 573.01 with loss1: 482.05, loss2: 90.96 and loss3: 0.00\n",
      "Epoch [2857], train_loss: 574.58 with loss1: 484.03, loss2: 90.55 and loss3: 0.00\n",
      "Epoch [2858], train_loss: 577.96 with loss1: 487.12, loss2: 90.85 and loss3: 0.00\n",
      "Epoch [2859], train_loss: 584.59 with loss1: 494.25, loss2: 90.35 and loss3: 0.00\n",
      "Epoch [2860], train_loss: 591.07 with loss1: 500.32, loss2: 90.75 and loss3: 0.00\n",
      "Epoch [2861], train_loss: 595.81 with loss1: 505.26, loss2: 90.55 and loss3: 0.00\n",
      "Epoch [2862], train_loss: 599.65 with loss1: 508.82, loss2: 90.83 and loss3: 0.00\n",
      "Epoch [2863], train_loss: 605.46 with loss1: 514.90, loss2: 90.55 and loss3: 0.00\n",
      "Epoch [2864], train_loss: 611.67 with loss1: 520.76, loss2: 90.91 and loss3: 0.00\n",
      "Epoch [2865], train_loss: 614.36 with loss1: 523.73, loss2: 90.64 and loss3: 0.00\n",
      "Epoch [2866], train_loss: 621.86 with loss1: 530.63, loss2: 91.24 and loss3: 0.00\n",
      "Epoch [2867], train_loss: 627.06 with loss1: 536.33, loss2: 90.73 and loss3: 0.00\n",
      "Epoch [2868], train_loss: 632.12 with loss1: 540.63, loss2: 91.49 and loss3: 0.00\n",
      "Epoch [2869], train_loss: 634.78 with loss1: 543.67, loss2: 91.11 and loss3: 0.00\n",
      "Epoch [2870], train_loss: 637.76 with loss1: 546.15, loss2: 91.61 and loss3: 0.00\n",
      "Epoch [2871], train_loss: 636.87 with loss1: 545.64, loss2: 91.23 and loss3: 0.00\n",
      "Epoch [2872], train_loss: 634.55 with loss1: 542.80, loss2: 91.75 and loss3: 0.00\n",
      "Epoch [2873], train_loss: 632.31 with loss1: 541.00, loss2: 91.31 and loss3: 0.00\n",
      "Epoch [2874], train_loss: 628.40 with loss1: 536.30, loss2: 92.10 and loss3: 0.00\n",
      "Epoch [2875], train_loss: 623.86 with loss1: 532.73, loss2: 91.14 and loss3: 0.00\n",
      "Epoch [2876], train_loss: 616.70 with loss1: 524.68, loss2: 92.03 and loss3: 0.00\n",
      "Epoch [2877], train_loss: 611.23 with loss1: 519.55, loss2: 91.68 and loss3: 0.00\n",
      "Epoch [2878], train_loss: 601.13 with loss1: 508.99, loss2: 92.14 and loss3: 0.00\n",
      "Epoch [2879], train_loss: 595.59 with loss1: 503.88, loss2: 91.71 and loss3: 0.00\n",
      "Epoch [2880], train_loss: 589.62 with loss1: 497.31, loss2: 92.31 and loss3: 0.00\n",
      "Epoch [2881], train_loss: 586.54 with loss1: 494.73, loss2: 91.81 and loss3: 0.00\n",
      "Epoch [2882], train_loss: 579.29 with loss1: 486.90, loss2: 92.39 and loss3: 0.00\n",
      "Epoch [2883], train_loss: 572.32 with loss1: 480.52, loss2: 91.80 and loss3: 0.00\n",
      "Epoch [2884], train_loss: 566.70 with loss1: 474.62, loss2: 92.08 and loss3: 0.00\n",
      "Epoch [2885], train_loss: 561.36 with loss1: 469.41, loss2: 91.95 and loss3: 0.00\n",
      "Epoch [2886], train_loss: 556.18 with loss1: 464.13, loss2: 92.05 and loss3: 0.00\n",
      "Epoch [2887], train_loss: 554.08 with loss1: 462.20, loss2: 91.88 and loss3: 0.00\n",
      "Epoch [2888], train_loss: 550.49 with loss1: 458.69, loss2: 91.80 and loss3: 0.00\n",
      "Epoch [2889], train_loss: 547.64 with loss1: 455.98, loss2: 91.65 and loss3: 0.00\n",
      "Epoch [2890], train_loss: 545.89 with loss1: 454.08, loss2: 91.81 and loss3: 0.00\n",
      "Epoch [2891], train_loss: 541.99 with loss1: 450.46, loss2: 91.53 and loss3: 0.00\n",
      "Epoch [2892], train_loss: 539.68 with loss1: 447.94, loss2: 91.74 and loss3: 0.00\n",
      "Epoch [2893], train_loss: 538.05 with loss1: 446.66, loss2: 91.39 and loss3: 0.00\n",
      "Epoch [2894], train_loss: 536.74 with loss1: 445.22, loss2: 91.52 and loss3: 0.00\n",
      "Epoch [2895], train_loss: 536.19 with loss1: 444.86, loss2: 91.32 and loss3: 0.00\n",
      "Epoch [2896], train_loss: 535.33 with loss1: 444.00, loss2: 91.33 and loss3: 0.00\n",
      "Epoch [2897], train_loss: 533.35 with loss1: 442.37, loss2: 90.98 and loss3: 0.00\n",
      "Epoch [2898], train_loss: 533.70 with loss1: 442.49, loss2: 91.20 and loss3: 0.00\n",
      "Epoch [2899], train_loss: 532.46 with loss1: 441.60, loss2: 90.85 and loss3: 0.00\n",
      "Epoch [2900], train_loss: 533.96 with loss1: 442.75, loss2: 91.21 and loss3: 0.00\n",
      "Epoch [2901], train_loss: 533.11 with loss1: 442.15, loss2: 90.96 and loss3: 0.00\n",
      "Epoch [2902], train_loss: 534.77 with loss1: 443.77, loss2: 91.00 and loss3: 0.00\n",
      "Epoch [2903], train_loss: 533.74 with loss1: 442.98, loss2: 90.76 and loss3: 0.00\n",
      "Epoch [2904], train_loss: 532.99 with loss1: 442.20, loss2: 90.79 and loss3: 0.00\n",
      "Epoch [2905], train_loss: 533.99 with loss1: 443.36, loss2: 90.63 and loss3: 0.00\n",
      "Epoch [2906], train_loss: 532.66 with loss1: 441.94, loss2: 90.72 and loss3: 0.00\n",
      "Epoch [2907], train_loss: 532.19 with loss1: 441.77, loss2: 90.42 and loss3: 0.00\n",
      "Epoch [2908], train_loss: 532.89 with loss1: 442.09, loss2: 90.80 and loss3: 0.00\n",
      "Epoch [2909], train_loss: 532.38 with loss1: 441.87, loss2: 90.51 and loss3: 0.00\n",
      "Epoch [2910], train_loss: 535.02 with loss1: 444.65, loss2: 90.36 and loss3: 0.00\n",
      "Epoch [2911], train_loss: 533.24 with loss1: 442.98, loss2: 90.26 and loss3: 0.00\n",
      "Epoch [2912], train_loss: 534.08 with loss1: 443.80, loss2: 90.29 and loss3: 0.00\n",
      "Epoch [2913], train_loss: 533.95 with loss1: 443.73, loss2: 90.22 and loss3: 0.00\n",
      "Epoch [2914], train_loss: 535.64 with loss1: 445.24, loss2: 90.40 and loss3: 0.00\n",
      "Epoch [2915], train_loss: 535.41 with loss1: 445.34, loss2: 90.07 and loss3: 0.00\n",
      "Epoch [2916], train_loss: 539.37 with loss1: 449.32, loss2: 90.05 and loss3: 0.00\n",
      "Epoch [2917], train_loss: 538.11 with loss1: 447.97, loss2: 90.14 and loss3: 0.00\n",
      "Epoch [2918], train_loss: 539.51 with loss1: 449.56, loss2: 89.95 and loss3: 0.00\n",
      "Epoch [2919], train_loss: 541.15 with loss1: 451.30, loss2: 89.85 and loss3: 0.00\n",
      "Epoch [2920], train_loss: 540.98 with loss1: 450.91, loss2: 90.07 and loss3: 0.00\n",
      "Epoch [2921], train_loss: 543.32 with loss1: 453.61, loss2: 89.71 and loss3: 0.00\n",
      "Epoch [2922], train_loss: 545.63 with loss1: 455.67, loss2: 89.95 and loss3: 0.00\n",
      "Epoch [2923], train_loss: 544.60 with loss1: 454.83, loss2: 89.78 and loss3: 0.00\n",
      "Epoch [2924], train_loss: 547.52 with loss1: 457.61, loss2: 89.91 and loss3: 0.00\n",
      "Epoch [2925], train_loss: 546.16 with loss1: 456.35, loss2: 89.81 and loss3: 0.00\n",
      "Epoch [2926], train_loss: 549.73 with loss1: 459.93, loss2: 89.80 and loss3: 0.00\n",
      "Epoch [2927], train_loss: 549.02 with loss1: 459.39, loss2: 89.63 and loss3: 0.00\n",
      "Epoch [2928], train_loss: 552.74 with loss1: 463.02, loss2: 89.72 and loss3: 0.00\n",
      "Epoch [2929], train_loss: 551.98 with loss1: 462.37, loss2: 89.62 and loss3: 0.00\n",
      "Epoch [2930], train_loss: 557.85 with loss1: 468.34, loss2: 89.51 and loss3: 0.00\n",
      "Epoch [2931], train_loss: 556.05 with loss1: 466.53, loss2: 89.52 and loss3: 0.00\n",
      "Epoch [2932], train_loss: 558.52 with loss1: 468.96, loss2: 89.55 and loss3: 0.00\n",
      "Epoch [2933], train_loss: 558.38 with loss1: 469.04, loss2: 89.34 and loss3: 0.00\n",
      "Epoch [2934], train_loss: 561.53 with loss1: 471.93, loss2: 89.61 and loss3: 0.00\n",
      "Epoch [2935], train_loss: 562.31 with loss1: 472.87, loss2: 89.43 and loss3: 0.00\n",
      "Epoch [2936], train_loss: 565.55 with loss1: 476.03, loss2: 89.52 and loss3: 0.00\n",
      "Epoch [2937], train_loss: 564.38 with loss1: 474.91, loss2: 89.47 and loss3: 0.00\n",
      "Epoch [2938], train_loss: 567.00 with loss1: 477.63, loss2: 89.36 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2939], train_loss: 566.55 with loss1: 477.22, loss2: 89.33 and loss3: 0.00\n",
      "Epoch [2940], train_loss: 572.07 with loss1: 482.50, loss2: 89.57 and loss3: 0.00\n",
      "Epoch [2941], train_loss: 570.60 with loss1: 481.10, loss2: 89.50 and loss3: 0.00\n",
      "Epoch [2942], train_loss: 573.68 with loss1: 484.29, loss2: 89.39 and loss3: 0.00\n",
      "Epoch [2943], train_loss: 572.96 with loss1: 483.69, loss2: 89.27 and loss3: 0.00\n",
      "Epoch [2944], train_loss: 577.98 with loss1: 488.61, loss2: 89.37 and loss3: 0.00\n",
      "Epoch [2945], train_loss: 576.05 with loss1: 486.52, loss2: 89.53 and loss3: 0.00\n",
      "Epoch [2946], train_loss: 578.92 with loss1: 489.32, loss2: 89.60 and loss3: 0.00\n",
      "Epoch [2947], train_loss: 577.65 with loss1: 488.04, loss2: 89.61 and loss3: 0.00\n",
      "Epoch [2948], train_loss: 582.05 with loss1: 492.28, loss2: 89.77 and loss3: 0.00\n",
      "Epoch [2949], train_loss: 579.48 with loss1: 489.76, loss2: 89.72 and loss3: 0.00\n",
      "Epoch [2950], train_loss: 581.16 with loss1: 491.53, loss2: 89.63 and loss3: 0.00\n",
      "Epoch [2951], train_loss: 579.24 with loss1: 489.61, loss2: 89.63 and loss3: 0.00\n",
      "Epoch [2952], train_loss: 581.07 with loss1: 491.22, loss2: 89.85 and loss3: 0.00\n",
      "Epoch [2953], train_loss: 577.36 with loss1: 487.62, loss2: 89.74 and loss3: 0.00\n",
      "Epoch [2954], train_loss: 577.93 with loss1: 488.22, loss2: 89.71 and loss3: 0.00\n",
      "Epoch [2955], train_loss: 576.27 with loss1: 486.61, loss2: 89.66 and loss3: 0.00\n",
      "Epoch [2956], train_loss: 579.54 with loss1: 489.81, loss2: 89.73 and loss3: 0.00\n",
      "Epoch [2957], train_loss: 576.78 with loss1: 487.12, loss2: 89.66 and loss3: 0.00\n",
      "Epoch [2958], train_loss: 579.59 with loss1: 489.94, loss2: 89.65 and loss3: 0.00\n",
      "Epoch [2959], train_loss: 577.63 with loss1: 487.83, loss2: 89.80 and loss3: 0.00\n",
      "Epoch [2960], train_loss: 579.17 with loss1: 489.47, loss2: 89.70 and loss3: 0.00\n",
      "Epoch [2961], train_loss: 576.61 with loss1: 486.83, loss2: 89.79 and loss3: 0.00\n",
      "Epoch [2962], train_loss: 577.31 with loss1: 487.35, loss2: 89.96 and loss3: 0.00\n",
      "Epoch [2963], train_loss: 575.96 with loss1: 486.03, loss2: 89.93 and loss3: 0.00\n",
      "Epoch [2964], train_loss: 575.63 with loss1: 485.68, loss2: 89.95 and loss3: 0.00\n",
      "Epoch [2965], train_loss: 571.62 with loss1: 481.92, loss2: 89.69 and loss3: 0.00\n",
      "Epoch [2966], train_loss: 570.14 with loss1: 480.36, loss2: 89.77 and loss3: 0.00\n",
      "Epoch [2967], train_loss: 567.42 with loss1: 477.67, loss2: 89.75 and loss3: 0.00\n",
      "Epoch [2968], train_loss: 568.20 with loss1: 478.19, loss2: 90.01 and loss3: 0.00\n",
      "Epoch [2969], train_loss: 564.78 with loss1: 475.04, loss2: 89.74 and loss3: 0.00\n",
      "Epoch [2970], train_loss: 561.22 with loss1: 471.31, loss2: 89.91 and loss3: 0.00\n",
      "Epoch [2971], train_loss: 559.15 with loss1: 469.49, loss2: 89.67 and loss3: 0.00\n",
      "Epoch [2972], train_loss: 560.31 with loss1: 470.53, loss2: 89.77 and loss3: 0.00\n",
      "Epoch [2973], train_loss: 555.57 with loss1: 465.98, loss2: 89.60 and loss3: 0.00\n",
      "Epoch [2974], train_loss: 555.02 with loss1: 465.28, loss2: 89.74 and loss3: 0.00\n",
      "Epoch [2975], train_loss: 552.72 with loss1: 463.11, loss2: 89.60 and loss3: 0.00\n",
      "Epoch [2976], train_loss: 553.46 with loss1: 463.63, loss2: 89.83 and loss3: 0.00\n",
      "Epoch [2977], train_loss: 550.04 with loss1: 460.35, loss2: 89.68 and loss3: 0.00\n",
      "Epoch [2978], train_loss: 549.30 with loss1: 459.76, loss2: 89.54 and loss3: 0.00\n",
      "Epoch [2979], train_loss: 549.33 with loss1: 459.87, loss2: 89.46 and loss3: 0.00\n",
      "Epoch [2980], train_loss: 549.35 with loss1: 459.64, loss2: 89.71 and loss3: 0.00\n",
      "Epoch [2981], train_loss: 547.97 with loss1: 458.48, loss2: 89.48 and loss3: 0.00\n",
      "Epoch [2982], train_loss: 548.65 with loss1: 459.20, loss2: 89.45 and loss3: 0.00\n",
      "Epoch [2983], train_loss: 546.21 with loss1: 456.74, loss2: 89.47 and loss3: 0.00\n",
      "Epoch [2984], train_loss: 546.04 with loss1: 456.65, loss2: 89.40 and loss3: 0.00\n",
      "Epoch [2985], train_loss: 544.19 with loss1: 454.76, loss2: 89.44 and loss3: 0.00\n",
      "Epoch [2986], train_loss: 542.75 with loss1: 453.42, loss2: 89.33 and loss3: 0.00\n",
      "Epoch [2987], train_loss: 540.61 with loss1: 451.45, loss2: 89.16 and loss3: 0.00\n",
      "Epoch [2988], train_loss: 541.21 with loss1: 452.07, loss2: 89.14 and loss3: 0.00\n",
      "Epoch [2989], train_loss: 541.34 with loss1: 452.15, loss2: 89.19 and loss3: 0.00\n",
      "Epoch [2990], train_loss: 540.00 with loss1: 450.69, loss2: 89.31 and loss3: 0.00\n",
      "Epoch [2991], train_loss: 538.04 with loss1: 449.01, loss2: 89.03 and loss3: 0.00\n",
      "Epoch [2992], train_loss: 538.74 with loss1: 449.60, loss2: 89.14 and loss3: 0.00\n",
      "Epoch [2993], train_loss: 537.56 with loss1: 448.31, loss2: 89.25 and loss3: 0.00\n",
      "Epoch [2994], train_loss: 537.96 with loss1: 448.83, loss2: 89.14 and loss3: 0.00\n",
      "Epoch [2995], train_loss: 539.68 with loss1: 450.67, loss2: 89.01 and loss3: 0.00\n",
      "Epoch [2996], train_loss: 538.31 with loss1: 449.21, loss2: 89.10 and loss3: 0.00\n",
      "Epoch [2997], train_loss: 539.41 with loss1: 450.49, loss2: 88.92 and loss3: 0.00\n",
      "Epoch [2998], train_loss: 538.13 with loss1: 449.09, loss2: 89.04 and loss3: 0.00\n",
      "Epoch [2999], train_loss: 538.70 with loss1: 449.85, loss2: 88.85 and loss3: 0.00\n",
      "Epoch [3000], train_loss: 539.37 with loss1: 450.40, loss2: 88.97 and loss3: 0.00\n",
      "Epoch [3001], train_loss: 538.01 with loss1: 449.20, loss2: 88.81 and loss3: 0.00\n",
      "Epoch [3002], train_loss: 540.17 with loss1: 451.20, loss2: 88.97 and loss3: 0.00\n",
      "Epoch [3003], train_loss: 539.19 with loss1: 450.51, loss2: 88.69 and loss3: 0.00\n",
      "Epoch [3004], train_loss: 539.28 with loss1: 450.40, loss2: 88.88 and loss3: 0.00\n",
      "Epoch [3005], train_loss: 538.78 with loss1: 450.19, loss2: 88.58 and loss3: 0.00\n",
      "Epoch [3006], train_loss: 541.48 with loss1: 452.69, loss2: 88.80 and loss3: 0.00\n",
      "Epoch [3007], train_loss: 539.68 with loss1: 451.04, loss2: 88.64 and loss3: 0.00\n",
      "Epoch [3008], train_loss: 540.86 with loss1: 452.16, loss2: 88.71 and loss3: 0.00\n",
      "Epoch [3009], train_loss: 540.42 with loss1: 452.09, loss2: 88.33 and loss3: 0.00\n",
      "Epoch [3010], train_loss: 542.46 with loss1: 453.97, loss2: 88.49 and loss3: 0.00\n",
      "Epoch [3011], train_loss: 540.29 with loss1: 451.99, loss2: 88.30 and loss3: 0.00\n",
      "Epoch [3012], train_loss: 543.19 with loss1: 454.63, loss2: 88.55 and loss3: 0.00\n",
      "Epoch [3013], train_loss: 542.22 with loss1: 454.06, loss2: 88.16 and loss3: 0.00\n",
      "Epoch [3014], train_loss: 546.45 with loss1: 457.94, loss2: 88.51 and loss3: 0.00\n",
      "Epoch [3015], train_loss: 542.05 with loss1: 453.84, loss2: 88.21 and loss3: 0.00\n",
      "Epoch [3016], train_loss: 544.21 with loss1: 456.06, loss2: 88.15 and loss3: 0.00\n",
      "Epoch [3017], train_loss: 542.01 with loss1: 453.83, loss2: 88.18 and loss3: 0.00\n",
      "Epoch [3018], train_loss: 543.93 with loss1: 455.52, loss2: 88.40 and loss3: 0.00\n",
      "Epoch [3019], train_loss: 539.52 with loss1: 451.61, loss2: 87.90 and loss3: 0.00\n",
      "Epoch [3020], train_loss: 542.29 with loss1: 454.06, loss2: 88.24 and loss3: 0.00\n",
      "Epoch [3021], train_loss: 539.58 with loss1: 451.82, loss2: 87.75 and loss3: 0.00\n",
      "Epoch [3022], train_loss: 540.81 with loss1: 452.68, loss2: 88.12 and loss3: 0.00\n",
      "Epoch [3023], train_loss: 539.77 with loss1: 452.13, loss2: 87.64 and loss3: 0.00\n",
      "Epoch [3024], train_loss: 540.01 with loss1: 451.93, loss2: 88.08 and loss3: 0.00\n",
      "Epoch [3025], train_loss: 538.28 with loss1: 450.57, loss2: 87.71 and loss3: 0.00\n",
      "Epoch [3026], train_loss: 540.12 with loss1: 452.33, loss2: 87.79 and loss3: 0.00\n",
      "Epoch [3027], train_loss: 538.76 with loss1: 451.25, loss2: 87.51 and loss3: 0.00\n",
      "Epoch [3028], train_loss: 541.63 with loss1: 453.94, loss2: 87.69 and loss3: 0.00\n",
      "Epoch [3029], train_loss: 539.47 with loss1: 452.07, loss2: 87.40 and loss3: 0.00\n",
      "Epoch [3030], train_loss: 541.21 with loss1: 453.58, loss2: 87.63 and loss3: 0.00\n",
      "Epoch [3031], train_loss: 540.34 with loss1: 452.90, loss2: 87.44 and loss3: 0.00\n",
      "Epoch [3032], train_loss: 542.06 with loss1: 454.53, loss2: 87.53 and loss3: 0.00\n",
      "Epoch [3033], train_loss: 541.80 with loss1: 454.61, loss2: 87.19 and loss3: 0.00\n",
      "Epoch [3034], train_loss: 545.59 with loss1: 458.00, loss2: 87.59 and loss3: 0.00\n",
      "Epoch [3035], train_loss: 544.95 with loss1: 457.78, loss2: 87.18 and loss3: 0.00\n",
      "Epoch [3036], train_loss: 547.16 with loss1: 459.76, loss2: 87.40 and loss3: 0.00\n",
      "Epoch [3037], train_loss: 547.89 with loss1: 460.78, loss2: 87.11 and loss3: 0.00\n",
      "Epoch [3038], train_loss: 551.74 with loss1: 464.40, loss2: 87.35 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3039], train_loss: 551.68 with loss1: 464.76, loss2: 86.92 and loss3: 0.00\n",
      "Epoch [3040], train_loss: 556.23 with loss1: 468.90, loss2: 87.32 and loss3: 0.00\n",
      "Epoch [3041], train_loss: 558.37 with loss1: 471.58, loss2: 86.79 and loss3: 0.00\n",
      "Epoch [3042], train_loss: 563.73 with loss1: 476.55, loss2: 87.18 and loss3: 0.00\n",
      "Epoch [3043], train_loss: 564.49 with loss1: 477.60, loss2: 86.89 and loss3: 0.00\n",
      "Epoch [3044], train_loss: 571.45 with loss1: 484.19, loss2: 87.27 and loss3: 0.00\n",
      "Epoch [3045], train_loss: 572.97 with loss1: 486.36, loss2: 86.61 and loss3: 0.00\n",
      "Epoch [3046], train_loss: 578.77 with loss1: 491.72, loss2: 87.05 and loss3: 0.00\n",
      "Epoch [3047], train_loss: 583.80 with loss1: 496.97, loss2: 86.83 and loss3: 0.00\n",
      "Epoch [3048], train_loss: 589.27 with loss1: 502.18, loss2: 87.09 and loss3: 0.00\n",
      "Epoch [3049], train_loss: 589.38 with loss1: 502.52, loss2: 86.86 and loss3: 0.00\n",
      "Epoch [3050], train_loss: 597.66 with loss1: 510.53, loss2: 87.12 and loss3: 0.00\n",
      "Epoch [3051], train_loss: 599.57 with loss1: 512.93, loss2: 86.64 and loss3: 0.00\n",
      "Epoch [3052], train_loss: 604.08 with loss1: 516.84, loss2: 87.24 and loss3: 0.00\n",
      "Epoch [3053], train_loss: 606.31 with loss1: 519.50, loss2: 86.81 and loss3: 0.00\n",
      "Epoch [3054], train_loss: 608.51 with loss1: 521.19, loss2: 87.32 and loss3: 0.00\n",
      "Epoch [3055], train_loss: 606.81 with loss1: 519.93, loss2: 86.88 and loss3: 0.00\n",
      "Epoch [3056], train_loss: 606.06 with loss1: 518.68, loss2: 87.37 and loss3: 0.00\n",
      "Epoch [3057], train_loss: 602.37 with loss1: 515.34, loss2: 87.04 and loss3: 0.00\n",
      "Epoch [3058], train_loss: 599.46 with loss1: 512.01, loss2: 87.46 and loss3: 0.00\n",
      "Epoch [3059], train_loss: 591.80 with loss1: 504.98, loss2: 86.82 and loss3: 0.00\n",
      "Epoch [3060], train_loss: 588.34 with loss1: 500.83, loss2: 87.52 and loss3: 0.00\n",
      "Epoch [3061], train_loss: 584.40 with loss1: 497.27, loss2: 87.13 and loss3: 0.00\n",
      "Epoch [3062], train_loss: 576.83 with loss1: 489.29, loss2: 87.53 and loss3: 0.00\n",
      "Epoch [3063], train_loss: 570.99 with loss1: 483.78, loss2: 87.21 and loss3: 0.00\n",
      "Epoch [3064], train_loss: 566.04 with loss1: 478.52, loss2: 87.52 and loss3: 0.00\n",
      "Epoch [3065], train_loss: 560.95 with loss1: 473.66, loss2: 87.29 and loss3: 0.00\n",
      "Epoch [3066], train_loss: 555.40 with loss1: 467.75, loss2: 87.65 and loss3: 0.00\n",
      "Epoch [3067], train_loss: 551.24 with loss1: 464.04, loss2: 87.20 and loss3: 0.00\n",
      "Epoch [3068], train_loss: 547.73 with loss1: 460.23, loss2: 87.50 and loss3: 0.00\n",
      "Epoch [3069], train_loss: 543.09 with loss1: 456.13, loss2: 86.96 and loss3: 0.00\n",
      "Epoch [3070], train_loss: 540.11 with loss1: 452.58, loss2: 87.52 and loss3: 0.00\n",
      "Epoch [3071], train_loss: 536.71 with loss1: 449.53, loss2: 87.18 and loss3: 0.00\n",
      "Epoch [3072], train_loss: 535.45 with loss1: 447.98, loss2: 87.47 and loss3: 0.00\n",
      "Epoch [3073], train_loss: 534.07 with loss1: 447.02, loss2: 87.05 and loss3: 0.00\n",
      "Epoch [3074], train_loss: 532.37 with loss1: 444.92, loss2: 87.46 and loss3: 0.00\n",
      "Epoch [3075], train_loss: 531.04 with loss1: 443.94, loss2: 87.10 and loss3: 0.00\n",
      "Epoch [3076], train_loss: 527.56 with loss1: 440.07, loss2: 87.49 and loss3: 0.00\n",
      "Epoch [3077], train_loss: 525.08 with loss1: 437.96, loss2: 87.12 and loss3: 0.00\n",
      "Epoch [3078], train_loss: 523.18 with loss1: 435.97, loss2: 87.21 and loss3: 0.00\n",
      "Epoch [3079], train_loss: 520.98 with loss1: 434.04, loss2: 86.94 and loss3: 0.00\n",
      "Epoch [3080], train_loss: 519.91 with loss1: 432.74, loss2: 87.17 and loss3: 0.00\n",
      "Epoch [3081], train_loss: 518.44 with loss1: 431.50, loss2: 86.95 and loss3: 0.00\n",
      "Epoch [3082], train_loss: 517.80 with loss1: 430.76, loss2: 87.04 and loss3: 0.00\n",
      "Epoch [3083], train_loss: 516.81 with loss1: 430.09, loss2: 86.73 and loss3: 0.00\n",
      "Epoch [3084], train_loss: 516.13 with loss1: 429.26, loss2: 86.87 and loss3: 0.00\n",
      "Epoch [3085], train_loss: 515.64 with loss1: 428.88, loss2: 86.75 and loss3: 0.00\n",
      "Epoch [3086], train_loss: 514.72 with loss1: 427.97, loss2: 86.76 and loss3: 0.00\n",
      "Epoch [3087], train_loss: 513.30 with loss1: 426.73, loss2: 86.57 and loss3: 0.00\n",
      "Epoch [3088], train_loss: 514.05 with loss1: 427.28, loss2: 86.77 and loss3: 0.00\n",
      "Epoch [3089], train_loss: 512.52 with loss1: 425.88, loss2: 86.64 and loss3: 0.00\n",
      "Epoch [3090], train_loss: 511.38 with loss1: 424.71, loss2: 86.67 and loss3: 0.00\n",
      "Epoch [3091], train_loss: 512.79 with loss1: 426.16, loss2: 86.63 and loss3: 0.00\n",
      "Epoch [3092], train_loss: 513.03 with loss1: 426.37, loss2: 86.66 and loss3: 0.00\n",
      "Epoch [3093], train_loss: 512.55 with loss1: 426.13, loss2: 86.43 and loss3: 0.00\n",
      "Epoch [3094], train_loss: 512.05 with loss1: 425.63, loss2: 86.42 and loss3: 0.00\n",
      "Epoch [3095], train_loss: 512.72 with loss1: 426.52, loss2: 86.20 and loss3: 0.00\n",
      "Epoch [3096], train_loss: 513.54 with loss1: 427.08, loss2: 86.45 and loss3: 0.00\n",
      "Epoch [3097], train_loss: 513.55 with loss1: 427.40, loss2: 86.15 and loss3: 0.00\n",
      "Epoch [3098], train_loss: 512.95 with loss1: 426.65, loss2: 86.31 and loss3: 0.00\n",
      "Epoch [3099], train_loss: 513.32 with loss1: 427.39, loss2: 85.93 and loss3: 0.00\n",
      "Epoch [3100], train_loss: 513.53 with loss1: 427.26, loss2: 86.27 and loss3: 0.00\n",
      "Epoch [3101], train_loss: 515.06 with loss1: 429.07, loss2: 85.99 and loss3: 0.00\n",
      "Epoch [3102], train_loss: 516.89 with loss1: 430.70, loss2: 86.19 and loss3: 0.00\n",
      "Epoch [3103], train_loss: 516.37 with loss1: 430.31, loss2: 86.05 and loss3: 0.00\n",
      "Epoch [3104], train_loss: 519.42 with loss1: 433.31, loss2: 86.11 and loss3: 0.00\n",
      "Epoch [3105], train_loss: 518.67 with loss1: 432.75, loss2: 85.92 and loss3: 0.00\n",
      "Epoch [3106], train_loss: 520.02 with loss1: 433.85, loss2: 86.16 and loss3: 0.00\n",
      "Epoch [3107], train_loss: 521.29 with loss1: 435.46, loss2: 85.83 and loss3: 0.00\n",
      "Epoch [3108], train_loss: 522.13 with loss1: 436.09, loss2: 86.04 and loss3: 0.00\n",
      "Epoch [3109], train_loss: 525.07 with loss1: 439.27, loss2: 85.80 and loss3: 0.00\n",
      "Epoch [3110], train_loss: 527.14 with loss1: 441.01, loss2: 86.13 and loss3: 0.00\n",
      "Epoch [3111], train_loss: 527.53 with loss1: 441.66, loss2: 85.87 and loss3: 0.00\n",
      "Epoch [3112], train_loss: 529.63 with loss1: 443.69, loss2: 85.95 and loss3: 0.00\n",
      "Epoch [3113], train_loss: 529.72 with loss1: 443.81, loss2: 85.91 and loss3: 0.00\n",
      "Epoch [3114], train_loss: 531.96 with loss1: 445.92, loss2: 86.04 and loss3: 0.00\n",
      "Epoch [3115], train_loss: 531.56 with loss1: 445.63, loss2: 85.93 and loss3: 0.00\n",
      "Epoch [3116], train_loss: 532.03 with loss1: 445.84, loss2: 86.19 and loss3: 0.00\n",
      "Epoch [3117], train_loss: 531.94 with loss1: 446.15, loss2: 85.79 and loss3: 0.00\n",
      "Epoch [3118], train_loss: 535.23 with loss1: 448.93, loss2: 86.30 and loss3: 0.00\n",
      "Epoch [3119], train_loss: 535.65 with loss1: 449.82, loss2: 85.83 and loss3: 0.00\n",
      "Epoch [3120], train_loss: 537.52 with loss1: 451.28, loss2: 86.24 and loss3: 0.00\n",
      "Epoch [3121], train_loss: 535.93 with loss1: 450.06, loss2: 85.87 and loss3: 0.00\n",
      "Epoch [3122], train_loss: 538.56 with loss1: 452.40, loss2: 86.16 and loss3: 0.00\n",
      "Epoch [3123], train_loss: 536.65 with loss1: 450.91, loss2: 85.74 and loss3: 0.00\n",
      "Epoch [3124], train_loss: 538.99 with loss1: 452.80, loss2: 86.20 and loss3: 0.00\n",
      "Epoch [3125], train_loss: 536.21 with loss1: 450.38, loss2: 85.83 and loss3: 0.00\n",
      "Epoch [3126], train_loss: 537.00 with loss1: 450.82, loss2: 86.18 and loss3: 0.00\n",
      "Epoch [3127], train_loss: 535.49 with loss1: 449.76, loss2: 85.73 and loss3: 0.00\n",
      "Epoch [3128], train_loss: 536.91 with loss1: 450.76, loss2: 86.15 and loss3: 0.00\n",
      "Epoch [3129], train_loss: 535.27 with loss1: 449.47, loss2: 85.79 and loss3: 0.00\n",
      "Epoch [3130], train_loss: 536.59 with loss1: 450.21, loss2: 86.38 and loss3: 0.00\n",
      "Epoch [3131], train_loss: 533.19 with loss1: 447.39, loss2: 85.81 and loss3: 0.00\n",
      "Epoch [3132], train_loss: 533.66 with loss1: 447.53, loss2: 86.13 and loss3: 0.00\n",
      "Epoch [3133], train_loss: 531.31 with loss1: 445.50, loss2: 85.82 and loss3: 0.00\n",
      "Epoch [3134], train_loss: 531.70 with loss1: 445.67, loss2: 86.03 and loss3: 0.00\n",
      "Epoch [3135], train_loss: 527.23 with loss1: 441.70, loss2: 85.52 and loss3: 0.00\n",
      "Epoch [3136], train_loss: 528.58 with loss1: 442.50, loss2: 86.08 and loss3: 0.00\n",
      "Epoch [3137], train_loss: 527.29 with loss1: 441.84, loss2: 85.46 and loss3: 0.00\n",
      "Epoch [3138], train_loss: 527.85 with loss1: 441.96, loss2: 85.89 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3139], train_loss: 528.02 with loss1: 442.67, loss2: 85.35 and loss3: 0.00\n",
      "Epoch [3140], train_loss: 528.45 with loss1: 442.71, loss2: 85.74 and loss3: 0.00\n",
      "Epoch [3141], train_loss: 527.43 with loss1: 442.00, loss2: 85.43 and loss3: 0.00\n",
      "Epoch [3142], train_loss: 529.30 with loss1: 443.53, loss2: 85.77 and loss3: 0.00\n",
      "Epoch [3143], train_loss: 530.10 with loss1: 444.71, loss2: 85.39 and loss3: 0.00\n",
      "Epoch [3144], train_loss: 531.07 with loss1: 445.45, loss2: 85.62 and loss3: 0.00\n",
      "Epoch [3145], train_loss: 530.38 with loss1: 445.34, loss2: 85.04 and loss3: 0.00\n",
      "Epoch [3146], train_loss: 529.78 with loss1: 444.48, loss2: 85.31 and loss3: 0.00\n",
      "Epoch [3147], train_loss: 531.90 with loss1: 446.68, loss2: 85.22 and loss3: 0.00\n",
      "Epoch [3148], train_loss: 534.19 with loss1: 448.78, loss2: 85.41 and loss3: 0.00\n",
      "Epoch [3149], train_loss: 533.12 with loss1: 448.15, loss2: 84.97 and loss3: 0.00\n",
      "Epoch [3150], train_loss: 534.95 with loss1: 449.65, loss2: 85.30 and loss3: 0.00\n",
      "Epoch [3151], train_loss: 532.50 with loss1: 447.78, loss2: 84.72 and loss3: 0.00\n",
      "Epoch [3152], train_loss: 536.13 with loss1: 450.82, loss2: 85.31 and loss3: 0.00\n",
      "Epoch [3153], train_loss: 534.79 with loss1: 450.02, loss2: 84.77 and loss3: 0.00\n",
      "Epoch [3154], train_loss: 537.57 with loss1: 452.53, loss2: 85.04 and loss3: 0.00\n",
      "Epoch [3155], train_loss: 538.17 with loss1: 453.49, loss2: 84.68 and loss3: 0.00\n",
      "Epoch [3156], train_loss: 539.14 with loss1: 453.99, loss2: 85.15 and loss3: 0.00\n",
      "Epoch [3157], train_loss: 538.88 with loss1: 454.26, loss2: 84.62 and loss3: 0.00\n",
      "Epoch [3158], train_loss: 541.17 with loss1: 456.23, loss2: 84.94 and loss3: 0.00\n",
      "Epoch [3159], train_loss: 540.69 with loss1: 456.08, loss2: 84.61 and loss3: 0.00\n",
      "Epoch [3160], train_loss: 543.93 with loss1: 458.97, loss2: 84.96 and loss3: 0.00\n",
      "Epoch [3161], train_loss: 545.19 with loss1: 460.55, loss2: 84.64 and loss3: 0.00\n",
      "Epoch [3162], train_loss: 548.07 with loss1: 463.20, loss2: 84.87 and loss3: 0.00\n",
      "Epoch [3163], train_loss: 545.56 with loss1: 461.01, loss2: 84.55 and loss3: 0.00\n",
      "Epoch [3164], train_loss: 549.31 with loss1: 464.57, loss2: 84.75 and loss3: 0.00\n",
      "Epoch [3165], train_loss: 549.35 with loss1: 464.84, loss2: 84.51 and loss3: 0.00\n",
      "Epoch [3166], train_loss: 552.46 with loss1: 467.75, loss2: 84.71 and loss3: 0.00\n",
      "Epoch [3167], train_loss: 553.60 with loss1: 469.03, loss2: 84.56 and loss3: 0.00\n",
      "Epoch [3168], train_loss: 556.64 with loss1: 471.82, loss2: 84.82 and loss3: 0.00\n",
      "Epoch [3169], train_loss: 556.08 with loss1: 471.50, loss2: 84.58 and loss3: 0.00\n",
      "Epoch [3170], train_loss: 558.22 with loss1: 473.42, loss2: 84.80 and loss3: 0.00\n",
      "Epoch [3171], train_loss: 557.97 with loss1: 473.29, loss2: 84.68 and loss3: 0.00\n",
      "Epoch [3172], train_loss: 560.26 with loss1: 475.37, loss2: 84.88 and loss3: 0.00\n",
      "Epoch [3173], train_loss: 558.13 with loss1: 473.35, loss2: 84.78 and loss3: 0.00\n",
      "Epoch [3174], train_loss: 559.80 with loss1: 474.70, loss2: 85.10 and loss3: 0.00\n",
      "Epoch [3175], train_loss: 555.93 with loss1: 471.26, loss2: 84.67 and loss3: 0.00\n",
      "Epoch [3176], train_loss: 557.03 with loss1: 472.08, loss2: 84.95 and loss3: 0.00\n",
      "Epoch [3177], train_loss: 554.67 with loss1: 469.95, loss2: 84.72 and loss3: 0.00\n",
      "Epoch [3178], train_loss: 554.87 with loss1: 470.12, loss2: 84.75 and loss3: 0.00\n",
      "Epoch [3179], train_loss: 552.22 with loss1: 467.60, loss2: 84.62 and loss3: 0.00\n",
      "Epoch [3180], train_loss: 552.52 with loss1: 467.81, loss2: 84.71 and loss3: 0.00\n",
      "Epoch [3181], train_loss: 549.62 with loss1: 464.93, loss2: 84.69 and loss3: 0.00\n",
      "Epoch [3182], train_loss: 549.54 with loss1: 464.85, loss2: 84.70 and loss3: 0.00\n",
      "Epoch [3183], train_loss: 546.46 with loss1: 461.97, loss2: 84.49 and loss3: 0.00\n",
      "Epoch [3184], train_loss: 548.92 with loss1: 464.04, loss2: 84.87 and loss3: 0.00\n",
      "Epoch [3185], train_loss: 544.22 with loss1: 459.78, loss2: 84.44 and loss3: 0.00\n",
      "Epoch [3186], train_loss: 545.18 with loss1: 460.47, loss2: 84.71 and loss3: 0.00\n",
      "Epoch [3187], train_loss: 542.45 with loss1: 458.10, loss2: 84.35 and loss3: 0.00\n",
      "Epoch [3188], train_loss: 542.09 with loss1: 457.65, loss2: 84.44 and loss3: 0.00\n",
      "Epoch [3189], train_loss: 539.14 with loss1: 454.81, loss2: 84.33 and loss3: 0.00\n",
      "Epoch [3190], train_loss: 541.85 with loss1: 457.26, loss2: 84.59 and loss3: 0.00\n",
      "Epoch [3191], train_loss: 537.19 with loss1: 452.87, loss2: 84.32 and loss3: 0.00\n",
      "Epoch [3192], train_loss: 535.55 with loss1: 450.92, loss2: 84.63 and loss3: 0.00\n",
      "Epoch [3193], train_loss: 534.54 with loss1: 450.39, loss2: 84.15 and loss3: 0.00\n",
      "Epoch [3194], train_loss: 534.96 with loss1: 450.55, loss2: 84.42 and loss3: 0.00\n",
      "Epoch [3195], train_loss: 531.89 with loss1: 447.68, loss2: 84.22 and loss3: 0.00\n",
      "Epoch [3196], train_loss: 532.35 with loss1: 447.79, loss2: 84.56 and loss3: 0.00\n",
      "Epoch [3197], train_loss: 530.31 with loss1: 446.26, loss2: 84.05 and loss3: 0.00\n",
      "Epoch [3198], train_loss: 529.88 with loss1: 445.44, loss2: 84.44 and loss3: 0.00\n",
      "Epoch [3199], train_loss: 529.65 with loss1: 445.61, loss2: 84.04 and loss3: 0.00\n",
      "Epoch [3200], train_loss: 530.22 with loss1: 446.06, loss2: 84.15 and loss3: 0.00\n",
      "Epoch [3201], train_loss: 527.50 with loss1: 443.41, loss2: 84.09 and loss3: 0.00\n",
      "Epoch [3202], train_loss: 531.04 with loss1: 446.78, loss2: 84.26 and loss3: 0.00\n",
      "Epoch [3203], train_loss: 528.23 with loss1: 444.07, loss2: 84.16 and loss3: 0.00\n",
      "Epoch [3204], train_loss: 528.68 with loss1: 444.42, loss2: 84.26 and loss3: 0.00\n",
      "Epoch [3205], train_loss: 526.60 with loss1: 442.51, loss2: 84.09 and loss3: 0.00\n",
      "Epoch [3206], train_loss: 526.82 with loss1: 442.83, loss2: 83.99 and loss3: 0.00\n",
      "Epoch [3207], train_loss: 524.84 with loss1: 440.86, loss2: 83.98 and loss3: 0.00\n",
      "Epoch [3208], train_loss: 524.47 with loss1: 440.41, loss2: 84.05 and loss3: 0.00\n",
      "Epoch [3209], train_loss: 523.32 with loss1: 439.51, loss2: 83.81 and loss3: 0.00\n",
      "Epoch [3210], train_loss: 521.93 with loss1: 437.99, loss2: 83.94 and loss3: 0.00\n",
      "Epoch [3211], train_loss: 521.69 with loss1: 437.88, loss2: 83.81 and loss3: 0.00\n",
      "Epoch [3212], train_loss: 521.57 with loss1: 437.45, loss2: 84.12 and loss3: 0.00\n",
      "Epoch [3213], train_loss: 520.72 with loss1: 437.09, loss2: 83.64 and loss3: 0.00\n",
      "Epoch [3214], train_loss: 521.72 with loss1: 437.96, loss2: 83.77 and loss3: 0.00\n",
      "Epoch [3215], train_loss: 522.68 with loss1: 438.97, loss2: 83.71 and loss3: 0.00\n",
      "Epoch [3216], train_loss: 522.85 with loss1: 438.88, loss2: 83.97 and loss3: 0.00\n",
      "Epoch [3217], train_loss: 521.49 with loss1: 437.81, loss2: 83.69 and loss3: 0.00\n",
      "Epoch [3218], train_loss: 522.62 with loss1: 438.88, loss2: 83.74 and loss3: 0.00\n",
      "Epoch [3219], train_loss: 523.06 with loss1: 439.43, loss2: 83.63 and loss3: 0.00\n",
      "Epoch [3220], train_loss: 522.40 with loss1: 438.60, loss2: 83.80 and loss3: 0.00\n",
      "Epoch [3221], train_loss: 523.46 with loss1: 439.86, loss2: 83.59 and loss3: 0.00\n",
      "Epoch [3222], train_loss: 525.37 with loss1: 441.66, loss2: 83.71 and loss3: 0.00\n",
      "Epoch [3223], train_loss: 528.59 with loss1: 445.11, loss2: 83.49 and loss3: 0.00\n",
      "Epoch [3224], train_loss: 531.09 with loss1: 447.37, loss2: 83.72 and loss3: 0.00\n",
      "Epoch [3225], train_loss: 531.84 with loss1: 448.31, loss2: 83.53 and loss3: 0.00\n",
      "Epoch [3226], train_loss: 534.81 with loss1: 450.82, loss2: 83.99 and loss3: 0.00\n",
      "Epoch [3227], train_loss: 534.01 with loss1: 450.34, loss2: 83.67 and loss3: 0.00\n",
      "Epoch [3228], train_loss: 536.43 with loss1: 452.59, loss2: 83.85 and loss3: 0.00\n",
      "Epoch [3229], train_loss: 537.96 with loss1: 454.20, loss2: 83.76 and loss3: 0.00\n",
      "Epoch [3230], train_loss: 539.44 with loss1: 455.41, loss2: 84.03 and loss3: 0.00\n",
      "Epoch [3231], train_loss: 542.92 with loss1: 459.20, loss2: 83.72 and loss3: 0.00\n",
      "Epoch [3232], train_loss: 543.96 with loss1: 459.73, loss2: 84.23 and loss3: 0.00\n",
      "Epoch [3233], train_loss: 547.42 with loss1: 463.68, loss2: 83.74 and loss3: 0.00\n",
      "Epoch [3234], train_loss: 547.16 with loss1: 462.78, loss2: 84.38 and loss3: 0.00\n",
      "Epoch [3235], train_loss: 548.65 with loss1: 464.62, loss2: 84.03 and loss3: 0.00\n",
      "Epoch [3236], train_loss: 550.33 with loss1: 465.81, loss2: 84.52 and loss3: 0.00\n",
      "Epoch [3237], train_loss: 548.15 with loss1: 463.90, loss2: 84.25 and loss3: 0.00\n",
      "Epoch [3238], train_loss: 548.02 with loss1: 463.66, loss2: 84.36 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3239], train_loss: 546.32 with loss1: 462.07, loss2: 84.25 and loss3: 0.00\n",
      "Epoch [3240], train_loss: 543.79 with loss1: 459.24, loss2: 84.55 and loss3: 0.00\n",
      "Epoch [3241], train_loss: 545.62 with loss1: 461.21, loss2: 84.41 and loss3: 0.00\n",
      "Epoch [3242], train_loss: 543.64 with loss1: 459.07, loss2: 84.56 and loss3: 0.00\n",
      "Epoch [3243], train_loss: 543.13 with loss1: 458.92, loss2: 84.22 and loss3: 0.00\n",
      "Epoch [3244], train_loss: 539.65 with loss1: 455.12, loss2: 84.53 and loss3: 0.00\n",
      "Epoch [3245], train_loss: 538.90 with loss1: 454.43, loss2: 84.47 and loss3: 0.00\n",
      "Epoch [3246], train_loss: 536.18 with loss1: 451.48, loss2: 84.71 and loss3: 0.00\n",
      "Epoch [3247], train_loss: 535.23 with loss1: 450.79, loss2: 84.45 and loss3: 0.00\n",
      "Epoch [3248], train_loss: 531.74 with loss1: 447.15, loss2: 84.59 and loss3: 0.00\n",
      "Epoch [3249], train_loss: 530.83 with loss1: 446.50, loss2: 84.33 and loss3: 0.00\n",
      "Epoch [3250], train_loss: 526.05 with loss1: 441.39, loss2: 84.66 and loss3: 0.00\n",
      "Epoch [3251], train_loss: 523.19 with loss1: 438.86, loss2: 84.33 and loss3: 0.00\n",
      "Epoch [3252], train_loss: 519.42 with loss1: 434.95, loss2: 84.47 and loss3: 0.00\n",
      "Epoch [3253], train_loss: 516.96 with loss1: 432.65, loss2: 84.31 and loss3: 0.00\n",
      "Epoch [3254], train_loss: 513.67 with loss1: 429.26, loss2: 84.41 and loss3: 0.00\n",
      "Epoch [3255], train_loss: 512.68 with loss1: 428.31, loss2: 84.37 and loss3: 0.00\n",
      "Epoch [3256], train_loss: 511.16 with loss1: 426.75, loss2: 84.41 and loss3: 0.00\n",
      "Epoch [3257], train_loss: 510.18 with loss1: 426.01, loss2: 84.16 and loss3: 0.00\n",
      "Epoch [3258], train_loss: 507.71 with loss1: 423.52, loss2: 84.19 and loss3: 0.00\n",
      "Epoch [3259], train_loss: 508.20 with loss1: 424.29, loss2: 83.91 and loss3: 0.00\n",
      "Epoch [3260], train_loss: 504.10 with loss1: 419.79, loss2: 84.30 and loss3: 0.00\n",
      "Epoch [3261], train_loss: 505.28 with loss1: 421.50, loss2: 83.78 and loss3: 0.00\n",
      "Epoch [3262], train_loss: 502.72 with loss1: 418.75, loss2: 83.97 and loss3: 0.00\n",
      "Epoch [3263], train_loss: 502.41 with loss1: 418.78, loss2: 83.64 and loss3: 0.00\n",
      "Epoch [3264], train_loss: 502.87 with loss1: 418.95, loss2: 83.92 and loss3: 0.00\n",
      "Epoch [3265], train_loss: 502.25 with loss1: 418.59, loss2: 83.66 and loss3: 0.00\n",
      "Epoch [3266], train_loss: 501.76 with loss1: 417.82, loss2: 83.94 and loss3: 0.00\n",
      "Epoch [3267], train_loss: 502.72 with loss1: 419.29, loss2: 83.42 and loss3: 0.00\n",
      "Epoch [3268], train_loss: 502.06 with loss1: 418.24, loss2: 83.82 and loss3: 0.00\n",
      "Epoch [3269], train_loss: 502.97 with loss1: 419.53, loss2: 83.43 and loss3: 0.00\n",
      "Epoch [3270], train_loss: 503.31 with loss1: 419.59, loss2: 83.72 and loss3: 0.00\n",
      "Epoch [3271], train_loss: 502.88 with loss1: 419.34, loss2: 83.54 and loss3: 0.00\n",
      "Epoch [3272], train_loss: 501.55 with loss1: 418.03, loss2: 83.52 and loss3: 0.00\n",
      "Epoch [3273], train_loss: 502.70 with loss1: 419.28, loss2: 83.42 and loss3: 0.00\n",
      "Epoch [3274], train_loss: 503.10 with loss1: 419.55, loss2: 83.54 and loss3: 0.00\n",
      "Epoch [3275], train_loss: 503.31 with loss1: 419.89, loss2: 83.42 and loss3: 0.00\n",
      "Epoch [3276], train_loss: 505.96 with loss1: 422.45, loss2: 83.52 and loss3: 0.00\n",
      "Epoch [3277], train_loss: 502.97 with loss1: 419.75, loss2: 83.22 and loss3: 0.00\n",
      "Epoch [3278], train_loss: 502.48 with loss1: 419.09, loss2: 83.39 and loss3: 0.00\n",
      "Epoch [3279], train_loss: 504.97 with loss1: 421.76, loss2: 83.21 and loss3: 0.00\n",
      "Epoch [3280], train_loss: 504.98 with loss1: 421.62, loss2: 83.35 and loss3: 0.00\n",
      "Epoch [3281], train_loss: 506.33 with loss1: 423.28, loss2: 83.05 and loss3: 0.00\n",
      "Epoch [3282], train_loss: 506.00 with loss1: 422.85, loss2: 83.16 and loss3: 0.00\n",
      "Epoch [3283], train_loss: 505.89 with loss1: 422.82, loss2: 83.07 and loss3: 0.00\n",
      "Epoch [3284], train_loss: 507.50 with loss1: 424.38, loss2: 83.12 and loss3: 0.00\n",
      "Epoch [3285], train_loss: 507.39 with loss1: 424.49, loss2: 82.90 and loss3: 0.00\n",
      "Epoch [3286], train_loss: 508.35 with loss1: 425.27, loss2: 83.08 and loss3: 0.00\n",
      "Epoch [3287], train_loss: 507.32 with loss1: 424.41, loss2: 82.91 and loss3: 0.00\n",
      "Epoch [3288], train_loss: 507.15 with loss1: 424.24, loss2: 82.91 and loss3: 0.00\n",
      "Epoch [3289], train_loss: 508.93 with loss1: 426.18, loss2: 82.75 and loss3: 0.00\n",
      "Epoch [3290], train_loss: 508.09 with loss1: 425.16, loss2: 82.94 and loss3: 0.00\n",
      "Epoch [3291], train_loss: 509.41 with loss1: 426.61, loss2: 82.80 and loss3: 0.00\n",
      "Epoch [3292], train_loss: 511.00 with loss1: 428.26, loss2: 82.73 and loss3: 0.00\n",
      "Epoch [3293], train_loss: 510.39 with loss1: 427.77, loss2: 82.62 and loss3: 0.00\n",
      "Epoch [3294], train_loss: 510.23 with loss1: 427.73, loss2: 82.50 and loss3: 0.00\n",
      "Epoch [3295], train_loss: 511.92 with loss1: 429.22, loss2: 82.70 and loss3: 0.00\n",
      "Epoch [3296], train_loss: 511.45 with loss1: 428.87, loss2: 82.58 and loss3: 0.00\n",
      "Epoch [3297], train_loss: 510.94 with loss1: 428.45, loss2: 82.49 and loss3: 0.00\n",
      "Epoch [3298], train_loss: 509.90 with loss1: 427.56, loss2: 82.34 and loss3: 0.00\n",
      "Epoch [3299], train_loss: 508.67 with loss1: 426.18, loss2: 82.49 and loss3: 0.00\n",
      "Epoch [3300], train_loss: 507.79 with loss1: 425.33, loss2: 82.46 and loss3: 0.00\n",
      "Epoch [3301], train_loss: 506.87 with loss1: 424.45, loss2: 82.42 and loss3: 0.00\n",
      "Epoch [3302], train_loss: 504.86 with loss1: 422.59, loss2: 82.27 and loss3: 0.00\n",
      "Epoch [3303], train_loss: 503.58 with loss1: 421.26, loss2: 82.32 and loss3: 0.00\n",
      "Epoch [3304], train_loss: 502.24 with loss1: 420.24, loss2: 82.01 and loss3: 0.00\n",
      "Epoch [3305], train_loss: 501.53 with loss1: 419.25, loss2: 82.29 and loss3: 0.00\n",
      "Epoch [3306], train_loss: 501.48 with loss1: 419.39, loss2: 82.09 and loss3: 0.00\n",
      "Epoch [3307], train_loss: 499.82 with loss1: 417.78, loss2: 82.04 and loss3: 0.00\n",
      "Epoch [3308], train_loss: 500.86 with loss1: 418.80, loss2: 82.06 and loss3: 0.00\n",
      "Epoch [3309], train_loss: 499.02 with loss1: 417.06, loss2: 81.95 and loss3: 0.00\n",
      "Epoch [3310], train_loss: 498.69 with loss1: 416.57, loss2: 82.12 and loss3: 0.00\n",
      "Epoch [3311], train_loss: 499.29 with loss1: 417.46, loss2: 81.83 and loss3: 0.00\n",
      "Epoch [3312], train_loss: 496.05 with loss1: 414.12, loss2: 81.93 and loss3: 0.00\n",
      "Epoch [3313], train_loss: 496.01 with loss1: 414.19, loss2: 81.82 and loss3: 0.00\n",
      "Epoch [3314], train_loss: 496.03 with loss1: 414.25, loss2: 81.78 and loss3: 0.00\n",
      "Epoch [3315], train_loss: 496.05 with loss1: 414.40, loss2: 81.66 and loss3: 0.00\n",
      "Epoch [3316], train_loss: 495.20 with loss1: 413.35, loss2: 81.84 and loss3: 0.00\n",
      "Epoch [3317], train_loss: 495.82 with loss1: 414.17, loss2: 81.65 and loss3: 0.00\n",
      "Epoch [3318], train_loss: 493.63 with loss1: 411.98, loss2: 81.66 and loss3: 0.00\n",
      "Epoch [3319], train_loss: 495.86 with loss1: 414.20, loss2: 81.66 and loss3: 0.00\n",
      "Epoch [3320], train_loss: 496.62 with loss1: 415.12, loss2: 81.50 and loss3: 0.00\n",
      "Epoch [3321], train_loss: 494.56 with loss1: 413.00, loss2: 81.55 and loss3: 0.00\n",
      "Epoch [3322], train_loss: 496.25 with loss1: 414.84, loss2: 81.41 and loss3: 0.00\n",
      "Epoch [3323], train_loss: 496.24 with loss1: 414.75, loss2: 81.49 and loss3: 0.00\n",
      "Epoch [3324], train_loss: 497.54 with loss1: 416.22, loss2: 81.32 and loss3: 0.00\n",
      "Epoch [3325], train_loss: 499.53 with loss1: 418.05, loss2: 81.48 and loss3: 0.00\n",
      "Epoch [3326], train_loss: 499.69 with loss1: 418.30, loss2: 81.39 and loss3: 0.00\n",
      "Epoch [3327], train_loss: 503.17 with loss1: 421.94, loss2: 81.24 and loss3: 0.00\n",
      "Epoch [3328], train_loss: 505.26 with loss1: 423.92, loss2: 81.35 and loss3: 0.00\n",
      "Epoch [3329], train_loss: 508.38 with loss1: 427.16, loss2: 81.22 and loss3: 0.00\n",
      "Epoch [3330], train_loss: 510.59 with loss1: 429.12, loss2: 81.48 and loss3: 0.00\n",
      "Epoch [3331], train_loss: 513.15 with loss1: 431.81, loss2: 81.35 and loss3: 0.00\n",
      "Epoch [3332], train_loss: 517.15 with loss1: 435.62, loss2: 81.53 and loss3: 0.00\n",
      "Epoch [3333], train_loss: 517.85 with loss1: 436.67, loss2: 81.19 and loss3: 0.00\n",
      "Epoch [3334], train_loss: 522.38 with loss1: 440.88, loss2: 81.50 and loss3: 0.00\n",
      "Epoch [3335], train_loss: 523.96 with loss1: 442.89, loss2: 81.07 and loss3: 0.00\n",
      "Epoch [3336], train_loss: 528.74 with loss1: 447.08, loss2: 81.66 and loss3: 0.00\n",
      "Epoch [3337], train_loss: 530.60 with loss1: 449.55, loss2: 81.05 and loss3: 0.00\n",
      "Epoch [3338], train_loss: 536.23 with loss1: 454.73, loss2: 81.50 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3339], train_loss: 537.51 with loss1: 456.46, loss2: 81.05 and loss3: 0.00\n",
      "Epoch [3340], train_loss: 543.96 with loss1: 462.44, loss2: 81.51 and loss3: 0.00\n",
      "Epoch [3341], train_loss: 544.94 with loss1: 464.01, loss2: 80.92 and loss3: 0.00\n",
      "Epoch [3342], train_loss: 552.72 with loss1: 471.11, loss2: 81.61 and loss3: 0.00\n",
      "Epoch [3343], train_loss: 553.83 with loss1: 472.73, loss2: 81.10 and loss3: 0.00\n",
      "Epoch [3344], train_loss: 560.73 with loss1: 479.05, loss2: 81.69 and loss3: 0.00\n",
      "Epoch [3345], train_loss: 562.02 with loss1: 480.97, loss2: 81.05 and loss3: 0.00\n",
      "Epoch [3346], train_loss: 567.76 with loss1: 486.26, loss2: 81.50 and loss3: 0.00\n",
      "Epoch [3347], train_loss: 570.12 with loss1: 489.01, loss2: 81.11 and loss3: 0.00\n",
      "Epoch [3348], train_loss: 576.96 with loss1: 495.30, loss2: 81.66 and loss3: 0.00\n",
      "Epoch [3349], train_loss: 575.13 with loss1: 493.98, loss2: 81.14 and loss3: 0.00\n",
      "Epoch [3350], train_loss: 581.25 with loss1: 499.52, loss2: 81.73 and loss3: 0.00\n",
      "Epoch [3351], train_loss: 579.10 with loss1: 498.03, loss2: 81.07 and loss3: 0.00\n",
      "Epoch [3352], train_loss: 583.15 with loss1: 501.48, loss2: 81.67 and loss3: 0.00\n",
      "Epoch [3353], train_loss: 578.41 with loss1: 497.31, loss2: 81.10 and loss3: 0.00\n",
      "Epoch [3354], train_loss: 581.58 with loss1: 500.05, loss2: 81.52 and loss3: 0.00\n",
      "Epoch [3355], train_loss: 573.88 with loss1: 492.85, loss2: 81.02 and loss3: 0.00\n",
      "Epoch [3356], train_loss: 575.93 with loss1: 494.35, loss2: 81.57 and loss3: 0.00\n",
      "Epoch [3357], train_loss: 570.54 with loss1: 489.60, loss2: 80.95 and loss3: 0.00\n",
      "Epoch [3358], train_loss: 572.78 with loss1: 491.28, loss2: 81.50 and loss3: 0.00\n",
      "Epoch [3359], train_loss: 568.26 with loss1: 487.30, loss2: 80.96 and loss3: 0.00\n",
      "Epoch [3360], train_loss: 569.04 with loss1: 487.53, loss2: 81.51 and loss3: 0.00\n",
      "Epoch [3361], train_loss: 562.36 with loss1: 481.32, loss2: 81.04 and loss3: 0.00\n",
      "Epoch [3362], train_loss: 563.67 with loss1: 482.19, loss2: 81.49 and loss3: 0.00\n",
      "Epoch [3363], train_loss: 560.60 with loss1: 479.57, loss2: 81.03 and loss3: 0.00\n",
      "Epoch [3364], train_loss: 561.04 with loss1: 479.44, loss2: 81.59 and loss3: 0.00\n",
      "Epoch [3365], train_loss: 555.56 with loss1: 474.23, loss2: 81.33 and loss3: 0.00\n",
      "Epoch [3366], train_loss: 555.62 with loss1: 474.12, loss2: 81.50 and loss3: 0.00\n",
      "Epoch [3367], train_loss: 552.05 with loss1: 470.83, loss2: 81.22 and loss3: 0.00\n",
      "Epoch [3368], train_loss: 551.20 with loss1: 469.75, loss2: 81.45 and loss3: 0.00\n",
      "Epoch [3369], train_loss: 546.40 with loss1: 465.03, loss2: 81.37 and loss3: 0.00\n",
      "Epoch [3370], train_loss: 546.50 with loss1: 464.81, loss2: 81.70 and loss3: 0.00\n",
      "Epoch [3371], train_loss: 542.17 with loss1: 460.84, loss2: 81.34 and loss3: 0.00\n",
      "Epoch [3372], train_loss: 541.37 with loss1: 459.78, loss2: 81.59 and loss3: 0.00\n",
      "Epoch [3373], train_loss: 537.26 with loss1: 455.80, loss2: 81.46 and loss3: 0.00\n",
      "Epoch [3374], train_loss: 534.99 with loss1: 453.37, loss2: 81.63 and loss3: 0.00\n",
      "Epoch [3375], train_loss: 532.59 with loss1: 451.07, loss2: 81.52 and loss3: 0.00\n",
      "Epoch [3376], train_loss: 528.50 with loss1: 446.90, loss2: 81.60 and loss3: 0.00\n",
      "Epoch [3377], train_loss: 524.82 with loss1: 443.49, loss2: 81.33 and loss3: 0.00\n",
      "Epoch [3378], train_loss: 523.64 with loss1: 442.06, loss2: 81.58 and loss3: 0.00\n",
      "Epoch [3379], train_loss: 520.49 with loss1: 439.08, loss2: 81.41 and loss3: 0.00\n",
      "Epoch [3380], train_loss: 518.90 with loss1: 437.37, loss2: 81.53 and loss3: 0.00\n",
      "Epoch [3381], train_loss: 516.63 with loss1: 435.25, loss2: 81.39 and loss3: 0.00\n",
      "Epoch [3382], train_loss: 515.82 with loss1: 434.29, loss2: 81.53 and loss3: 0.00\n",
      "Epoch [3383], train_loss: 512.42 with loss1: 431.19, loss2: 81.23 and loss3: 0.00\n",
      "Epoch [3384], train_loss: 511.68 with loss1: 430.33, loss2: 81.34 and loss3: 0.00\n",
      "Epoch [3385], train_loss: 511.19 with loss1: 430.01, loss2: 81.18 and loss3: 0.00\n",
      "Epoch [3386], train_loss: 509.83 with loss1: 428.55, loss2: 81.29 and loss3: 0.00\n",
      "Epoch [3387], train_loss: 507.72 with loss1: 426.51, loss2: 81.22 and loss3: 0.00\n",
      "Epoch [3388], train_loss: 507.73 with loss1: 426.61, loss2: 81.12 and loss3: 0.00\n",
      "Epoch [3389], train_loss: 505.33 with loss1: 424.18, loss2: 81.15 and loss3: 0.00\n",
      "Epoch [3390], train_loss: 504.32 with loss1: 423.13, loss2: 81.18 and loss3: 0.00\n",
      "Epoch [3391], train_loss: 502.47 with loss1: 421.39, loss2: 81.08 and loss3: 0.00\n",
      "Epoch [3392], train_loss: 502.29 with loss1: 421.18, loss2: 81.11 and loss3: 0.00\n",
      "Epoch [3393], train_loss: 500.31 with loss1: 419.31, loss2: 81.00 and loss3: 0.00\n",
      "Epoch [3394], train_loss: 498.70 with loss1: 417.77, loss2: 80.93 and loss3: 0.00\n",
      "Epoch [3395], train_loss: 499.27 with loss1: 418.49, loss2: 80.78 and loss3: 0.00\n",
      "Epoch [3396], train_loss: 500.64 with loss1: 419.76, loss2: 80.88 and loss3: 0.00\n",
      "Epoch [3397], train_loss: 498.40 with loss1: 417.57, loss2: 80.83 and loss3: 0.00\n",
      "Epoch [3398], train_loss: 500.03 with loss1: 419.31, loss2: 80.72 and loss3: 0.00\n",
      "Epoch [3399], train_loss: 499.19 with loss1: 418.36, loss2: 80.82 and loss3: 0.00\n",
      "Epoch [3400], train_loss: 498.45 with loss1: 417.58, loss2: 80.86 and loss3: 0.00\n",
      "Epoch [3401], train_loss: 499.23 with loss1: 418.51, loss2: 80.71 and loss3: 0.00\n",
      "Epoch [3402], train_loss: 498.58 with loss1: 417.74, loss2: 80.85 and loss3: 0.00\n",
      "Epoch [3403], train_loss: 496.26 with loss1: 415.76, loss2: 80.50 and loss3: 0.00\n",
      "Epoch [3404], train_loss: 497.05 with loss1: 416.43, loss2: 80.62 and loss3: 0.00\n",
      "Epoch [3405], train_loss: 497.38 with loss1: 416.92, loss2: 80.46 and loss3: 0.00\n",
      "Epoch [3406], train_loss: 498.02 with loss1: 417.46, loss2: 80.56 and loss3: 0.00\n",
      "Epoch [3407], train_loss: 497.16 with loss1: 416.73, loss2: 80.43 and loss3: 0.00\n",
      "Epoch [3408], train_loss: 498.03 with loss1: 417.45, loss2: 80.57 and loss3: 0.00\n",
      "Epoch [3409], train_loss: 497.65 with loss1: 417.23, loss2: 80.41 and loss3: 0.00\n",
      "Epoch [3410], train_loss: 499.53 with loss1: 419.01, loss2: 80.52 and loss3: 0.00\n",
      "Epoch [3411], train_loss: 500.98 with loss1: 420.59, loss2: 80.39 and loss3: 0.00\n",
      "Epoch [3412], train_loss: 502.76 with loss1: 422.41, loss2: 80.35 and loss3: 0.00\n",
      "Epoch [3413], train_loss: 501.70 with loss1: 421.37, loss2: 80.33 and loss3: 0.00\n",
      "Epoch [3414], train_loss: 503.32 with loss1: 422.75, loss2: 80.57 and loss3: 0.00\n",
      "Epoch [3415], train_loss: 502.10 with loss1: 421.82, loss2: 80.27 and loss3: 0.00\n",
      "Epoch [3416], train_loss: 502.63 with loss1: 422.18, loss2: 80.44 and loss3: 0.00\n",
      "Epoch [3417], train_loss: 503.49 with loss1: 423.15, loss2: 80.34 and loss3: 0.00\n",
      "Epoch [3418], train_loss: 503.35 with loss1: 422.79, loss2: 80.56 and loss3: 0.00\n",
      "Epoch [3419], train_loss: 503.45 with loss1: 423.11, loss2: 80.34 and loss3: 0.00\n",
      "Epoch [3420], train_loss: 504.52 with loss1: 424.11, loss2: 80.41 and loss3: 0.00\n",
      "Epoch [3421], train_loss: 502.93 with loss1: 422.67, loss2: 80.26 and loss3: 0.00\n",
      "Epoch [3422], train_loss: 503.44 with loss1: 423.25, loss2: 80.19 and loss3: 0.00\n",
      "Epoch [3423], train_loss: 503.49 with loss1: 423.26, loss2: 80.23 and loss3: 0.00\n",
      "Epoch [3424], train_loss: 504.65 with loss1: 424.38, loss2: 80.27 and loss3: 0.00\n",
      "Epoch [3425], train_loss: 505.03 with loss1: 424.81, loss2: 80.23 and loss3: 0.00\n",
      "Epoch [3426], train_loss: 506.58 with loss1: 426.24, loss2: 80.34 and loss3: 0.00\n",
      "Epoch [3427], train_loss: 507.50 with loss1: 427.28, loss2: 80.23 and loss3: 0.00\n",
      "Epoch [3428], train_loss: 505.55 with loss1: 425.24, loss2: 80.31 and loss3: 0.00\n",
      "Epoch [3429], train_loss: 506.20 with loss1: 426.03, loss2: 80.17 and loss3: 0.00\n",
      "Epoch [3430], train_loss: 507.44 with loss1: 427.00, loss2: 80.45 and loss3: 0.00\n",
      "Epoch [3431], train_loss: 506.40 with loss1: 426.27, loss2: 80.13 and loss3: 0.00\n",
      "Epoch [3432], train_loss: 506.21 with loss1: 425.76, loss2: 80.45 and loss3: 0.00\n",
      "Epoch [3433], train_loss: 504.90 with loss1: 424.82, loss2: 80.08 and loss3: 0.00\n",
      "Epoch [3434], train_loss: 505.26 with loss1: 425.08, loss2: 80.19 and loss3: 0.00\n",
      "Epoch [3435], train_loss: 505.91 with loss1: 425.93, loss2: 79.98 and loss3: 0.00\n",
      "Epoch [3436], train_loss: 506.47 with loss1: 426.05, loss2: 80.43 and loss3: 0.00\n",
      "Epoch [3437], train_loss: 506.35 with loss1: 426.33, loss2: 80.02 and loss3: 0.00\n",
      "Epoch [3438], train_loss: 504.80 with loss1: 424.58, loss2: 80.22 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3439], train_loss: 505.60 with loss1: 425.53, loss2: 80.07 and loss3: 0.00\n",
      "Epoch [3440], train_loss: 504.30 with loss1: 424.07, loss2: 80.23 and loss3: 0.00\n",
      "Epoch [3441], train_loss: 503.52 with loss1: 423.44, loss2: 80.09 and loss3: 0.00\n",
      "Epoch [3442], train_loss: 503.70 with loss1: 423.62, loss2: 80.08 and loss3: 0.00\n",
      "Epoch [3443], train_loss: 503.13 with loss1: 423.15, loss2: 79.98 and loss3: 0.00\n",
      "Epoch [3444], train_loss: 504.11 with loss1: 424.00, loss2: 80.11 and loss3: 0.00\n",
      "Epoch [3445], train_loss: 504.00 with loss1: 423.99, loss2: 80.01 and loss3: 0.00\n",
      "Epoch [3446], train_loss: 503.49 with loss1: 423.35, loss2: 80.14 and loss3: 0.00\n",
      "Epoch [3447], train_loss: 502.96 with loss1: 422.89, loss2: 80.07 and loss3: 0.00\n",
      "Epoch [3448], train_loss: 503.27 with loss1: 423.14, loss2: 80.13 and loss3: 0.00\n",
      "Epoch [3449], train_loss: 504.39 with loss1: 424.40, loss2: 79.99 and loss3: 0.00\n",
      "Epoch [3450], train_loss: 505.26 with loss1: 425.20, loss2: 80.07 and loss3: 0.00\n",
      "Epoch [3451], train_loss: 503.82 with loss1: 423.82, loss2: 79.99 and loss3: 0.00\n",
      "Epoch [3452], train_loss: 504.40 with loss1: 424.28, loss2: 80.12 and loss3: 0.00\n",
      "Epoch [3453], train_loss: 505.44 with loss1: 425.43, loss2: 80.00 and loss3: 0.00\n",
      "Epoch [3454], train_loss: 504.37 with loss1: 424.27, loss2: 80.11 and loss3: 0.00\n",
      "Epoch [3455], train_loss: 504.91 with loss1: 425.02, loss2: 79.88 and loss3: 0.00\n",
      "Epoch [3456], train_loss: 504.19 with loss1: 424.21, loss2: 79.98 and loss3: 0.00\n",
      "Epoch [3457], train_loss: 503.57 with loss1: 423.80, loss2: 79.78 and loss3: 0.00\n",
      "Epoch [3458], train_loss: 504.11 with loss1: 424.32, loss2: 79.79 and loss3: 0.00\n",
      "Epoch [3459], train_loss: 503.00 with loss1: 423.13, loss2: 79.86 and loss3: 0.00\n",
      "Epoch [3460], train_loss: 503.88 with loss1: 424.10, loss2: 79.77 and loss3: 0.00\n",
      "Epoch [3461], train_loss: 505.81 with loss1: 425.99, loss2: 79.83 and loss3: 0.00\n",
      "Epoch [3462], train_loss: 507.57 with loss1: 427.72, loss2: 79.86 and loss3: 0.00\n",
      "Epoch [3463], train_loss: 506.83 with loss1: 426.99, loss2: 79.85 and loss3: 0.00\n",
      "Epoch [3464], train_loss: 508.66 with loss1: 428.67, loss2: 80.00 and loss3: 0.00\n",
      "Epoch [3465], train_loss: 509.26 with loss1: 429.39, loss2: 79.87 and loss3: 0.00\n",
      "Epoch [3466], train_loss: 510.30 with loss1: 430.45, loss2: 79.85 and loss3: 0.00\n",
      "Epoch [3467], train_loss: 511.68 with loss1: 431.62, loss2: 80.06 and loss3: 0.00\n",
      "Epoch [3468], train_loss: 511.88 with loss1: 432.00, loss2: 79.88 and loss3: 0.00\n",
      "Epoch [3469], train_loss: 513.68 with loss1: 433.67, loss2: 80.01 and loss3: 0.00\n",
      "Epoch [3470], train_loss: 514.23 with loss1: 434.34, loss2: 79.90 and loss3: 0.00\n",
      "Epoch [3471], train_loss: 514.77 with loss1: 434.67, loss2: 80.10 and loss3: 0.00\n",
      "Epoch [3472], train_loss: 516.87 with loss1: 436.86, loss2: 80.02 and loss3: 0.00\n",
      "Epoch [3473], train_loss: 519.17 with loss1: 439.21, loss2: 79.96 and loss3: 0.00\n",
      "Epoch [3474], train_loss: 521.29 with loss1: 441.33, loss2: 79.96 and loss3: 0.00\n",
      "Epoch [3475], train_loss: 523.07 with loss1: 442.95, loss2: 80.11 and loss3: 0.00\n",
      "Epoch [3476], train_loss: 523.63 with loss1: 443.53, loss2: 80.10 and loss3: 0.00\n",
      "Epoch [3477], train_loss: 525.54 with loss1: 445.25, loss2: 80.29 and loss3: 0.00\n",
      "Epoch [3478], train_loss: 525.59 with loss1: 445.53, loss2: 80.06 and loss3: 0.00\n",
      "Epoch [3479], train_loss: 525.87 with loss1: 445.64, loss2: 80.23 and loss3: 0.00\n",
      "Epoch [3480], train_loss: 526.88 with loss1: 446.74, loss2: 80.14 and loss3: 0.00\n",
      "Epoch [3481], train_loss: 527.35 with loss1: 446.97, loss2: 80.37 and loss3: 0.00\n",
      "Epoch [3482], train_loss: 529.93 with loss1: 449.79, loss2: 80.14 and loss3: 0.00\n",
      "Epoch [3483], train_loss: 532.49 with loss1: 452.12, loss2: 80.37 and loss3: 0.00\n",
      "Epoch [3484], train_loss: 533.64 with loss1: 453.41, loss2: 80.23 and loss3: 0.00\n",
      "Epoch [3485], train_loss: 533.37 with loss1: 452.97, loss2: 80.39 and loss3: 0.00\n",
      "Epoch [3486], train_loss: 534.69 with loss1: 454.57, loss2: 80.12 and loss3: 0.00\n",
      "Epoch [3487], train_loss: 534.05 with loss1: 453.64, loss2: 80.41 and loss3: 0.00\n",
      "Epoch [3488], train_loss: 533.25 with loss1: 453.24, loss2: 80.01 and loss3: 0.00\n",
      "Epoch [3489], train_loss: 533.59 with loss1: 453.27, loss2: 80.32 and loss3: 0.00\n",
      "Epoch [3490], train_loss: 531.61 with loss1: 451.49, loss2: 80.13 and loss3: 0.00\n",
      "Epoch [3491], train_loss: 531.23 with loss1: 450.91, loss2: 80.32 and loss3: 0.00\n",
      "Epoch [3492], train_loss: 528.55 with loss1: 448.45, loss2: 80.10 and loss3: 0.00\n",
      "Epoch [3493], train_loss: 529.46 with loss1: 449.15, loss2: 80.32 and loss3: 0.00\n",
      "Epoch [3494], train_loss: 528.59 with loss1: 448.61, loss2: 79.98 and loss3: 0.00\n",
      "Epoch [3495], train_loss: 529.53 with loss1: 449.07, loss2: 80.46 and loss3: 0.00\n",
      "Epoch [3496], train_loss: 527.05 with loss1: 447.06, loss2: 79.99 and loss3: 0.00\n",
      "Epoch [3497], train_loss: 526.85 with loss1: 446.45, loss2: 80.40 and loss3: 0.00\n",
      "Epoch [3498], train_loss: 525.65 with loss1: 445.81, loss2: 79.84 and loss3: 0.00\n",
      "Epoch [3499], train_loss: 525.11 with loss1: 444.97, loss2: 80.15 and loss3: 0.00\n",
      "Epoch [3500], train_loss: 520.32 with loss1: 440.38, loss2: 79.93 and loss3: 0.00\n",
      "Epoch [3501], train_loss: 521.20 with loss1: 441.09, loss2: 80.11 and loss3: 0.00\n",
      "Epoch [3502], train_loss: 520.03 with loss1: 440.27, loss2: 79.76 and loss3: 0.00\n",
      "Epoch [3503], train_loss: 519.74 with loss1: 439.53, loss2: 80.21 and loss3: 0.00\n",
      "Epoch [3504], train_loss: 516.62 with loss1: 437.10, loss2: 79.52 and loss3: 0.00\n",
      "Epoch [3505], train_loss: 517.66 with loss1: 437.56, loss2: 80.10 and loss3: 0.00\n",
      "Epoch [3506], train_loss: 513.24 with loss1: 433.82, loss2: 79.42 and loss3: 0.00\n",
      "Epoch [3507], train_loss: 513.89 with loss1: 434.09, loss2: 79.80 and loss3: 0.00\n",
      "Epoch [3508], train_loss: 512.73 with loss1: 433.36, loss2: 79.36 and loss3: 0.00\n",
      "Epoch [3509], train_loss: 512.40 with loss1: 432.84, loss2: 79.56 and loss3: 0.00\n",
      "Epoch [3510], train_loss: 510.24 with loss1: 430.94, loss2: 79.30 and loss3: 0.00\n",
      "Epoch [3511], train_loss: 509.82 with loss1: 430.18, loss2: 79.65 and loss3: 0.00\n",
      "Epoch [3512], train_loss: 509.23 with loss1: 430.00, loss2: 79.23 and loss3: 0.00\n",
      "Epoch [3513], train_loss: 511.06 with loss1: 431.50, loss2: 79.56 and loss3: 0.00\n",
      "Epoch [3514], train_loss: 508.10 with loss1: 429.12, loss2: 78.98 and loss3: 0.00\n",
      "Epoch [3515], train_loss: 508.11 with loss1: 428.74, loss2: 79.37 and loss3: 0.00\n",
      "Epoch [3516], train_loss: 506.91 with loss1: 427.85, loss2: 79.06 and loss3: 0.00\n",
      "Epoch [3517], train_loss: 507.98 with loss1: 428.67, loss2: 79.30 and loss3: 0.00\n",
      "Epoch [3518], train_loss: 504.99 with loss1: 426.08, loss2: 78.91 and loss3: 0.00\n",
      "Epoch [3519], train_loss: 504.77 with loss1: 425.76, loss2: 79.01 and loss3: 0.00\n",
      "Epoch [3520], train_loss: 504.42 with loss1: 425.55, loss2: 78.87 and loss3: 0.00\n",
      "Epoch [3521], train_loss: 503.21 with loss1: 424.17, loss2: 79.04 and loss3: 0.00\n",
      "Epoch [3522], train_loss: 502.27 with loss1: 423.47, loss2: 78.80 and loss3: 0.00\n",
      "Epoch [3523], train_loss: 503.27 with loss1: 424.21, loss2: 79.06 and loss3: 0.00\n",
      "Epoch [3524], train_loss: 501.71 with loss1: 423.12, loss2: 78.59 and loss3: 0.00\n",
      "Epoch [3525], train_loss: 503.75 with loss1: 425.01, loss2: 78.74 and loss3: 0.00\n",
      "Epoch [3526], train_loss: 501.59 with loss1: 422.96, loss2: 78.63 and loss3: 0.00\n",
      "Epoch [3527], train_loss: 502.77 with loss1: 424.09, loss2: 78.68 and loss3: 0.00\n",
      "Epoch [3528], train_loss: 503.40 with loss1: 424.83, loss2: 78.57 and loss3: 0.00\n",
      "Epoch [3529], train_loss: 504.96 with loss1: 426.25, loss2: 78.71 and loss3: 0.00\n",
      "Epoch [3530], train_loss: 504.59 with loss1: 426.24, loss2: 78.35 and loss3: 0.00\n",
      "Epoch [3531], train_loss: 507.18 with loss1: 428.58, loss2: 78.60 and loss3: 0.00\n",
      "Epoch [3532], train_loss: 504.74 with loss1: 426.37, loss2: 78.37 and loss3: 0.00\n",
      "Epoch [3533], train_loss: 506.86 with loss1: 428.24, loss2: 78.62 and loss3: 0.00\n",
      "Epoch [3534], train_loss: 506.89 with loss1: 428.64, loss2: 78.24 and loss3: 0.00\n",
      "Epoch [3535], train_loss: 508.34 with loss1: 429.77, loss2: 78.57 and loss3: 0.00\n",
      "Epoch [3536], train_loss: 507.83 with loss1: 429.61, loss2: 78.21 and loss3: 0.00\n",
      "Epoch [3537], train_loss: 507.33 with loss1: 428.82, loss2: 78.51 and loss3: 0.00\n",
      "Epoch [3538], train_loss: 504.86 with loss1: 426.75, loss2: 78.11 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3539], train_loss: 506.49 with loss1: 428.06, loss2: 78.43 and loss3: 0.00\n",
      "Epoch [3540], train_loss: 505.17 with loss1: 427.03, loss2: 78.15 and loss3: 0.00\n",
      "Epoch [3541], train_loss: 506.33 with loss1: 428.05, loss2: 78.29 and loss3: 0.00\n",
      "Epoch [3542], train_loss: 504.23 with loss1: 426.12, loss2: 78.11 and loss3: 0.00\n",
      "Epoch [3543], train_loss: 504.50 with loss1: 426.12, loss2: 78.38 and loss3: 0.00\n",
      "Epoch [3544], train_loss: 502.73 with loss1: 424.52, loss2: 78.22 and loss3: 0.00\n",
      "Epoch [3545], train_loss: 502.16 with loss1: 423.98, loss2: 78.19 and loss3: 0.00\n",
      "Epoch [3546], train_loss: 502.72 with loss1: 424.65, loss2: 78.07 and loss3: 0.00\n",
      "Epoch [3547], train_loss: 501.79 with loss1: 423.70, loss2: 78.08 and loss3: 0.00\n",
      "Epoch [3548], train_loss: 500.39 with loss1: 422.38, loss2: 78.01 and loss3: 0.00\n",
      "Epoch [3549], train_loss: 500.52 with loss1: 422.45, loss2: 78.08 and loss3: 0.00\n",
      "Epoch [3550], train_loss: 500.31 with loss1: 422.37, loss2: 77.93 and loss3: 0.00\n",
      "Epoch [3551], train_loss: 500.96 with loss1: 423.04, loss2: 77.92 and loss3: 0.00\n",
      "Epoch [3552], train_loss: 501.18 with loss1: 423.23, loss2: 77.95 and loss3: 0.00\n",
      "Epoch [3553], train_loss: 501.50 with loss1: 423.50, loss2: 78.00 and loss3: 0.00\n",
      "Epoch [3554], train_loss: 499.54 with loss1: 421.79, loss2: 77.75 and loss3: 0.00\n",
      "Epoch [3555], train_loss: 501.98 with loss1: 424.02, loss2: 77.96 and loss3: 0.00\n",
      "Epoch [3556], train_loss: 499.54 with loss1: 421.85, loss2: 77.69 and loss3: 0.00\n",
      "Epoch [3557], train_loss: 499.98 with loss1: 422.01, loss2: 77.97 and loss3: 0.00\n",
      "Epoch [3558], train_loss: 499.11 with loss1: 421.30, loss2: 77.81 and loss3: 0.00\n",
      "Epoch [3559], train_loss: 499.18 with loss1: 421.31, loss2: 77.86 and loss3: 0.00\n",
      "Epoch [3560], train_loss: 498.46 with loss1: 420.76, loss2: 77.70 and loss3: 0.00\n",
      "Epoch [3561], train_loss: 498.16 with loss1: 420.37, loss2: 77.78 and loss3: 0.00\n",
      "Epoch [3562], train_loss: 498.58 with loss1: 420.96, loss2: 77.62 and loss3: 0.00\n",
      "Epoch [3563], train_loss: 499.21 with loss1: 421.53, loss2: 77.68 and loss3: 0.00\n",
      "Epoch [3564], train_loss: 499.14 with loss1: 421.49, loss2: 77.66 and loss3: 0.00\n",
      "Epoch [3565], train_loss: 502.17 with loss1: 424.31, loss2: 77.86 and loss3: 0.00\n",
      "Epoch [3566], train_loss: 500.31 with loss1: 422.54, loss2: 77.77 and loss3: 0.00\n",
      "Epoch [3567], train_loss: 498.65 with loss1: 420.96, loss2: 77.69 and loss3: 0.00\n",
      "Epoch [3568], train_loss: 498.00 with loss1: 420.52, loss2: 77.48 and loss3: 0.00\n",
      "Epoch [3569], train_loss: 498.23 with loss1: 420.53, loss2: 77.70 and loss3: 0.00\n",
      "Epoch [3570], train_loss: 498.76 with loss1: 421.38, loss2: 77.38 and loss3: 0.00\n",
      "Epoch [3571], train_loss: 500.85 with loss1: 423.12, loss2: 77.73 and loss3: 0.00\n",
      "Epoch [3572], train_loss: 499.42 with loss1: 421.90, loss2: 77.52 and loss3: 0.00\n",
      "Epoch [3573], train_loss: 500.91 with loss1: 423.49, loss2: 77.42 and loss3: 0.00\n",
      "Epoch [3574], train_loss: 500.73 with loss1: 423.34, loss2: 77.39 and loss3: 0.00\n",
      "Epoch [3575], train_loss: 499.75 with loss1: 422.13, loss2: 77.62 and loss3: 0.00\n",
      "Epoch [3576], train_loss: 498.46 with loss1: 421.06, loss2: 77.41 and loss3: 0.00\n",
      "Epoch [3577], train_loss: 498.35 with loss1: 420.84, loss2: 77.51 and loss3: 0.00\n",
      "Epoch [3578], train_loss: 496.35 with loss1: 418.84, loss2: 77.52 and loss3: 0.00\n",
      "Epoch [3579], train_loss: 497.72 with loss1: 420.14, loss2: 77.58 and loss3: 0.00\n",
      "Epoch [3580], train_loss: 496.54 with loss1: 419.38, loss2: 77.16 and loss3: 0.00\n",
      "Epoch [3581], train_loss: 498.69 with loss1: 421.13, loss2: 77.56 and loss3: 0.00\n",
      "Epoch [3582], train_loss: 497.76 with loss1: 420.47, loss2: 77.29 and loss3: 0.00\n",
      "Epoch [3583], train_loss: 496.82 with loss1: 419.41, loss2: 77.41 and loss3: 0.00\n",
      "Epoch [3584], train_loss: 495.16 with loss1: 417.95, loss2: 77.21 and loss3: 0.00\n",
      "Epoch [3585], train_loss: 497.24 with loss1: 420.00, loss2: 77.24 and loss3: 0.00\n",
      "Epoch [3586], train_loss: 495.60 with loss1: 418.57, loss2: 77.04 and loss3: 0.00\n",
      "Epoch [3587], train_loss: 496.59 with loss1: 419.30, loss2: 77.29 and loss3: 0.00\n",
      "Epoch [3588], train_loss: 494.22 with loss1: 417.18, loss2: 77.04 and loss3: 0.00\n",
      "Epoch [3589], train_loss: 492.63 with loss1: 415.48, loss2: 77.15 and loss3: 0.00\n",
      "Epoch [3590], train_loss: 492.83 with loss1: 415.80, loss2: 77.03 and loss3: 0.00\n",
      "Epoch [3591], train_loss: 493.53 with loss1: 416.20, loss2: 77.33 and loss3: 0.00\n",
      "Epoch [3592], train_loss: 492.18 with loss1: 415.20, loss2: 76.98 and loss3: 0.00\n",
      "Epoch [3593], train_loss: 490.65 with loss1: 413.64, loss2: 77.01 and loss3: 0.00\n",
      "Epoch [3594], train_loss: 490.53 with loss1: 413.75, loss2: 76.78 and loss3: 0.00\n",
      "Epoch [3595], train_loss: 490.40 with loss1: 413.29, loss2: 77.10 and loss3: 0.00\n",
      "Epoch [3596], train_loss: 488.33 with loss1: 411.64, loss2: 76.69 and loss3: 0.00\n",
      "Epoch [3597], train_loss: 491.33 with loss1: 414.36, loss2: 76.97 and loss3: 0.00\n",
      "Epoch [3598], train_loss: 490.06 with loss1: 413.43, loss2: 76.63 and loss3: 0.00\n",
      "Epoch [3599], train_loss: 490.08 with loss1: 413.26, loss2: 76.82 and loss3: 0.00\n",
      "Epoch [3600], train_loss: 488.82 with loss1: 412.22, loss2: 76.60 and loss3: 0.00\n",
      "Epoch [3601], train_loss: 489.76 with loss1: 412.98, loss2: 76.77 and loss3: 0.00\n",
      "Epoch [3602], train_loss: 489.15 with loss1: 412.57, loss2: 76.58 and loss3: 0.00\n",
      "Epoch [3603], train_loss: 490.20 with loss1: 413.59, loss2: 76.61 and loss3: 0.00\n",
      "Epoch [3604], train_loss: 487.79 with loss1: 411.19, loss2: 76.59 and loss3: 0.00\n",
      "Epoch [3605], train_loss: 489.21 with loss1: 412.44, loss2: 76.77 and loss3: 0.00\n",
      "Epoch [3606], train_loss: 488.59 with loss1: 412.18, loss2: 76.41 and loss3: 0.00\n",
      "Epoch [3607], train_loss: 490.18 with loss1: 413.38, loss2: 76.80 and loss3: 0.00\n",
      "Epoch [3608], train_loss: 489.62 with loss1: 413.11, loss2: 76.51 and loss3: 0.00\n",
      "Epoch [3609], train_loss: 489.70 with loss1: 413.00, loss2: 76.70 and loss3: 0.00\n",
      "Epoch [3610], train_loss: 489.76 with loss1: 413.52, loss2: 76.24 and loss3: 0.00\n",
      "Epoch [3611], train_loss: 489.67 with loss1: 413.01, loss2: 76.66 and loss3: 0.00\n",
      "Epoch [3612], train_loss: 490.97 with loss1: 414.60, loss2: 76.37 and loss3: 0.00\n",
      "Epoch [3613], train_loss: 491.42 with loss1: 414.81, loss2: 76.61 and loss3: 0.00\n",
      "Epoch [3614], train_loss: 491.27 with loss1: 415.00, loss2: 76.28 and loss3: 0.00\n",
      "Epoch [3615], train_loss: 493.17 with loss1: 416.76, loss2: 76.41 and loss3: 0.00\n",
      "Epoch [3616], train_loss: 492.01 with loss1: 415.88, loss2: 76.13 and loss3: 0.00\n",
      "Epoch [3617], train_loss: 493.26 with loss1: 416.87, loss2: 76.39 and loss3: 0.00\n",
      "Epoch [3618], train_loss: 492.57 with loss1: 416.36, loss2: 76.21 and loss3: 0.00\n",
      "Epoch [3619], train_loss: 493.93 with loss1: 417.50, loss2: 76.43 and loss3: 0.00\n",
      "Epoch [3620], train_loss: 492.77 with loss1: 416.59, loss2: 76.17 and loss3: 0.00\n",
      "Epoch [3621], train_loss: 495.21 with loss1: 418.70, loss2: 76.51 and loss3: 0.00\n",
      "Epoch [3622], train_loss: 494.83 with loss1: 418.80, loss2: 76.03 and loss3: 0.00\n",
      "Epoch [3623], train_loss: 495.68 with loss1: 419.24, loss2: 76.44 and loss3: 0.00\n",
      "Epoch [3624], train_loss: 494.14 with loss1: 417.92, loss2: 76.22 and loss3: 0.00\n",
      "Epoch [3625], train_loss: 495.50 with loss1: 419.05, loss2: 76.45 and loss3: 0.00\n",
      "Epoch [3626], train_loss: 494.39 with loss1: 418.21, loss2: 76.18 and loss3: 0.00\n",
      "Epoch [3627], train_loss: 496.64 with loss1: 420.16, loss2: 76.48 and loss3: 0.00\n",
      "Epoch [3628], train_loss: 495.93 with loss1: 419.85, loss2: 76.07 and loss3: 0.00\n",
      "Epoch [3629], train_loss: 497.31 with loss1: 420.94, loss2: 76.37 and loss3: 0.00\n",
      "Epoch [3630], train_loss: 497.87 with loss1: 421.80, loss2: 76.07 and loss3: 0.00\n",
      "Epoch [3631], train_loss: 498.77 with loss1: 422.24, loss2: 76.53 and loss3: 0.00\n",
      "Epoch [3632], train_loss: 495.35 with loss1: 419.21, loss2: 76.14 and loss3: 0.00\n",
      "Epoch [3633], train_loss: 496.36 with loss1: 420.02, loss2: 76.34 and loss3: 0.00\n",
      "Epoch [3634], train_loss: 495.43 with loss1: 419.27, loss2: 76.17 and loss3: 0.00\n",
      "Epoch [3635], train_loss: 495.92 with loss1: 419.45, loss2: 76.47 and loss3: 0.00\n",
      "Epoch [3636], train_loss: 494.53 with loss1: 418.38, loss2: 76.15 and loss3: 0.00\n",
      "Epoch [3637], train_loss: 492.95 with loss1: 416.59, loss2: 76.36 and loss3: 0.00\n",
      "Epoch [3638], train_loss: 494.22 with loss1: 418.07, loss2: 76.15 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3639], train_loss: 495.37 with loss1: 418.97, loss2: 76.40 and loss3: 0.00\n",
      "Epoch [3640], train_loss: 494.01 with loss1: 417.92, loss2: 76.09 and loss3: 0.00\n",
      "Epoch [3641], train_loss: 495.30 with loss1: 418.71, loss2: 76.59 and loss3: 0.00\n",
      "Epoch [3642], train_loss: 493.53 with loss1: 417.54, loss2: 75.99 and loss3: 0.00\n",
      "Epoch [3643], train_loss: 492.64 with loss1: 416.21, loss2: 76.43 and loss3: 0.00\n",
      "Epoch [3644], train_loss: 491.79 with loss1: 415.68, loss2: 76.12 and loss3: 0.00\n",
      "Epoch [3645], train_loss: 492.32 with loss1: 415.79, loss2: 76.52 and loss3: 0.00\n",
      "Epoch [3646], train_loss: 489.33 with loss1: 413.33, loss2: 76.01 and loss3: 0.00\n",
      "Epoch [3647], train_loss: 488.01 with loss1: 411.50, loss2: 76.52 and loss3: 0.00\n",
      "Epoch [3648], train_loss: 486.64 with loss1: 410.60, loss2: 76.04 and loss3: 0.00\n",
      "Epoch [3649], train_loss: 487.21 with loss1: 410.68, loss2: 76.52 and loss3: 0.00\n",
      "Epoch [3650], train_loss: 484.84 with loss1: 408.87, loss2: 75.97 and loss3: 0.00\n",
      "Epoch [3651], train_loss: 483.85 with loss1: 407.30, loss2: 76.55 and loss3: 0.00\n",
      "Epoch [3652], train_loss: 482.69 with loss1: 406.60, loss2: 76.09 and loss3: 0.00\n",
      "Epoch [3653], train_loss: 481.61 with loss1: 405.11, loss2: 76.50 and loss3: 0.00\n",
      "Epoch [3654], train_loss: 479.92 with loss1: 403.68, loss2: 76.23 and loss3: 0.00\n",
      "Epoch [3655], train_loss: 478.58 with loss1: 402.11, loss2: 76.47 and loss3: 0.00\n",
      "Epoch [3656], train_loss: 476.73 with loss1: 400.62, loss2: 76.11 and loss3: 0.00\n",
      "Epoch [3657], train_loss: 476.02 with loss1: 399.61, loss2: 76.42 and loss3: 0.00\n",
      "Epoch [3658], train_loss: 475.62 with loss1: 399.53, loss2: 76.09 and loss3: 0.00\n",
      "Epoch [3659], train_loss: 474.35 with loss1: 397.97, loss2: 76.38 and loss3: 0.00\n",
      "Epoch [3660], train_loss: 474.55 with loss1: 398.60, loss2: 75.95 and loss3: 0.00\n",
      "Epoch [3661], train_loss: 474.19 with loss1: 397.92, loss2: 76.27 and loss3: 0.00\n",
      "Epoch [3662], train_loss: 473.13 with loss1: 397.27, loss2: 75.86 and loss3: 0.00\n",
      "Epoch [3663], train_loss: 473.15 with loss1: 397.04, loss2: 76.11 and loss3: 0.00\n",
      "Epoch [3664], train_loss: 471.36 with loss1: 395.40, loss2: 75.96 and loss3: 0.00\n",
      "Epoch [3665], train_loss: 471.27 with loss1: 395.20, loss2: 76.07 and loss3: 0.00\n",
      "Epoch [3666], train_loss: 470.25 with loss1: 394.38, loss2: 75.87 and loss3: 0.00\n",
      "Epoch [3667], train_loss: 470.46 with loss1: 394.51, loss2: 75.95 and loss3: 0.00\n",
      "Epoch [3668], train_loss: 469.17 with loss1: 393.43, loss2: 75.74 and loss3: 0.00\n",
      "Epoch [3669], train_loss: 470.88 with loss1: 395.15, loss2: 75.73 and loss3: 0.00\n",
      "Epoch [3670], train_loss: 470.06 with loss1: 394.31, loss2: 75.75 and loss3: 0.00\n",
      "Epoch [3671], train_loss: 471.23 with loss1: 395.49, loss2: 75.74 and loss3: 0.00\n",
      "Epoch [3672], train_loss: 471.85 with loss1: 396.24, loss2: 75.61 and loss3: 0.00\n",
      "Epoch [3673], train_loss: 474.62 with loss1: 398.85, loss2: 75.78 and loss3: 0.00\n",
      "Epoch [3674], train_loss: 473.63 with loss1: 398.05, loss2: 75.59 and loss3: 0.00\n",
      "Epoch [3675], train_loss: 475.91 with loss1: 400.09, loss2: 75.81 and loss3: 0.00\n",
      "Epoch [3676], train_loss: 475.15 with loss1: 399.50, loss2: 75.65 and loss3: 0.00\n",
      "Epoch [3677], train_loss: 477.67 with loss1: 401.89, loss2: 75.78 and loss3: 0.00\n",
      "Epoch [3678], train_loss: 478.72 with loss1: 403.06, loss2: 75.65 and loss3: 0.00\n",
      "Epoch [3679], train_loss: 481.76 with loss1: 405.86, loss2: 75.89 and loss3: 0.00\n",
      "Epoch [3680], train_loss: 482.71 with loss1: 407.06, loss2: 75.65 and loss3: 0.00\n",
      "Epoch [3681], train_loss: 484.77 with loss1: 408.99, loss2: 75.78 and loss3: 0.00\n",
      "Epoch [3682], train_loss: 485.91 with loss1: 410.31, loss2: 75.60 and loss3: 0.00\n",
      "Epoch [3683], train_loss: 487.29 with loss1: 411.29, loss2: 76.00 and loss3: 0.00\n",
      "Epoch [3684], train_loss: 487.18 with loss1: 411.49, loss2: 75.69 and loss3: 0.00\n",
      "Epoch [3685], train_loss: 492.71 with loss1: 416.79, loss2: 75.92 and loss3: 0.00\n",
      "Epoch [3686], train_loss: 494.33 with loss1: 418.63, loss2: 75.70 and loss3: 0.00\n",
      "Epoch [3687], train_loss: 497.54 with loss1: 421.45, loss2: 76.09 and loss3: 0.00\n",
      "Epoch [3688], train_loss: 497.12 with loss1: 421.21, loss2: 75.91 and loss3: 0.00\n",
      "Epoch [3689], train_loss: 501.68 with loss1: 425.37, loss2: 76.31 and loss3: 0.00\n",
      "Epoch [3690], train_loss: 500.60 with loss1: 424.70, loss2: 75.90 and loss3: 0.00\n",
      "Epoch [3691], train_loss: 503.65 with loss1: 427.14, loss2: 76.51 and loss3: 0.00\n",
      "Epoch [3692], train_loss: 501.76 with loss1: 425.81, loss2: 75.95 and loss3: 0.00\n",
      "Epoch [3693], train_loss: 504.44 with loss1: 427.91, loss2: 76.53 and loss3: 0.00\n",
      "Epoch [3694], train_loss: 502.51 with loss1: 426.37, loss2: 76.14 and loss3: 0.00\n",
      "Epoch [3695], train_loss: 504.84 with loss1: 428.34, loss2: 76.51 and loss3: 0.00\n",
      "Epoch [3696], train_loss: 501.29 with loss1: 425.20, loss2: 76.09 and loss3: 0.00\n",
      "Epoch [3697], train_loss: 504.20 with loss1: 427.86, loss2: 76.34 and loss3: 0.00\n",
      "Epoch [3698], train_loss: 499.25 with loss1: 423.31, loss2: 75.94 and loss3: 0.00\n",
      "Epoch [3699], train_loss: 503.37 with loss1: 426.85, loss2: 76.53 and loss3: 0.00\n",
      "Epoch [3700], train_loss: 500.16 with loss1: 424.21, loss2: 75.95 and loss3: 0.00\n",
      "Epoch [3701], train_loss: 502.38 with loss1: 426.12, loss2: 76.26 and loss3: 0.00\n",
      "Epoch [3702], train_loss: 500.88 with loss1: 424.92, loss2: 75.97 and loss3: 0.00\n",
      "Epoch [3703], train_loss: 503.64 with loss1: 427.45, loss2: 76.19 and loss3: 0.00\n",
      "Epoch [3704], train_loss: 503.33 with loss1: 427.42, loss2: 75.90 and loss3: 0.00\n",
      "Epoch [3705], train_loss: 506.74 with loss1: 430.71, loss2: 76.03 and loss3: 0.00\n",
      "Epoch [3706], train_loss: 505.59 with loss1: 429.58, loss2: 76.01 and loss3: 0.00\n",
      "Epoch [3707], train_loss: 508.58 with loss1: 432.42, loss2: 76.15 and loss3: 0.00\n",
      "Epoch [3708], train_loss: 509.83 with loss1: 433.92, loss2: 75.91 and loss3: 0.00\n",
      "Epoch [3709], train_loss: 512.65 with loss1: 436.41, loss2: 76.24 and loss3: 0.00\n",
      "Epoch [3710], train_loss: 511.12 with loss1: 435.25, loss2: 75.88 and loss3: 0.00\n",
      "Epoch [3711], train_loss: 515.47 with loss1: 439.41, loss2: 76.06 and loss3: 0.00\n",
      "Epoch [3712], train_loss: 515.71 with loss1: 439.95, loss2: 75.76 and loss3: 0.00\n",
      "Epoch [3713], train_loss: 518.12 with loss1: 441.87, loss2: 76.25 and loss3: 0.00\n",
      "Epoch [3714], train_loss: 517.72 with loss1: 441.95, loss2: 75.77 and loss3: 0.00\n",
      "Epoch [3715], train_loss: 521.08 with loss1: 444.97, loss2: 76.11 and loss3: 0.00\n",
      "Epoch [3716], train_loss: 518.52 with loss1: 442.90, loss2: 75.62 and loss3: 0.00\n",
      "Epoch [3717], train_loss: 522.71 with loss1: 446.53, loss2: 76.19 and loss3: 0.00\n",
      "Epoch [3718], train_loss: 520.82 with loss1: 445.06, loss2: 75.76 and loss3: 0.00\n",
      "Epoch [3719], train_loss: 523.64 with loss1: 447.54, loss2: 76.10 and loss3: 0.00\n",
      "Epoch [3720], train_loss: 521.85 with loss1: 446.23, loss2: 75.62 and loss3: 0.00\n",
      "Epoch [3721], train_loss: 524.65 with loss1: 448.48, loss2: 76.16 and loss3: 0.00\n",
      "Epoch [3722], train_loss: 520.38 with loss1: 444.74, loss2: 75.64 and loss3: 0.00\n",
      "Epoch [3723], train_loss: 523.23 with loss1: 447.12, loss2: 76.11 and loss3: 0.00\n",
      "Epoch [3724], train_loss: 520.13 with loss1: 444.41, loss2: 75.72 and loss3: 0.00\n",
      "Epoch [3725], train_loss: 521.27 with loss1: 445.31, loss2: 75.96 and loss3: 0.00\n",
      "Epoch [3726], train_loss: 517.62 with loss1: 442.00, loss2: 75.62 and loss3: 0.00\n",
      "Epoch [3727], train_loss: 518.80 with loss1: 442.78, loss2: 76.02 and loss3: 0.00\n",
      "Epoch [3728], train_loss: 513.42 with loss1: 437.62, loss2: 75.79 and loss3: 0.00\n",
      "Epoch [3729], train_loss: 514.97 with loss1: 438.87, loss2: 76.10 and loss3: 0.00\n",
      "Epoch [3730], train_loss: 510.57 with loss1: 434.86, loss2: 75.71 and loss3: 0.00\n",
      "Epoch [3731], train_loss: 511.45 with loss1: 435.53, loss2: 75.92 and loss3: 0.00\n",
      "Epoch [3732], train_loss: 508.10 with loss1: 432.47, loss2: 75.63 and loss3: 0.00\n",
      "Epoch [3733], train_loss: 507.83 with loss1: 431.90, loss2: 75.94 and loss3: 0.00\n",
      "Epoch [3734], train_loss: 503.65 with loss1: 427.92, loss2: 75.73 and loss3: 0.00\n",
      "Epoch [3735], train_loss: 503.70 with loss1: 427.88, loss2: 75.82 and loss3: 0.00\n",
      "Epoch [3736], train_loss: 498.39 with loss1: 422.86, loss2: 75.53 and loss3: 0.00\n",
      "Epoch [3737], train_loss: 498.73 with loss1: 422.94, loss2: 75.79 and loss3: 0.00\n",
      "Epoch [3738], train_loss: 495.48 with loss1: 419.90, loss2: 75.58 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3739], train_loss: 496.56 with loss1: 420.89, loss2: 75.67 and loss3: 0.00\n",
      "Epoch [3740], train_loss: 493.15 with loss1: 417.78, loss2: 75.37 and loss3: 0.00\n",
      "Epoch [3741], train_loss: 493.07 with loss1: 417.29, loss2: 75.79 and loss3: 0.00\n",
      "Epoch [3742], train_loss: 489.23 with loss1: 413.89, loss2: 75.33 and loss3: 0.00\n",
      "Epoch [3743], train_loss: 489.68 with loss1: 414.13, loss2: 75.55 and loss3: 0.00\n",
      "Epoch [3744], train_loss: 486.79 with loss1: 411.48, loss2: 75.31 and loss3: 0.00\n",
      "Epoch [3745], train_loss: 487.34 with loss1: 411.79, loss2: 75.55 and loss3: 0.00\n",
      "Epoch [3746], train_loss: 484.74 with loss1: 409.45, loss2: 75.29 and loss3: 0.00\n",
      "Epoch [3747], train_loss: 483.25 with loss1: 407.75, loss2: 75.50 and loss3: 0.00\n",
      "Epoch [3748], train_loss: 483.29 with loss1: 408.10, loss2: 75.19 and loss3: 0.00\n",
      "Epoch [3749], train_loss: 483.74 with loss1: 408.39, loss2: 75.34 and loss3: 0.00\n",
      "Epoch [3750], train_loss: 482.44 with loss1: 407.33, loss2: 75.11 and loss3: 0.00\n",
      "Epoch [3751], train_loss: 481.78 with loss1: 406.49, loss2: 75.29 and loss3: 0.00\n",
      "Epoch [3752], train_loss: 479.94 with loss1: 404.88, loss2: 75.06 and loss3: 0.00\n",
      "Epoch [3753], train_loss: 479.14 with loss1: 403.88, loss2: 75.26 and loss3: 0.00\n",
      "Epoch [3754], train_loss: 478.71 with loss1: 403.89, loss2: 74.83 and loss3: 0.00\n",
      "Epoch [3755], train_loss: 480.07 with loss1: 405.07, loss2: 75.00 and loss3: 0.00\n",
      "Epoch [3756], train_loss: 478.79 with loss1: 403.93, loss2: 74.87 and loss3: 0.00\n",
      "Epoch [3757], train_loss: 479.92 with loss1: 404.89, loss2: 75.03 and loss3: 0.00\n",
      "Epoch [3758], train_loss: 479.99 with loss1: 405.10, loss2: 74.90 and loss3: 0.00\n",
      "Epoch [3759], train_loss: 480.87 with loss1: 405.91, loss2: 74.97 and loss3: 0.00\n",
      "Epoch [3760], train_loss: 479.79 with loss1: 405.20, loss2: 74.59 and loss3: 0.00\n",
      "Epoch [3761], train_loss: 480.53 with loss1: 405.53, loss2: 74.99 and loss3: 0.00\n",
      "Epoch [3762], train_loss: 480.91 with loss1: 406.16, loss2: 74.76 and loss3: 0.00\n",
      "Epoch [3763], train_loss: 481.13 with loss1: 406.16, loss2: 74.97 and loss3: 0.00\n",
      "Epoch [3764], train_loss: 480.79 with loss1: 406.17, loss2: 74.62 and loss3: 0.00\n",
      "Epoch [3765], train_loss: 482.83 with loss1: 408.13, loss2: 74.70 and loss3: 0.00\n",
      "Epoch [3766], train_loss: 481.57 with loss1: 406.95, loss2: 74.61 and loss3: 0.00\n",
      "Epoch [3767], train_loss: 484.24 with loss1: 409.41, loss2: 74.83 and loss3: 0.00\n",
      "Epoch [3768], train_loss: 483.81 with loss1: 409.34, loss2: 74.47 and loss3: 0.00\n",
      "Epoch [3769], train_loss: 484.97 with loss1: 410.31, loss2: 74.66 and loss3: 0.00\n",
      "Epoch [3770], train_loss: 483.66 with loss1: 409.35, loss2: 74.31 and loss3: 0.00\n",
      "Epoch [3771], train_loss: 486.91 with loss1: 412.24, loss2: 74.67 and loss3: 0.00\n",
      "Epoch [3772], train_loss: 488.60 with loss1: 414.25, loss2: 74.34 and loss3: 0.00\n",
      "Epoch [3773], train_loss: 490.69 with loss1: 416.01, loss2: 74.69 and loss3: 0.00\n",
      "Epoch [3774], train_loss: 491.52 with loss1: 417.14, loss2: 74.38 and loss3: 0.00\n",
      "Epoch [3775], train_loss: 492.03 with loss1: 417.43, loss2: 74.61 and loss3: 0.00\n",
      "Epoch [3776], train_loss: 493.75 with loss1: 419.43, loss2: 74.32 and loss3: 0.00\n",
      "Epoch [3777], train_loss: 494.29 with loss1: 419.80, loss2: 74.49 and loss3: 0.00\n",
      "Epoch [3778], train_loss: 494.75 with loss1: 420.46, loss2: 74.29 and loss3: 0.00\n",
      "Epoch [3779], train_loss: 496.25 with loss1: 421.72, loss2: 74.54 and loss3: 0.00\n",
      "Epoch [3780], train_loss: 495.73 with loss1: 421.39, loss2: 74.33 and loss3: 0.00\n",
      "Epoch [3781], train_loss: 498.75 with loss1: 424.31, loss2: 74.43 and loss3: 0.00\n",
      "Epoch [3782], train_loss: 498.17 with loss1: 423.83, loss2: 74.34 and loss3: 0.00\n",
      "Epoch [3783], train_loss: 501.02 with loss1: 426.46, loss2: 74.56 and loss3: 0.00\n",
      "Epoch [3784], train_loss: 501.69 with loss1: 427.34, loss2: 74.35 and loss3: 0.00\n",
      "Epoch [3785], train_loss: 502.67 with loss1: 428.25, loss2: 74.42 and loss3: 0.00\n",
      "Epoch [3786], train_loss: 504.30 with loss1: 429.97, loss2: 74.33 and loss3: 0.00\n",
      "Epoch [3787], train_loss: 503.20 with loss1: 428.78, loss2: 74.41 and loss3: 0.00\n",
      "Epoch [3788], train_loss: 501.78 with loss1: 427.54, loss2: 74.24 and loss3: 0.00\n",
      "Epoch [3789], train_loss: 501.36 with loss1: 427.01, loss2: 74.35 and loss3: 0.00\n",
      "Epoch [3790], train_loss: 500.59 with loss1: 426.27, loss2: 74.32 and loss3: 0.00\n",
      "Epoch [3791], train_loss: 501.83 with loss1: 427.33, loss2: 74.50 and loss3: 0.00\n",
      "Epoch [3792], train_loss: 501.31 with loss1: 427.05, loss2: 74.26 and loss3: 0.00\n",
      "Epoch [3793], train_loss: 503.40 with loss1: 428.72, loss2: 74.68 and loss3: 0.00\n",
      "Epoch [3794], train_loss: 499.36 with loss1: 425.09, loss2: 74.27 and loss3: 0.00\n",
      "Epoch [3795], train_loss: 500.18 with loss1: 425.72, loss2: 74.46 and loss3: 0.00\n",
      "Epoch [3796], train_loss: 500.29 with loss1: 426.02, loss2: 74.28 and loss3: 0.00\n",
      "Epoch [3797], train_loss: 498.28 with loss1: 423.82, loss2: 74.46 and loss3: 0.00\n",
      "Epoch [3798], train_loss: 496.74 with loss1: 422.41, loss2: 74.33 and loss3: 0.00\n",
      "Epoch [3799], train_loss: 496.54 with loss1: 422.02, loss2: 74.52 and loss3: 0.00\n",
      "Epoch [3800], train_loss: 496.19 with loss1: 422.13, loss2: 74.06 and loss3: 0.00\n",
      "Epoch [3801], train_loss: 497.26 with loss1: 422.69, loss2: 74.57 and loss3: 0.00\n",
      "Epoch [3802], train_loss: 493.99 with loss1: 419.82, loss2: 74.17 and loss3: 0.00\n",
      "Epoch [3803], train_loss: 493.18 with loss1: 418.73, loss2: 74.46 and loss3: 0.00\n",
      "Epoch [3804], train_loss: 492.45 with loss1: 418.13, loss2: 74.32 and loss3: 0.00\n",
      "Epoch [3805], train_loss: 490.95 with loss1: 416.38, loss2: 74.56 and loss3: 0.00\n",
      "Epoch [3806], train_loss: 488.36 with loss1: 414.02, loss2: 74.33 and loss3: 0.00\n",
      "Epoch [3807], train_loss: 489.07 with loss1: 414.68, loss2: 74.39 and loss3: 0.00\n",
      "Epoch [3808], train_loss: 487.40 with loss1: 413.03, loss2: 74.37 and loss3: 0.00\n",
      "Epoch [3809], train_loss: 486.44 with loss1: 412.12, loss2: 74.32 and loss3: 0.00\n",
      "Epoch [3810], train_loss: 486.03 with loss1: 411.70, loss2: 74.33 and loss3: 0.00\n",
      "Epoch [3811], train_loss: 485.39 with loss1: 411.00, loss2: 74.40 and loss3: 0.00\n",
      "Epoch [3812], train_loss: 483.26 with loss1: 409.18, loss2: 74.08 and loss3: 0.00\n",
      "Epoch [3813], train_loss: 481.86 with loss1: 407.53, loss2: 74.34 and loss3: 0.00\n",
      "Epoch [3814], train_loss: 481.02 with loss1: 406.80, loss2: 74.22 and loss3: 0.00\n",
      "Epoch [3815], train_loss: 479.89 with loss1: 405.60, loss2: 74.29 and loss3: 0.00\n",
      "Epoch [3816], train_loss: 477.47 with loss1: 403.30, loss2: 74.17 and loss3: 0.00\n",
      "Epoch [3817], train_loss: 478.74 with loss1: 404.56, loss2: 74.18 and loss3: 0.00\n",
      "Epoch [3818], train_loss: 477.38 with loss1: 403.50, loss2: 73.88 and loss3: 0.00\n",
      "Epoch [3819], train_loss: 476.54 with loss1: 402.51, loss2: 74.03 and loss3: 0.00\n",
      "Epoch [3820], train_loss: 475.19 with loss1: 401.18, loss2: 74.01 and loss3: 0.00\n",
      "Epoch [3821], train_loss: 474.99 with loss1: 400.96, loss2: 74.03 and loss3: 0.00\n",
      "Epoch [3822], train_loss: 475.13 with loss1: 401.33, loss2: 73.80 and loss3: 0.00\n",
      "Epoch [3823], train_loss: 473.65 with loss1: 399.62, loss2: 74.03 and loss3: 0.00\n",
      "Epoch [3824], train_loss: 472.90 with loss1: 399.22, loss2: 73.68 and loss3: 0.00\n",
      "Epoch [3825], train_loss: 473.76 with loss1: 399.91, loss2: 73.84 and loss3: 0.00\n",
      "Epoch [3826], train_loss: 473.94 with loss1: 400.06, loss2: 73.88 and loss3: 0.00\n",
      "Epoch [3827], train_loss: 471.98 with loss1: 398.19, loss2: 73.79 and loss3: 0.00\n",
      "Epoch [3828], train_loss: 470.23 with loss1: 396.49, loss2: 73.74 and loss3: 0.00\n",
      "Epoch [3829], train_loss: 468.79 with loss1: 395.04, loss2: 73.76 and loss3: 0.00\n",
      "Epoch [3830], train_loss: 467.37 with loss1: 393.72, loss2: 73.65 and loss3: 0.00\n",
      "Epoch [3831], train_loss: 467.01 with loss1: 393.27, loss2: 73.74 and loss3: 0.00\n",
      "Epoch [3832], train_loss: 466.86 with loss1: 393.30, loss2: 73.56 and loss3: 0.00\n",
      "Epoch [3833], train_loss: 465.29 with loss1: 391.76, loss2: 73.52 and loss3: 0.00\n",
      "Epoch [3834], train_loss: 465.13 with loss1: 391.63, loss2: 73.50 and loss3: 0.00\n",
      "Epoch [3835], train_loss: 466.45 with loss1: 392.93, loss2: 73.52 and loss3: 0.00\n",
      "Epoch [3836], train_loss: 464.45 with loss1: 391.11, loss2: 73.34 and loss3: 0.00\n",
      "Epoch [3837], train_loss: 464.88 with loss1: 391.47, loss2: 73.41 and loss3: 0.00\n",
      "Epoch [3838], train_loss: 465.94 with loss1: 392.64, loss2: 73.31 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3839], train_loss: 465.26 with loss1: 391.99, loss2: 73.27 and loss3: 0.00\n",
      "Epoch [3840], train_loss: 463.74 with loss1: 390.33, loss2: 73.41 and loss3: 0.00\n",
      "Epoch [3841], train_loss: 463.51 with loss1: 390.18, loss2: 73.33 and loss3: 0.00\n",
      "Epoch [3842], train_loss: 464.20 with loss1: 391.04, loss2: 73.16 and loss3: 0.00\n",
      "Epoch [3843], train_loss: 464.95 with loss1: 391.68, loss2: 73.27 and loss3: 0.00\n",
      "Epoch [3844], train_loss: 464.49 with loss1: 391.27, loss2: 73.22 and loss3: 0.00\n",
      "Epoch [3845], train_loss: 465.45 with loss1: 392.30, loss2: 73.15 and loss3: 0.00\n",
      "Epoch [3846], train_loss: 465.28 with loss1: 392.27, loss2: 73.01 and loss3: 0.00\n",
      "Epoch [3847], train_loss: 466.09 with loss1: 393.00, loss2: 73.09 and loss3: 0.00\n",
      "Epoch [3848], train_loss: 467.13 with loss1: 394.11, loss2: 73.02 and loss3: 0.00\n",
      "Epoch [3849], train_loss: 468.43 with loss1: 395.43, loss2: 73.00 and loss3: 0.00\n",
      "Epoch [3850], train_loss: 468.99 with loss1: 395.91, loss2: 73.08 and loss3: 0.00\n",
      "Epoch [3851], train_loss: 471.51 with loss1: 398.46, loss2: 73.05 and loss3: 0.00\n",
      "Epoch [3852], train_loss: 470.63 with loss1: 397.50, loss2: 73.13 and loss3: 0.00\n",
      "Epoch [3853], train_loss: 473.29 with loss1: 400.37, loss2: 72.93 and loss3: 0.00\n",
      "Epoch [3854], train_loss: 472.56 with loss1: 399.43, loss2: 73.13 and loss3: 0.00\n",
      "Epoch [3855], train_loss: 473.74 with loss1: 400.66, loss2: 73.08 and loss3: 0.00\n",
      "Epoch [3856], train_loss: 476.19 with loss1: 403.11, loss2: 73.08 and loss3: 0.00\n",
      "Epoch [3857], train_loss: 479.60 with loss1: 406.46, loss2: 73.14 and loss3: 0.00\n",
      "Epoch [3858], train_loss: 478.14 with loss1: 405.01, loss2: 73.14 and loss3: 0.00\n",
      "Epoch [3859], train_loss: 480.54 with loss1: 407.32, loss2: 73.22 and loss3: 0.00\n",
      "Epoch [3860], train_loss: 481.46 with loss1: 408.27, loss2: 73.18 and loss3: 0.00\n",
      "Epoch [3861], train_loss: 481.89 with loss1: 408.69, loss2: 73.20 and loss3: 0.00\n",
      "Epoch [3862], train_loss: 483.27 with loss1: 409.96, loss2: 73.31 and loss3: 0.00\n",
      "Epoch [3863], train_loss: 486.15 with loss1: 412.78, loss2: 73.36 and loss3: 0.00\n",
      "Epoch [3864], train_loss: 485.62 with loss1: 412.24, loss2: 73.38 and loss3: 0.00\n",
      "Epoch [3865], train_loss: 488.02 with loss1: 414.57, loss2: 73.46 and loss3: 0.00\n",
      "Epoch [3866], train_loss: 486.36 with loss1: 412.91, loss2: 73.45 and loss3: 0.00\n",
      "Epoch [3867], train_loss: 487.96 with loss1: 414.52, loss2: 73.44 and loss3: 0.00\n",
      "Epoch [3868], train_loss: 488.98 with loss1: 415.43, loss2: 73.55 and loss3: 0.00\n",
      "Epoch [3869], train_loss: 491.45 with loss1: 417.95, loss2: 73.50 and loss3: 0.00\n",
      "Epoch [3870], train_loss: 494.31 with loss1: 420.68, loss2: 73.63 and loss3: 0.00\n",
      "Epoch [3871], train_loss: 497.36 with loss1: 423.69, loss2: 73.67 and loss3: 0.00\n",
      "Epoch [3872], train_loss: 497.35 with loss1: 423.62, loss2: 73.73 and loss3: 0.00\n",
      "Epoch [3873], train_loss: 498.96 with loss1: 424.92, loss2: 74.04 and loss3: 0.00\n",
      "Epoch [3874], train_loss: 497.66 with loss1: 423.85, loss2: 73.82 and loss3: 0.00\n",
      "Epoch [3875], train_loss: 497.51 with loss1: 423.56, loss2: 73.95 and loss3: 0.00\n",
      "Epoch [3876], train_loss: 496.06 with loss1: 422.00, loss2: 74.06 and loss3: 0.00\n",
      "Epoch [3877], train_loss: 495.64 with loss1: 421.72, loss2: 73.92 and loss3: 0.00\n",
      "Epoch [3878], train_loss: 495.39 with loss1: 421.57, loss2: 73.82 and loss3: 0.00\n",
      "Epoch [3879], train_loss: 495.95 with loss1: 422.04, loss2: 73.91 and loss3: 0.00\n",
      "Epoch [3880], train_loss: 495.41 with loss1: 421.56, loss2: 73.85 and loss3: 0.00\n",
      "Epoch [3881], train_loss: 494.49 with loss1: 420.55, loss2: 73.94 and loss3: 0.00\n",
      "Epoch [3882], train_loss: 492.15 with loss1: 418.33, loss2: 73.83 and loss3: 0.00\n",
      "Epoch [3883], train_loss: 493.38 with loss1: 419.45, loss2: 73.92 and loss3: 0.00\n",
      "Epoch [3884], train_loss: 490.93 with loss1: 417.27, loss2: 73.67 and loss3: 0.00\n",
      "Epoch [3885], train_loss: 492.68 with loss1: 418.87, loss2: 73.81 and loss3: 0.00\n",
      "Epoch [3886], train_loss: 490.33 with loss1: 416.56, loss2: 73.77 and loss3: 0.00\n",
      "Epoch [3887], train_loss: 492.35 with loss1: 418.50, loss2: 73.86 and loss3: 0.00\n",
      "Epoch [3888], train_loss: 490.73 with loss1: 416.81, loss2: 73.92 and loss3: 0.00\n",
      "Epoch [3889], train_loss: 491.47 with loss1: 417.64, loss2: 73.83 and loss3: 0.00\n",
      "Epoch [3890], train_loss: 490.16 with loss1: 416.43, loss2: 73.72 and loss3: 0.00\n",
      "Epoch [3891], train_loss: 490.90 with loss1: 417.05, loss2: 73.86 and loss3: 0.00\n",
      "Epoch [3892], train_loss: 489.52 with loss1: 415.84, loss2: 73.68 and loss3: 0.00\n",
      "Epoch [3893], train_loss: 488.68 with loss1: 414.88, loss2: 73.80 and loss3: 0.00\n",
      "Epoch [3894], train_loss: 486.18 with loss1: 412.43, loss2: 73.75 and loss3: 0.00\n",
      "Epoch [3895], train_loss: 487.93 with loss1: 414.16, loss2: 73.78 and loss3: 0.00\n",
      "Epoch [3896], train_loss: 484.26 with loss1: 410.70, loss2: 73.56 and loss3: 0.00\n",
      "Epoch [3897], train_loss: 482.74 with loss1: 408.90, loss2: 73.83 and loss3: 0.00\n",
      "Epoch [3898], train_loss: 479.89 with loss1: 406.27, loss2: 73.62 and loss3: 0.00\n",
      "Epoch [3899], train_loss: 479.74 with loss1: 406.09, loss2: 73.65 and loss3: 0.00\n",
      "Epoch [3900], train_loss: 478.09 with loss1: 404.66, loss2: 73.43 and loss3: 0.00\n",
      "Epoch [3901], train_loss: 475.54 with loss1: 401.94, loss2: 73.61 and loss3: 0.00\n",
      "Epoch [3902], train_loss: 475.07 with loss1: 401.68, loss2: 73.39 and loss3: 0.00\n",
      "Epoch [3903], train_loss: 473.35 with loss1: 399.84, loss2: 73.52 and loss3: 0.00\n",
      "Epoch [3904], train_loss: 472.41 with loss1: 398.99, loss2: 73.42 and loss3: 0.00\n",
      "Epoch [3905], train_loss: 471.97 with loss1: 398.68, loss2: 73.29 and loss3: 0.00\n",
      "Epoch [3906], train_loss: 471.63 with loss1: 398.34, loss2: 73.29 and loss3: 0.00\n",
      "Epoch [3907], train_loss: 471.28 with loss1: 398.04, loss2: 73.24 and loss3: 0.00\n",
      "Epoch [3908], train_loss: 470.17 with loss1: 396.91, loss2: 73.26 and loss3: 0.00\n",
      "Epoch [3909], train_loss: 470.41 with loss1: 397.22, loss2: 73.20 and loss3: 0.00\n",
      "Epoch [3910], train_loss: 469.04 with loss1: 395.88, loss2: 73.16 and loss3: 0.00\n",
      "Epoch [3911], train_loss: 468.25 with loss1: 395.08, loss2: 73.17 and loss3: 0.00\n",
      "Epoch [3912], train_loss: 468.13 with loss1: 395.07, loss2: 73.06 and loss3: 0.00\n",
      "Epoch [3913], train_loss: 468.35 with loss1: 395.16, loss2: 73.19 and loss3: 0.00\n",
      "Epoch [3914], train_loss: 467.91 with loss1: 394.97, loss2: 72.94 and loss3: 0.00\n",
      "Epoch [3915], train_loss: 469.16 with loss1: 396.13, loss2: 73.04 and loss3: 0.00\n",
      "Epoch [3916], train_loss: 467.09 with loss1: 394.24, loss2: 72.85 and loss3: 0.00\n",
      "Epoch [3917], train_loss: 467.72 with loss1: 394.71, loss2: 73.01 and loss3: 0.00\n",
      "Epoch [3918], train_loss: 467.31 with loss1: 394.53, loss2: 72.78 and loss3: 0.00\n",
      "Epoch [3919], train_loss: 467.70 with loss1: 394.70, loss2: 73.00 and loss3: 0.00\n",
      "Epoch [3920], train_loss: 468.45 with loss1: 395.77, loss2: 72.68 and loss3: 0.00\n",
      "Epoch [3921], train_loss: 468.21 with loss1: 395.37, loss2: 72.85 and loss3: 0.00\n",
      "Epoch [3922], train_loss: 467.42 with loss1: 394.78, loss2: 72.63 and loss3: 0.00\n",
      "Epoch [3923], train_loss: 467.88 with loss1: 395.12, loss2: 72.76 and loss3: 0.00\n",
      "Epoch [3924], train_loss: 467.76 with loss1: 395.19, loss2: 72.57 and loss3: 0.00\n",
      "Epoch [3925], train_loss: 468.46 with loss1: 395.81, loss2: 72.66 and loss3: 0.00\n",
      "Epoch [3926], train_loss: 469.68 with loss1: 397.31, loss2: 72.37 and loss3: 0.00\n",
      "Epoch [3927], train_loss: 471.99 with loss1: 399.37, loss2: 72.62 and loss3: 0.00\n",
      "Epoch [3928], train_loss: 471.15 with loss1: 398.85, loss2: 72.30 and loss3: 0.00\n",
      "Epoch [3929], train_loss: 475.37 with loss1: 402.85, loss2: 72.52 and loss3: 0.00\n",
      "Epoch [3930], train_loss: 475.02 with loss1: 402.75, loss2: 72.27 and loss3: 0.00\n",
      "Epoch [3931], train_loss: 478.05 with loss1: 405.63, loss2: 72.42 and loss3: 0.00\n",
      "Epoch [3932], train_loss: 476.99 with loss1: 404.92, loss2: 72.08 and loss3: 0.00\n",
      "Epoch [3933], train_loss: 480.39 with loss1: 408.05, loss2: 72.34 and loss3: 0.00\n",
      "Epoch [3934], train_loss: 479.39 with loss1: 407.26, loss2: 72.13 and loss3: 0.00\n",
      "Epoch [3935], train_loss: 483.95 with loss1: 411.57, loss2: 72.38 and loss3: 0.00\n",
      "Epoch [3936], train_loss: 483.89 with loss1: 411.92, loss2: 71.96 and loss3: 0.00\n",
      "Epoch [3937], train_loss: 486.17 with loss1: 413.88, loss2: 72.28 and loss3: 0.00\n",
      "Epoch [3938], train_loss: 485.99 with loss1: 414.07, loss2: 71.92 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3939], train_loss: 490.63 with loss1: 418.36, loss2: 72.27 and loss3: 0.00\n",
      "Epoch [3940], train_loss: 491.23 with loss1: 419.29, loss2: 71.94 and loss3: 0.00\n",
      "Epoch [3941], train_loss: 494.77 with loss1: 422.46, loss2: 72.31 and loss3: 0.00\n",
      "Epoch [3942], train_loss: 493.20 with loss1: 421.27, loss2: 71.93 and loss3: 0.00\n",
      "Epoch [3943], train_loss: 497.68 with loss1: 425.48, loss2: 72.19 and loss3: 0.00\n",
      "Epoch [3944], train_loss: 497.52 with loss1: 425.73, loss2: 71.79 and loss3: 0.00\n",
      "Epoch [3945], train_loss: 501.25 with loss1: 429.12, loss2: 72.13 and loss3: 0.00\n",
      "Epoch [3946], train_loss: 501.58 with loss1: 429.73, loss2: 71.85 and loss3: 0.00\n",
      "Epoch [3947], train_loss: 504.64 with loss1: 432.53, loss2: 72.11 and loss3: 0.00\n",
      "Epoch [3948], train_loss: 502.76 with loss1: 431.12, loss2: 71.64 and loss3: 0.00\n",
      "Epoch [3949], train_loss: 507.63 with loss1: 435.47, loss2: 72.16 and loss3: 0.00\n",
      "Epoch [3950], train_loss: 507.50 with loss1: 435.83, loss2: 71.67 and loss3: 0.00\n",
      "Epoch [3951], train_loss: 508.94 with loss1: 436.85, loss2: 72.09 and loss3: 0.00\n",
      "Epoch [3952], train_loss: 506.50 with loss1: 434.76, loss2: 71.75 and loss3: 0.00\n",
      "Epoch [3953], train_loss: 508.41 with loss1: 436.33, loss2: 72.07 and loss3: 0.00\n",
      "Epoch [3954], train_loss: 505.74 with loss1: 434.03, loss2: 71.70 and loss3: 0.00\n",
      "Epoch [3955], train_loss: 505.89 with loss1: 433.87, loss2: 72.03 and loss3: 0.00\n",
      "Epoch [3956], train_loss: 502.12 with loss1: 430.52, loss2: 71.60 and loss3: 0.00\n",
      "Epoch [3957], train_loss: 500.92 with loss1: 428.89, loss2: 72.03 and loss3: 0.00\n",
      "Epoch [3958], train_loss: 500.70 with loss1: 429.08, loss2: 71.62 and loss3: 0.00\n",
      "Epoch [3959], train_loss: 499.04 with loss1: 426.87, loss2: 72.17 and loss3: 0.00\n",
      "Epoch [3960], train_loss: 494.59 with loss1: 422.85, loss2: 71.74 and loss3: 0.00\n",
      "Epoch [3961], train_loss: 491.86 with loss1: 419.80, loss2: 72.05 and loss3: 0.00\n",
      "Epoch [3962], train_loss: 488.53 with loss1: 416.82, loss2: 71.71 and loss3: 0.00\n",
      "Epoch [3963], train_loss: 486.05 with loss1: 413.96, loss2: 72.09 and loss3: 0.00\n",
      "Epoch [3964], train_loss: 482.63 with loss1: 411.00, loss2: 71.63 and loss3: 0.00\n",
      "Epoch [3965], train_loss: 481.42 with loss1: 409.48, loss2: 71.93 and loss3: 0.00\n",
      "Epoch [3966], train_loss: 477.98 with loss1: 406.31, loss2: 71.67 and loss3: 0.00\n",
      "Epoch [3967], train_loss: 475.87 with loss1: 403.95, loss2: 71.92 and loss3: 0.00\n",
      "Epoch [3968], train_loss: 473.69 with loss1: 402.13, loss2: 71.57 and loss3: 0.00\n",
      "Epoch [3969], train_loss: 474.39 with loss1: 402.42, loss2: 71.97 and loss3: 0.00\n",
      "Epoch [3970], train_loss: 471.42 with loss1: 399.92, loss2: 71.49 and loss3: 0.00\n",
      "Epoch [3971], train_loss: 470.77 with loss1: 398.77, loss2: 72.00 and loss3: 0.00\n",
      "Epoch [3972], train_loss: 466.86 with loss1: 395.29, loss2: 71.56 and loss3: 0.00\n",
      "Epoch [3973], train_loss: 466.15 with loss1: 394.41, loss2: 71.75 and loss3: 0.00\n",
      "Epoch [3974], train_loss: 465.24 with loss1: 393.96, loss2: 71.28 and loss3: 0.00\n",
      "Epoch [3975], train_loss: 463.64 with loss1: 391.95, loss2: 71.69 and loss3: 0.00\n",
      "Epoch [3976], train_loss: 461.12 with loss1: 389.80, loss2: 71.32 and loss3: 0.00\n",
      "Epoch [3977], train_loss: 461.05 with loss1: 389.49, loss2: 71.56 and loss3: 0.00\n",
      "Epoch [3978], train_loss: 458.44 with loss1: 387.06, loss2: 71.38 and loss3: 0.00\n",
      "Epoch [3979], train_loss: 459.51 with loss1: 387.98, loss2: 71.52 and loss3: 0.00\n",
      "Epoch [3980], train_loss: 457.05 with loss1: 385.58, loss2: 71.47 and loss3: 0.00\n",
      "Epoch [3981], train_loss: 457.35 with loss1: 385.90, loss2: 71.45 and loss3: 0.00\n",
      "Epoch [3982], train_loss: 455.70 with loss1: 384.38, loss2: 71.32 and loss3: 0.00\n",
      "Epoch [3983], train_loss: 456.24 with loss1: 384.70, loss2: 71.54 and loss3: 0.00\n",
      "Epoch [3984], train_loss: 456.57 with loss1: 385.35, loss2: 71.21 and loss3: 0.00\n",
      "Epoch [3985], train_loss: 455.77 with loss1: 384.52, loss2: 71.25 and loss3: 0.00\n",
      "Epoch [3986], train_loss: 454.26 with loss1: 383.12, loss2: 71.14 and loss3: 0.00\n",
      "Epoch [3987], train_loss: 455.55 with loss1: 384.29, loss2: 71.26 and loss3: 0.00\n",
      "Epoch [3988], train_loss: 454.38 with loss1: 383.25, loss2: 71.13 and loss3: 0.00\n",
      "Epoch [3989], train_loss: 454.52 with loss1: 383.22, loss2: 71.31 and loss3: 0.00\n",
      "Epoch [3990], train_loss: 453.03 with loss1: 381.96, loss2: 71.07 and loss3: 0.00\n",
      "Epoch [3991], train_loss: 454.33 with loss1: 383.16, loss2: 71.16 and loss3: 0.00\n",
      "Epoch [3992], train_loss: 452.95 with loss1: 382.05, loss2: 70.90 and loss3: 0.00\n",
      "Epoch [3993], train_loss: 453.99 with loss1: 382.84, loss2: 71.15 and loss3: 0.00\n",
      "Epoch [3994], train_loss: 452.58 with loss1: 381.81, loss2: 70.78 and loss3: 0.00\n",
      "Epoch [3995], train_loss: 454.51 with loss1: 383.52, loss2: 70.99 and loss3: 0.00\n",
      "Epoch [3996], train_loss: 454.13 with loss1: 383.43, loss2: 70.70 and loss3: 0.00\n",
      "Epoch [3997], train_loss: 455.56 with loss1: 384.52, loss2: 71.04 and loss3: 0.00\n",
      "Epoch [3998], train_loss: 454.68 with loss1: 383.95, loss2: 70.73 and loss3: 0.00\n",
      "Epoch [3999], train_loss: 456.75 with loss1: 385.82, loss2: 70.93 and loss3: 0.00\n",
      "Epoch [4000], train_loss: 455.64 with loss1: 384.97, loss2: 70.67 and loss3: 0.00\n",
      "Epoch [4001], train_loss: 457.37 with loss1: 386.64, loss2: 70.73 and loss3: 0.00\n",
      "Epoch [4002], train_loss: 455.54 with loss1: 384.79, loss2: 70.74 and loss3: 0.00\n",
      "Epoch [4003], train_loss: 455.72 with loss1: 384.89, loss2: 70.83 and loss3: 0.00\n",
      "Epoch [4004], train_loss: 455.66 with loss1: 385.01, loss2: 70.65 and loss3: 0.00\n",
      "Epoch [4005], train_loss: 456.62 with loss1: 385.86, loss2: 70.76 and loss3: 0.00\n",
      "Epoch [4006], train_loss: 457.34 with loss1: 386.70, loss2: 70.63 and loss3: 0.00\n",
      "Epoch [4007], train_loss: 456.74 with loss1: 386.00, loss2: 70.74 and loss3: 0.00\n",
      "Epoch [4008], train_loss: 458.82 with loss1: 388.40, loss2: 70.42 and loss3: 0.00\n",
      "Epoch [4009], train_loss: 459.07 with loss1: 388.29, loss2: 70.79 and loss3: 0.00\n",
      "Epoch [4010], train_loss: 458.78 with loss1: 388.41, loss2: 70.37 and loss3: 0.00\n",
      "Epoch [4011], train_loss: 458.91 with loss1: 388.29, loss2: 70.61 and loss3: 0.00\n",
      "Epoch [4012], train_loss: 458.25 with loss1: 388.01, loss2: 70.23 and loss3: 0.00\n",
      "Epoch [4013], train_loss: 459.59 with loss1: 388.98, loss2: 70.61 and loss3: 0.00\n",
      "Epoch [4014], train_loss: 459.12 with loss1: 388.98, loss2: 70.14 and loss3: 0.00\n",
      "Epoch [4015], train_loss: 461.87 with loss1: 391.30, loss2: 70.57 and loss3: 0.00\n",
      "Epoch [4016], train_loss: 460.44 with loss1: 390.15, loss2: 70.29 and loss3: 0.00\n",
      "Epoch [4017], train_loss: 461.53 with loss1: 390.86, loss2: 70.67 and loss3: 0.00\n",
      "Epoch [4018], train_loss: 457.90 with loss1: 387.61, loss2: 70.30 and loss3: 0.00\n",
      "Epoch [4019], train_loss: 460.34 with loss1: 389.91, loss2: 70.44 and loss3: 0.00\n",
      "Epoch [4020], train_loss: 460.54 with loss1: 390.34, loss2: 70.20 and loss3: 0.00\n",
      "Epoch [4021], train_loss: 461.54 with loss1: 391.06, loss2: 70.48 and loss3: 0.00\n",
      "Epoch [4022], train_loss: 461.12 with loss1: 390.86, loss2: 70.25 and loss3: 0.00\n",
      "Epoch [4023], train_loss: 461.24 with loss1: 390.91, loss2: 70.33 and loss3: 0.00\n",
      "Epoch [4024], train_loss: 461.89 with loss1: 391.74, loss2: 70.14 and loss3: 0.00\n",
      "Epoch [4025], train_loss: 462.65 with loss1: 392.16, loss2: 70.48 and loss3: 0.00\n",
      "Epoch [4026], train_loss: 461.72 with loss1: 391.71, loss2: 70.01 and loss3: 0.00\n",
      "Epoch [4027], train_loss: 463.91 with loss1: 393.73, loss2: 70.18 and loss3: 0.00\n",
      "Epoch [4028], train_loss: 464.86 with loss1: 394.87, loss2: 69.99 and loss3: 0.00\n",
      "Epoch [4029], train_loss: 466.42 with loss1: 396.00, loss2: 70.42 and loss3: 0.00\n",
      "Epoch [4030], train_loss: 465.78 with loss1: 395.73, loss2: 70.05 and loss3: 0.00\n",
      "Epoch [4031], train_loss: 467.19 with loss1: 396.83, loss2: 70.36 and loss3: 0.00\n",
      "Epoch [4032], train_loss: 466.48 with loss1: 396.57, loss2: 69.91 and loss3: 0.00\n",
      "Epoch [4033], train_loss: 468.08 with loss1: 397.85, loss2: 70.22 and loss3: 0.00\n",
      "Epoch [4034], train_loss: 466.97 with loss1: 397.14, loss2: 69.83 and loss3: 0.00\n",
      "Epoch [4035], train_loss: 468.44 with loss1: 398.11, loss2: 70.33 and loss3: 0.00\n",
      "Epoch [4036], train_loss: 466.95 with loss1: 397.06, loss2: 69.89 and loss3: 0.00\n",
      "Epoch [4037], train_loss: 467.47 with loss1: 397.28, loss2: 70.19 and loss3: 0.00\n",
      "Epoch [4038], train_loss: 465.94 with loss1: 396.04, loss2: 69.90 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4039], train_loss: 468.00 with loss1: 397.87, loss2: 70.13 and loss3: 0.00\n",
      "Epoch [4040], train_loss: 465.78 with loss1: 395.88, loss2: 69.90 and loss3: 0.00\n",
      "Epoch [4041], train_loss: 467.52 with loss1: 397.35, loss2: 70.16 and loss3: 0.00\n",
      "Epoch [4042], train_loss: 466.20 with loss1: 396.36, loss2: 69.84 and loss3: 0.00\n",
      "Epoch [4043], train_loss: 467.67 with loss1: 397.52, loss2: 70.15 and loss3: 0.00\n",
      "Epoch [4044], train_loss: 466.48 with loss1: 396.63, loss2: 69.85 and loss3: 0.00\n",
      "Epoch [4045], train_loss: 468.79 with loss1: 398.80, loss2: 69.99 and loss3: 0.00\n",
      "Epoch [4046], train_loss: 469.41 with loss1: 399.67, loss2: 69.75 and loss3: 0.00\n",
      "Epoch [4047], train_loss: 471.04 with loss1: 400.74, loss2: 70.30 and loss3: 0.00\n",
      "Epoch [4048], train_loss: 470.86 with loss1: 401.09, loss2: 69.77 and loss3: 0.00\n",
      "Epoch [4049], train_loss: 474.38 with loss1: 404.07, loss2: 70.31 and loss3: 0.00\n",
      "Epoch [4050], train_loss: 472.40 with loss1: 402.55, loss2: 69.85 and loss3: 0.00\n",
      "Epoch [4051], train_loss: 474.59 with loss1: 404.32, loss2: 70.27 and loss3: 0.00\n",
      "Epoch [4052], train_loss: 472.44 with loss1: 402.47, loss2: 69.98 and loss3: 0.00\n",
      "Epoch [4053], train_loss: 475.12 with loss1: 404.82, loss2: 70.31 and loss3: 0.00\n",
      "Epoch [4054], train_loss: 474.23 with loss1: 404.22, loss2: 70.01 and loss3: 0.00\n",
      "Epoch [4055], train_loss: 476.17 with loss1: 405.83, loss2: 70.34 and loss3: 0.00\n",
      "Epoch [4056], train_loss: 474.73 with loss1: 404.65, loss2: 70.09 and loss3: 0.00\n",
      "Epoch [4057], train_loss: 476.45 with loss1: 405.98, loss2: 70.47 and loss3: 0.00\n",
      "Epoch [4058], train_loss: 475.54 with loss1: 405.50, loss2: 70.03 and loss3: 0.00\n",
      "Epoch [4059], train_loss: 477.19 with loss1: 406.64, loss2: 70.54 and loss3: 0.00\n",
      "Epoch [4060], train_loss: 477.27 with loss1: 407.19, loss2: 70.08 and loss3: 0.00\n",
      "Epoch [4061], train_loss: 476.44 with loss1: 405.82, loss2: 70.63 and loss3: 0.00\n",
      "Epoch [4062], train_loss: 476.99 with loss1: 406.75, loss2: 70.24 and loss3: 0.00\n",
      "Epoch [4063], train_loss: 477.60 with loss1: 407.05, loss2: 70.55 and loss3: 0.00\n",
      "Epoch [4064], train_loss: 477.27 with loss1: 407.03, loss2: 70.24 and loss3: 0.00\n",
      "Epoch [4065], train_loss: 477.30 with loss1: 406.66, loss2: 70.64 and loss3: 0.00\n",
      "Epoch [4066], train_loss: 475.19 with loss1: 404.90, loss2: 70.29 and loss3: 0.00\n",
      "Epoch [4067], train_loss: 477.72 with loss1: 407.12, loss2: 70.60 and loss3: 0.00\n",
      "Epoch [4068], train_loss: 476.39 with loss1: 406.08, loss2: 70.31 and loss3: 0.00\n",
      "Epoch [4069], train_loss: 478.20 with loss1: 407.66, loss2: 70.53 and loss3: 0.00\n",
      "Epoch [4070], train_loss: 478.09 with loss1: 407.78, loss2: 70.31 and loss3: 0.00\n",
      "Epoch [4071], train_loss: 478.42 with loss1: 407.81, loss2: 70.61 and loss3: 0.00\n",
      "Epoch [4072], train_loss: 479.70 with loss1: 409.21, loss2: 70.49 and loss3: 0.00\n",
      "Epoch [4073], train_loss: 480.14 with loss1: 409.51, loss2: 70.63 and loss3: 0.00\n",
      "Epoch [4074], train_loss: 480.54 with loss1: 410.17, loss2: 70.37 and loss3: 0.00\n",
      "Epoch [4075], train_loss: 481.02 with loss1: 410.28, loss2: 70.74 and loss3: 0.00\n",
      "Epoch [4076], train_loss: 481.26 with loss1: 410.93, loss2: 70.33 and loss3: 0.00\n",
      "Epoch [4077], train_loss: 484.04 with loss1: 413.35, loss2: 70.69 and loss3: 0.00\n",
      "Epoch [4078], train_loss: 482.03 with loss1: 411.70, loss2: 70.33 and loss3: 0.00\n",
      "Epoch [4079], train_loss: 483.67 with loss1: 413.05, loss2: 70.62 and loss3: 0.00\n",
      "Epoch [4080], train_loss: 485.34 with loss1: 415.11, loss2: 70.23 and loss3: 0.00\n",
      "Epoch [4081], train_loss: 489.74 with loss1: 419.07, loss2: 70.67 and loss3: 0.00\n",
      "Epoch [4082], train_loss: 489.35 with loss1: 419.02, loss2: 70.34 and loss3: 0.00\n",
      "Epoch [4083], train_loss: 491.33 with loss1: 420.65, loss2: 70.68 and loss3: 0.00\n",
      "Epoch [4084], train_loss: 489.37 with loss1: 418.99, loss2: 70.38 and loss3: 0.00\n",
      "Epoch [4085], train_loss: 492.08 with loss1: 421.50, loss2: 70.58 and loss3: 0.00\n",
      "Epoch [4086], train_loss: 490.83 with loss1: 420.45, loss2: 70.39 and loss3: 0.00\n",
      "Epoch [4087], train_loss: 491.41 with loss1: 420.77, loss2: 70.65 and loss3: 0.00\n",
      "Epoch [4088], train_loss: 490.53 with loss1: 420.16, loss2: 70.38 and loss3: 0.00\n",
      "Epoch [4089], train_loss: 492.39 with loss1: 421.86, loss2: 70.54 and loss3: 0.00\n",
      "Epoch [4090], train_loss: 489.97 with loss1: 419.47, loss2: 70.50 and loss3: 0.00\n",
      "Epoch [4091], train_loss: 493.26 with loss1: 422.66, loss2: 70.60 and loss3: 0.00\n",
      "Epoch [4092], train_loss: 489.27 with loss1: 418.80, loss2: 70.47 and loss3: 0.00\n",
      "Epoch [4093], train_loss: 488.47 with loss1: 417.78, loss2: 70.69 and loss3: 0.00\n",
      "Epoch [4094], train_loss: 485.32 with loss1: 414.71, loss2: 70.61 and loss3: 0.00\n",
      "Epoch [4095], train_loss: 484.93 with loss1: 414.25, loss2: 70.67 and loss3: 0.00\n",
      "Epoch [4096], train_loss: 482.68 with loss1: 412.13, loss2: 70.55 and loss3: 0.00\n",
      "Epoch [4097], train_loss: 482.07 with loss1: 411.47, loss2: 70.59 and loss3: 0.00\n",
      "Epoch [4098], train_loss: 477.73 with loss1: 407.26, loss2: 70.46 and loss3: 0.00\n",
      "Epoch [4099], train_loss: 476.34 with loss1: 405.63, loss2: 70.71 and loss3: 0.00\n",
      "Epoch [4100], train_loss: 473.30 with loss1: 402.86, loss2: 70.45 and loss3: 0.00\n",
      "Epoch [4101], train_loss: 472.64 with loss1: 402.08, loss2: 70.56 and loss3: 0.00\n",
      "Epoch [4102], train_loss: 468.63 with loss1: 398.32, loss2: 70.31 and loss3: 0.00\n",
      "Epoch [4103], train_loss: 467.68 with loss1: 397.17, loss2: 70.51 and loss3: 0.00\n",
      "Epoch [4104], train_loss: 465.60 with loss1: 395.33, loss2: 70.27 and loss3: 0.00\n",
      "Epoch [4105], train_loss: 464.65 with loss1: 394.21, loss2: 70.45 and loss3: 0.00\n",
      "Epoch [4106], train_loss: 462.52 with loss1: 392.28, loss2: 70.24 and loss3: 0.00\n",
      "Epoch [4107], train_loss: 463.08 with loss1: 392.81, loss2: 70.26 and loss3: 0.00\n",
      "Epoch [4108], train_loss: 461.32 with loss1: 391.08, loss2: 70.24 and loss3: 0.00\n",
      "Epoch [4109], train_loss: 459.90 with loss1: 389.52, loss2: 70.38 and loss3: 0.00\n",
      "Epoch [4110], train_loss: 458.44 with loss1: 388.38, loss2: 70.06 and loss3: 0.00\n",
      "Epoch [4111], train_loss: 456.17 with loss1: 385.77, loss2: 70.40 and loss3: 0.00\n",
      "Epoch [4112], train_loss: 454.31 with loss1: 384.21, loss2: 70.10 and loss3: 0.00\n",
      "Epoch [4113], train_loss: 453.77 with loss1: 383.54, loss2: 70.23 and loss3: 0.00\n",
      "Epoch [4114], train_loss: 451.53 with loss1: 381.51, loss2: 70.02 and loss3: 0.00\n",
      "Epoch [4115], train_loss: 452.05 with loss1: 381.95, loss2: 70.10 and loss3: 0.00\n",
      "Epoch [4116], train_loss: 450.93 with loss1: 380.97, loss2: 69.96 and loss3: 0.00\n",
      "Epoch [4117], train_loss: 450.91 with loss1: 380.81, loss2: 70.09 and loss3: 0.00\n",
      "Epoch [4118], train_loss: 449.32 with loss1: 379.38, loss2: 69.94 and loss3: 0.00\n",
      "Epoch [4119], train_loss: 449.36 with loss1: 379.32, loss2: 70.04 and loss3: 0.00\n",
      "Epoch [4120], train_loss: 447.74 with loss1: 378.01, loss2: 69.73 and loss3: 0.00\n",
      "Epoch [4121], train_loss: 448.52 with loss1: 378.61, loss2: 69.91 and loss3: 0.00\n",
      "Epoch [4122], train_loss: 447.71 with loss1: 378.05, loss2: 69.66 and loss3: 0.00\n",
      "Epoch [4123], train_loss: 447.72 with loss1: 377.89, loss2: 69.83 and loss3: 0.00\n",
      "Epoch [4124], train_loss: 447.01 with loss1: 377.35, loss2: 69.67 and loss3: 0.00\n",
      "Epoch [4125], train_loss: 447.88 with loss1: 378.06, loss2: 69.82 and loss3: 0.00\n",
      "Epoch [4126], train_loss: 446.08 with loss1: 376.48, loss2: 69.60 and loss3: 0.00\n",
      "Epoch [4127], train_loss: 447.29 with loss1: 377.57, loss2: 69.72 and loss3: 0.00\n",
      "Epoch [4128], train_loss: 446.43 with loss1: 376.93, loss2: 69.50 and loss3: 0.00\n",
      "Epoch [4129], train_loss: 448.49 with loss1: 378.91, loss2: 69.58 and loss3: 0.00\n",
      "Epoch [4130], train_loss: 446.55 with loss1: 377.24, loss2: 69.31 and loss3: 0.00\n",
      "Epoch [4131], train_loss: 447.82 with loss1: 378.29, loss2: 69.53 and loss3: 0.00\n",
      "Epoch [4132], train_loss: 448.40 with loss1: 379.13, loss2: 69.27 and loss3: 0.00\n",
      "Epoch [4133], train_loss: 449.51 with loss1: 380.04, loss2: 69.47 and loss3: 0.00\n",
      "Epoch [4134], train_loss: 448.08 with loss1: 378.73, loss2: 69.35 and loss3: 0.00\n",
      "Epoch [4135], train_loss: 449.11 with loss1: 379.73, loss2: 69.38 and loss3: 0.00\n",
      "Epoch [4136], train_loss: 449.02 with loss1: 379.84, loss2: 69.17 and loss3: 0.00\n",
      "Epoch [4137], train_loss: 450.77 with loss1: 381.46, loss2: 69.31 and loss3: 0.00\n",
      "Epoch [4138], train_loss: 451.21 with loss1: 382.08, loss2: 69.13 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4139], train_loss: 452.75 with loss1: 383.46, loss2: 69.29 and loss3: 0.00\n",
      "Epoch [4140], train_loss: 452.71 with loss1: 383.55, loss2: 69.16 and loss3: 0.00\n",
      "Epoch [4141], train_loss: 455.62 with loss1: 386.37, loss2: 69.26 and loss3: 0.00\n",
      "Epoch [4142], train_loss: 455.45 with loss1: 386.35, loss2: 69.10 and loss3: 0.00\n",
      "Epoch [4143], train_loss: 459.36 with loss1: 390.09, loss2: 69.27 and loss3: 0.00\n",
      "Epoch [4144], train_loss: 460.09 with loss1: 391.04, loss2: 69.05 and loss3: 0.00\n",
      "Epoch [4145], train_loss: 462.54 with loss1: 393.23, loss2: 69.31 and loss3: 0.00\n",
      "Epoch [4146], train_loss: 464.77 with loss1: 395.84, loss2: 68.92 and loss3: 0.00\n",
      "Epoch [4147], train_loss: 466.53 with loss1: 397.30, loss2: 69.23 and loss3: 0.00\n",
      "Epoch [4148], train_loss: 469.63 with loss1: 400.56, loss2: 69.07 and loss3: 0.00\n",
      "Epoch [4149], train_loss: 472.62 with loss1: 403.34, loss2: 69.28 and loss3: 0.00\n",
      "Epoch [4150], train_loss: 473.58 with loss1: 404.58, loss2: 69.00 and loss3: 0.00\n",
      "Epoch [4151], train_loss: 476.84 with loss1: 407.74, loss2: 69.10 and loss3: 0.00\n",
      "Epoch [4152], train_loss: 476.71 with loss1: 407.75, loss2: 68.96 and loss3: 0.00\n",
      "Epoch [4153], train_loss: 483.20 with loss1: 414.08, loss2: 69.11 and loss3: 0.00\n",
      "Epoch [4154], train_loss: 483.76 with loss1: 414.89, loss2: 68.87 and loss3: 0.00\n",
      "Epoch [4155], train_loss: 489.07 with loss1: 419.96, loss2: 69.11 and loss3: 0.00\n",
      "Epoch [4156], train_loss: 489.94 with loss1: 421.06, loss2: 68.88 and loss3: 0.00\n",
      "Epoch [4157], train_loss: 493.82 with loss1: 424.63, loss2: 69.18 and loss3: 0.00\n",
      "Epoch [4158], train_loss: 493.42 with loss1: 424.46, loss2: 68.96 and loss3: 0.00\n",
      "Epoch [4159], train_loss: 497.75 with loss1: 428.58, loss2: 69.17 and loss3: 0.00\n",
      "Epoch [4160], train_loss: 496.53 with loss1: 427.52, loss2: 69.01 and loss3: 0.00\n",
      "Epoch [4161], train_loss: 499.45 with loss1: 430.20, loss2: 69.25 and loss3: 0.00\n",
      "Epoch [4162], train_loss: 497.81 with loss1: 428.75, loss2: 69.06 and loss3: 0.00\n",
      "Epoch [4163], train_loss: 500.28 with loss1: 430.98, loss2: 69.30 and loss3: 0.00\n",
      "Epoch [4164], train_loss: 497.94 with loss1: 428.93, loss2: 69.01 and loss3: 0.00\n",
      "Epoch [4165], train_loss: 498.14 with loss1: 428.70, loss2: 69.44 and loss3: 0.00\n",
      "Epoch [4166], train_loss: 495.07 with loss1: 426.08, loss2: 68.99 and loss3: 0.00\n",
      "Epoch [4167], train_loss: 495.76 with loss1: 426.49, loss2: 69.27 and loss3: 0.00\n",
      "Epoch [4168], train_loss: 492.91 with loss1: 423.82, loss2: 69.09 and loss3: 0.00\n",
      "Epoch [4169], train_loss: 494.60 with loss1: 425.25, loss2: 69.35 and loss3: 0.00\n",
      "Epoch [4170], train_loss: 489.95 with loss1: 420.87, loss2: 69.09 and loss3: 0.00\n",
      "Epoch [4171], train_loss: 489.89 with loss1: 420.58, loss2: 69.32 and loss3: 0.00\n",
      "Epoch [4172], train_loss: 486.93 with loss1: 417.81, loss2: 69.13 and loss3: 0.00\n",
      "Epoch [4173], train_loss: 485.08 with loss1: 415.74, loss2: 69.34 and loss3: 0.00\n",
      "Epoch [4174], train_loss: 481.54 with loss1: 412.37, loss2: 69.17 and loss3: 0.00\n",
      "Epoch [4175], train_loss: 479.63 with loss1: 410.24, loss2: 69.40 and loss3: 0.00\n",
      "Epoch [4176], train_loss: 474.54 with loss1: 405.25, loss2: 69.28 and loss3: 0.00\n",
      "Epoch [4177], train_loss: 473.15 with loss1: 403.76, loss2: 69.39 and loss3: 0.00\n",
      "Epoch [4178], train_loss: 470.09 with loss1: 401.00, loss2: 69.08 and loss3: 0.00\n",
      "Epoch [4179], train_loss: 467.24 with loss1: 397.90, loss2: 69.34 and loss3: 0.00\n",
      "Epoch [4180], train_loss: 465.29 with loss1: 396.19, loss2: 69.10 and loss3: 0.00\n",
      "Epoch [4181], train_loss: 463.66 with loss1: 394.22, loss2: 69.43 and loss3: 0.00\n",
      "Epoch [4182], train_loss: 460.69 with loss1: 391.64, loss2: 69.05 and loss3: 0.00\n",
      "Epoch [4183], train_loss: 460.83 with loss1: 391.51, loss2: 69.32 and loss3: 0.00\n",
      "Epoch [4184], train_loss: 456.01 with loss1: 387.03, loss2: 68.97 and loss3: 0.00\n",
      "Epoch [4185], train_loss: 456.12 with loss1: 386.89, loss2: 69.23 and loss3: 0.00\n",
      "Epoch [4186], train_loss: 452.51 with loss1: 383.60, loss2: 68.91 and loss3: 0.00\n",
      "Epoch [4187], train_loss: 451.56 with loss1: 382.47, loss2: 69.10 and loss3: 0.00\n",
      "Epoch [4188], train_loss: 449.99 with loss1: 381.10, loss2: 68.88 and loss3: 0.00\n",
      "Epoch [4189], train_loss: 450.53 with loss1: 381.42, loss2: 69.11 and loss3: 0.00\n",
      "Epoch [4190], train_loss: 448.81 with loss1: 379.84, loss2: 68.98 and loss3: 0.00\n",
      "Epoch [4191], train_loss: 449.25 with loss1: 380.20, loss2: 69.05 and loss3: 0.00\n",
      "Epoch [4192], train_loss: 446.56 with loss1: 377.87, loss2: 68.69 and loss3: 0.00\n",
      "Epoch [4193], train_loss: 447.26 with loss1: 378.41, loss2: 68.84 and loss3: 0.00\n",
      "Epoch [4194], train_loss: 444.37 with loss1: 375.69, loss2: 68.68 and loss3: 0.00\n",
      "Epoch [4195], train_loss: 445.00 with loss1: 376.17, loss2: 68.83 and loss3: 0.00\n",
      "Epoch [4196], train_loss: 442.43 with loss1: 373.68, loss2: 68.75 and loss3: 0.00\n",
      "Epoch [4197], train_loss: 442.18 with loss1: 373.34, loss2: 68.83 and loss3: 0.00\n",
      "Epoch [4198], train_loss: 441.57 with loss1: 372.92, loss2: 68.64 and loss3: 0.00\n",
      "Epoch [4199], train_loss: 441.30 with loss1: 372.52, loss2: 68.78 and loss3: 0.00\n",
      "Epoch [4200], train_loss: 441.97 with loss1: 373.36, loss2: 68.61 and loss3: 0.00\n",
      "Epoch [4201], train_loss: 439.70 with loss1: 371.04, loss2: 68.66 and loss3: 0.00\n",
      "Epoch [4202], train_loss: 441.66 with loss1: 373.24, loss2: 68.42 and loss3: 0.00\n",
      "Epoch [4203], train_loss: 440.87 with loss1: 372.39, loss2: 68.49 and loss3: 0.00\n",
      "Epoch [4204], train_loss: 441.35 with loss1: 372.99, loss2: 68.35 and loss3: 0.00\n",
      "Epoch [4205], train_loss: 442.53 with loss1: 374.06, loss2: 68.47 and loss3: 0.00\n",
      "Epoch [4206], train_loss: 441.92 with loss1: 373.64, loss2: 68.28 and loss3: 0.00\n",
      "Epoch [4207], train_loss: 441.24 with loss1: 372.72, loss2: 68.52 and loss3: 0.00\n",
      "Epoch [4208], train_loss: 442.56 with loss1: 374.27, loss2: 68.30 and loss3: 0.00\n",
      "Epoch [4209], train_loss: 442.06 with loss1: 373.76, loss2: 68.29 and loss3: 0.00\n",
      "Epoch [4210], train_loss: 441.49 with loss1: 373.18, loss2: 68.32 and loss3: 0.00\n",
      "Epoch [4211], train_loss: 441.12 with loss1: 372.67, loss2: 68.44 and loss3: 0.00\n",
      "Epoch [4212], train_loss: 442.47 with loss1: 374.28, loss2: 68.19 and loss3: 0.00\n",
      "Epoch [4213], train_loss: 443.34 with loss1: 375.12, loss2: 68.22 and loss3: 0.00\n",
      "Epoch [4214], train_loss: 444.19 with loss1: 375.91, loss2: 68.28 and loss3: 0.00\n",
      "Epoch [4215], train_loss: 444.75 with loss1: 376.52, loss2: 68.23 and loss3: 0.00\n",
      "Epoch [4216], train_loss: 444.25 with loss1: 376.08, loss2: 68.17 and loss3: 0.00\n",
      "Epoch [4217], train_loss: 444.25 with loss1: 375.91, loss2: 68.34 and loss3: 0.00\n",
      "Epoch [4218], train_loss: 444.60 with loss1: 376.45, loss2: 68.15 and loss3: 0.00\n",
      "Epoch [4219], train_loss: 445.26 with loss1: 377.14, loss2: 68.12 and loss3: 0.00\n",
      "Epoch [4220], train_loss: 445.86 with loss1: 377.74, loss2: 68.11 and loss3: 0.00\n",
      "Epoch [4221], train_loss: 445.55 with loss1: 377.51, loss2: 68.04 and loss3: 0.00\n",
      "Epoch [4222], train_loss: 446.08 with loss1: 377.88, loss2: 68.20 and loss3: 0.00\n",
      "Epoch [4223], train_loss: 447.96 with loss1: 379.85, loss2: 68.11 and loss3: 0.00\n",
      "Epoch [4224], train_loss: 448.68 with loss1: 380.68, loss2: 68.01 and loss3: 0.00\n",
      "Epoch [4225], train_loss: 448.83 with loss1: 380.71, loss2: 68.12 and loss3: 0.00\n",
      "Epoch [4226], train_loss: 450.20 with loss1: 382.25, loss2: 67.95 and loss3: 0.00\n",
      "Epoch [4227], train_loss: 450.26 with loss1: 382.23, loss2: 68.03 and loss3: 0.00\n",
      "Epoch [4228], train_loss: 450.14 with loss1: 382.14, loss2: 68.00 and loss3: 0.00\n",
      "Epoch [4229], train_loss: 450.99 with loss1: 382.91, loss2: 68.09 and loss3: 0.00\n",
      "Epoch [4230], train_loss: 449.95 with loss1: 381.95, loss2: 68.00 and loss3: 0.00\n",
      "Epoch [4231], train_loss: 452.34 with loss1: 384.36, loss2: 67.97 and loss3: 0.00\n",
      "Epoch [4232], train_loss: 451.20 with loss1: 383.31, loss2: 67.89 and loss3: 0.00\n",
      "Epoch [4233], train_loss: 450.31 with loss1: 382.33, loss2: 67.98 and loss3: 0.00\n",
      "Epoch [4234], train_loss: 450.59 with loss1: 382.77, loss2: 67.81 and loss3: 0.00\n",
      "Epoch [4235], train_loss: 452.50 with loss1: 384.51, loss2: 67.98 and loss3: 0.00\n",
      "Epoch [4236], train_loss: 452.92 with loss1: 385.05, loss2: 67.86 and loss3: 0.00\n",
      "Epoch [4237], train_loss: 453.77 with loss1: 385.78, loss2: 67.99 and loss3: 0.00\n",
      "Epoch [4238], train_loss: 454.31 with loss1: 386.57, loss2: 67.74 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4239], train_loss: 453.90 with loss1: 385.85, loss2: 68.05 and loss3: 0.00\n",
      "Epoch [4240], train_loss: 451.98 with loss1: 384.18, loss2: 67.80 and loss3: 0.00\n",
      "Epoch [4241], train_loss: 452.51 with loss1: 384.66, loss2: 67.85 and loss3: 0.00\n",
      "Epoch [4242], train_loss: 452.65 with loss1: 384.83, loss2: 67.82 and loss3: 0.00\n",
      "Epoch [4243], train_loss: 451.93 with loss1: 384.07, loss2: 67.86 and loss3: 0.00\n",
      "Epoch [4244], train_loss: 453.42 with loss1: 385.67, loss2: 67.75 and loss3: 0.00\n",
      "Epoch [4245], train_loss: 453.64 with loss1: 385.74, loss2: 67.90 and loss3: 0.00\n",
      "Epoch [4246], train_loss: 453.50 with loss1: 385.79, loss2: 67.72 and loss3: 0.00\n",
      "Epoch [4247], train_loss: 452.86 with loss1: 384.93, loss2: 67.93 and loss3: 0.00\n",
      "Epoch [4248], train_loss: 453.15 with loss1: 385.44, loss2: 67.70 and loss3: 0.00\n",
      "Epoch [4249], train_loss: 452.91 with loss1: 385.16, loss2: 67.75 and loss3: 0.00\n",
      "Epoch [4250], train_loss: 452.89 with loss1: 385.32, loss2: 67.57 and loss3: 0.00\n",
      "Epoch [4251], train_loss: 453.68 with loss1: 385.80, loss2: 67.88 and loss3: 0.00\n",
      "Epoch [4252], train_loss: 453.00 with loss1: 385.36, loss2: 67.64 and loss3: 0.00\n",
      "Epoch [4253], train_loss: 452.71 with loss1: 384.98, loss2: 67.73 and loss3: 0.00\n",
      "Epoch [4254], train_loss: 452.41 with loss1: 384.85, loss2: 67.55 and loss3: 0.00\n",
      "Epoch [4255], train_loss: 452.70 with loss1: 385.07, loss2: 67.63 and loss3: 0.00\n",
      "Epoch [4256], train_loss: 450.42 with loss1: 382.97, loss2: 67.45 and loss3: 0.00\n",
      "Epoch [4257], train_loss: 452.44 with loss1: 384.78, loss2: 67.66 and loss3: 0.00\n",
      "Epoch [4258], train_loss: 452.63 with loss1: 385.23, loss2: 67.40 and loss3: 0.00\n",
      "Epoch [4259], train_loss: 452.54 with loss1: 384.86, loss2: 67.67 and loss3: 0.00\n",
      "Epoch [4260], train_loss: 452.05 with loss1: 384.67, loss2: 67.37 and loss3: 0.00\n",
      "Epoch [4261], train_loss: 453.01 with loss1: 385.23, loss2: 67.78 and loss3: 0.00\n",
      "Epoch [4262], train_loss: 453.78 with loss1: 386.57, loss2: 67.21 and loss3: 0.00\n",
      "Epoch [4263], train_loss: 455.08 with loss1: 387.51, loss2: 67.57 and loss3: 0.00\n",
      "Epoch [4264], train_loss: 455.15 with loss1: 388.00, loss2: 67.15 and loss3: 0.00\n",
      "Epoch [4265], train_loss: 455.90 with loss1: 388.20, loss2: 67.70 and loss3: 0.00\n",
      "Epoch [4266], train_loss: 457.04 with loss1: 389.90, loss2: 67.14 and loss3: 0.00\n",
      "Epoch [4267], train_loss: 459.99 with loss1: 392.39, loss2: 67.60 and loss3: 0.00\n",
      "Epoch [4268], train_loss: 460.76 with loss1: 393.52, loss2: 67.24 and loss3: 0.00\n",
      "Epoch [4269], train_loss: 463.87 with loss1: 396.20, loss2: 67.67 and loss3: 0.00\n",
      "Epoch [4270], train_loss: 462.45 with loss1: 395.31, loss2: 67.14 and loss3: 0.00\n",
      "Epoch [4271], train_loss: 464.23 with loss1: 396.66, loss2: 67.57 and loss3: 0.00\n",
      "Epoch [4272], train_loss: 463.19 with loss1: 396.02, loss2: 67.17 and loss3: 0.00\n",
      "Epoch [4273], train_loss: 466.24 with loss1: 398.57, loss2: 67.67 and loss3: 0.00\n",
      "Epoch [4274], train_loss: 463.89 with loss1: 396.59, loss2: 67.30 and loss3: 0.00\n",
      "Epoch [4275], train_loss: 466.71 with loss1: 399.14, loss2: 67.57 and loss3: 0.00\n",
      "Epoch [4276], train_loss: 464.45 with loss1: 397.31, loss2: 67.14 and loss3: 0.00\n",
      "Epoch [4277], train_loss: 466.14 with loss1: 398.58, loss2: 67.56 and loss3: 0.00\n",
      "Epoch [4278], train_loss: 464.76 with loss1: 397.66, loss2: 67.10 and loss3: 0.00\n",
      "Epoch [4279], train_loss: 467.05 with loss1: 399.47, loss2: 67.58 and loss3: 0.00\n",
      "Epoch [4280], train_loss: 464.76 with loss1: 397.59, loss2: 67.17 and loss3: 0.00\n",
      "Epoch [4281], train_loss: 466.52 with loss1: 399.00, loss2: 67.53 and loss3: 0.00\n",
      "Epoch [4282], train_loss: 465.73 with loss1: 398.56, loss2: 67.17 and loss3: 0.00\n",
      "Epoch [4283], train_loss: 468.14 with loss1: 400.49, loss2: 67.65 and loss3: 0.00\n",
      "Epoch [4284], train_loss: 462.94 with loss1: 395.73, loss2: 67.21 and loss3: 0.00\n",
      "Epoch [4285], train_loss: 465.53 with loss1: 397.89, loss2: 67.64 and loss3: 0.00\n",
      "Epoch [4286], train_loss: 462.07 with loss1: 394.81, loss2: 67.27 and loss3: 0.00\n",
      "Epoch [4287], train_loss: 462.29 with loss1: 394.71, loss2: 67.58 and loss3: 0.00\n",
      "Epoch [4288], train_loss: 461.21 with loss1: 393.98, loss2: 67.23 and loss3: 0.00\n",
      "Epoch [4289], train_loss: 461.22 with loss1: 393.64, loss2: 67.59 and loss3: 0.00\n",
      "Epoch [4290], train_loss: 459.97 with loss1: 392.88, loss2: 67.10 and loss3: 0.00\n",
      "Epoch [4291], train_loss: 460.01 with loss1: 392.39, loss2: 67.62 and loss3: 0.00\n",
      "Epoch [4292], train_loss: 459.51 with loss1: 392.38, loss2: 67.13 and loss3: 0.00\n",
      "Epoch [4293], train_loss: 460.92 with loss1: 393.30, loss2: 67.62 and loss3: 0.00\n",
      "Epoch [4294], train_loss: 459.00 with loss1: 391.66, loss2: 67.34 and loss3: 0.00\n",
      "Epoch [4295], train_loss: 461.80 with loss1: 394.27, loss2: 67.52 and loss3: 0.00\n",
      "Epoch [4296], train_loss: 458.51 with loss1: 391.36, loss2: 67.14 and loss3: 0.00\n",
      "Epoch [4297], train_loss: 459.97 with loss1: 392.39, loss2: 67.58 and loss3: 0.00\n",
      "Epoch [4298], train_loss: 458.77 with loss1: 391.63, loss2: 67.14 and loss3: 0.00\n",
      "Epoch [4299], train_loss: 458.64 with loss1: 391.19, loss2: 67.46 and loss3: 0.00\n",
      "Epoch [4300], train_loss: 457.41 with loss1: 390.25, loss2: 67.15 and loss3: 0.00\n",
      "Epoch [4301], train_loss: 458.56 with loss1: 391.04, loss2: 67.52 and loss3: 0.00\n",
      "Epoch [4302], train_loss: 457.34 with loss1: 390.26, loss2: 67.08 and loss3: 0.00\n",
      "Epoch [4303], train_loss: 458.09 with loss1: 390.70, loss2: 67.39 and loss3: 0.00\n",
      "Epoch [4304], train_loss: 456.54 with loss1: 389.31, loss2: 67.23 and loss3: 0.00\n",
      "Epoch [4305], train_loss: 457.59 with loss1: 390.18, loss2: 67.41 and loss3: 0.00\n",
      "Epoch [4306], train_loss: 456.61 with loss1: 389.58, loss2: 67.03 and loss3: 0.00\n",
      "Epoch [4307], train_loss: 457.24 with loss1: 389.78, loss2: 67.46 and loss3: 0.00\n",
      "Epoch [4308], train_loss: 459.03 with loss1: 392.09, loss2: 66.95 and loss3: 0.00\n",
      "Epoch [4309], train_loss: 459.71 with loss1: 392.33, loss2: 67.38 and loss3: 0.00\n",
      "Epoch [4310], train_loss: 458.95 with loss1: 391.91, loss2: 67.04 and loss3: 0.00\n",
      "Epoch [4311], train_loss: 461.28 with loss1: 394.04, loss2: 67.24 and loss3: 0.00\n",
      "Epoch [4312], train_loss: 460.73 with loss1: 393.86, loss2: 66.87 and loss3: 0.00\n",
      "Epoch [4313], train_loss: 463.97 with loss1: 396.83, loss2: 67.14 and loss3: 0.00\n",
      "Epoch [4314], train_loss: 463.36 with loss1: 396.49, loss2: 66.87 and loss3: 0.00\n",
      "Epoch [4315], train_loss: 465.28 with loss1: 398.07, loss2: 67.22 and loss3: 0.00\n",
      "Epoch [4316], train_loss: 465.33 with loss1: 398.45, loss2: 66.88 and loss3: 0.00\n",
      "Epoch [4317], train_loss: 467.78 with loss1: 400.46, loss2: 67.32 and loss3: 0.00\n",
      "Epoch [4318], train_loss: 465.71 with loss1: 398.95, loss2: 66.76 and loss3: 0.00\n",
      "Epoch [4319], train_loss: 467.19 with loss1: 399.88, loss2: 67.30 and loss3: 0.00\n",
      "Epoch [4320], train_loss: 466.75 with loss1: 399.91, loss2: 66.84 and loss3: 0.00\n",
      "Epoch [4321], train_loss: 470.92 with loss1: 403.75, loss2: 67.17 and loss3: 0.00\n",
      "Epoch [4322], train_loss: 470.52 with loss1: 403.75, loss2: 66.77 and loss3: 0.00\n",
      "Epoch [4323], train_loss: 473.62 with loss1: 406.47, loss2: 67.15 and loss3: 0.00\n",
      "Epoch [4324], train_loss: 473.97 with loss1: 407.19, loss2: 66.78 and loss3: 0.00\n",
      "Epoch [4325], train_loss: 475.42 with loss1: 408.28, loss2: 67.14 and loss3: 0.00\n",
      "Epoch [4326], train_loss: 473.40 with loss1: 406.66, loss2: 66.74 and loss3: 0.00\n",
      "Epoch [4327], train_loss: 476.39 with loss1: 409.23, loss2: 67.16 and loss3: 0.00\n",
      "Epoch [4328], train_loss: 475.15 with loss1: 408.30, loss2: 66.86 and loss3: 0.00\n",
      "Epoch [4329], train_loss: 478.06 with loss1: 410.98, loss2: 67.07 and loss3: 0.00\n",
      "Epoch [4330], train_loss: 477.39 with loss1: 410.60, loss2: 66.79 and loss3: 0.00\n",
      "Epoch [4331], train_loss: 479.98 with loss1: 412.80, loss2: 67.18 and loss3: 0.00\n",
      "Epoch [4332], train_loss: 477.52 with loss1: 410.60, loss2: 66.92 and loss3: 0.00\n",
      "Epoch [4333], train_loss: 478.49 with loss1: 411.36, loss2: 67.13 and loss3: 0.00\n",
      "Epoch [4334], train_loss: 476.98 with loss1: 410.02, loss2: 66.95 and loss3: 0.00\n",
      "Epoch [4335], train_loss: 478.22 with loss1: 411.17, loss2: 67.05 and loss3: 0.00\n",
      "Epoch [4336], train_loss: 476.17 with loss1: 409.42, loss2: 66.75 and loss3: 0.00\n",
      "Epoch [4337], train_loss: 474.90 with loss1: 407.93, loss2: 66.97 and loss3: 0.00\n",
      "Epoch [4338], train_loss: 472.97 with loss1: 406.09, loss2: 66.88 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4339], train_loss: 473.05 with loss1: 405.90, loss2: 67.15 and loss3: 0.00\n",
      "Epoch [4340], train_loss: 469.85 with loss1: 403.01, loss2: 66.83 and loss3: 0.00\n",
      "Epoch [4341], train_loss: 469.48 with loss1: 402.36, loss2: 67.12 and loss3: 0.00\n",
      "Epoch [4342], train_loss: 466.05 with loss1: 399.33, loss2: 66.72 and loss3: 0.00\n",
      "Epoch [4343], train_loss: 465.91 with loss1: 398.89, loss2: 67.02 and loss3: 0.00\n",
      "Epoch [4344], train_loss: 463.80 with loss1: 396.98, loss2: 66.81 and loss3: 0.00\n",
      "Epoch [4345], train_loss: 464.05 with loss1: 397.00, loss2: 67.05 and loss3: 0.00\n",
      "Epoch [4346], train_loss: 462.03 with loss1: 395.26, loss2: 66.76 and loss3: 0.00\n",
      "Epoch [4347], train_loss: 460.08 with loss1: 393.13, loss2: 66.94 and loss3: 0.00\n",
      "Epoch [4348], train_loss: 458.55 with loss1: 391.79, loss2: 66.76 and loss3: 0.00\n",
      "Epoch [4349], train_loss: 456.61 with loss1: 389.64, loss2: 66.96 and loss3: 0.00\n",
      "Epoch [4350], train_loss: 456.14 with loss1: 389.43, loss2: 66.71 and loss3: 0.00\n",
      "Epoch [4351], train_loss: 453.00 with loss1: 386.06, loss2: 66.95 and loss3: 0.00\n",
      "Epoch [4352], train_loss: 451.23 with loss1: 384.60, loss2: 66.63 and loss3: 0.00\n",
      "Epoch [4353], train_loss: 452.79 with loss1: 385.95, loss2: 66.84 and loss3: 0.00\n",
      "Epoch [4354], train_loss: 451.62 with loss1: 384.95, loss2: 66.67 and loss3: 0.00\n",
      "Epoch [4355], train_loss: 450.79 with loss1: 383.98, loss2: 66.81 and loss3: 0.00\n",
      "Epoch [4356], train_loss: 450.78 with loss1: 384.25, loss2: 66.52 and loss3: 0.00\n",
      "Epoch [4357], train_loss: 448.87 with loss1: 382.19, loss2: 66.67 and loss3: 0.00\n",
      "Epoch [4358], train_loss: 449.04 with loss1: 382.47, loss2: 66.57 and loss3: 0.00\n",
      "Epoch [4359], train_loss: 448.50 with loss1: 381.84, loss2: 66.65 and loss3: 0.00\n",
      "Epoch [4360], train_loss: 445.44 with loss1: 378.91, loss2: 66.53 and loss3: 0.00\n",
      "Epoch [4361], train_loss: 444.81 with loss1: 378.28, loss2: 66.53 and loss3: 0.00\n",
      "Epoch [4362], train_loss: 444.82 with loss1: 378.21, loss2: 66.61 and loss3: 0.00\n",
      "Epoch [4363], train_loss: 442.79 with loss1: 376.16, loss2: 66.63 and loss3: 0.00\n",
      "Epoch [4364], train_loss: 441.68 with loss1: 375.20, loss2: 66.48 and loss3: 0.00\n",
      "Epoch [4365], train_loss: 443.09 with loss1: 376.45, loss2: 66.64 and loss3: 0.00\n",
      "Epoch [4366], train_loss: 442.73 with loss1: 376.35, loss2: 66.38 and loss3: 0.00\n",
      "Epoch [4367], train_loss: 442.50 with loss1: 375.97, loss2: 66.53 and loss3: 0.00\n",
      "Epoch [4368], train_loss: 440.62 with loss1: 374.19, loss2: 66.42 and loss3: 0.00\n",
      "Epoch [4369], train_loss: 441.59 with loss1: 374.96, loss2: 66.63 and loss3: 0.00\n",
      "Epoch [4370], train_loss: 440.10 with loss1: 373.79, loss2: 66.31 and loss3: 0.00\n",
      "Epoch [4371], train_loss: 439.49 with loss1: 373.04, loss2: 66.45 and loss3: 0.00\n",
      "Epoch [4372], train_loss: 438.81 with loss1: 372.62, loss2: 66.19 and loss3: 0.00\n",
      "Epoch [4373], train_loss: 437.38 with loss1: 370.95, loss2: 66.43 and loss3: 0.00\n",
      "Epoch [4374], train_loss: 438.34 with loss1: 372.04, loss2: 66.30 and loss3: 0.00\n",
      "Epoch [4375], train_loss: 438.07 with loss1: 371.81, loss2: 66.26 and loss3: 0.00\n",
      "Epoch [4376], train_loss: 439.06 with loss1: 372.83, loss2: 66.23 and loss3: 0.00\n",
      "Epoch [4377], train_loss: 440.44 with loss1: 374.30, loss2: 66.13 and loss3: 0.00\n",
      "Epoch [4378], train_loss: 439.20 with loss1: 373.06, loss2: 66.14 and loss3: 0.00\n",
      "Epoch [4379], train_loss: 438.68 with loss1: 372.47, loss2: 66.20 and loss3: 0.00\n",
      "Epoch [4380], train_loss: 439.20 with loss1: 373.13, loss2: 66.08 and loss3: 0.00\n",
      "Epoch [4381], train_loss: 439.74 with loss1: 373.43, loss2: 66.31 and loss3: 0.00\n",
      "Epoch [4382], train_loss: 439.03 with loss1: 372.84, loss2: 66.18 and loss3: 0.00\n",
      "Epoch [4383], train_loss: 441.29 with loss1: 375.21, loss2: 66.09 and loss3: 0.00\n",
      "Epoch [4384], train_loss: 441.10 with loss1: 375.09, loss2: 66.01 and loss3: 0.00\n",
      "Epoch [4385], train_loss: 440.54 with loss1: 374.41, loss2: 66.12 and loss3: 0.00\n",
      "Epoch [4386], train_loss: 439.45 with loss1: 373.52, loss2: 65.93 and loss3: 0.00\n",
      "Epoch [4387], train_loss: 442.42 with loss1: 376.44, loss2: 65.98 and loss3: 0.00\n",
      "Epoch [4388], train_loss: 442.41 with loss1: 376.46, loss2: 65.95 and loss3: 0.00\n",
      "Epoch [4389], train_loss: 442.93 with loss1: 376.90, loss2: 66.03 and loss3: 0.00\n",
      "Epoch [4390], train_loss: 441.67 with loss1: 375.65, loss2: 66.02 and loss3: 0.00\n",
      "Epoch [4391], train_loss: 443.31 with loss1: 377.27, loss2: 66.04 and loss3: 0.00\n",
      "Epoch [4392], train_loss: 443.38 with loss1: 377.57, loss2: 65.81 and loss3: 0.00\n",
      "Epoch [4393], train_loss: 443.70 with loss1: 377.63, loss2: 66.07 and loss3: 0.00\n",
      "Epoch [4394], train_loss: 442.55 with loss1: 376.63, loss2: 65.92 and loss3: 0.00\n",
      "Epoch [4395], train_loss: 446.14 with loss1: 380.28, loss2: 65.85 and loss3: 0.00\n",
      "Epoch [4396], train_loss: 444.72 with loss1: 378.92, loss2: 65.80 and loss3: 0.00\n",
      "Epoch [4397], train_loss: 446.89 with loss1: 380.94, loss2: 65.95 and loss3: 0.00\n",
      "Epoch [4398], train_loss: 446.12 with loss1: 380.37, loss2: 65.75 and loss3: 0.00\n",
      "Epoch [4399], train_loss: 447.41 with loss1: 381.35, loss2: 66.06 and loss3: 0.00\n",
      "Epoch [4400], train_loss: 446.86 with loss1: 381.17, loss2: 65.69 and loss3: 0.00\n",
      "Epoch [4401], train_loss: 447.71 with loss1: 381.76, loss2: 65.96 and loss3: 0.00\n",
      "Epoch [4402], train_loss: 447.80 with loss1: 382.14, loss2: 65.66 and loss3: 0.00\n",
      "Epoch [4403], train_loss: 449.15 with loss1: 383.17, loss2: 65.98 and loss3: 0.00\n",
      "Epoch [4404], train_loss: 449.24 with loss1: 383.46, loss2: 65.78 and loss3: 0.00\n",
      "Epoch [4405], train_loss: 450.70 with loss1: 384.64, loss2: 66.06 and loss3: 0.00\n",
      "Epoch [4406], train_loss: 449.30 with loss1: 383.59, loss2: 65.71 and loss3: 0.00\n",
      "Epoch [4407], train_loss: 451.87 with loss1: 385.96, loss2: 65.91 and loss3: 0.00\n",
      "Epoch [4408], train_loss: 452.45 with loss1: 386.81, loss2: 65.64 and loss3: 0.00\n",
      "Epoch [4409], train_loss: 454.37 with loss1: 388.45, loss2: 65.92 and loss3: 0.00\n",
      "Epoch [4410], train_loss: 452.70 with loss1: 386.97, loss2: 65.73 and loss3: 0.00\n",
      "Epoch [4411], train_loss: 453.43 with loss1: 387.56, loss2: 65.87 and loss3: 0.00\n",
      "Epoch [4412], train_loss: 451.56 with loss1: 385.76, loss2: 65.80 and loss3: 0.00\n",
      "Epoch [4413], train_loss: 452.55 with loss1: 386.69, loss2: 65.86 and loss3: 0.00\n",
      "Epoch [4414], train_loss: 451.72 with loss1: 386.09, loss2: 65.63 and loss3: 0.00\n",
      "Epoch [4415], train_loss: 452.12 with loss1: 386.25, loss2: 65.87 and loss3: 0.00\n",
      "Epoch [4416], train_loss: 449.56 with loss1: 383.81, loss2: 65.75 and loss3: 0.00\n",
      "Epoch [4417], train_loss: 451.61 with loss1: 385.73, loss2: 65.88 and loss3: 0.00\n",
      "Epoch [4418], train_loss: 449.20 with loss1: 383.50, loss2: 65.70 and loss3: 0.00\n",
      "Epoch [4419], train_loss: 450.19 with loss1: 384.29, loss2: 65.91 and loss3: 0.00\n",
      "Epoch [4420], train_loss: 448.79 with loss1: 383.05, loss2: 65.74 and loss3: 0.00\n",
      "Epoch [4421], train_loss: 449.89 with loss1: 384.14, loss2: 65.75 and loss3: 0.00\n",
      "Epoch [4422], train_loss: 449.81 with loss1: 384.11, loss2: 65.70 and loss3: 0.00\n",
      "Epoch [4423], train_loss: 450.17 with loss1: 384.29, loss2: 65.88 and loss3: 0.00\n",
      "Epoch [4424], train_loss: 448.03 with loss1: 382.56, loss2: 65.47 and loss3: 0.00\n",
      "Epoch [4425], train_loss: 448.24 with loss1: 382.35, loss2: 65.89 and loss3: 0.00\n",
      "Epoch [4426], train_loss: 446.82 with loss1: 381.33, loss2: 65.49 and loss3: 0.00\n",
      "Epoch [4427], train_loss: 446.80 with loss1: 381.09, loss2: 65.72 and loss3: 0.00\n",
      "Epoch [4428], train_loss: 444.54 with loss1: 378.95, loss2: 65.59 and loss3: 0.00\n",
      "Epoch [4429], train_loss: 444.31 with loss1: 378.47, loss2: 65.83 and loss3: 0.00\n",
      "Epoch [4430], train_loss: 444.11 with loss1: 378.61, loss2: 65.50 and loss3: 0.00\n",
      "Epoch [4431], train_loss: 444.36 with loss1: 378.66, loss2: 65.70 and loss3: 0.00\n",
      "Epoch [4432], train_loss: 442.71 with loss1: 377.36, loss2: 65.36 and loss3: 0.00\n",
      "Epoch [4433], train_loss: 443.00 with loss1: 377.45, loss2: 65.56 and loss3: 0.00\n",
      "Epoch [4434], train_loss: 441.62 with loss1: 376.27, loss2: 65.35 and loss3: 0.00\n",
      "Epoch [4435], train_loss: 443.11 with loss1: 377.55, loss2: 65.57 and loss3: 0.00\n",
      "Epoch [4436], train_loss: 442.73 with loss1: 377.63, loss2: 65.10 and loss3: 0.00\n",
      "Epoch [4437], train_loss: 443.38 with loss1: 377.79, loss2: 65.59 and loss3: 0.00\n",
      "Epoch [4438], train_loss: 442.31 with loss1: 377.07, loss2: 65.23 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4439], train_loss: 443.30 with loss1: 377.83, loss2: 65.47 and loss3: 0.00\n",
      "Epoch [4440], train_loss: 444.73 with loss1: 379.53, loss2: 65.19 and loss3: 0.00\n",
      "Epoch [4441], train_loss: 446.00 with loss1: 380.48, loss2: 65.52 and loss3: 0.00\n",
      "Epoch [4442], train_loss: 446.50 with loss1: 381.36, loss2: 65.13 and loss3: 0.00\n",
      "Epoch [4443], train_loss: 448.89 with loss1: 383.54, loss2: 65.35 and loss3: 0.00\n",
      "Epoch [4444], train_loss: 448.70 with loss1: 383.55, loss2: 65.15 and loss3: 0.00\n",
      "Epoch [4445], train_loss: 453.43 with loss1: 388.15, loss2: 65.28 and loss3: 0.00\n",
      "Epoch [4446], train_loss: 453.11 with loss1: 387.94, loss2: 65.16 and loss3: 0.00\n",
      "Epoch [4447], train_loss: 457.98 with loss1: 392.74, loss2: 65.24 and loss3: 0.00\n",
      "Epoch [4448], train_loss: 458.70 with loss1: 393.66, loss2: 65.04 and loss3: 0.00\n",
      "Epoch [4449], train_loss: 461.92 with loss1: 396.56, loss2: 65.36 and loss3: 0.00\n",
      "Epoch [4450], train_loss: 461.59 with loss1: 396.47, loss2: 65.11 and loss3: 0.00\n",
      "Epoch [4451], train_loss: 465.72 with loss1: 400.49, loss2: 65.23 and loss3: 0.00\n",
      "Epoch [4452], train_loss: 467.75 with loss1: 402.57, loss2: 65.17 and loss3: 0.00\n",
      "Epoch [4453], train_loss: 471.10 with loss1: 405.75, loss2: 65.35 and loss3: 0.00\n",
      "Epoch [4454], train_loss: 471.53 with loss1: 406.33, loss2: 65.21 and loss3: 0.00\n",
      "Epoch [4455], train_loss: 475.79 with loss1: 410.41, loss2: 65.38 and loss3: 0.00\n",
      "Epoch [4456], train_loss: 476.75 with loss1: 411.59, loss2: 65.16 and loss3: 0.00\n",
      "Epoch [4457], train_loss: 481.31 with loss1: 415.77, loss2: 65.54 and loss3: 0.00\n",
      "Epoch [4458], train_loss: 480.68 with loss1: 415.38, loss2: 65.30 and loss3: 0.00\n",
      "Epoch [4459], train_loss: 480.98 with loss1: 415.26, loss2: 65.72 and loss3: 0.00\n",
      "Epoch [4460], train_loss: 480.02 with loss1: 414.62, loss2: 65.40 and loss3: 0.00\n",
      "Epoch [4461], train_loss: 480.37 with loss1: 414.56, loss2: 65.81 and loss3: 0.00\n",
      "Epoch [4462], train_loss: 479.72 with loss1: 414.19, loss2: 65.54 and loss3: 0.00\n",
      "Epoch [4463], train_loss: 478.20 with loss1: 412.33, loss2: 65.87 and loss3: 0.00\n",
      "Epoch [4464], train_loss: 475.90 with loss1: 410.21, loss2: 65.69 and loss3: 0.00\n",
      "Epoch [4465], train_loss: 473.62 with loss1: 407.58, loss2: 66.05 and loss3: 0.00\n",
      "Epoch [4466], train_loss: 470.22 with loss1: 404.47, loss2: 65.75 and loss3: 0.00\n",
      "Epoch [4467], train_loss: 467.14 with loss1: 401.14, loss2: 66.00 and loss3: 0.00\n",
      "Epoch [4468], train_loss: 464.25 with loss1: 398.45, loss2: 65.80 and loss3: 0.00\n",
      "Epoch [4469], train_loss: 463.17 with loss1: 397.10, loss2: 66.07 and loss3: 0.00\n",
      "Epoch [4470], train_loss: 458.89 with loss1: 393.08, loss2: 65.81 and loss3: 0.00\n",
      "Epoch [4471], train_loss: 453.73 with loss1: 387.77, loss2: 65.96 and loss3: 0.00\n",
      "Epoch [4472], train_loss: 452.12 with loss1: 386.23, loss2: 65.89 and loss3: 0.00\n",
      "Epoch [4473], train_loss: 449.75 with loss1: 383.75, loss2: 65.99 and loss3: 0.00\n",
      "Epoch [4474], train_loss: 447.06 with loss1: 381.14, loss2: 65.92 and loss3: 0.00\n",
      "Epoch [4475], train_loss: 443.95 with loss1: 377.89, loss2: 66.06 and loss3: 0.00\n",
      "Epoch [4476], train_loss: 440.36 with loss1: 374.49, loss2: 65.87 and loss3: 0.00\n",
      "Epoch [4477], train_loss: 438.96 with loss1: 373.03, loss2: 65.93 and loss3: 0.00\n",
      "Epoch [4478], train_loss: 436.74 with loss1: 371.05, loss2: 65.69 and loss3: 0.00\n",
      "Epoch [4479], train_loss: 437.06 with loss1: 371.16, loss2: 65.90 and loss3: 0.00\n",
      "Epoch [4480], train_loss: 435.67 with loss1: 369.99, loss2: 65.68 and loss3: 0.00\n",
      "Epoch [4481], train_loss: 434.73 with loss1: 368.81, loss2: 65.92 and loss3: 0.00\n",
      "Epoch [4482], train_loss: 432.74 with loss1: 367.08, loss2: 65.66 and loss3: 0.00\n",
      "Epoch [4483], train_loss: 431.35 with loss1: 365.49, loss2: 65.87 and loss3: 0.00\n",
      "Epoch [4484], train_loss: 430.92 with loss1: 365.33, loss2: 65.59 and loss3: 0.00\n",
      "Epoch [4485], train_loss: 429.77 with loss1: 364.00, loss2: 65.77 and loss3: 0.00\n",
      "Epoch [4486], train_loss: 429.98 with loss1: 364.50, loss2: 65.48 and loss3: 0.00\n",
      "Epoch [4487], train_loss: 429.26 with loss1: 363.48, loss2: 65.78 and loss3: 0.00\n",
      "Epoch [4488], train_loss: 428.54 with loss1: 363.18, loss2: 65.36 and loss3: 0.00\n",
      "Epoch [4489], train_loss: 427.83 with loss1: 362.24, loss2: 65.59 and loss3: 0.00\n",
      "Epoch [4490], train_loss: 428.16 with loss1: 362.89, loss2: 65.27 and loss3: 0.00\n",
      "Epoch [4491], train_loss: 427.18 with loss1: 361.69, loss2: 65.50 and loss3: 0.00\n",
      "Epoch [4492], train_loss: 426.92 with loss1: 361.54, loss2: 65.38 and loss3: 0.00\n",
      "Epoch [4493], train_loss: 427.37 with loss1: 361.87, loss2: 65.51 and loss3: 0.00\n",
      "Epoch [4494], train_loss: 425.82 with loss1: 360.52, loss2: 65.29 and loss3: 0.00\n",
      "Epoch [4495], train_loss: 426.86 with loss1: 361.51, loss2: 65.35 and loss3: 0.00\n",
      "Epoch [4496], train_loss: 425.50 with loss1: 360.25, loss2: 65.26 and loss3: 0.00\n",
      "Epoch [4497], train_loss: 425.05 with loss1: 359.81, loss2: 65.25 and loss3: 0.00\n",
      "Epoch [4498], train_loss: 424.35 with loss1: 359.39, loss2: 64.96 and loss3: 0.00\n",
      "Epoch [4499], train_loss: 426.23 with loss1: 360.98, loss2: 65.25 and loss3: 0.00\n",
      "Epoch [4500], train_loss: 425.04 with loss1: 360.00, loss2: 65.03 and loss3: 0.00\n",
      "Epoch [4501], train_loss: 424.95 with loss1: 359.76, loss2: 65.20 and loss3: 0.00\n",
      "Epoch [4502], train_loss: 424.43 with loss1: 359.35, loss2: 65.08 and loss3: 0.00\n",
      "Epoch [4503], train_loss: 424.79 with loss1: 359.61, loss2: 65.18 and loss3: 0.00\n",
      "Epoch [4504], train_loss: 423.69 with loss1: 358.73, loss2: 64.96 and loss3: 0.00\n",
      "Epoch [4505], train_loss: 424.77 with loss1: 359.80, loss2: 64.97 and loss3: 0.00\n",
      "Epoch [4506], train_loss: 424.61 with loss1: 359.65, loss2: 64.97 and loss3: 0.00\n",
      "Epoch [4507], train_loss: 423.94 with loss1: 358.98, loss2: 64.96 and loss3: 0.00\n",
      "Epoch [4508], train_loss: 423.44 with loss1: 358.56, loss2: 64.88 and loss3: 0.00\n",
      "Epoch [4509], train_loss: 423.88 with loss1: 358.94, loss2: 64.95 and loss3: 0.00\n",
      "Epoch [4510], train_loss: 424.46 with loss1: 359.74, loss2: 64.72 and loss3: 0.00\n",
      "Epoch [4511], train_loss: 425.16 with loss1: 360.31, loss2: 64.86 and loss3: 0.00\n",
      "Epoch [4512], train_loss: 424.59 with loss1: 359.88, loss2: 64.71 and loss3: 0.00\n",
      "Epoch [4513], train_loss: 425.09 with loss1: 360.23, loss2: 64.86 and loss3: 0.00\n",
      "Epoch [4514], train_loss: 424.44 with loss1: 359.79, loss2: 64.65 and loss3: 0.00\n",
      "Epoch [4515], train_loss: 425.94 with loss1: 361.15, loss2: 64.79 and loss3: 0.00\n",
      "Epoch [4516], train_loss: 424.39 with loss1: 359.87, loss2: 64.52 and loss3: 0.00\n",
      "Epoch [4517], train_loss: 424.29 with loss1: 359.56, loss2: 64.74 and loss3: 0.00\n",
      "Epoch [4518], train_loss: 423.72 with loss1: 359.22, loss2: 64.49 and loss3: 0.00\n",
      "Epoch [4519], train_loss: 425.33 with loss1: 360.81, loss2: 64.52 and loss3: 0.00\n",
      "Epoch [4520], train_loss: 424.01 with loss1: 359.59, loss2: 64.42 and loss3: 0.00\n",
      "Epoch [4521], train_loss: 424.54 with loss1: 360.08, loss2: 64.46 and loss3: 0.00\n",
      "Epoch [4522], train_loss: 424.13 with loss1: 359.77, loss2: 64.35 and loss3: 0.00\n",
      "Epoch [4523], train_loss: 424.89 with loss1: 360.40, loss2: 64.49 and loss3: 0.00\n",
      "Epoch [4524], train_loss: 424.80 with loss1: 360.51, loss2: 64.29 and loss3: 0.00\n",
      "Epoch [4525], train_loss: 424.92 with loss1: 360.56, loss2: 64.37 and loss3: 0.00\n",
      "Epoch [4526], train_loss: 424.46 with loss1: 360.11, loss2: 64.35 and loss3: 0.00\n",
      "Epoch [4527], train_loss: 424.92 with loss1: 360.49, loss2: 64.43 and loss3: 0.00\n",
      "Epoch [4528], train_loss: 423.41 with loss1: 359.37, loss2: 64.04 and loss3: 0.00\n",
      "Epoch [4529], train_loss: 424.22 with loss1: 359.97, loss2: 64.25 and loss3: 0.00\n",
      "Epoch [4530], train_loss: 424.46 with loss1: 360.34, loss2: 64.12 and loss3: 0.00\n",
      "Epoch [4531], train_loss: 424.65 with loss1: 360.31, loss2: 64.34 and loss3: 0.00\n",
      "Epoch [4532], train_loss: 423.58 with loss1: 359.46, loss2: 64.12 and loss3: 0.00\n",
      "Epoch [4533], train_loss: 424.41 with loss1: 360.13, loss2: 64.27 and loss3: 0.00\n",
      "Epoch [4534], train_loss: 424.41 with loss1: 360.37, loss2: 64.04 and loss3: 0.00\n",
      "Epoch [4535], train_loss: 426.53 with loss1: 362.28, loss2: 64.25 and loss3: 0.00\n",
      "Epoch [4536], train_loss: 423.62 with loss1: 359.55, loss2: 64.07 and loss3: 0.00\n",
      "Epoch [4537], train_loss: 423.49 with loss1: 359.46, loss2: 64.03 and loss3: 0.00\n",
      "Epoch [4538], train_loss: 424.47 with loss1: 360.54, loss2: 63.92 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4539], train_loss: 425.73 with loss1: 361.62, loss2: 64.10 and loss3: 0.00\n",
      "Epoch [4540], train_loss: 425.08 with loss1: 361.12, loss2: 63.95 and loss3: 0.00\n",
      "Epoch [4541], train_loss: 426.52 with loss1: 362.39, loss2: 64.12 and loss3: 0.00\n",
      "Epoch [4542], train_loss: 425.52 with loss1: 361.53, loss2: 64.00 and loss3: 0.00\n",
      "Epoch [4543], train_loss: 427.86 with loss1: 363.84, loss2: 64.01 and loss3: 0.00\n",
      "Epoch [4544], train_loss: 426.67 with loss1: 362.99, loss2: 63.68 and loss3: 0.00\n",
      "Epoch [4545], train_loss: 427.26 with loss1: 363.28, loss2: 63.98 and loss3: 0.00\n",
      "Epoch [4546], train_loss: 425.53 with loss1: 361.81, loss2: 63.72 and loss3: 0.00\n",
      "Epoch [4547], train_loss: 428.88 with loss1: 365.03, loss2: 63.85 and loss3: 0.00\n",
      "Epoch [4548], train_loss: 428.51 with loss1: 364.97, loss2: 63.54 and loss3: 0.00\n",
      "Epoch [4549], train_loss: 428.87 with loss1: 364.90, loss2: 63.97 and loss3: 0.00\n",
      "Epoch [4550], train_loss: 428.78 with loss1: 365.28, loss2: 63.50 and loss3: 0.00\n",
      "Epoch [4551], train_loss: 432.11 with loss1: 368.36, loss2: 63.75 and loss3: 0.00\n",
      "Epoch [4552], train_loss: 429.69 with loss1: 366.20, loss2: 63.50 and loss3: 0.00\n",
      "Epoch [4553], train_loss: 432.08 with loss1: 368.24, loss2: 63.84 and loss3: 0.00\n",
      "Epoch [4554], train_loss: 432.05 with loss1: 368.62, loss2: 63.44 and loss3: 0.00\n",
      "Epoch [4555], train_loss: 434.43 with loss1: 370.72, loss2: 63.71 and loss3: 0.00\n",
      "Epoch [4556], train_loss: 434.34 with loss1: 371.02, loss2: 63.32 and loss3: 0.00\n",
      "Epoch [4557], train_loss: 436.52 with loss1: 372.69, loss2: 63.83 and loss3: 0.00\n",
      "Epoch [4558], train_loss: 436.15 with loss1: 372.84, loss2: 63.32 and loss3: 0.00\n",
      "Epoch [4559], train_loss: 439.22 with loss1: 375.53, loss2: 63.69 and loss3: 0.00\n",
      "Epoch [4560], train_loss: 439.55 with loss1: 376.13, loss2: 63.42 and loss3: 0.00\n",
      "Epoch [4561], train_loss: 441.38 with loss1: 377.68, loss2: 63.70 and loss3: 0.00\n",
      "Epoch [4562], train_loss: 441.11 with loss1: 377.82, loss2: 63.29 and loss3: 0.00\n",
      "Epoch [4563], train_loss: 445.22 with loss1: 381.55, loss2: 63.67 and loss3: 0.00\n",
      "Epoch [4564], train_loss: 444.28 with loss1: 381.03, loss2: 63.25 and loss3: 0.00\n",
      "Epoch [4565], train_loss: 447.83 with loss1: 384.19, loss2: 63.65 and loss3: 0.00\n",
      "Epoch [4566], train_loss: 448.83 with loss1: 385.58, loss2: 63.25 and loss3: 0.00\n",
      "Epoch [4567], train_loss: 451.89 with loss1: 388.27, loss2: 63.62 and loss3: 0.00\n",
      "Epoch [4568], train_loss: 453.19 with loss1: 390.07, loss2: 63.12 and loss3: 0.00\n",
      "Epoch [4569], train_loss: 455.80 with loss1: 392.15, loss2: 63.65 and loss3: 0.00\n",
      "Epoch [4570], train_loss: 456.85 with loss1: 393.68, loss2: 63.17 and loss3: 0.00\n",
      "Epoch [4571], train_loss: 460.79 with loss1: 397.18, loss2: 63.61 and loss3: 0.00\n",
      "Epoch [4572], train_loss: 463.03 with loss1: 399.89, loss2: 63.14 and loss3: 0.00\n",
      "Epoch [4573], train_loss: 468.66 with loss1: 405.05, loss2: 63.61 and loss3: 0.00\n",
      "Epoch [4574], train_loss: 470.45 with loss1: 407.21, loss2: 63.24 and loss3: 0.00\n",
      "Epoch [4575], train_loss: 474.16 with loss1: 410.39, loss2: 63.77 and loss3: 0.00\n",
      "Epoch [4576], train_loss: 476.37 with loss1: 413.22, loss2: 63.15 and loss3: 0.00\n",
      "Epoch [4577], train_loss: 482.09 with loss1: 418.47, loss2: 63.63 and loss3: 0.00\n",
      "Epoch [4578], train_loss: 482.02 with loss1: 418.82, loss2: 63.20 and loss3: 0.00\n",
      "Epoch [4579], train_loss: 485.11 with loss1: 421.50, loss2: 63.60 and loss3: 0.00\n",
      "Epoch [4580], train_loss: 483.09 with loss1: 419.98, loss2: 63.11 and loss3: 0.00\n",
      "Epoch [4581], train_loss: 485.67 with loss1: 422.02, loss2: 63.65 and loss3: 0.00\n",
      "Epoch [4582], train_loss: 484.61 with loss1: 421.25, loss2: 63.36 and loss3: 0.00\n",
      "Epoch [4583], train_loss: 487.68 with loss1: 424.02, loss2: 63.66 and loss3: 0.00\n",
      "Epoch [4584], train_loss: 486.66 with loss1: 423.44, loss2: 63.23 and loss3: 0.00\n",
      "Epoch [4585], train_loss: 490.70 with loss1: 426.98, loss2: 63.72 and loss3: 0.00\n",
      "Epoch [4586], train_loss: 488.91 with loss1: 425.51, loss2: 63.40 and loss3: 0.00\n",
      "Epoch [4587], train_loss: 490.73 with loss1: 426.90, loss2: 63.83 and loss3: 0.00\n",
      "Epoch [4588], train_loss: 488.25 with loss1: 424.83, loss2: 63.43 and loss3: 0.00\n",
      "Epoch [4589], train_loss: 487.73 with loss1: 423.74, loss2: 64.00 and loss3: 0.00\n",
      "Epoch [4590], train_loss: 484.07 with loss1: 420.51, loss2: 63.56 and loss3: 0.00\n",
      "Epoch [4591], train_loss: 483.24 with loss1: 419.47, loss2: 63.77 and loss3: 0.00\n",
      "Epoch [4592], train_loss: 479.62 with loss1: 416.15, loss2: 63.47 and loss3: 0.00\n",
      "Epoch [4593], train_loss: 478.79 with loss1: 414.83, loss2: 63.96 and loss3: 0.00\n",
      "Epoch [4594], train_loss: 474.54 with loss1: 410.91, loss2: 63.63 and loss3: 0.00\n",
      "Epoch [4595], train_loss: 472.95 with loss1: 409.06, loss2: 63.89 and loss3: 0.00\n",
      "Epoch [4596], train_loss: 468.09 with loss1: 404.45, loss2: 63.64 and loss3: 0.00\n",
      "Epoch [4597], train_loss: 467.64 with loss1: 403.81, loss2: 63.84 and loss3: 0.00\n",
      "Epoch [4598], train_loss: 464.94 with loss1: 401.24, loss2: 63.70 and loss3: 0.00\n",
      "Epoch [4599], train_loss: 462.52 with loss1: 398.59, loss2: 63.93 and loss3: 0.00\n",
      "Epoch [4600], train_loss: 458.98 with loss1: 395.31, loss2: 63.68 and loss3: 0.00\n",
      "Epoch [4601], train_loss: 457.06 with loss1: 393.04, loss2: 64.02 and loss3: 0.00\n",
      "Epoch [4602], train_loss: 454.44 with loss1: 390.73, loss2: 63.70 and loss3: 0.00\n",
      "Epoch [4603], train_loss: 451.78 with loss1: 387.89, loss2: 63.90 and loss3: 0.00\n",
      "Epoch [4604], train_loss: 448.98 with loss1: 385.36, loss2: 63.62 and loss3: 0.00\n",
      "Epoch [4605], train_loss: 448.72 with loss1: 384.85, loss2: 63.87 and loss3: 0.00\n",
      "Epoch [4606], train_loss: 445.40 with loss1: 381.74, loss2: 63.66 and loss3: 0.00\n",
      "Epoch [4607], train_loss: 445.20 with loss1: 381.32, loss2: 63.88 and loss3: 0.00\n",
      "Epoch [4608], train_loss: 441.44 with loss1: 377.68, loss2: 63.76 and loss3: 0.00\n",
      "Epoch [4609], train_loss: 440.66 with loss1: 376.90, loss2: 63.77 and loss3: 0.00\n",
      "Epoch [4610], train_loss: 439.07 with loss1: 375.49, loss2: 63.58 and loss3: 0.00\n",
      "Epoch [4611], train_loss: 439.39 with loss1: 375.68, loss2: 63.70 and loss3: 0.00\n",
      "Epoch [4612], train_loss: 437.03 with loss1: 373.44, loss2: 63.58 and loss3: 0.00\n",
      "Epoch [4613], train_loss: 436.43 with loss1: 372.79, loss2: 63.64 and loss3: 0.00\n",
      "Epoch [4614], train_loss: 434.20 with loss1: 370.66, loss2: 63.54 and loss3: 0.00\n",
      "Epoch [4615], train_loss: 433.48 with loss1: 369.96, loss2: 63.52 and loss3: 0.00\n",
      "Epoch [4616], train_loss: 432.13 with loss1: 368.64, loss2: 63.49 and loss3: 0.00\n",
      "Epoch [4617], train_loss: 431.53 with loss1: 367.94, loss2: 63.59 and loss3: 0.00\n",
      "Epoch [4618], train_loss: 429.44 with loss1: 365.98, loss2: 63.46 and loss3: 0.00\n",
      "Epoch [4619], train_loss: 430.99 with loss1: 367.46, loss2: 63.52 and loss3: 0.00\n",
      "Epoch [4620], train_loss: 428.83 with loss1: 365.40, loss2: 63.44 and loss3: 0.00\n",
      "Epoch [4621], train_loss: 426.22 with loss1: 362.66, loss2: 63.56 and loss3: 0.00\n",
      "Epoch [4622], train_loss: 426.64 with loss1: 363.22, loss2: 63.41 and loss3: 0.00\n",
      "Epoch [4623], train_loss: 426.05 with loss1: 362.54, loss2: 63.51 and loss3: 0.00\n",
      "Epoch [4624], train_loss: 425.30 with loss1: 362.05, loss2: 63.25 and loss3: 0.00\n",
      "Epoch [4625], train_loss: 425.10 with loss1: 361.61, loss2: 63.49 and loss3: 0.00\n",
      "Epoch [4626], train_loss: 423.40 with loss1: 360.14, loss2: 63.26 and loss3: 0.00\n",
      "Epoch [4627], train_loss: 422.99 with loss1: 359.61, loss2: 63.38 and loss3: 0.00\n",
      "Epoch [4628], train_loss: 422.37 with loss1: 359.16, loss2: 63.21 and loss3: 0.00\n",
      "Epoch [4629], train_loss: 423.47 with loss1: 360.11, loss2: 63.35 and loss3: 0.00\n",
      "Epoch [4630], train_loss: 422.03 with loss1: 358.94, loss2: 63.08 and loss3: 0.00\n",
      "Epoch [4631], train_loss: 423.32 with loss1: 360.01, loss2: 63.31 and loss3: 0.00\n",
      "Epoch [4632], train_loss: 422.21 with loss1: 358.98, loss2: 63.23 and loss3: 0.00\n",
      "Epoch [4633], train_loss: 421.87 with loss1: 358.58, loss2: 63.29 and loss3: 0.00\n",
      "Epoch [4634], train_loss: 421.76 with loss1: 358.75, loss2: 63.01 and loss3: 0.00\n",
      "Epoch [4635], train_loss: 423.12 with loss1: 359.92, loss2: 63.20 and loss3: 0.00\n",
      "Epoch [4636], train_loss: 423.20 with loss1: 360.07, loss2: 63.13 and loss3: 0.00\n",
      "Epoch [4637], train_loss: 422.71 with loss1: 359.50, loss2: 63.21 and loss3: 0.00\n",
      "Epoch [4638], train_loss: 421.45 with loss1: 358.41, loss2: 63.05 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4639], train_loss: 423.00 with loss1: 359.72, loss2: 63.28 and loss3: 0.00\n",
      "Epoch [4640], train_loss: 421.95 with loss1: 358.98, loss2: 62.97 and loss3: 0.00\n",
      "Epoch [4641], train_loss: 422.81 with loss1: 359.77, loss2: 63.04 and loss3: 0.00\n",
      "Epoch [4642], train_loss: 421.99 with loss1: 359.01, loss2: 62.98 and loss3: 0.00\n",
      "Epoch [4643], train_loss: 423.85 with loss1: 360.76, loss2: 63.09 and loss3: 0.00\n",
      "Epoch [4644], train_loss: 422.19 with loss1: 359.22, loss2: 62.97 and loss3: 0.00\n",
      "Epoch [4645], train_loss: 423.60 with loss1: 360.56, loss2: 63.04 and loss3: 0.00\n",
      "Epoch [4646], train_loss: 424.43 with loss1: 361.54, loss2: 62.90 and loss3: 0.00\n",
      "Epoch [4647], train_loss: 425.15 with loss1: 362.12, loss2: 63.03 and loss3: 0.00\n",
      "Epoch [4648], train_loss: 425.75 with loss1: 362.79, loss2: 62.96 and loss3: 0.00\n",
      "Epoch [4649], train_loss: 425.96 with loss1: 362.94, loss2: 63.02 and loss3: 0.00\n",
      "Epoch [4650], train_loss: 426.56 with loss1: 363.60, loss2: 62.95 and loss3: 0.00\n",
      "Epoch [4651], train_loss: 428.16 with loss1: 365.10, loss2: 63.07 and loss3: 0.00\n",
      "Epoch [4652], train_loss: 428.27 with loss1: 365.48, loss2: 62.79 and loss3: 0.00\n",
      "Epoch [4653], train_loss: 429.55 with loss1: 366.56, loss2: 62.99 and loss3: 0.00\n",
      "Epoch [4654], train_loss: 428.83 with loss1: 365.99, loss2: 62.84 and loss3: 0.00\n",
      "Epoch [4655], train_loss: 430.71 with loss1: 367.73, loss2: 62.99 and loss3: 0.00\n",
      "Epoch [4656], train_loss: 431.11 with loss1: 368.34, loss2: 62.77 and loss3: 0.00\n",
      "Epoch [4657], train_loss: 431.70 with loss1: 368.65, loss2: 63.05 and loss3: 0.00\n",
      "Epoch [4658], train_loss: 431.07 with loss1: 368.16, loss2: 62.91 and loss3: 0.00\n",
      "Epoch [4659], train_loss: 432.48 with loss1: 369.53, loss2: 62.94 and loss3: 0.00\n",
      "Epoch [4660], train_loss: 432.15 with loss1: 369.21, loss2: 62.95 and loss3: 0.00\n",
      "Epoch [4661], train_loss: 434.73 with loss1: 371.72, loss2: 63.01 and loss3: 0.00\n",
      "Epoch [4662], train_loss: 433.48 with loss1: 370.56, loss2: 62.92 and loss3: 0.00\n",
      "Epoch [4663], train_loss: 435.36 with loss1: 372.40, loss2: 62.95 and loss3: 0.00\n",
      "Epoch [4664], train_loss: 435.97 with loss1: 373.03, loss2: 62.93 and loss3: 0.00\n",
      "Epoch [4665], train_loss: 437.59 with loss1: 374.62, loss2: 62.96 and loss3: 0.00\n",
      "Epoch [4666], train_loss: 437.32 with loss1: 374.53, loss2: 62.78 and loss3: 0.00\n",
      "Epoch [4667], train_loss: 439.85 with loss1: 376.94, loss2: 62.91 and loss3: 0.00\n",
      "Epoch [4668], train_loss: 439.33 with loss1: 376.31, loss2: 63.02 and loss3: 0.00\n",
      "Epoch [4669], train_loss: 442.25 with loss1: 379.28, loss2: 62.97 and loss3: 0.00\n",
      "Epoch [4670], train_loss: 442.24 with loss1: 379.29, loss2: 62.95 and loss3: 0.00\n",
      "Epoch [4671], train_loss: 443.84 with loss1: 380.88, loss2: 62.96 and loss3: 0.00\n",
      "Epoch [4672], train_loss: 444.79 with loss1: 381.83, loss2: 62.95 and loss3: 0.00\n",
      "Epoch [4673], train_loss: 446.30 with loss1: 383.32, loss2: 62.97 and loss3: 0.00\n",
      "Epoch [4674], train_loss: 447.67 with loss1: 384.72, loss2: 62.95 and loss3: 0.00\n",
      "Epoch [4675], train_loss: 450.61 with loss1: 387.52, loss2: 63.09 and loss3: 0.00\n",
      "Epoch [4676], train_loss: 450.57 with loss1: 387.50, loss2: 63.07 and loss3: 0.00\n",
      "Epoch [4677], train_loss: 453.62 with loss1: 390.51, loss2: 63.11 and loss3: 0.00\n",
      "Epoch [4678], train_loss: 456.22 with loss1: 393.21, loss2: 63.01 and loss3: 0.00\n",
      "Epoch [4679], train_loss: 459.10 with loss1: 395.81, loss2: 63.29 and loss3: 0.00\n",
      "Epoch [4680], train_loss: 459.61 with loss1: 396.42, loss2: 63.19 and loss3: 0.00\n",
      "Epoch [4681], train_loss: 459.94 with loss1: 396.61, loss2: 63.34 and loss3: 0.00\n",
      "Epoch [4682], train_loss: 460.74 with loss1: 397.34, loss2: 63.41 and loss3: 0.00\n",
      "Epoch [4683], train_loss: 461.26 with loss1: 397.84, loss2: 63.43 and loss3: 0.00\n",
      "Epoch [4684], train_loss: 457.44 with loss1: 394.02, loss2: 63.42 and loss3: 0.00\n",
      "Epoch [4685], train_loss: 457.62 with loss1: 394.05, loss2: 63.57 and loss3: 0.00\n",
      "Epoch [4686], train_loss: 456.79 with loss1: 393.35, loss2: 63.43 and loss3: 0.00\n",
      "Epoch [4687], train_loss: 455.81 with loss1: 392.12, loss2: 63.69 and loss3: 0.00\n",
      "Epoch [4688], train_loss: 454.89 with loss1: 391.23, loss2: 63.66 and loss3: 0.00\n",
      "Epoch [4689], train_loss: 454.30 with loss1: 390.58, loss2: 63.72 and loss3: 0.00\n",
      "Epoch [4690], train_loss: 451.93 with loss1: 388.29, loss2: 63.65 and loss3: 0.00\n",
      "Epoch [4691], train_loss: 450.16 with loss1: 386.32, loss2: 63.84 and loss3: 0.00\n",
      "Epoch [4692], train_loss: 447.41 with loss1: 383.82, loss2: 63.59 and loss3: 0.00\n",
      "Epoch [4693], train_loss: 443.63 with loss1: 380.02, loss2: 63.61 and loss3: 0.00\n",
      "Epoch [4694], train_loss: 441.24 with loss1: 377.51, loss2: 63.73 and loss3: 0.00\n",
      "Epoch [4695], train_loss: 439.39 with loss1: 375.61, loss2: 63.78 and loss3: 0.00\n",
      "Epoch [4696], train_loss: 437.56 with loss1: 373.87, loss2: 63.69 and loss3: 0.00\n",
      "Epoch [4697], train_loss: 435.90 with loss1: 372.21, loss2: 63.68 and loss3: 0.00\n",
      "Epoch [4698], train_loss: 434.64 with loss1: 370.91, loss2: 63.73 and loss3: 0.00\n",
      "Epoch [4699], train_loss: 433.40 with loss1: 369.65, loss2: 63.75 and loss3: 0.00\n",
      "Epoch [4700], train_loss: 430.61 with loss1: 366.97, loss2: 63.63 and loss3: 0.00\n",
      "Epoch [4701], train_loss: 429.65 with loss1: 366.02, loss2: 63.63 and loss3: 0.00\n",
      "Epoch [4702], train_loss: 426.94 with loss1: 363.41, loss2: 63.53 and loss3: 0.00\n",
      "Epoch [4703], train_loss: 426.87 with loss1: 363.30, loss2: 63.57 and loss3: 0.00\n",
      "Epoch [4704], train_loss: 426.76 with loss1: 363.08, loss2: 63.68 and loss3: 0.00\n",
      "Epoch [4705], train_loss: 425.64 with loss1: 362.10, loss2: 63.54 and loss3: 0.00\n",
      "Epoch [4706], train_loss: 425.27 with loss1: 361.79, loss2: 63.48 and loss3: 0.00\n",
      "Epoch [4707], train_loss: 422.88 with loss1: 359.26, loss2: 63.62 and loss3: 0.00\n",
      "Epoch [4708], train_loss: 424.21 with loss1: 360.68, loss2: 63.53 and loss3: 0.00\n",
      "Epoch [4709], train_loss: 423.08 with loss1: 359.63, loss2: 63.45 and loss3: 0.00\n",
      "Epoch [4710], train_loss: 422.70 with loss1: 359.26, loss2: 63.44 and loss3: 0.00\n",
      "Epoch [4711], train_loss: 421.98 with loss1: 358.58, loss2: 63.39 and loss3: 0.00\n",
      "Epoch [4712], train_loss: 422.57 with loss1: 359.04, loss2: 63.53 and loss3: 0.00\n",
      "Epoch [4713], train_loss: 421.37 with loss1: 358.09, loss2: 63.28 and loss3: 0.00\n",
      "Epoch [4714], train_loss: 422.38 with loss1: 358.97, loss2: 63.40 and loss3: 0.00\n",
      "Epoch [4715], train_loss: 421.38 with loss1: 358.14, loss2: 63.24 and loss3: 0.00\n",
      "Epoch [4716], train_loss: 422.58 with loss1: 359.22, loss2: 63.36 and loss3: 0.00\n",
      "Epoch [4717], train_loss: 422.93 with loss1: 359.83, loss2: 63.10 and loss3: 0.00\n",
      "Epoch [4718], train_loss: 425.29 with loss1: 361.91, loss2: 63.38 and loss3: 0.00\n",
      "Epoch [4719], train_loss: 424.29 with loss1: 361.21, loss2: 63.08 and loss3: 0.00\n",
      "Epoch [4720], train_loss: 426.04 with loss1: 362.83, loss2: 63.21 and loss3: 0.00\n",
      "Epoch [4721], train_loss: 426.02 with loss1: 363.06, loss2: 62.96 and loss3: 0.00\n",
      "Epoch [4722], train_loss: 428.04 with loss1: 364.87, loss2: 63.17 and loss3: 0.00\n",
      "Epoch [4723], train_loss: 428.31 with loss1: 365.43, loss2: 62.88 and loss3: 0.00\n",
      "Epoch [4724], train_loss: 431.31 with loss1: 368.05, loss2: 63.26 and loss3: 0.00\n",
      "Epoch [4725], train_loss: 430.95 with loss1: 368.08, loss2: 62.87 and loss3: 0.00\n",
      "Epoch [4726], train_loss: 433.74 with loss1: 370.58, loss2: 63.16 and loss3: 0.00\n",
      "Epoch [4727], train_loss: 434.87 with loss1: 371.96, loss2: 62.91 and loss3: 0.00\n",
      "Epoch [4728], train_loss: 437.48 with loss1: 374.35, loss2: 63.14 and loss3: 0.00\n",
      "Epoch [4729], train_loss: 437.99 with loss1: 375.25, loss2: 62.74 and loss3: 0.00\n",
      "Epoch [4730], train_loss: 441.24 with loss1: 378.16, loss2: 63.07 and loss3: 0.00\n",
      "Epoch [4731], train_loss: 442.12 with loss1: 379.41, loss2: 62.72 and loss3: 0.00\n",
      "Epoch [4732], train_loss: 444.34 with loss1: 381.28, loss2: 63.06 and loss3: 0.00\n",
      "Epoch [4733], train_loss: 444.41 with loss1: 381.78, loss2: 62.64 and loss3: 0.00\n",
      "Epoch [4734], train_loss: 446.89 with loss1: 383.92, loss2: 62.97 and loss3: 0.00\n",
      "Epoch [4735], train_loss: 448.02 with loss1: 385.41, loss2: 62.61 and loss3: 0.00\n",
      "Epoch [4736], train_loss: 449.31 with loss1: 386.38, loss2: 62.92 and loss3: 0.00\n",
      "Epoch [4737], train_loss: 451.16 with loss1: 388.58, loss2: 62.58 and loss3: 0.00\n",
      "Epoch [4738], train_loss: 453.62 with loss1: 390.72, loss2: 62.91 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4739], train_loss: 451.58 with loss1: 389.05, loss2: 62.54 and loss3: 0.00\n",
      "Epoch [4740], train_loss: 454.92 with loss1: 392.15, loss2: 62.77 and loss3: 0.00\n",
      "Epoch [4741], train_loss: 453.43 with loss1: 391.07, loss2: 62.35 and loss3: 0.00\n",
      "Epoch [4742], train_loss: 454.54 with loss1: 391.79, loss2: 62.75 and loss3: 0.00\n",
      "Epoch [4743], train_loss: 454.50 with loss1: 392.15, loss2: 62.35 and loss3: 0.00\n",
      "Epoch [4744], train_loss: 455.48 with loss1: 392.68, loss2: 62.80 and loss3: 0.00\n",
      "Epoch [4745], train_loss: 453.56 with loss1: 391.23, loss2: 62.33 and loss3: 0.00\n",
      "Epoch [4746], train_loss: 455.99 with loss1: 393.32, loss2: 62.68 and loss3: 0.00\n",
      "Epoch [4747], train_loss: 453.32 with loss1: 391.17, loss2: 62.15 and loss3: 0.00\n",
      "Epoch [4748], train_loss: 454.08 with loss1: 391.40, loss2: 62.68 and loss3: 0.00\n",
      "Epoch [4749], train_loss: 450.30 with loss1: 388.08, loss2: 62.22 and loss3: 0.00\n",
      "Epoch [4750], train_loss: 450.66 with loss1: 388.12, loss2: 62.54 and loss3: 0.00\n",
      "Epoch [4751], train_loss: 447.34 with loss1: 385.20, loss2: 62.14 and loss3: 0.00\n",
      "Epoch [4752], train_loss: 447.42 with loss1: 384.87, loss2: 62.56 and loss3: 0.00\n",
      "Epoch [4753], train_loss: 444.06 with loss1: 381.93, loss2: 62.13 and loss3: 0.00\n",
      "Epoch [4754], train_loss: 444.84 with loss1: 382.40, loss2: 62.44 and loss3: 0.00\n",
      "Epoch [4755], train_loss: 442.02 with loss1: 380.03, loss2: 61.99 and loss3: 0.00\n",
      "Epoch [4756], train_loss: 442.36 with loss1: 379.90, loss2: 62.46 and loss3: 0.00\n",
      "Epoch [4757], train_loss: 438.77 with loss1: 376.80, loss2: 61.96 and loss3: 0.00\n",
      "Epoch [4758], train_loss: 440.83 with loss1: 378.55, loss2: 62.28 and loss3: 0.00\n",
      "Epoch [4759], train_loss: 438.44 with loss1: 376.47, loss2: 61.97 and loss3: 0.00\n",
      "Epoch [4760], train_loss: 437.56 with loss1: 375.28, loss2: 62.28 and loss3: 0.00\n",
      "Epoch [4761], train_loss: 435.20 with loss1: 373.39, loss2: 61.81 and loss3: 0.00\n",
      "Epoch [4762], train_loss: 435.13 with loss1: 372.87, loss2: 62.26 and loss3: 0.00\n",
      "Epoch [4763], train_loss: 433.70 with loss1: 371.85, loss2: 61.85 and loss3: 0.00\n",
      "Epoch [4764], train_loss: 433.13 with loss1: 370.93, loss2: 62.20 and loss3: 0.00\n",
      "Epoch [4765], train_loss: 431.29 with loss1: 369.55, loss2: 61.74 and loss3: 0.00\n",
      "Epoch [4766], train_loss: 431.81 with loss1: 369.70, loss2: 62.11 and loss3: 0.00\n",
      "Epoch [4767], train_loss: 430.92 with loss1: 369.24, loss2: 61.68 and loss3: 0.00\n",
      "Epoch [4768], train_loss: 430.77 with loss1: 368.61, loss2: 62.16 and loss3: 0.00\n",
      "Epoch [4769], train_loss: 430.76 with loss1: 369.11, loss2: 61.65 and loss3: 0.00\n",
      "Epoch [4770], train_loss: 430.73 with loss1: 368.79, loss2: 61.94 and loss3: 0.00\n",
      "Epoch [4771], train_loss: 429.22 with loss1: 367.56, loss2: 61.66 and loss3: 0.00\n",
      "Epoch [4772], train_loss: 429.25 with loss1: 367.30, loss2: 61.95 and loss3: 0.00\n",
      "Epoch [4773], train_loss: 429.12 with loss1: 367.45, loss2: 61.66 and loss3: 0.00\n",
      "Epoch [4774], train_loss: 429.04 with loss1: 367.11, loss2: 61.94 and loss3: 0.00\n",
      "Epoch [4775], train_loss: 429.30 with loss1: 367.63, loss2: 61.67 and loss3: 0.00\n",
      "Epoch [4776], train_loss: 428.67 with loss1: 366.68, loss2: 61.98 and loss3: 0.00\n",
      "Epoch [4777], train_loss: 428.38 with loss1: 366.86, loss2: 61.52 and loss3: 0.00\n",
      "Epoch [4778], train_loss: 428.96 with loss1: 367.01, loss2: 61.95 and loss3: 0.00\n",
      "Epoch [4779], train_loss: 428.27 with loss1: 366.73, loss2: 61.55 and loss3: 0.00\n",
      "Epoch [4780], train_loss: 428.48 with loss1: 366.64, loss2: 61.85 and loss3: 0.00\n",
      "Epoch [4781], train_loss: 427.49 with loss1: 366.02, loss2: 61.47 and loss3: 0.00\n",
      "Epoch [4782], train_loss: 427.20 with loss1: 365.34, loss2: 61.85 and loss3: 0.00\n",
      "Epoch [4783], train_loss: 427.22 with loss1: 365.74, loss2: 61.49 and loss3: 0.00\n",
      "Epoch [4784], train_loss: 427.62 with loss1: 365.87, loss2: 61.75 and loss3: 0.00\n",
      "Epoch [4785], train_loss: 426.11 with loss1: 364.75, loss2: 61.37 and loss3: 0.00\n",
      "Epoch [4786], train_loss: 427.80 with loss1: 366.15, loss2: 61.65 and loss3: 0.00\n",
      "Epoch [4787], train_loss: 426.05 with loss1: 364.68, loss2: 61.36 and loss3: 0.00\n",
      "Epoch [4788], train_loss: 426.79 with loss1: 365.13, loss2: 61.66 and loss3: 0.00\n",
      "Epoch [4789], train_loss: 427.21 with loss1: 365.83, loss2: 61.38 and loss3: 0.00\n",
      "Epoch [4790], train_loss: 427.37 with loss1: 365.74, loss2: 61.63 and loss3: 0.00\n",
      "Epoch [4791], train_loss: 425.22 with loss1: 363.96, loss2: 61.26 and loss3: 0.00\n",
      "Epoch [4792], train_loss: 426.20 with loss1: 364.43, loss2: 61.77 and loss3: 0.00\n",
      "Epoch [4793], train_loss: 425.45 with loss1: 364.10, loss2: 61.35 and loss3: 0.00\n",
      "Epoch [4794], train_loss: 427.18 with loss1: 365.56, loss2: 61.62 and loss3: 0.00\n",
      "Epoch [4795], train_loss: 425.77 with loss1: 364.55, loss2: 61.22 and loss3: 0.00\n",
      "Epoch [4796], train_loss: 428.02 with loss1: 366.45, loss2: 61.58 and loss3: 0.00\n",
      "Epoch [4797], train_loss: 425.94 with loss1: 364.75, loss2: 61.18 and loss3: 0.00\n",
      "Epoch [4798], train_loss: 429.68 with loss1: 368.18, loss2: 61.50 and loss3: 0.00\n",
      "Epoch [4799], train_loss: 427.48 with loss1: 366.33, loss2: 61.15 and loss3: 0.00\n",
      "Epoch [4800], train_loss: 429.89 with loss1: 368.44, loss2: 61.45 and loss3: 0.00\n",
      "Epoch [4801], train_loss: 427.28 with loss1: 366.10, loss2: 61.18 and loss3: 0.00\n",
      "Epoch [4802], train_loss: 429.28 with loss1: 367.85, loss2: 61.44 and loss3: 0.00\n",
      "Epoch [4803], train_loss: 429.56 with loss1: 368.45, loss2: 61.11 and loss3: 0.00\n",
      "Epoch [4804], train_loss: 432.59 with loss1: 371.10, loss2: 61.49 and loss3: 0.00\n",
      "Epoch [4805], train_loss: 432.13 with loss1: 371.06, loss2: 61.07 and loss3: 0.00\n",
      "Epoch [4806], train_loss: 432.63 with loss1: 371.19, loss2: 61.43 and loss3: 0.00\n",
      "Epoch [4807], train_loss: 431.81 with loss1: 370.82, loss2: 60.99 and loss3: 0.00\n",
      "Epoch [4808], train_loss: 432.81 with loss1: 371.36, loss2: 61.46 and loss3: 0.00\n",
      "Epoch [4809], train_loss: 431.01 with loss1: 370.05, loss2: 60.96 and loss3: 0.00\n",
      "Epoch [4810], train_loss: 432.74 with loss1: 371.26, loss2: 61.48 and loss3: 0.00\n",
      "Epoch [4811], train_loss: 430.45 with loss1: 369.38, loss2: 61.07 and loss3: 0.00\n",
      "Epoch [4812], train_loss: 432.19 with loss1: 370.88, loss2: 61.31 and loss3: 0.00\n",
      "Epoch [4813], train_loss: 429.74 with loss1: 368.86, loss2: 60.87 and loss3: 0.00\n",
      "Epoch [4814], train_loss: 430.88 with loss1: 369.58, loss2: 61.29 and loss3: 0.00\n",
      "Epoch [4815], train_loss: 429.32 with loss1: 368.45, loss2: 60.87 and loss3: 0.00\n",
      "Epoch [4816], train_loss: 429.28 with loss1: 367.98, loss2: 61.29 and loss3: 0.00\n",
      "Epoch [4817], train_loss: 426.27 with loss1: 365.29, loss2: 60.98 and loss3: 0.00\n",
      "Epoch [4818], train_loss: 428.83 with loss1: 367.59, loss2: 61.23 and loss3: 0.00\n",
      "Epoch [4819], train_loss: 426.99 with loss1: 366.08, loss2: 60.91 and loss3: 0.00\n",
      "Epoch [4820], train_loss: 427.75 with loss1: 366.50, loss2: 61.25 and loss3: 0.00\n",
      "Epoch [4821], train_loss: 424.15 with loss1: 363.35, loss2: 60.80 and loss3: 0.00\n",
      "Epoch [4822], train_loss: 426.17 with loss1: 365.14, loss2: 61.04 and loss3: 0.00\n",
      "Epoch [4823], train_loss: 423.21 with loss1: 362.43, loss2: 60.78 and loss3: 0.00\n",
      "Epoch [4824], train_loss: 424.68 with loss1: 363.53, loss2: 61.15 and loss3: 0.00\n",
      "Epoch [4825], train_loss: 422.54 with loss1: 361.72, loss2: 60.83 and loss3: 0.00\n",
      "Epoch [4826], train_loss: 423.64 with loss1: 362.64, loss2: 61.00 and loss3: 0.00\n",
      "Epoch [4827], train_loss: 421.70 with loss1: 361.06, loss2: 60.64 and loss3: 0.00\n",
      "Epoch [4828], train_loss: 422.86 with loss1: 361.94, loss2: 60.92 and loss3: 0.00\n",
      "Epoch [4829], train_loss: 420.64 with loss1: 359.94, loss2: 60.70 and loss3: 0.00\n",
      "Epoch [4830], train_loss: 422.67 with loss1: 361.72, loss2: 60.95 and loss3: 0.00\n",
      "Epoch [4831], train_loss: 419.73 with loss1: 359.02, loss2: 60.70 and loss3: 0.00\n",
      "Epoch [4832], train_loss: 421.71 with loss1: 360.81, loss2: 60.90 and loss3: 0.00\n",
      "Epoch [4833], train_loss: 418.96 with loss1: 358.24, loss2: 60.72 and loss3: 0.00\n",
      "Epoch [4834], train_loss: 419.66 with loss1: 358.81, loss2: 60.85 and loss3: 0.00\n",
      "Epoch [4835], train_loss: 418.48 with loss1: 357.92, loss2: 60.57 and loss3: 0.00\n",
      "Epoch [4836], train_loss: 418.33 with loss1: 357.51, loss2: 60.82 and loss3: 0.00\n",
      "Epoch [4837], train_loss: 417.00 with loss1: 356.44, loss2: 60.56 and loss3: 0.00\n",
      "Epoch [4838], train_loss: 417.85 with loss1: 356.98, loss2: 60.87 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4839], train_loss: 416.11 with loss1: 355.66, loss2: 60.45 and loss3: 0.00\n",
      "Epoch [4840], train_loss: 417.44 with loss1: 356.65, loss2: 60.79 and loss3: 0.00\n",
      "Epoch [4841], train_loss: 415.10 with loss1: 354.59, loss2: 60.51 and loss3: 0.00\n",
      "Epoch [4842], train_loss: 418.16 with loss1: 357.41, loss2: 60.75 and loss3: 0.00\n",
      "Epoch [4843], train_loss: 415.87 with loss1: 355.48, loss2: 60.39 and loss3: 0.00\n",
      "Epoch [4844], train_loss: 416.82 with loss1: 356.08, loss2: 60.74 and loss3: 0.00\n",
      "Epoch [4845], train_loss: 415.85 with loss1: 355.53, loss2: 60.31 and loss3: 0.00\n",
      "Epoch [4846], train_loss: 416.35 with loss1: 355.79, loss2: 60.56 and loss3: 0.00\n",
      "Epoch [4847], train_loss: 415.40 with loss1: 355.08, loss2: 60.32 and loss3: 0.00\n",
      "Epoch [4848], train_loss: 416.96 with loss1: 356.40, loss2: 60.55 and loss3: 0.00\n",
      "Epoch [4849], train_loss: 416.80 with loss1: 356.57, loss2: 60.23 and loss3: 0.00\n",
      "Epoch [4850], train_loss: 417.57 with loss1: 357.09, loss2: 60.47 and loss3: 0.00\n",
      "Epoch [4851], train_loss: 417.19 with loss1: 356.87, loss2: 60.32 and loss3: 0.00\n",
      "Epoch [4852], train_loss: 418.21 with loss1: 357.77, loss2: 60.45 and loss3: 0.00\n",
      "Epoch [4853], train_loss: 418.79 with loss1: 358.56, loss2: 60.23 and loss3: 0.00\n",
      "Epoch [4854], train_loss: 421.12 with loss1: 360.74, loss2: 60.39 and loss3: 0.00\n",
      "Epoch [4855], train_loss: 422.17 with loss1: 361.97, loss2: 60.20 and loss3: 0.00\n",
      "Epoch [4856], train_loss: 424.09 with loss1: 363.55, loss2: 60.53 and loss3: 0.00\n",
      "Epoch [4857], train_loss: 423.88 with loss1: 363.72, loss2: 60.15 and loss3: 0.00\n",
      "Epoch [4858], train_loss: 426.64 with loss1: 366.19, loss2: 60.44 and loss3: 0.00\n",
      "Epoch [4859], train_loss: 427.13 with loss1: 367.02, loss2: 60.11 and loss3: 0.00\n",
      "Epoch [4860], train_loss: 430.91 with loss1: 370.42, loss2: 60.49 and loss3: 0.00\n",
      "Epoch [4861], train_loss: 432.41 with loss1: 372.31, loss2: 60.10 and loss3: 0.00\n",
      "Epoch [4862], train_loss: 437.10 with loss1: 376.62, loss2: 60.48 and loss3: 0.00\n",
      "Epoch [4863], train_loss: 440.35 with loss1: 380.22, loss2: 60.13 and loss3: 0.00\n",
      "Epoch [4864], train_loss: 443.08 with loss1: 382.49, loss2: 60.59 and loss3: 0.00\n",
      "Epoch [4865], train_loss: 445.67 with loss1: 385.28, loss2: 60.39 and loss3: 0.00\n",
      "Epoch [4866], train_loss: 451.21 with loss1: 390.52, loss2: 60.69 and loss3: 0.00\n",
      "Epoch [4867], train_loss: 453.71 with loss1: 393.08, loss2: 60.63 and loss3: 0.00\n",
      "Epoch [4868], train_loss: 459.25 with loss1: 398.35, loss2: 60.90 and loss3: 0.00\n",
      "Epoch [4869], train_loss: 461.04 with loss1: 400.21, loss2: 60.83 and loss3: 0.00\n",
      "Epoch [4870], train_loss: 461.66 with loss1: 400.59, loss2: 61.07 and loss3: 0.00\n",
      "Epoch [4871], train_loss: 463.15 with loss1: 402.14, loss2: 61.01 and loss3: 0.00\n",
      "Epoch [4872], train_loss: 464.37 with loss1: 403.14, loss2: 61.23 and loss3: 0.00\n",
      "Epoch [4873], train_loss: 462.70 with loss1: 401.43, loss2: 61.27 and loss3: 0.00\n",
      "Epoch [4874], train_loss: 460.52 with loss1: 399.04, loss2: 61.48 and loss3: 0.00\n",
      "Epoch [4875], train_loss: 456.01 with loss1: 394.62, loss2: 61.39 and loss3: 0.00\n",
      "Epoch [4876], train_loss: 453.53 with loss1: 391.94, loss2: 61.59 and loss3: 0.00\n",
      "Epoch [4877], train_loss: 449.29 with loss1: 387.73, loss2: 61.56 and loss3: 0.00\n",
      "Epoch [4878], train_loss: 444.96 with loss1: 383.24, loss2: 61.72 and loss3: 0.00\n",
      "Epoch [4879], train_loss: 439.75 with loss1: 378.24, loss2: 61.51 and loss3: 0.00\n",
      "Epoch [4880], train_loss: 437.46 with loss1: 375.77, loss2: 61.69 and loss3: 0.00\n",
      "Epoch [4881], train_loss: 433.22 with loss1: 371.65, loss2: 61.58 and loss3: 0.00\n",
      "Epoch [4882], train_loss: 428.83 with loss1: 367.16, loss2: 61.67 and loss3: 0.00\n",
      "Epoch [4883], train_loss: 425.56 with loss1: 364.13, loss2: 61.43 and loss3: 0.00\n",
      "Epoch [4884], train_loss: 423.92 with loss1: 362.27, loss2: 61.65 and loss3: 0.00\n",
      "Epoch [4885], train_loss: 421.78 with loss1: 360.33, loss2: 61.44 and loss3: 0.00\n",
      "Epoch [4886], train_loss: 418.55 with loss1: 356.98, loss2: 61.57 and loss3: 0.00\n",
      "Epoch [4887], train_loss: 417.08 with loss1: 355.78, loss2: 61.31 and loss3: 0.00\n",
      "Epoch [4888], train_loss: 415.70 with loss1: 354.17, loss2: 61.53 and loss3: 0.00\n",
      "Epoch [4889], train_loss: 413.60 with loss1: 352.33, loss2: 61.27 and loss3: 0.00\n",
      "Epoch [4890], train_loss: 413.34 with loss1: 352.00, loss2: 61.34 and loss3: 0.00\n",
      "Epoch [4891], train_loss: 412.13 with loss1: 351.03, loss2: 61.10 and loss3: 0.00\n",
      "Epoch [4892], train_loss: 412.62 with loss1: 351.29, loss2: 61.33 and loss3: 0.00\n",
      "Epoch [4893], train_loss: 411.05 with loss1: 349.99, loss2: 61.06 and loss3: 0.00\n",
      "Epoch [4894], train_loss: 410.36 with loss1: 349.15, loss2: 61.21 and loss3: 0.00\n",
      "Epoch [4895], train_loss: 408.55 with loss1: 347.56, loss2: 60.99 and loss3: 0.00\n",
      "Epoch [4896], train_loss: 410.23 with loss1: 349.04, loss2: 61.19 and loss3: 0.00\n",
      "Epoch [4897], train_loss: 408.64 with loss1: 347.68, loss2: 60.95 and loss3: 0.00\n",
      "Epoch [4898], train_loss: 408.07 with loss1: 347.01, loss2: 61.06 and loss3: 0.00\n",
      "Epoch [4899], train_loss: 408.33 with loss1: 347.48, loss2: 60.85 and loss3: 0.00\n",
      "Epoch [4900], train_loss: 408.48 with loss1: 347.48, loss2: 61.00 and loss3: 0.00\n",
      "Epoch [4901], train_loss: 409.33 with loss1: 348.49, loss2: 60.84 and loss3: 0.00\n",
      "Epoch [4902], train_loss: 408.87 with loss1: 347.96, loss2: 60.92 and loss3: 0.00\n",
      "Epoch [4903], train_loss: 408.53 with loss1: 347.76, loss2: 60.77 and loss3: 0.00\n",
      "Epoch [4904], train_loss: 410.21 with loss1: 349.45, loss2: 60.76 and loss3: 0.00\n",
      "Epoch [4905], train_loss: 410.20 with loss1: 349.42, loss2: 60.78 and loss3: 0.00\n",
      "Epoch [4906], train_loss: 411.31 with loss1: 350.45, loss2: 60.86 and loss3: 0.00\n",
      "Epoch [4907], train_loss: 411.88 with loss1: 351.34, loss2: 60.54 and loss3: 0.00\n",
      "Epoch [4908], train_loss: 413.54 with loss1: 352.83, loss2: 60.71 and loss3: 0.00\n",
      "Epoch [4909], train_loss: 411.82 with loss1: 351.22, loss2: 60.60 and loss3: 0.00\n",
      "Epoch [4910], train_loss: 413.95 with loss1: 353.18, loss2: 60.77 and loss3: 0.00\n",
      "Epoch [4911], train_loss: 413.93 with loss1: 353.45, loss2: 60.48 and loss3: 0.00\n",
      "Epoch [4912], train_loss: 414.15 with loss1: 353.38, loss2: 60.78 and loss3: 0.00\n",
      "Epoch [4913], train_loss: 413.21 with loss1: 352.71, loss2: 60.50 and loss3: 0.00\n",
      "Epoch [4914], train_loss: 415.10 with loss1: 354.38, loss2: 60.72 and loss3: 0.00\n",
      "Epoch [4915], train_loss: 415.21 with loss1: 354.77, loss2: 60.44 and loss3: 0.00\n",
      "Epoch [4916], train_loss: 416.45 with loss1: 355.76, loss2: 60.69 and loss3: 0.00\n",
      "Epoch [4917], train_loss: 417.58 with loss1: 357.22, loss2: 60.36 and loss3: 0.00\n",
      "Epoch [4918], train_loss: 418.01 with loss1: 357.38, loss2: 60.63 and loss3: 0.00\n",
      "Epoch [4919], train_loss: 418.70 with loss1: 358.25, loss2: 60.45 and loss3: 0.00\n",
      "Epoch [4920], train_loss: 418.10 with loss1: 357.53, loss2: 60.57 and loss3: 0.00\n",
      "Epoch [4921], train_loss: 418.87 with loss1: 358.71, loss2: 60.16 and loss3: 0.00\n",
      "Epoch [4922], train_loss: 419.91 with loss1: 359.46, loss2: 60.45 and loss3: 0.00\n",
      "Epoch [4923], train_loss: 420.35 with loss1: 360.28, loss2: 60.07 and loss3: 0.00\n",
      "Epoch [4924], train_loss: 422.65 with loss1: 362.24, loss2: 60.41 and loss3: 0.00\n",
      "Epoch [4925], train_loss: 423.04 with loss1: 362.86, loss2: 60.18 and loss3: 0.00\n",
      "Epoch [4926], train_loss: 425.07 with loss1: 364.64, loss2: 60.44 and loss3: 0.00\n",
      "Epoch [4927], train_loss: 424.31 with loss1: 364.10, loss2: 60.21 and loss3: 0.00\n",
      "Epoch [4928], train_loss: 425.98 with loss1: 365.63, loss2: 60.35 and loss3: 0.00\n",
      "Epoch [4929], train_loss: 426.33 with loss1: 366.32, loss2: 60.02 and loss3: 0.00\n",
      "Epoch [4930], train_loss: 429.29 with loss1: 368.96, loss2: 60.34 and loss3: 0.00\n",
      "Epoch [4931], train_loss: 429.33 with loss1: 369.27, loss2: 60.06 and loss3: 0.00\n",
      "Epoch [4932], train_loss: 432.13 with loss1: 371.74, loss2: 60.40 and loss3: 0.00\n",
      "Epoch [4933], train_loss: 429.83 with loss1: 369.76, loss2: 60.07 and loss3: 0.00\n",
      "Epoch [4934], train_loss: 432.70 with loss1: 372.33, loss2: 60.37 and loss3: 0.00\n",
      "Epoch [4935], train_loss: 432.12 with loss1: 371.99, loss2: 60.14 and loss3: 0.00\n",
      "Epoch [4936], train_loss: 434.81 with loss1: 374.45, loss2: 60.36 and loss3: 0.00\n",
      "Epoch [4937], train_loss: 434.69 with loss1: 374.56, loss2: 60.13 and loss3: 0.00\n",
      "Epoch [4938], train_loss: 437.86 with loss1: 377.60, loss2: 60.27 and loss3: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4939], train_loss: 436.24 with loss1: 376.12, loss2: 60.12 and loss3: 0.00\n",
      "Epoch [4940], train_loss: 437.91 with loss1: 377.64, loss2: 60.27 and loss3: 0.00\n",
      "Epoch [4941], train_loss: 436.72 with loss1: 376.58, loss2: 60.15 and loss3: 0.00\n",
      "Epoch [4942], train_loss: 439.26 with loss1: 379.07, loss2: 60.19 and loss3: 0.00\n",
      "Epoch [4943], train_loss: 437.60 with loss1: 377.48, loss2: 60.12 and loss3: 0.00\n",
      "Epoch [4944], train_loss: 440.34 with loss1: 380.02, loss2: 60.32 and loss3: 0.00\n",
      "Epoch [4945], train_loss: 438.35 with loss1: 378.18, loss2: 60.17 and loss3: 0.00\n",
      "Epoch [4946], train_loss: 438.30 with loss1: 377.95, loss2: 60.35 and loss3: 0.00\n",
      "Epoch [4947], train_loss: 436.89 with loss1: 376.67, loss2: 60.22 and loss3: 0.00\n",
      "Epoch [4948], train_loss: 437.97 with loss1: 377.52, loss2: 60.45 and loss3: 0.00\n",
      "Epoch [4949], train_loss: 436.52 with loss1: 376.33, loss2: 60.19 and loss3: 0.00\n",
      "Epoch [4950], train_loss: 437.30 with loss1: 376.96, loss2: 60.34 and loss3: 0.00\n",
      "Epoch [4951], train_loss: 436.18 with loss1: 375.98, loss2: 60.20 and loss3: 0.00\n",
      "Epoch [4952], train_loss: 437.53 with loss1: 377.20, loss2: 60.33 and loss3: 0.00\n",
      "Epoch [4953], train_loss: 434.88 with loss1: 374.61, loss2: 60.26 and loss3: 0.00\n",
      "Epoch [4954], train_loss: 435.45 with loss1: 375.13, loss2: 60.31 and loss3: 0.00\n",
      "Epoch [4955], train_loss: 433.31 with loss1: 373.04, loss2: 60.27 and loss3: 0.00\n",
      "Epoch [4956], train_loss: 433.45 with loss1: 373.22, loss2: 60.23 and loss3: 0.00\n",
      "Epoch [4957], train_loss: 431.30 with loss1: 371.00, loss2: 60.30 and loss3: 0.00\n",
      "Epoch [4958], train_loss: 432.50 with loss1: 372.12, loss2: 60.39 and loss3: 0.00\n",
      "Epoch [4959], train_loss: 430.27 with loss1: 370.00, loss2: 60.27 and loss3: 0.00\n",
      "Epoch [4960], train_loss: 430.88 with loss1: 370.51, loss2: 60.37 and loss3: 0.00\n",
      "Epoch [4961], train_loss: 430.22 with loss1: 369.99, loss2: 60.23 and loss3: 0.00\n",
      "Epoch [4962], train_loss: 431.78 with loss1: 371.52, loss2: 60.26 and loss3: 0.00\n",
      "Epoch [4963], train_loss: 431.26 with loss1: 371.05, loss2: 60.21 and loss3: 0.00\n",
      "Epoch [4964], train_loss: 432.10 with loss1: 371.84, loss2: 60.25 and loss3: 0.00\n",
      "Epoch [4965], train_loss: 431.24 with loss1: 371.10, loss2: 60.15 and loss3: 0.00\n",
      "Epoch [4966], train_loss: 432.01 with loss1: 371.81, loss2: 60.21 and loss3: 0.00\n",
      "Epoch [4967], train_loss: 430.62 with loss1: 370.37, loss2: 60.25 and loss3: 0.00\n",
      "Epoch [4968], train_loss: 430.85 with loss1: 370.59, loss2: 60.26 and loss3: 0.00\n",
      "Epoch [4969], train_loss: 429.13 with loss1: 369.02, loss2: 60.12 and loss3: 0.00\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1e-1, lamb=1.\n",
    "# model version 3.5\n",
    "# with loss2 and loss3\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history = fit(epochs=5000, lr=5e-6, h0=h0, model=model, lamb=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573eb15",
   "metadata": {},
   "source": [
    "New decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9387e4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 194742.62 with loss1: 194742.62 and loss2: 0.00\n",
      "Epoch [1], train_loss: 104011.70 with loss1: 104011.70 and loss2: 0.00\n",
      "Epoch [2], train_loss: 80626.57 with loss1: 80626.57 and loss2: 0.00\n",
      "Epoch [3], train_loss: 69839.09 with loss1: 69839.09 and loss2: 0.00\n",
      "Epoch [4], train_loss: 58970.20 with loss1: 58970.20 and loss2: 0.00\n",
      "Epoch [5], train_loss: 54786.08 with loss1: 54786.08 and loss2: 0.00\n",
      "Epoch [6], train_loss: 58546.38 with loss1: 58546.38 and loss2: 0.00\n",
      "Epoch [7], train_loss: 53538.83 with loss1: 53538.83 and loss2: 0.00\n",
      "Epoch [8], train_loss: 48686.29 with loss1: 48686.29 and loss2: 0.00\n",
      "Epoch [9], train_loss: 51127.53 with loss1: 51127.53 and loss2: 0.00\n",
      "Epoch [10], train_loss: 48807.86 with loss1: 48807.86 and loss2: 0.00\n",
      "Epoch [11], train_loss: 41193.52 with loss1: 41193.52 and loss2: 0.00\n",
      "Epoch [12], train_loss: 43514.22 with loss1: 43514.22 and loss2: 0.00\n",
      "Epoch [13], train_loss: 40934.07 with loss1: 40934.07 and loss2: 0.00\n",
      "Epoch [14], train_loss: 38137.68 with loss1: 38137.68 and loss2: 0.00\n",
      "Epoch [15], train_loss: 32408.09 with loss1: 32408.09 and loss2: 0.00\n",
      "Epoch [16], train_loss: 33115.66 with loss1: 33115.66 and loss2: 0.00\n",
      "Epoch [17], train_loss: 29396.16 with loss1: 29396.16 and loss2: 0.00\n",
      "Epoch [18], train_loss: 28845.22 with loss1: 28845.22 and loss2: 0.00\n",
      "Epoch [19], train_loss: 25065.18 with loss1: 25065.18 and loss2: 0.00\n",
      "Epoch [20], train_loss: 22716.36 with loss1: 22716.36 and loss2: 0.00\n",
      "Epoch [21], train_loss: 20502.84 with loss1: 20502.84 and loss2: 0.00\n",
      "Epoch [22], train_loss: 19228.43 with loss1: 19228.43 and loss2: 0.00\n",
      "Epoch [23], train_loss: 19060.25 with loss1: 19060.25 and loss2: 0.00\n",
      "Epoch [24], train_loss: 17091.86 with loss1: 17091.86 and loss2: 0.00\n",
      "Epoch [25], train_loss: 16761.11 with loss1: 16761.11 and loss2: 0.00\n",
      "Epoch [26], train_loss: 17228.99 with loss1: 17228.99 and loss2: 0.00\n",
      "Epoch [27], train_loss: 16435.29 with loss1: 16435.29 and loss2: 0.00\n",
      "Epoch [28], train_loss: 14640.27 with loss1: 14640.27 and loss2: 0.00\n",
      "Epoch [29], train_loss: 13449.30 with loss1: 13449.30 and loss2: 0.00\n",
      "Epoch [30], train_loss: 13697.95 with loss1: 13697.95 and loss2: 0.00\n",
      "Epoch [31], train_loss: 13244.63 with loss1: 13244.63 and loss2: 0.00\n",
      "Epoch [32], train_loss: 13737.46 with loss1: 13737.46 and loss2: 0.00\n",
      "Epoch [33], train_loss: 13349.33 with loss1: 13349.33 and loss2: 0.00\n",
      "Epoch [34], train_loss: 12943.78 with loss1: 12943.78 and loss2: 0.00\n",
      "Epoch [35], train_loss: 11275.55 with loss1: 11275.55 and loss2: 0.00\n",
      "Epoch [36], train_loss: 11726.54 with loss1: 11726.54 and loss2: 0.00\n",
      "Epoch [37], train_loss: 10757.09 with loss1: 10757.09 and loss2: 0.00\n",
      "Epoch [38], train_loss: 10511.19 with loss1: 10511.19 and loss2: 0.00\n",
      "Epoch [39], train_loss: 11099.97 with loss1: 11099.97 and loss2: 0.00\n",
      "Epoch [40], train_loss: 10569.46 with loss1: 10569.46 and loss2: 0.00\n",
      "Epoch [41], train_loss: 11211.12 with loss1: 11211.12 and loss2: 0.00\n",
      "Epoch [42], train_loss: 10999.90 with loss1: 10999.90 and loss2: 0.00\n",
      "Epoch [43], train_loss: 10829.20 with loss1: 10829.20 and loss2: 0.00\n",
      "Epoch [44], train_loss: 10775.65 with loss1: 10775.65 and loss2: 0.00\n",
      "Epoch [45], train_loss: 10559.92 with loss1: 10559.92 and loss2: 0.00\n",
      "Epoch [46], train_loss: 10419.84 with loss1: 10419.84 and loss2: 0.00\n",
      "Epoch [47], train_loss: 9621.46 with loss1: 9621.46 and loss2: 0.00\n",
      "Epoch [48], train_loss: 10343.14 with loss1: 10343.14 and loss2: 0.00\n",
      "Epoch [49], train_loss: 10060.47 with loss1: 10060.47 and loss2: 0.00\n",
      "Epoch [50], train_loss: 8889.33 with loss1: 8889.33 and loss2: 0.00\n",
      "Epoch [51], train_loss: 8756.17 with loss1: 8756.17 and loss2: 0.00\n",
      "Epoch [52], train_loss: 8951.83 with loss1: 8951.83 and loss2: 0.00\n",
      "Epoch [53], train_loss: 9634.51 with loss1: 9634.51 and loss2: 0.00\n",
      "Epoch [54], train_loss: 8747.74 with loss1: 8747.74 and loss2: 0.00\n",
      "Epoch [55], train_loss: 9599.93 with loss1: 9599.93 and loss2: 0.00\n",
      "Epoch [56], train_loss: 9325.59 with loss1: 9325.59 and loss2: 0.00\n",
      "Epoch [57], train_loss: 9428.04 with loss1: 9428.04 and loss2: 0.00\n",
      "Epoch [58], train_loss: 8682.48 with loss1: 8682.48 and loss2: 0.00\n",
      "Epoch [59], train_loss: 8811.44 with loss1: 8811.44 and loss2: 0.00\n",
      "Epoch [60], train_loss: 9042.88 with loss1: 9042.88 and loss2: 0.00\n",
      "Epoch [61], train_loss: 8278.22 with loss1: 8278.22 and loss2: 0.00\n",
      "Epoch [62], train_loss: 8385.64 with loss1: 8385.64 and loss2: 0.00\n",
      "Epoch [63], train_loss: 9186.87 with loss1: 9186.87 and loss2: 0.00\n",
      "Epoch [64], train_loss: 7904.42 with loss1: 7904.42 and loss2: 0.00\n",
      "Epoch [65], train_loss: 7830.85 with loss1: 7830.85 and loss2: 0.00\n",
      "Epoch [66], train_loss: 7769.09 with loss1: 7769.09 and loss2: 0.00\n",
      "Epoch [67], train_loss: 7974.37 with loss1: 7974.37 and loss2: 0.00\n",
      "Epoch [68], train_loss: 8837.91 with loss1: 8837.91 and loss2: 0.00\n",
      "Epoch [69], train_loss: 8302.98 with loss1: 8302.98 and loss2: 0.00\n",
      "Epoch [70], train_loss: 7823.85 with loss1: 7823.85 and loss2: 0.00\n",
      "Epoch [71], train_loss: 8698.94 with loss1: 8698.94 and loss2: 0.00\n",
      "Epoch [72], train_loss: 7553.86 with loss1: 7553.86 and loss2: 0.00\n",
      "Epoch [73], train_loss: 8104.88 with loss1: 8104.88 and loss2: 0.00\n",
      "Epoch [74], train_loss: 8034.07 with loss1: 8034.07 and loss2: 0.00\n",
      "Epoch [75], train_loss: 7635.71 with loss1: 7635.71 and loss2: 0.00\n",
      "Epoch [76], train_loss: 7582.24 with loss1: 7582.24 and loss2: 0.00\n",
      "Epoch [77], train_loss: 7530.48 with loss1: 7530.48 and loss2: 0.00\n",
      "Epoch [78], train_loss: 7305.24 with loss1: 7305.24 and loss2: 0.00\n",
      "Epoch [79], train_loss: 7486.90 with loss1: 7486.90 and loss2: 0.00\n",
      "Epoch [80], train_loss: 7440.60 with loss1: 7440.60 and loss2: 0.00\n",
      "Epoch [81], train_loss: 8041.72 with loss1: 8041.72 and loss2: 0.00\n",
      "Epoch [82], train_loss: 7189.42 with loss1: 7189.42 and loss2: 0.00\n",
      "Epoch [83], train_loss: 7165.35 with loss1: 7165.35 and loss2: 0.00\n",
      "Epoch [84], train_loss: 7745.06 with loss1: 7745.06 and loss2: 0.00\n",
      "Epoch [85], train_loss: 7693.42 with loss1: 7693.42 and loss2: 0.00\n",
      "Epoch [86], train_loss: 7084.18 with loss1: 7084.18 and loss2: 0.00\n",
      "Epoch [87], train_loss: 7261.84 with loss1: 7261.84 and loss2: 0.00\n",
      "Epoch [88], train_loss: 7635.50 with loss1: 7635.50 and loss2: 0.00\n",
      "Epoch [89], train_loss: 7406.49 with loss1: 7406.49 and loss2: 0.00\n",
      "Epoch [90], train_loss: 7532.07 with loss1: 7532.07 and loss2: 0.00\n",
      "Epoch [91], train_loss: 7178.54 with loss1: 7178.54 and loss2: 0.00\n",
      "Epoch [92], train_loss: 7513.91 with loss1: 7513.91 and loss2: 0.00\n",
      "Epoch [93], train_loss: 7462.85 with loss1: 7462.85 and loss2: 0.00\n",
      "Epoch [94], train_loss: 7460.85 with loss1: 7460.85 and loss2: 0.00\n",
      "Epoch [95], train_loss: 8160.20 with loss1: 8160.20 and loss2: 0.00\n",
      "Epoch [96], train_loss: 7600.59 with loss1: 7600.59 and loss2: 0.00\n",
      "Epoch [97], train_loss: 7053.51 with loss1: 7053.51 and loss2: 0.00\n",
      "Epoch [98], train_loss: 7998.14 with loss1: 7998.14 and loss2: 0.00\n",
      "Epoch [99], train_loss: 6970.09 with loss1: 6970.09 and loss2: 0.00\n",
      "Epoch [100], train_loss: 7920.81 with loss1: 7920.81 and loss2: 0.00\n",
      "Epoch [101], train_loss: 7382.90 with loss1: 7382.90 and loss2: 0.00\n",
      "Epoch [102], train_loss: 7989.37 with loss1: 7989.37 and loss2: 0.00\n",
      "Epoch [103], train_loss: 7442.37 with loss1: 7442.37 and loss2: 0.00\n",
      "Epoch [104], train_loss: 6720.16 with loss1: 6720.16 and loss2: 0.00\n",
      "Epoch [105], train_loss: 6682.93 with loss1: 6682.93 and loss2: 0.00\n",
      "Epoch [106], train_loss: 7900.03 with loss1: 7900.03 and loss2: 0.00\n",
      "Epoch [107], train_loss: 7728.93 with loss1: 7728.93 and loss2: 0.00\n",
      "Epoch [108], train_loss: 7700.05 with loss1: 7700.05 and loss2: 0.00\n",
      "Epoch [109], train_loss: 7246.24 with loss1: 7246.24 and loss2: 0.00\n",
      "Epoch [110], train_loss: 7855.61 with loss1: 7855.61 and loss2: 0.00\n",
      "Epoch [111], train_loss: 6753.65 with loss1: 6753.65 and loss2: 0.00\n",
      "Epoch [112], train_loss: 7018.31 with loss1: 7018.31 and loss2: 0.00\n",
      "Epoch [113], train_loss: 7318.23 with loss1: 7318.23 and loss2: 0.00\n",
      "Epoch [114], train_loss: 6570.70 with loss1: 6570.70 and loss2: 0.00\n",
      "Epoch [115], train_loss: 6524.32 with loss1: 6524.32 and loss2: 0.00\n",
      "Epoch [116], train_loss: 7066.75 with loss1: 7066.75 and loss2: 0.00\n",
      "Epoch [117], train_loss: 7035.44 with loss1: 7035.44 and loss2: 0.00\n",
      "Epoch [118], train_loss: 6871.52 with loss1: 6871.52 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [119], train_loss: 6989.63 with loss1: 6989.63 and loss2: 0.00\n",
      "Epoch [120], train_loss: 6976.28 with loss1: 6976.28 and loss2: 0.00\n",
      "Epoch [121], train_loss: 7180.56 with loss1: 7180.56 and loss2: 0.00\n",
      "Epoch [122], train_loss: 6838.21 with loss1: 6838.21 and loss2: 0.00\n",
      "Epoch [123], train_loss: 7150.52 with loss1: 7150.52 and loss2: 0.00\n",
      "Epoch [124], train_loss: 7097.39 with loss1: 7097.39 and loss2: 0.00\n",
      "Epoch [125], train_loss: 6617.31 with loss1: 6617.31 and loss2: 0.00\n",
      "Epoch [126], train_loss: 6398.66 with loss1: 6398.66 and loss2: 0.00\n",
      "Epoch [127], train_loss: 6778.54 with loss1: 6778.54 and loss2: 0.00\n",
      "Epoch [128], train_loss: 6379.75 with loss1: 6379.75 and loss2: 0.00\n",
      "Epoch [129], train_loss: 6341.45 with loss1: 6341.45 and loss2: 0.00\n",
      "Epoch [130], train_loss: 7607.26 with loss1: 7607.26 and loss2: 0.00\n",
      "Epoch [131], train_loss: 6340.53 with loss1: 6340.53 and loss2: 0.00\n",
      "Epoch [132], train_loss: 6514.80 with loss1: 6514.80 and loss2: 0.00\n",
      "Epoch [133], train_loss: 6753.79 with loss1: 6753.79 and loss2: 0.00\n",
      "Epoch [134], train_loss: 6685.75 with loss1: 6685.75 and loss2: 0.00\n",
      "Epoch [135], train_loss: 6509.31 with loss1: 6509.31 and loss2: 0.00\n",
      "Epoch [136], train_loss: 6958.25 with loss1: 6958.25 and loss2: 0.00\n",
      "Epoch [137], train_loss: 6802.32 with loss1: 6802.32 and loss2: 0.00\n",
      "Epoch [138], train_loss: 7536.68 with loss1: 7536.68 and loss2: 0.00\n",
      "Epoch [139], train_loss: 6261.38 with loss1: 6261.38 and loss2: 0.00\n",
      "Epoch [140], train_loss: 6767.85 with loss1: 6767.85 and loss2: 0.00\n",
      "Epoch [141], train_loss: 6937.23 with loss1: 6937.23 and loss2: 0.00\n",
      "Epoch [142], train_loss: 6637.89 with loss1: 6637.89 and loss2: 0.00\n",
      "Epoch [143], train_loss: 6432.12 with loss1: 6432.12 and loss2: 0.00\n",
      "Epoch [144], train_loss: 6381.83 with loss1: 6381.83 and loss2: 0.00\n",
      "Epoch [145], train_loss: 7383.49 with loss1: 7383.49 and loss2: 0.00\n",
      "Epoch [146], train_loss: 6660.02 with loss1: 6660.02 and loss2: 0.00\n",
      "Epoch [147], train_loss: 6177.93 with loss1: 6177.93 and loss2: 0.00\n",
      "Epoch [148], train_loss: 6837.03 with loss1: 6837.03 and loss2: 0.00\n",
      "Epoch [149], train_loss: 7369.13 with loss1: 7369.13 and loss2: 0.00\n",
      "Epoch [150], train_loss: 6783.33 with loss1: 6783.33 and loss2: 0.00\n",
      "Epoch [151], train_loss: 6668.83 with loss1: 6668.83 and loss2: 0.00\n",
      "Epoch [152], train_loss: 6331.56 with loss1: 6331.56 and loss2: 0.00\n",
      "Epoch [153], train_loss: 6652.77 with loss1: 6652.77 and loss2: 0.00\n",
      "Epoch [154], train_loss: 6631.54 with loss1: 6631.54 and loss2: 0.00\n",
      "Epoch [155], train_loss: 7398.86 with loss1: 7398.86 and loss2: 0.00\n",
      "Epoch [156], train_loss: 6105.39 with loss1: 6105.39 and loss2: 0.00\n",
      "Epoch [157], train_loss: 6074.42 with loss1: 6074.42 and loss2: 0.00\n",
      "Epoch [158], train_loss: 6273.13 with loss1: 6273.13 and loss2: 0.00\n",
      "Epoch [159], train_loss: 6058.48 with loss1: 6058.48 and loss2: 0.00\n",
      "Epoch [160], train_loss: 6475.55 with loss1: 6475.55 and loss2: 0.00\n",
      "Epoch [161], train_loss: 6036.31 with loss1: 6036.31 and loss2: 0.00\n",
      "Epoch [162], train_loss: 6017.57 with loss1: 6017.57 and loss2: 0.00\n",
      "Epoch [163], train_loss: 6563.59 with loss1: 6563.59 and loss2: 0.00\n",
      "Epoch [164], train_loss: 6701.77 with loss1: 6701.77 and loss2: 0.00\n",
      "Epoch [165], train_loss: 6018.94 with loss1: 6018.94 and loss2: 0.00\n",
      "Epoch [166], train_loss: 6681.49 with loss1: 6681.49 and loss2: 0.00\n",
      "Epoch [167], train_loss: 6001.42 with loss1: 6001.42 and loss2: 0.00\n",
      "Epoch [168], train_loss: 5993.56 with loss1: 5993.56 and loss2: 0.00\n",
      "Epoch [169], train_loss: 6502.33 with loss1: 6502.33 and loss2: 0.00\n",
      "Epoch [170], train_loss: 6478.73 with loss1: 6478.73 and loss2: 0.00\n",
      "Epoch [171], train_loss: 5964.95 with loss1: 5964.95 and loss2: 0.00\n",
      "Epoch [172], train_loss: 6473.80 with loss1: 6473.80 and loss2: 0.00\n",
      "Epoch [173], train_loss: 5949.84 with loss1: 5949.84 and loss2: 0.00\n",
      "Epoch [174], train_loss: 6449.17 with loss1: 6449.17 and loss2: 0.00\n",
      "Epoch [175], train_loss: 5941.30 with loss1: 5941.30 and loss2: 0.00\n",
      "Epoch [176], train_loss: 5915.31 with loss1: 5915.31 and loss2: 0.00\n",
      "Epoch [177], train_loss: 7236.97 with loss1: 7236.97 and loss2: 0.00\n",
      "Epoch [178], train_loss: 6407.74 with loss1: 6407.74 and loss2: 0.00\n",
      "Epoch [179], train_loss: 6586.21 with loss1: 6586.21 and loss2: 0.00\n",
      "Epoch [180], train_loss: 6337.51 with loss1: 6337.51 and loss2: 0.00\n",
      "Epoch [181], train_loss: 6140.47 with loss1: 6140.47 and loss2: 0.00\n",
      "Epoch [182], train_loss: 6341.57 with loss1: 6341.57 and loss2: 0.00\n",
      "Epoch [183], train_loss: 6385.81 with loss1: 6385.81 and loss2: 0.00\n",
      "Epoch [184], train_loss: 5892.50 with loss1: 5892.50 and loss2: 0.00\n",
      "Epoch [185], train_loss: 6092.02 with loss1: 6092.02 and loss2: 0.00\n",
      "Epoch [186], train_loss: 6308.91 with loss1: 6308.91 and loss2: 0.00\n",
      "Epoch [187], train_loss: 6369.07 with loss1: 6369.07 and loss2: 0.00\n",
      "Epoch [188], train_loss: 6090.59 with loss1: 6090.59 and loss2: 0.00\n",
      "Epoch [189], train_loss: 6039.53 with loss1: 6039.53 and loss2: 0.00\n",
      "Epoch [190], train_loss: 6489.64 with loss1: 6489.64 and loss2: 0.00\n",
      "Epoch [191], train_loss: 7090.41 with loss1: 7090.41 and loss2: 0.00\n",
      "Epoch [192], train_loss: 6961.68 with loss1: 6961.68 and loss2: 0.00\n",
      "Epoch [193], train_loss: 6440.01 with loss1: 6440.01 and loss2: 0.00\n",
      "Epoch [194], train_loss: 6437.49 with loss1: 6437.49 and loss2: 0.00\n",
      "Epoch [195], train_loss: 6425.17 with loss1: 6425.17 and loss2: 0.00\n",
      "Epoch [196], train_loss: 6325.58 with loss1: 6325.58 and loss2: 0.00\n",
      "Epoch [197], train_loss: 6023.76 with loss1: 6023.76 and loss2: 0.00\n",
      "Epoch [198], train_loss: 5974.80 with loss1: 5974.80 and loss2: 0.00\n",
      "Epoch [199], train_loss: 5794.22 with loss1: 5794.22 and loss2: 0.00\n",
      "Epoch [200], train_loss: 6304.65 with loss1: 6304.65 and loss2: 0.00\n",
      "Epoch [201], train_loss: 6430.90 with loss1: 6430.90 and loss2: 0.00\n",
      "Epoch [202], train_loss: 5967.33 with loss1: 5967.33 and loss2: 0.00\n",
      "Epoch [203], train_loss: 5766.46 with loss1: 5766.46 and loss2: 0.00\n",
      "Epoch [204], train_loss: 6408.46 with loss1: 6408.46 and loss2: 0.00\n",
      "Epoch [205], train_loss: 7014.46 with loss1: 7014.46 and loss2: 0.00\n",
      "Epoch [206], train_loss: 6896.50 with loss1: 6896.50 and loss2: 0.00\n",
      "Epoch [207], train_loss: 6302.49 with loss1: 6302.49 and loss2: 0.00\n",
      "Epoch [208], train_loss: 6244.44 with loss1: 6244.44 and loss2: 0.00\n",
      "Epoch [209], train_loss: 5721.98 with loss1: 5721.98 and loss2: 0.00\n",
      "Epoch [210], train_loss: 6384.16 with loss1: 6384.16 and loss2: 0.00\n",
      "Epoch [211], train_loss: 6985.58 with loss1: 6985.58 and loss2: 0.00\n",
      "Epoch [212], train_loss: 6863.76 with loss1: 6863.76 and loss2: 0.00\n",
      "Epoch [213], train_loss: 6260.48 with loss1: 6260.48 and loss2: 0.00\n",
      "Epoch [214], train_loss: 6112.70 with loss1: 6112.70 and loss2: 0.00\n",
      "Epoch [215], train_loss: 7017.00 with loss1: 7017.00 and loss2: 0.00\n",
      "Epoch [216], train_loss: 6299.30 with loss1: 6299.30 and loss2: 0.00\n",
      "Epoch [217], train_loss: 6107.06 with loss1: 6107.06 and loss2: 0.00\n",
      "Epoch [218], train_loss: 5914.20 with loss1: 5914.20 and loss2: 0.00\n",
      "Epoch [219], train_loss: 6195.56 with loss1: 6195.56 and loss2: 0.00\n",
      "Epoch [220], train_loss: 6319.31 with loss1: 6319.31 and loss2: 0.00\n",
      "Epoch [221], train_loss: 5671.13 with loss1: 5671.13 and loss2: 0.00\n",
      "Epoch [222], train_loss: 5890.92 with loss1: 5890.92 and loss2: 0.00\n",
      "Epoch [223], train_loss: 6879.74 with loss1: 6879.74 and loss2: 0.00\n",
      "Epoch [224], train_loss: 5793.50 with loss1: 5793.50 and loss2: 0.00\n",
      "Epoch [225], train_loss: 5806.68 with loss1: 5806.68 and loss2: 0.00\n",
      "Epoch [226], train_loss: 6294.19 with loss1: 6294.19 and loss2: 0.00\n",
      "Epoch [227], train_loss: 6860.96 with loss1: 6860.96 and loss2: 0.00\n",
      "Epoch [228], train_loss: 6123.39 with loss1: 6123.39 and loss2: 0.00\n",
      "Epoch [229], train_loss: 6049.35 with loss1: 6049.35 and loss2: 0.00\n",
      "Epoch [230], train_loss: 5836.12 with loss1: 5836.12 and loss2: 0.00\n",
      "Epoch [231], train_loss: 6252.15 with loss1: 6252.15 and loss2: 0.00\n",
      "Epoch [232], train_loss: 6040.23 with loss1: 6040.23 and loss2: 0.00\n",
      "Epoch [233], train_loss: 6096.81 with loss1: 6096.81 and loss2: 0.00\n",
      "Epoch [234], train_loss: 5601.45 with loss1: 5601.45 and loss2: 0.00\n",
      "Epoch [235], train_loss: 6093.20 with loss1: 6093.20 and loss2: 0.00\n",
      "Epoch [236], train_loss: 6004.10 with loss1: 6004.10 and loss2: 0.00\n",
      "Epoch [237], train_loss: 6073.46 with loss1: 6073.46 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [238], train_loss: 6887.66 with loss1: 6887.66 and loss2: 0.00\n",
      "Epoch [239], train_loss: 6725.36 with loss1: 6725.36 and loss2: 0.00\n",
      "Epoch [240], train_loss: 6113.94 with loss1: 6113.94 and loss2: 0.00\n",
      "Epoch [241], train_loss: 5772.13 with loss1: 5772.13 and loss2: 0.00\n",
      "Epoch [242], train_loss: 5997.25 with loss1: 5997.25 and loss2: 0.00\n",
      "Epoch [243], train_loss: 6848.79 with loss1: 6848.79 and loss2: 0.00\n",
      "Epoch [244], train_loss: 6185.34 with loss1: 6185.34 and loss2: 0.00\n",
      "Epoch [245], train_loss: 6069.22 with loss1: 6069.22 and loss2: 0.00\n",
      "Epoch [246], train_loss: 5953.27 with loss1: 5953.27 and loss2: 0.00\n",
      "Epoch [247], train_loss: 6846.04 with loss1: 6846.04 and loss2: 0.00\n",
      "Epoch [248], train_loss: 6065.34 with loss1: 6065.34 and loss2: 0.00\n",
      "Epoch [249], train_loss: 5525.98 with loss1: 5525.98 and loss2: 0.00\n",
      "Epoch [250], train_loss: 6198.79 with loss1: 6198.79 and loss2: 0.00\n",
      "Epoch [251], train_loss: 5710.17 with loss1: 5710.17 and loss2: 0.00\n",
      "Epoch [252], train_loss: 6035.80 with loss1: 6035.80 and loss2: 0.00\n",
      "Epoch [253], train_loss: 5498.65 with loss1: 5498.65 and loss2: 0.00\n",
      "Epoch [254], train_loss: 5487.83 with loss1: 5487.83 and loss2: 0.00\n",
      "Epoch [255], train_loss: 6765.01 with loss1: 6765.01 and loss2: 0.00\n",
      "Epoch [256], train_loss: 5647.95 with loss1: 5647.95 and loss2: 0.00\n",
      "Epoch [257], train_loss: 5463.98 with loss1: 5463.98 and loss2: 0.00\n",
      "Epoch [258], train_loss: 6192.35 with loss1: 6192.35 and loss2: 0.00\n",
      "Epoch [259], train_loss: 6026.38 with loss1: 6026.38 and loss2: 0.00\n",
      "Epoch [260], train_loss: 6766.40 with loss1: 6766.40 and loss2: 0.00\n",
      "Epoch [261], train_loss: 5467.27 with loss1: 5467.27 and loss2: 0.00\n",
      "Epoch [262], train_loss: 5457.43 with loss1: 5457.43 and loss2: 0.00\n",
      "Epoch [263], train_loss: 6733.34 with loss1: 6733.34 and loss2: 0.00\n",
      "Epoch [264], train_loss: 5605.40 with loss1: 5605.40 and loss2: 0.00\n",
      "Epoch [265], train_loss: 5913.35 with loss1: 5913.35 and loss2: 0.00\n",
      "Epoch [266], train_loss: 5965.94 with loss1: 5965.94 and loss2: 0.00\n",
      "Epoch [267], train_loss: 5940.83 with loss1: 5940.83 and loss2: 0.00\n",
      "Epoch [268], train_loss: 5869.13 with loss1: 5869.13 and loss2: 0.00\n",
      "Epoch [269], train_loss: 6744.62 with loss1: 6744.62 and loss2: 0.00\n",
      "Epoch [270], train_loss: 6098.20 with loss1: 6098.20 and loss2: 0.00\n",
      "Epoch [271], train_loss: 6652.06 with loss1: 6652.06 and loss2: 0.00\n",
      "Epoch [272], train_loss: 5907.03 with loss1: 5907.03 and loss2: 0.00\n",
      "Epoch [273], train_loss: 6109.98 with loss1: 6109.98 and loss2: 0.00\n",
      "Epoch [274], train_loss: 6083.06 with loss1: 6083.06 and loss2: 0.00\n",
      "Epoch [275], train_loss: 5401.59 with loss1: 5401.59 and loss2: 0.00\n",
      "Epoch [276], train_loss: 5924.22 with loss1: 5924.22 and loss2: 0.00\n",
      "Epoch [277], train_loss: 5596.05 with loss1: 5596.05 and loss2: 0.00\n",
      "Epoch [278], train_loss: 6605.64 with loss1: 6605.64 and loss2: 0.00\n",
      "Epoch [279], train_loss: 5949.93 with loss1: 5949.93 and loss2: 0.00\n",
      "Epoch [280], train_loss: 5357.70 with loss1: 5357.70 and loss2: 0.00\n",
      "Epoch [281], train_loss: 5571.79 with loss1: 5571.79 and loss2: 0.00\n",
      "Epoch [282], train_loss: 5331.34 with loss1: 5331.34 and loss2: 0.00\n",
      "Epoch [283], train_loss: 6103.35 with loss1: 6103.35 and loss2: 0.00\n",
      "Epoch [284], train_loss: 5554.28 with loss1: 5554.28 and loss2: 0.00\n",
      "Epoch [285], train_loss: 6058.69 with loss1: 6058.69 and loss2: 0.00\n",
      "Epoch [286], train_loss: 6044.41 with loss1: 6044.41 and loss2: 0.00\n",
      "Epoch [287], train_loss: 5535.88 with loss1: 5535.88 and loss2: 0.00\n",
      "Epoch [288], train_loss: 5503.39 with loss1: 5503.39 and loss2: 0.00\n",
      "Epoch [289], train_loss: 6552.32 with loss1: 6552.32 and loss2: 0.00\n",
      "Epoch [290], train_loss: 6017.54 with loss1: 6017.54 and loss2: 0.00\n",
      "Epoch [291], train_loss: 5786.32 with loss1: 5786.32 and loss2: 0.00\n",
      "Epoch [292], train_loss: 5307.90 with loss1: 5307.90 and loss2: 0.00\n",
      "Epoch [293], train_loss: 5769.29 with loss1: 5769.29 and loss2: 0.00\n",
      "Epoch [294], train_loss: 5295.86 with loss1: 5295.86 and loss2: 0.00\n",
      "Epoch [295], train_loss: 5277.54 with loss1: 5277.54 and loss2: 0.00\n",
      "Epoch [296], train_loss: 6049.86 with loss1: 6049.86 and loss2: 0.00\n",
      "Epoch [297], train_loss: 5487.41 with loss1: 5487.41 and loss2: 0.00\n",
      "Epoch [298], train_loss: 5848.63 with loss1: 5848.63 and loss2: 0.00\n",
      "Epoch [299], train_loss: 5833.30 with loss1: 5833.30 and loss2: 0.00\n",
      "Epoch [300], train_loss: 6563.59 with loss1: 6563.59 and loss2: 0.00\n",
      "Epoch [301], train_loss: 5998.34 with loss1: 5998.34 and loss2: 0.00\n",
      "Epoch [302], train_loss: 5247.47 with loss1: 5247.47 and loss2: 0.00\n",
      "Epoch [303], train_loss: 5238.17 with loss1: 5238.17 and loss2: 0.00\n",
      "Epoch [304], train_loss: 6530.01 with loss1: 6530.01 and loss2: 0.00\n",
      "Epoch [305], train_loss: 5230.53 with loss1: 5230.53 and loss2: 0.00\n",
      "Epoch [306], train_loss: 5722.50 with loss1: 5722.50 and loss2: 0.00\n",
      "Epoch [307], train_loss: 6529.75 with loss1: 6529.75 and loss2: 0.00\n",
      "Epoch [308], train_loss: 5404.26 with loss1: 5404.26 and loss2: 0.00\n",
      "Epoch [309], train_loss: 6007.31 with loss1: 6007.31 and loss2: 0.00\n",
      "Epoch [310], train_loss: 5707.11 with loss1: 5707.11 and loss2: 0.00\n",
      "Epoch [311], train_loss: 5779.64 with loss1: 5779.64 and loss2: 0.00\n",
      "Epoch [312], train_loss: 5674.58 with loss1: 5674.58 and loss2: 0.00\n",
      "Epoch [313], train_loss: 6515.80 with loss1: 6515.80 and loss2: 0.00\n",
      "Epoch [314], train_loss: 5958.84 with loss1: 5958.84 and loss2: 0.00\n",
      "Epoch [315], train_loss: 5769.43 with loss1: 5769.43 and loss2: 0.00\n",
      "Epoch [316], train_loss: 5756.06 with loss1: 5756.06 and loss2: 0.00\n",
      "Epoch [317], train_loss: 5754.58 with loss1: 5754.58 and loss2: 0.00\n",
      "Epoch [318], train_loss: 6458.44 with loss1: 6458.44 and loss2: 0.00\n",
      "Epoch [319], train_loss: 5367.42 with loss1: 5367.42 and loss2: 0.00\n",
      "Epoch [320], train_loss: 5166.03 with loss1: 5166.03 and loss2: 0.00\n",
      "Epoch [321], train_loss: 5375.47 with loss1: 5375.47 and loss2: 0.00\n",
      "Epoch [322], train_loss: 5667.59 with loss1: 5667.59 and loss2: 0.00\n",
      "Epoch [323], train_loss: 5678.17 with loss1: 5678.17 and loss2: 0.00\n",
      "Epoch [324], train_loss: 5654.99 with loss1: 5654.99 and loss2: 0.00\n",
      "Epoch [325], train_loss: 5971.37 with loss1: 5971.37 and loss2: 0.00\n",
      "Epoch [326], train_loss: 5931.98 with loss1: 5931.98 and loss2: 0.00\n",
      "Epoch [327], train_loss: 5146.69 with loss1: 5146.69 and loss2: 0.00\n",
      "Epoch [328], train_loss: 5944.15 with loss1: 5944.15 and loss2: 0.00\n",
      "Epoch [329], train_loss: 5137.46 with loss1: 5137.46 and loss2: 0.00\n",
      "Epoch [330], train_loss: 5111.41 with loss1: 5111.41 and loss2: 0.00\n",
      "Epoch [331], train_loss: 6367.91 with loss1: 6367.91 and loss2: 0.00\n",
      "Epoch [332], train_loss: 6266.88 with loss1: 6266.88 and loss2: 0.00\n",
      "Epoch [333], train_loss: 6252.75 with loss1: 6252.75 and loss2: 0.00\n",
      "Epoch [334], train_loss: 5738.22 with loss1: 5738.22 and loss2: 0.00\n",
      "Epoch [335], train_loss: 5306.41 with loss1: 5306.41 and loss2: 0.00\n",
      "Epoch [336], train_loss: 5289.03 with loss1: 5289.03 and loss2: 0.00\n",
      "Epoch [337], train_loss: 5700.90 with loss1: 5700.90 and loss2: 0.00\n",
      "Epoch [338], train_loss: 5679.99 with loss1: 5679.99 and loss2: 0.00\n",
      "Epoch [339], train_loss: 5565.78 with loss1: 5565.78 and loss2: 0.00\n",
      "Epoch [340], train_loss: 5655.50 with loss1: 5655.50 and loss2: 0.00\n",
      "Epoch [341], train_loss: 5638.82 with loss1: 5638.82 and loss2: 0.00\n",
      "Epoch [342], train_loss: 6330.49 with loss1: 6330.49 and loss2: 0.00\n",
      "Epoch [343], train_loss: 5577.91 with loss1: 5577.91 and loss2: 0.00\n",
      "Epoch [344], train_loss: 5558.63 with loss1: 5558.63 and loss2: 0.00\n",
      "Epoch [345], train_loss: 6312.31 with loss1: 6312.31 and loss2: 0.00\n",
      "Epoch [346], train_loss: 5887.88 with loss1: 5887.88 and loss2: 0.00\n",
      "Epoch [347], train_loss: 5879.30 with loss1: 5879.30 and loss2: 0.00\n",
      "Epoch [348], train_loss: 5534.16 with loss1: 5534.16 and loss2: 0.00\n",
      "Epoch [349], train_loss: 6271.14 with loss1: 6271.14 and loss2: 0.00\n",
      "Epoch [350], train_loss: 5202.60 with loss1: 5202.60 and loss2: 0.00\n",
      "Epoch [351], train_loss: 5200.80 with loss1: 5200.80 and loss2: 0.00\n",
      "Epoch [352], train_loss: 6195.75 with loss1: 6195.75 and loss2: 0.00\n",
      "Epoch [353], train_loss: 6131.59 with loss1: 6131.59 and loss2: 0.00\n",
      "Epoch [354], train_loss: 6124.34 with loss1: 6124.34 and loss2: 0.00\n",
      "Epoch [355], train_loss: 5640.85 with loss1: 5640.85 and loss2: 0.00\n",
      "Epoch [356], train_loss: 5483.34 with loss1: 5483.34 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [357], train_loss: 6200.45 with loss1: 6200.45 and loss2: 0.00\n",
      "Epoch [358], train_loss: 6102.05 with loss1: 6102.05 and loss2: 0.00\n",
      "Epoch [359], train_loss: 5853.05 with loss1: 5853.05 and loss2: 0.00\n",
      "Epoch [360], train_loss: 5593.33 with loss1: 5593.33 and loss2: 0.00\n",
      "Epoch [361], train_loss: 5455.84 with loss1: 5455.84 and loss2: 0.00\n",
      "Epoch [362], train_loss: 4957.16 with loss1: 4957.16 and loss2: 0.00\n",
      "Epoch [363], train_loss: 6151.73 with loss1: 6151.73 and loss2: 0.00\n",
      "Epoch [364], train_loss: 5132.87 with loss1: 5132.87 and loss2: 0.00\n",
      "Epoch [365], train_loss: 5127.86 with loss1: 5127.86 and loss2: 0.00\n",
      "Epoch [366], train_loss: 5112.59 with loss1: 5112.59 and loss2: 0.00\n",
      "Epoch [367], train_loss: 5453.71 with loss1: 5453.71 and loss2: 0.00\n",
      "Epoch [368], train_loss: 5553.46 with loss1: 5553.46 and loss2: 0.00\n",
      "Epoch [369], train_loss: 5129.71 with loss1: 5129.71 and loss2: 0.00\n",
      "Epoch [370], train_loss: 5098.52 with loss1: 5098.52 and loss2: 0.00\n",
      "Epoch [371], train_loss: 6052.64 with loss1: 6052.64 and loss2: 0.00\n",
      "Epoch [372], train_loss: 4869.38 with loss1: 4869.38 and loss2: 0.00\n",
      "Epoch [373], train_loss: 5549.30 with loss1: 5549.30 and loss2: 0.00\n",
      "Epoch [374], train_loss: 5394.80 with loss1: 5394.80 and loss2: 0.00\n",
      "Epoch [375], train_loss: 5394.36 with loss1: 5394.36 and loss2: 0.00\n",
      "Epoch [376], train_loss: 5079.59 with loss1: 5079.59 and loss2: 0.00\n",
      "Epoch [377], train_loss: 5533.51 with loss1: 5533.51 and loss2: 0.00\n",
      "Epoch [378], train_loss: 5361.95 with loss1: 5361.95 and loss2: 0.00\n",
      "Epoch [379], train_loss: 5065.94 with loss1: 5065.94 and loss2: 0.00\n",
      "Epoch [380], train_loss: 5504.25 with loss1: 5504.25 and loss2: 0.00\n",
      "Epoch [381], train_loss: 5051.55 with loss1: 5051.55 and loss2: 0.00\n",
      "Epoch [382], train_loss: 5024.75 with loss1: 5024.75 and loss2: 0.00\n",
      "Epoch [383], train_loss: 5504.52 with loss1: 5504.52 and loss2: 0.00\n",
      "Epoch [384], train_loss: 5347.50 with loss1: 5347.50 and loss2: 0.00\n",
      "Epoch [385], train_loss: 5845.85 with loss1: 5845.85 and loss2: 0.00\n",
      "Epoch [386], train_loss: 5322.98 with loss1: 5322.98 and loss2: 0.00\n",
      "Epoch [387], train_loss: 4828.54 with loss1: 4828.54 and loss2: 0.00\n",
      "Epoch [388], train_loss: 5480.08 with loss1: 5480.08 and loss2: 0.00\n",
      "Epoch [389], train_loss: 5302.91 with loss1: 5302.91 and loss2: 0.00\n",
      "Epoch [390], train_loss: 5323.32 with loss1: 5323.32 and loss2: 0.00\n",
      "Epoch [391], train_loss: 5833.98 with loss1: 5833.98 and loss2: 0.00\n",
      "Epoch [392], train_loss: 5271.75 with loss1: 5271.75 and loss2: 0.00\n",
      "Epoch [393], train_loss: 5937.35 with loss1: 5937.35 and loss2: 0.00\n",
      "Epoch [394], train_loss: 5473.96 with loss1: 5473.96 and loss2: 0.00\n",
      "Epoch [395], train_loss: 4744.70 with loss1: 4744.70 and loss2: 0.00\n",
      "Epoch [396], train_loss: 4729.88 with loss1: 4729.88 and loss2: 0.00\n",
      "Epoch [397], train_loss: 5859.88 with loss1: 5859.88 and loss2: 0.00\n",
      "Epoch [398], train_loss: 5257.37 with loss1: 5257.37 and loss2: 0.00\n",
      "Epoch [399], train_loss: 5912.96 with loss1: 5912.96 and loss2: 0.00\n",
      "Epoch [400], train_loss: 4945.22 with loss1: 4945.22 and loss2: 0.00\n",
      "Epoch [401], train_loss: 5794.99 with loss1: 5794.99 and loss2: 0.00\n",
      "Epoch [402], train_loss: 5763.59 with loss1: 5763.59 and loss2: 0.00\n",
      "Epoch [403], train_loss: 4699.61 with loss1: 4699.61 and loss2: 0.00\n",
      "Epoch [404], train_loss: 4705.41 with loss1: 4705.41 and loss2: 0.00\n",
      "Epoch [405], train_loss: 5242.81 with loss1: 5242.81 and loss2: 0.00\n",
      "Epoch [406], train_loss: 5394.99 with loss1: 5394.99 and loss2: 0.00\n",
      "Epoch [407], train_loss: 4912.01 with loss1: 4912.01 and loss2: 0.00\n",
      "Epoch [408], train_loss: 4655.47 with loss1: 4655.47 and loss2: 0.00\n",
      "Epoch [409], train_loss: 4903.77 with loss1: 4903.77 and loss2: 0.00\n",
      "Epoch [410], train_loss: 5822.30 with loss1: 5822.30 and loss2: 0.00\n",
      "Epoch [411], train_loss: 5792.59 with loss1: 5792.59 and loss2: 0.00\n",
      "Epoch [412], train_loss: 5757.93 with loss1: 5757.93 and loss2: 0.00\n",
      "Epoch [413], train_loss: 5407.10 with loss1: 5407.10 and loss2: 0.00\n",
      "Epoch [414], train_loss: 5187.74 with loss1: 5187.74 and loss2: 0.00\n",
      "Epoch [415], train_loss: 5776.92 with loss1: 5776.92 and loss2: 0.00\n",
      "Epoch [416], train_loss: 5369.72 with loss1: 5369.72 and loss2: 0.00\n",
      "Epoch [417], train_loss: 5784.08 with loss1: 5784.08 and loss2: 0.00\n",
      "Epoch [418], train_loss: 4599.70 with loss1: 4599.70 and loss2: 0.00\n",
      "Epoch [419], train_loss: 5358.30 with loss1: 5358.30 and loss2: 0.00\n",
      "Epoch [420], train_loss: 5152.35 with loss1: 5152.35 and loss2: 0.00\n",
      "Epoch [421], train_loss: 4847.57 with loss1: 4847.57 and loss2: 0.00\n",
      "Epoch [422], train_loss: 5746.06 with loss1: 5746.06 and loss2: 0.00\n",
      "Epoch [423], train_loss: 5717.80 with loss1: 5717.80 and loss2: 0.00\n",
      "Epoch [424], train_loss: 5684.42 with loss1: 5684.42 and loss2: 0.00\n",
      "Epoch [425], train_loss: 5134.57 with loss1: 5134.57 and loss2: 0.00\n",
      "Epoch [426], train_loss: 5752.50 with loss1: 5752.50 and loss2: 0.00\n",
      "Epoch [427], train_loss: 5699.41 with loss1: 5699.41 and loss2: 0.00\n",
      "Epoch [428], train_loss: 5684.83 with loss1: 5684.83 and loss2: 0.00\n",
      "Epoch [429], train_loss: 5320.43 with loss1: 5320.43 and loss2: 0.00\n",
      "Epoch [430], train_loss: 5109.87 with loss1: 5109.87 and loss2: 0.00\n",
      "Epoch [431], train_loss: 5668.52 with loss1: 5668.52 and loss2: 0.00\n",
      "Epoch [432], train_loss: 5619.82 with loss1: 5619.82 and loss2: 0.00\n",
      "Epoch [433], train_loss: 5614.78 with loss1: 5614.78 and loss2: 0.00\n",
      "Epoch [434], train_loss: 5075.93 with loss1: 5075.93 and loss2: 0.00\n",
      "Epoch [435], train_loss: 5098.75 with loss1: 5098.75 and loss2: 0.00\n",
      "Epoch [436], train_loss: 5747.07 with loss1: 5747.07 and loss2: 0.00\n",
      "Epoch [437], train_loss: 5600.07 with loss1: 5600.07 and loss2: 0.00\n",
      "Epoch [438], train_loss: 4507.82 with loss1: 4507.82 and loss2: 0.00\n",
      "Epoch [439], train_loss: 5059.01 with loss1: 5059.01 and loss2: 0.00\n",
      "Epoch [440], train_loss: 4726.57 with loss1: 4726.57 and loss2: 0.00\n",
      "Epoch [441], train_loss: 5587.82 with loss1: 5587.82 and loss2: 0.00\n",
      "Epoch [442], train_loss: 5023.18 with loss1: 5023.18 and loss2: 0.00\n",
      "Epoch [443], train_loss: 4715.07 with loss1: 4715.07 and loss2: 0.00\n",
      "Epoch [444], train_loss: 5567.05 with loss1: 5567.05 and loss2: 0.00\n",
      "Epoch [445], train_loss: 5685.20 with loss1: 5685.20 and loss2: 0.00\n",
      "Epoch [446], train_loss: 5012.40 with loss1: 5012.40 and loss2: 0.00\n",
      "Epoch [447], train_loss: 5708.06 with loss1: 5708.06 and loss2: 0.00\n",
      "Epoch [448], train_loss: 5250.49 with loss1: 5250.49 and loss2: 0.00\n",
      "Epoch [449], train_loss: 5021.35 with loss1: 5021.35 and loss2: 0.00\n",
      "Epoch [450], train_loss: 5009.33 with loss1: 5009.33 and loss2: 0.00\n",
      "Epoch [451], train_loss: 5529.03 with loss1: 5529.03 and loss2: 0.00\n",
      "Epoch [452], train_loss: 5252.14 with loss1: 5252.14 and loss2: 0.00\n",
      "Epoch [453], train_loss: 4676.72 with loss1: 4676.72 and loss2: 0.00\n",
      "Epoch [454], train_loss: 4435.18 with loss1: 4435.18 and loss2: 0.00\n",
      "Epoch [455], train_loss: 5522.40 with loss1: 5522.40 and loss2: 0.00\n",
      "Epoch [456], train_loss: 4966.89 with loss1: 4966.89 and loss2: 0.00\n",
      "Epoch [457], train_loss: 4661.41 with loss1: 4661.41 and loss2: 0.00\n",
      "Epoch [458], train_loss: 5503.32 with loss1: 5503.32 and loss2: 0.00\n",
      "Epoch [459], train_loss: 5220.48 with loss1: 5220.48 and loss2: 0.00\n",
      "Epoch [460], train_loss: 5486.19 with loss1: 5486.19 and loss2: 0.00\n",
      "Epoch [461], train_loss: 4927.07 with loss1: 4927.07 and loss2: 0.00\n",
      "Epoch [462], train_loss: 5213.19 with loss1: 5213.19 and loss2: 0.00\n",
      "Epoch [463], train_loss: 4643.08 with loss1: 4643.08 and loss2: 0.00\n",
      "Epoch [464], train_loss: 5683.24 with loss1: 5683.24 and loss2: 0.00\n",
      "Epoch [465], train_loss: 5460.05 with loss1: 5460.05 and loss2: 0.00\n",
      "Epoch [466], train_loss: 5440.10 with loss1: 5440.10 and loss2: 0.00\n",
      "Epoch [467], train_loss: 5191.16 with loss1: 5191.16 and loss2: 0.00\n",
      "Epoch [468], train_loss: 4380.84 with loss1: 4380.84 and loss2: 0.00\n",
      "Epoch [469], train_loss: 4373.73 with loss1: 4373.73 and loss2: 0.00\n",
      "Epoch [470], train_loss: 4357.31 with loss1: 4357.31 and loss2: 0.00\n",
      "Epoch [471], train_loss: 5449.80 with loss1: 5449.80 and loss2: 0.00\n",
      "Epoch [472], train_loss: 4345.80 with loss1: 4345.80 and loss2: 0.00\n",
      "Epoch [473], train_loss: 4343.00 with loss1: 4343.00 and loss2: 0.00\n",
      "Epoch [474], train_loss: 4589.04 with loss1: 4589.04 and loss2: 0.00\n",
      "Epoch [475], train_loss: 5645.04 with loss1: 5645.04 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [476], train_loss: 4584.70 with loss1: 4584.70 and loss2: 0.00\n",
      "Epoch [477], train_loss: 4335.53 with loss1: 4335.53 and loss2: 0.00\n",
      "Epoch [478], train_loss: 4567.22 with loss1: 4567.22 and loss2: 0.00\n",
      "Epoch [479], train_loss: 4309.35 with loss1: 4309.35 and loss2: 0.00\n",
      "Epoch [480], train_loss: 4892.52 with loss1: 4892.52 and loss2: 0.00\n",
      "Epoch [481], train_loss: 5392.86 with loss1: 5392.86 and loss2: 0.00\n",
      "Epoch [482], train_loss: 5607.52 with loss1: 5607.52 and loss2: 0.00\n",
      "Epoch [483], train_loss: 4563.71 with loss1: 4563.71 and loss2: 0.00\n",
      "Epoch [484], train_loss: 5370.04 with loss1: 5370.04 and loss2: 0.00\n",
      "Epoch [485], train_loss: 4294.14 with loss1: 4294.14 and loss2: 0.00\n",
      "Epoch [486], train_loss: 5623.77 with loss1: 5623.77 and loss2: 0.00\n",
      "Epoch [487], train_loss: 4547.22 with loss1: 4547.22 and loss2: 0.00\n",
      "Epoch [488], train_loss: 4301.48 with loss1: 4301.48 and loss2: 0.00\n",
      "Epoch [489], train_loss: 4870.01 with loss1: 4870.01 and loss2: 0.00\n",
      "Epoch [490], train_loss: 5636.11 with loss1: 5636.11 and loss2: 0.00\n",
      "Epoch [491], train_loss: 4832.18 with loss1: 4832.18 and loss2: 0.00\n",
      "Epoch [492], train_loss: 4519.82 with loss1: 4519.82 and loss2: 0.00\n",
      "Epoch [493], train_loss: 5605.39 with loss1: 5605.39 and loss2: 0.00\n",
      "Epoch [494], train_loss: 5572.50 with loss1: 5572.50 and loss2: 0.00\n",
      "Epoch [495], train_loss: 5342.36 with loss1: 5342.36 and loss2: 0.00\n",
      "Epoch [496], train_loss: 4507.60 with loss1: 4507.60 and loss2: 0.00\n",
      "Epoch [497], train_loss: 4264.04 with loss1: 4264.04 and loss2: 0.00\n",
      "Epoch [498], train_loss: 4821.93 with loss1: 4821.93 and loss2: 0.00\n",
      "Epoch [499], train_loss: 5613.67 with loss1: 5613.67 and loss2: 0.00\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=200, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# new model\n",
    "# delete loss2\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history = fit(epochs=500, lr=1e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9692d86c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 26639.27 with loss1: 26639.27 and loss2: 0.00\n",
      "Epoch [1], train_loss: 99967.50 with loss1: 99967.50 and loss2: 0.00\n",
      "Epoch [2], train_loss: 90690.41 with loss1: 90690.41 and loss2: 0.00\n",
      "Epoch [3], train_loss: 90513.84 with loss1: 90513.84 and loss2: 0.00\n",
      "Epoch [4], train_loss: 96516.62 with loss1: 96516.62 and loss2: 0.00\n",
      "Epoch [5], train_loss: 99967.50 with loss1: 99967.50 and loss2: 0.00\n",
      "Epoch [6], train_loss: 105793.66 with loss1: 105793.66 and loss2: 0.00\n",
      "Epoch [7], train_loss: 90513.84 with loss1: 90513.84 and loss2: 0.00\n",
      "Epoch [8], train_loss: 105793.66 with loss1: 105793.66 and loss2: 0.00\n",
      "Epoch [9], train_loss: 105793.66 with loss1: 105793.66 and loss2: 0.00\n",
      "Epoch [10], train_loss: 105970.28 with loss1: 105970.28 and loss2: 0.00\n",
      "Epoch [11], train_loss: 90513.84 with loss1: 90513.84 and loss2: 0.00\n",
      "Epoch [12], train_loss: 105793.66 with loss1: 105793.66 and loss2: 0.00\n",
      "Epoch [13], train_loss: 105793.66 with loss1: 105793.66 and loss2: 0.00\n",
      "Epoch [14], train_loss: 96516.62 with loss1: 96516.62 and loss2: 0.00\n",
      "Epoch [15], train_loss: 105970.28 with loss1: 105970.28 and loss2: 0.00\n",
      "Epoch [16], train_loss: 99967.50 with loss1: 99967.50 and loss2: 0.00\n",
      "Epoch [17], train_loss: 96516.62 with loss1: 96516.62 and loss2: 0.00\n",
      "Epoch [18], train_loss: 90513.85 with loss1: 90513.85 and loss2: 0.00\n",
      "Epoch [19], train_loss: 99967.50 with loss1: 99967.50 and loss2: 0.00\n",
      "Epoch [20], train_loss: 99967.52 with loss1: 99967.52 and loss2: 0.00\n",
      "Epoch [21], train_loss: 90513.84 with loss1: 90513.84 and loss2: 0.00\n",
      "Epoch [22], train_loss: 105970.28 with loss1: 105970.28 and loss2: 0.00\n",
      "Epoch [23], train_loss: 90690.41 with loss1: 90690.41 and loss2: 0.00\n",
      "Epoch [24], train_loss: 90513.84 with loss1: 90513.84 and loss2: 0.00\n",
      "Epoch [25], train_loss: 90513.85 with loss1: 90513.85 and loss2: 0.00\n",
      "Epoch [26], train_loss: 90513.85 with loss1: 90513.85 and loss2: 0.00\n",
      "Epoch [27], train_loss: 90513.85 with loss1: 90513.85 and loss2: 0.00\n",
      "Epoch [28], train_loss: 90690.41 with loss1: 90690.41 and loss2: 0.00\n",
      "Epoch [29], train_loss: 90690.41 with loss1: 90690.41 and loss2: 0.00\n",
      "Epoch [30], train_loss: 105970.28 with loss1: 105970.28 and loss2: 0.00\n",
      "Epoch [31], train_loss: 96516.62 with loss1: 96516.62 and loss2: 0.00\n",
      "Epoch [32], train_loss: 90513.84 with loss1: 90513.84 and loss2: 0.00\n",
      "Epoch [33], train_loss: 105970.28 with loss1: 105970.28 and loss2: 0.00\n",
      "Epoch [34], train_loss: 96516.62 with loss1: 96516.62 and loss2: 0.00\n",
      "Epoch [35], train_loss: 90690.41 with loss1: 90690.41 and loss2: 0.00\n",
      "Epoch [36], train_loss: 90513.84 with loss1: 90513.84 and loss2: 0.00\n",
      "Epoch [37], train_loss: 99967.52 with loss1: 99967.52 and loss2: 0.00\n",
      "Epoch [38], train_loss: 96516.62 with loss1: 96516.62 and loss2: 0.00\n",
      "Epoch [39], train_loss: 99967.52 with loss1: 99967.52 and loss2: 0.00\n",
      "Epoch [40], train_loss: 90690.41 with loss1: 90690.41 and loss2: 0.00\n",
      "Epoch [41], train_loss: 90690.41 with loss1: 90690.41 and loss2: 0.00\n",
      "Epoch [42], train_loss: 105970.28 with loss1: 105970.28 and loss2: 0.00\n",
      "Epoch [43], train_loss: 90513.84 with loss1: 90513.84 and loss2: 0.00\n",
      "Epoch [44], train_loss: 105793.66 with loss1: 105793.66 and loss2: 0.00\n",
      "Epoch [45], train_loss: 90690.41 with loss1: 90690.41 and loss2: 0.00\n",
      "Epoch [46], train_loss: 90513.84 with loss1: 90513.84 and loss2: 0.00\n",
      "Epoch [47], train_loss: 99967.50 with loss1: 99967.50 and loss2: 0.00\n",
      "Epoch [48], train_loss: 96516.62 with loss1: 96516.62 and loss2: 0.00\n",
      "Epoch [49], train_loss: 105793.66 with loss1: 105793.66 and loss2: 0.00\n",
      "Epoch [50], train_loss: 90513.85 with loss1: 90513.85 and loss2: 0.00\n",
      "Epoch [51], train_loss: 96516.62 with loss1: 96516.62 and loss2: 0.00\n",
      "Epoch [52], train_loss: 105970.28 with loss1: 105970.28 and loss2: 0.00\n",
      "Epoch [53], train_loss: 90690.41 with loss1: 90690.41 and loss2: 0.00\n",
      "Epoch [54], train_loss: 90513.84 with loss1: 90513.84 and loss2: 0.00\n",
      "Epoch [55], train_loss: 99967.50 with loss1: 99967.50 and loss2: 0.00\n",
      "Epoch [56], train_loss: 90513.84 with loss1: 90513.84 and loss2: 0.00\n",
      "Epoch [57], train_loss: 90513.85 with loss1: 90513.85 and loss2: 0.00\n",
      "Epoch [58], train_loss: 90690.41 with loss1: 90690.41 and loss2: 0.00\n",
      "Epoch [59], train_loss: 105970.28 with loss1: 105970.28 and loss2: 0.00\n",
      "Epoch [60], train_loss: 99967.52 with loss1: 99967.52 and loss2: 0.00\n",
      "Epoch [61], train_loss: 105970.28 with loss1: 105970.28 and loss2: 0.00\n",
      "Epoch [62], train_loss: 99967.50 with loss1: 99967.50 and loss2: 0.00\n",
      "Epoch [63], train_loss: 90513.85 with loss1: 90513.85 and loss2: 0.00\n",
      "Epoch [64], train_loss: 105793.66 with loss1: 105793.66 and loss2: 0.00\n",
      "Epoch [65], train_loss: 90513.85 with loss1: 90513.85 and loss2: 0.00\n",
      "Epoch [66], train_loss: 90513.84 with loss1: 90513.84 and loss2: 0.00\n",
      "Epoch [67], train_loss: 105793.66 with loss1: 105793.66 and loss2: 0.00\n",
      "Epoch [68], train_loss: 96516.62 with loss1: 96516.62 and loss2: 0.00\n",
      "Epoch [69], train_loss: 105793.66 with loss1: 105793.66 and loss2: 0.00\n",
      "Epoch [70], train_loss: 90513.85 with loss1: 90513.85 and loss2: 0.00\n",
      "Epoch [71], train_loss: 90513.84 with loss1: 90513.84 and loss2: 0.00\n",
      "Epoch [72], train_loss: 90513.85 with loss1: 90513.85 and loss2: 0.00\n",
      "Epoch [73], train_loss: 90513.85 with loss1: 90513.85 and loss2: 0.00\n",
      "Epoch [74], train_loss: 99967.50 with loss1: 99967.50 and loss2: 0.00\n",
      "Epoch [75], train_loss: 99967.50 with loss1: 99967.50 and loss2: 0.00\n",
      "Epoch [76], train_loss: 96516.62 with loss1: 96516.62 and loss2: 0.00\n",
      "Epoch [77], train_loss: 105970.28 with loss1: 105970.28 and loss2: 0.00\n",
      "Epoch [78], train_loss: 105970.28 with loss1: 105970.28 and loss2: 0.00\n",
      "Epoch [79], train_loss: 90690.41 with loss1: 90690.41 and loss2: 0.00\n",
      "Epoch [80], train_loss: 90513.84 with loss1: 90513.84 and loss2: 0.00\n",
      "Epoch [81], train_loss: 105970.28 with loss1: 105970.28 and loss2: 0.00\n",
      "Epoch [82], train_loss: 105970.28 with loss1: 105970.28 and loss2: 0.00\n",
      "Epoch [83], train_loss: 105793.66 with loss1: 105793.66 and loss2: 0.00\n",
      "Epoch [84], train_loss: 96516.62 with loss1: 96516.62 and loss2: 0.00\n",
      "Epoch [85], train_loss: 90690.41 with loss1: 90690.41 and loss2: 0.00\n",
      "Epoch [86], train_loss: 96516.62 with loss1: 96516.62 and loss2: 0.00\n",
      "Epoch [87], train_loss: 99967.52 with loss1: 99967.52 and loss2: 0.00\n",
      "Epoch [88], train_loss: 105970.28 with loss1: 105970.28 and loss2: 0.00\n",
      "Epoch [89], train_loss: 90513.84 with loss1: 90513.84 and loss2: 0.00\n",
      "Epoch [90], train_loss: 99967.52 with loss1: 99967.52 and loss2: 0.00\n",
      "Epoch [91], train_loss: 90690.41 with loss1: 90690.41 and loss2: 0.00\n",
      "Epoch [92], train_loss: 105970.28 with loss1: 105970.28 and loss2: 0.00\n",
      "Epoch [93], train_loss: 105793.66 with loss1: 105793.66 and loss2: 0.00\n",
      "Epoch [94], train_loss: 105970.28 with loss1: 105970.28 and loss2: 0.00\n",
      "Epoch [95], train_loss: 105970.28 with loss1: 105970.28 and loss2: 0.00\n",
      "Epoch [96], train_loss: 96516.62 with loss1: 96516.62 and loss2: 0.00\n",
      "Epoch [97], train_loss: 96516.62 with loss1: 96516.62 and loss2: 0.00\n",
      "Epoch [98], train_loss: 90690.41 with loss1: 90690.41 and loss2: 0.00\n",
      "Epoch [99], train_loss: 90690.41 with loss1: 90690.41 and loss2: 0.00\n",
      "Epoch [100], train_loss: 105970.28 with loss1: 105970.28 and loss2: 0.00\n",
      "Epoch [101], train_loss: 96516.62 with loss1: 96516.62 and loss2: 0.00\n",
      "Epoch [102], train_loss: 105793.66 with loss1: 105793.66 and loss2: 0.00\n",
      "Epoch [103], train_loss: 90690.41 with loss1: 90690.41 and loss2: 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/39197986/ipykernel_199641/1562137790.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# delete loss2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# dataset: 4 images (IMCL not included)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/local/39197986/ipykernel_199641/4066282492.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, lr, h0, model, train_loader, val_loader, opt_func)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;31m#print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.10/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.10/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# continued\n",
    "# enc_out_dim=200, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# new model\n",
    "# delete loss2\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history = fit(epochs=500, lr=5e-5, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f15e75e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 99433.16 with loss1: 99433.16 and loss2: 0.00\n",
      "Epoch [1], train_loss: 59789.14 with loss1: 59789.14 and loss2: 0.00\n",
      "Epoch [2], train_loss: 58515.09 with loss1: 58515.09 and loss2: 0.00\n",
      "Epoch [3], train_loss: 47473.84 with loss1: 47473.84 and loss2: 0.00\n",
      "Epoch [4], train_loss: 45237.55 with loss1: 45237.55 and loss2: 0.00\n",
      "Epoch [5], train_loss: 36444.26 with loss1: 36444.26 and loss2: 0.00\n",
      "Epoch [6], train_loss: 27717.99 with loss1: 27717.99 and loss2: 0.00\n",
      "Epoch [7], train_loss: 18050.49 with loss1: 18050.49 and loss2: 0.00\n",
      "Epoch [8], train_loss: 21614.59 with loss1: 21614.59 and loss2: 0.00\n",
      "Epoch [9], train_loss: 13650.98 with loss1: 13650.98 and loss2: 0.00\n",
      "Epoch [10], train_loss: 10935.08 with loss1: 10935.08 and loss2: 0.00\n",
      "Epoch [11], train_loss: 11332.80 with loss1: 11332.80 and loss2: 0.00\n",
      "Epoch [12], train_loss: 9497.66 with loss1: 9497.66 and loss2: 0.00\n",
      "Epoch [13], train_loss: 9187.84 with loss1: 9187.84 and loss2: 0.00\n",
      "Epoch [14], train_loss: 8991.74 with loss1: 8991.74 and loss2: 0.00\n",
      "Epoch [15], train_loss: 9609.16 with loss1: 9609.16 and loss2: 0.00\n",
      "Epoch [16], train_loss: 9526.54 with loss1: 9526.54 and loss2: 0.00\n",
      "Epoch [17], train_loss: 9614.19 with loss1: 9614.19 and loss2: 0.00\n",
      "Epoch [18], train_loss: 8830.48 with loss1: 8830.48 and loss2: 0.00\n",
      "Epoch [19], train_loss: 9245.67 with loss1: 9245.67 and loss2: 0.00\n",
      "Epoch [20], train_loss: 8857.99 with loss1: 8857.99 and loss2: 0.00\n",
      "Epoch [21], train_loss: 7756.08 with loss1: 7756.08 and loss2: 0.00\n",
      "Epoch [22], train_loss: 8536.14 with loss1: 8536.14 and loss2: 0.00\n",
      "Epoch [23], train_loss: 8273.60 with loss1: 8273.60 and loss2: 0.00\n",
      "Epoch [24], train_loss: 7466.97 with loss1: 7466.97 and loss2: 0.00\n",
      "Epoch [25], train_loss: 8157.69 with loss1: 8157.69 and loss2: 0.00\n",
      "Epoch [26], train_loss: 8103.13 with loss1: 8103.13 and loss2: 0.00\n",
      "Epoch [27], train_loss: 7279.67 with loss1: 7279.67 and loss2: 0.00\n",
      "Epoch [28], train_loss: 7273.60 with loss1: 7273.60 and loss2: 0.00\n",
      "Epoch [29], train_loss: 7009.44 with loss1: 7009.44 and loss2: 0.00\n",
      "Epoch [30], train_loss: 7786.05 with loss1: 7786.05 and loss2: 0.00\n",
      "Epoch [31], train_loss: 8276.09 with loss1: 8276.09 and loss2: 0.00\n",
      "Epoch [32], train_loss: 7616.31 with loss1: 7616.31 and loss2: 0.00\n",
      "Epoch [33], train_loss: 7175.06 with loss1: 7175.06 and loss2: 0.00\n",
      "Epoch [34], train_loss: 6854.10 with loss1: 6854.10 and loss2: 0.00\n",
      "Epoch [35], train_loss: 6793.89 with loss1: 6793.89 and loss2: 0.00\n",
      "Epoch [36], train_loss: 6810.77 with loss1: 6810.77 and loss2: 0.00\n",
      "Epoch [37], train_loss: 8041.55 with loss1: 8041.55 and loss2: 0.00\n",
      "Epoch [38], train_loss: 7802.56 with loss1: 7802.56 and loss2: 0.00\n",
      "Epoch [39], train_loss: 7627.00 with loss1: 7627.00 and loss2: 0.00\n",
      "Epoch [40], train_loss: 8001.25 with loss1: 8001.25 and loss2: 0.00\n",
      "Epoch [41], train_loss: 6971.19 with loss1: 6971.19 and loss2: 0.00\n",
      "Epoch [42], train_loss: 6636.12 with loss1: 6636.12 and loss2: 0.00\n",
      "Epoch [43], train_loss: 7138.39 with loss1: 7138.39 and loss2: 0.00\n",
      "Epoch [44], train_loss: 6588.85 with loss1: 6588.85 and loss2: 0.00\n",
      "Epoch [45], train_loss: 7106.55 with loss1: 7106.55 and loss2: 0.00\n",
      "Epoch [46], train_loss: 7110.71 with loss1: 7110.71 and loss2: 0.00\n",
      "Epoch [47], train_loss: 7739.09 with loss1: 7739.09 and loss2: 0.00\n",
      "Epoch [48], train_loss: 6742.32 with loss1: 6742.32 and loss2: 0.00\n",
      "Epoch [49], train_loss: 6929.70 with loss1: 6929.70 and loss2: 0.00\n",
      "Epoch [50], train_loss: 6890.85 with loss1: 6890.85 and loss2: 0.00\n",
      "Epoch [51], train_loss: 6829.36 with loss1: 6829.36 and loss2: 0.00\n",
      "Epoch [52], train_loss: 7381.87 with loss1: 7381.87 and loss2: 0.00\n",
      "Epoch [53], train_loss: 6741.14 with loss1: 6741.14 and loss2: 0.00\n",
      "Epoch [54], train_loss: 6711.62 with loss1: 6711.62 and loss2: 0.00\n",
      "Epoch [55], train_loss: 6688.05 with loss1: 6688.05 and loss2: 0.00\n",
      "Epoch [56], train_loss: 6761.76 with loss1: 6761.76 and loss2: 0.00\n",
      "Epoch [57], train_loss: 7224.26 with loss1: 7224.26 and loss2: 0.00\n",
      "Epoch [58], train_loss: 6805.33 with loss1: 6805.33 and loss2: 0.00\n",
      "Epoch [59], train_loss: 6321.40 with loss1: 6321.40 and loss2: 0.00\n",
      "Epoch [60], train_loss: 7020.69 with loss1: 7020.69 and loss2: 0.00\n",
      "Epoch [61], train_loss: 6789.27 with loss1: 6789.27 and loss2: 0.00\n",
      "Epoch [62], train_loss: 7526.29 with loss1: 7526.29 and loss2: 0.00\n",
      "Epoch [63], train_loss: 7813.93 with loss1: 7813.93 and loss2: 0.00\n",
      "Epoch [64], train_loss: 6185.34 with loss1: 6185.34 and loss2: 0.00\n",
      "Epoch [65], train_loss: 7521.69 with loss1: 7521.69 and loss2: 0.00\n",
      "Epoch [66], train_loss: 6464.61 with loss1: 6464.61 and loss2: 0.00\n",
      "Epoch [67], train_loss: 6816.87 with loss1: 6816.87 and loss2: 0.00\n",
      "Epoch [68], train_loss: 7411.86 with loss1: 7411.86 and loss2: 0.00\n",
      "Epoch [69], train_loss: 6120.81 with loss1: 6120.81 and loss2: 0.00\n",
      "Epoch [70], train_loss: 7452.08 with loss1: 7452.08 and loss2: 0.00\n",
      "Epoch [71], train_loss: 6667.50 with loss1: 6667.50 and loss2: 0.00\n",
      "Epoch [72], train_loss: 6527.37 with loss1: 6527.37 and loss2: 0.00\n",
      "Epoch [73], train_loss: 6782.91 with loss1: 6782.91 and loss2: 0.00\n",
      "Epoch [74], train_loss: 7089.78 with loss1: 7089.78 and loss2: 0.00\n",
      "Epoch [75], train_loss: 6513.19 with loss1: 6513.19 and loss2: 0.00\n",
      "Epoch [76], train_loss: 6418.20 with loss1: 6418.20 and loss2: 0.00\n",
      "Epoch [77], train_loss: 7017.13 with loss1: 7017.13 and loss2: 0.00\n",
      "Epoch [78], train_loss: 6166.03 with loss1: 6166.03 and loss2: 0.00\n",
      "Epoch [79], train_loss: 6435.05 with loss1: 6435.05 and loss2: 0.00\n",
      "Epoch [80], train_loss: 6295.91 with loss1: 6295.91 and loss2: 0.00\n",
      "Epoch [81], train_loss: 6150.96 with loss1: 6150.96 and loss2: 0.00\n",
      "Epoch [82], train_loss: 6377.16 with loss1: 6377.16 and loss2: 0.00\n",
      "Epoch [83], train_loss: 6023.62 with loss1: 6023.62 and loss2: 0.00\n",
      "Epoch [84], train_loss: 5927.10 with loss1: 5927.10 and loss2: 0.00\n",
      "Epoch [85], train_loss: 6425.50 with loss1: 6425.50 and loss2: 0.00\n",
      "Epoch [86], train_loss: 6469.57 with loss1: 6469.57 and loss2: 0.00\n",
      "Epoch [87], train_loss: 6074.91 with loss1: 6074.91 and loss2: 0.00\n",
      "Epoch [88], train_loss: 6734.94 with loss1: 6734.94 and loss2: 0.00\n",
      "Epoch [89], train_loss: 6854.35 with loss1: 6854.35 and loss2: 0.00\n",
      "Epoch [90], train_loss: 6353.49 with loss1: 6353.49 and loss2: 0.00\n",
      "Epoch [91], train_loss: 6096.57 with loss1: 6096.57 and loss2: 0.00\n",
      "Epoch [92], train_loss: 5985.41 with loss1: 5985.41 and loss2: 0.00\n",
      "Epoch [93], train_loss: 6241.70 with loss1: 6241.70 and loss2: 0.00\n",
      "Epoch [94], train_loss: 5915.79 with loss1: 5915.79 and loss2: 0.00\n",
      "Epoch [95], train_loss: 7487.97 with loss1: 7487.97 and loss2: 0.00\n",
      "Epoch [96], train_loss: 5841.94 with loss1: 5841.94 and loss2: 0.00\n",
      "Epoch [97], train_loss: 5866.50 with loss1: 5866.50 and loss2: 0.00\n",
      "Epoch [98], train_loss: 5734.75 with loss1: 5734.75 and loss2: 0.00\n",
      "Epoch [99], train_loss: 7183.76 with loss1: 7183.76 and loss2: 0.00\n",
      "Epoch [100], train_loss: 5951.44 with loss1: 5951.44 and loss2: 0.00\n",
      "Epoch [101], train_loss: 5845.99 with loss1: 5845.99 and loss2: 0.00\n",
      "Epoch [102], train_loss: 5718.85 with loss1: 5718.85 and loss2: 0.00\n",
      "Epoch [103], train_loss: 5740.28 with loss1: 5740.28 and loss2: 0.00\n",
      "Epoch [104], train_loss: 6615.27 with loss1: 6615.27 and loss2: 0.00\n",
      "Epoch [105], train_loss: 7059.15 with loss1: 7059.15 and loss2: 0.00\n",
      "Epoch [106], train_loss: 5856.30 with loss1: 5856.30 and loss2: 0.00\n",
      "Epoch [107], train_loss: 6109.48 with loss1: 6109.48 and loss2: 0.00\n",
      "Epoch [108], train_loss: 6452.87 with loss1: 6452.87 and loss2: 0.00\n",
      "Epoch [109], train_loss: 6082.67 with loss1: 6082.67 and loss2: 0.00\n",
      "Epoch [110], train_loss: 6441.06 with loss1: 6441.06 and loss2: 0.00\n",
      "Epoch [111], train_loss: 5770.08 with loss1: 5770.08 and loss2: 0.00\n",
      "Epoch [112], train_loss: 7190.02 with loss1: 7190.02 and loss2: 0.00\n",
      "Epoch [113], train_loss: 5822.29 with loss1: 5822.29 and loss2: 0.00\n",
      "Epoch [114], train_loss: 5741.33 with loss1: 5741.33 and loss2: 0.00\n",
      "Epoch [115], train_loss: 6134.46 with loss1: 6134.46 and loss2: 0.00\n",
      "Epoch [116], train_loss: 6011.51 with loss1: 6011.51 and loss2: 0.00\n",
      "Epoch [117], train_loss: 5981.27 with loss1: 5981.27 and loss2: 0.00\n",
      "Epoch [118], train_loss: 6357.63 with loss1: 6357.63 and loss2: 0.00\n",
      "Epoch [119], train_loss: 5760.97 with loss1: 5760.97 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120], train_loss: 6037.15 with loss1: 6037.15 and loss2: 0.00\n",
      "Epoch [121], train_loss: 7012.71 with loss1: 7012.71 and loss2: 0.00\n",
      "Epoch [122], train_loss: 5950.04 with loss1: 5950.04 and loss2: 0.00\n",
      "Epoch [123], train_loss: 5759.92 with loss1: 5759.92 and loss2: 0.00\n",
      "Epoch [124], train_loss: 5950.07 with loss1: 5950.07 and loss2: 0.00\n",
      "Epoch [125], train_loss: 5730.65 with loss1: 5730.65 and loss2: 0.00\n",
      "Epoch [126], train_loss: 5683.56 with loss1: 5683.56 and loss2: 0.00\n",
      "Epoch [127], train_loss: 7233.43 with loss1: 7233.43 and loss2: 0.00\n",
      "Epoch [128], train_loss: 5954.90 with loss1: 5954.90 and loss2: 0.00\n",
      "Epoch [129], train_loss: 5735.08 with loss1: 5735.08 and loss2: 0.00\n",
      "Epoch [130], train_loss: 5556.94 with loss1: 5556.94 and loss2: 0.00\n",
      "Epoch [131], train_loss: 6010.86 with loss1: 6010.86 and loss2: 0.00\n",
      "Epoch [132], train_loss: 7252.82 with loss1: 7252.82 and loss2: 0.00\n",
      "Epoch [133], train_loss: 5574.28 with loss1: 5574.28 and loss2: 0.00\n",
      "Epoch [134], train_loss: 5974.77 with loss1: 5974.77 and loss2: 0.00\n",
      "Epoch [135], train_loss: 5665.38 with loss1: 5665.38 and loss2: 0.00\n",
      "Epoch [136], train_loss: 6908.71 with loss1: 6908.71 and loss2: 0.00\n",
      "Epoch [137], train_loss: 5862.11 with loss1: 5862.11 and loss2: 0.00\n",
      "Epoch [138], train_loss: 5916.88 with loss1: 5916.88 and loss2: 0.00\n",
      "Epoch [139], train_loss: 6339.45 with loss1: 6339.45 and loss2: 0.00\n",
      "Epoch [140], train_loss: 5903.11 with loss1: 5903.11 and loss2: 0.00\n",
      "Epoch [141], train_loss: 5618.64 with loss1: 5618.64 and loss2: 0.00\n",
      "Epoch [142], train_loss: 7085.78 with loss1: 7085.78 and loss2: 0.00\n",
      "Epoch [143], train_loss: 5988.72 with loss1: 5988.72 and loss2: 0.00\n",
      "Epoch [144], train_loss: 5591.41 with loss1: 5591.41 and loss2: 0.00\n",
      "Epoch [145], train_loss: 6159.02 with loss1: 6159.02 and loss2: 0.00\n",
      "Epoch [146], train_loss: 6615.18 with loss1: 6615.18 and loss2: 0.00\n",
      "Epoch [147], train_loss: 6346.06 with loss1: 6346.06 and loss2: 0.00\n",
      "Epoch [148], train_loss: 5802.96 with loss1: 5802.96 and loss2: 0.00\n",
      "Epoch [149], train_loss: 5755.81 with loss1: 5755.81 and loss2: 0.00\n",
      "Epoch [150], train_loss: 5770.48 with loss1: 5770.48 and loss2: 0.00\n",
      "Epoch [151], train_loss: 5573.31 with loss1: 5573.31 and loss2: 0.00\n",
      "Epoch [152], train_loss: 5604.75 with loss1: 5604.75 and loss2: 0.00\n",
      "Epoch [153], train_loss: 5814.30 with loss1: 5814.30 and loss2: 0.00\n",
      "Epoch [154], train_loss: 5850.87 with loss1: 5850.87 and loss2: 0.00\n",
      "Epoch [155], train_loss: 6832.85 with loss1: 6832.85 and loss2: 0.00\n",
      "Epoch [156], train_loss: 5912.92 with loss1: 5912.92 and loss2: 0.00\n",
      "Epoch [157], train_loss: 5774.58 with loss1: 5774.58 and loss2: 0.00\n",
      "Epoch [158], train_loss: 6862.29 with loss1: 6862.29 and loss2: 0.00\n",
      "Epoch [159], train_loss: 7016.63 with loss1: 7016.63 and loss2: 0.00\n",
      "Epoch [160], train_loss: 5867.04 with loss1: 5867.04 and loss2: 0.00\n",
      "Epoch [161], train_loss: 5513.28 with loss1: 5513.28 and loss2: 0.00\n",
      "Epoch [162], train_loss: 5544.34 with loss1: 5544.34 and loss2: 0.00\n",
      "Epoch [163], train_loss: 6940.28 with loss1: 6940.28 and loss2: 0.00\n",
      "Epoch [164], train_loss: 5582.25 with loss1: 5582.25 and loss2: 0.00\n",
      "Epoch [165], train_loss: 5800.49 with loss1: 5800.49 and loss2: 0.00\n",
      "Epoch [166], train_loss: 6785.51 with loss1: 6785.51 and loss2: 0.00\n",
      "Epoch [167], train_loss: 5718.82 with loss1: 5718.82 and loss2: 0.00\n",
      "Epoch [168], train_loss: 6021.91 with loss1: 6021.91 and loss2: 0.00\n",
      "Epoch [169], train_loss: 5689.18 with loss1: 5689.18 and loss2: 0.00\n",
      "Epoch [170], train_loss: 6732.10 with loss1: 6732.10 and loss2: 0.00\n",
      "Epoch [171], train_loss: 5384.13 with loss1: 5384.13 and loss2: 0.00\n",
      "Epoch [172], train_loss: 5401.02 with loss1: 5401.02 and loss2: 0.00\n",
      "Epoch [173], train_loss: 5358.56 with loss1: 5358.56 and loss2: 0.00\n",
      "Epoch [174], train_loss: 5808.43 with loss1: 5808.43 and loss2: 0.00\n",
      "Epoch [175], train_loss: 5706.76 with loss1: 5706.76 and loss2: 0.00\n",
      "Epoch [176], train_loss: 5485.87 with loss1: 5485.87 and loss2: 0.00\n",
      "Epoch [177], train_loss: 5369.11 with loss1: 5369.11 and loss2: 0.00\n",
      "Epoch [178], train_loss: 5794.53 with loss1: 5794.53 and loss2: 0.00\n",
      "Epoch [179], train_loss: 5429.37 with loss1: 5429.37 and loss2: 0.00\n",
      "Epoch [180], train_loss: 5795.89 with loss1: 5795.89 and loss2: 0.00\n",
      "Epoch [181], train_loss: 6722.18 with loss1: 6722.18 and loss2: 0.00\n",
      "Epoch [182], train_loss: 5811.28 with loss1: 5811.28 and loss2: 0.00\n",
      "Epoch [183], train_loss: 5666.93 with loss1: 5666.93 and loss2: 0.00\n",
      "Epoch [184], train_loss: 5415.16 with loss1: 5415.16 and loss2: 0.00\n",
      "Epoch [185], train_loss: 5686.61 with loss1: 5686.61 and loss2: 0.00\n",
      "Epoch [186], train_loss: 5579.23 with loss1: 5579.23 and loss2: 0.00\n",
      "Epoch [187], train_loss: 6972.74 with loss1: 6972.74 and loss2: 0.00\n",
      "Epoch [188], train_loss: 6800.18 with loss1: 6800.18 and loss2: 0.00\n",
      "Epoch [189], train_loss: 5679.79 with loss1: 5679.79 and loss2: 0.00\n",
      "Epoch [190], train_loss: 6446.72 with loss1: 6446.72 and loss2: 0.00\n",
      "Epoch [191], train_loss: 5740.79 with loss1: 5740.79 and loss2: 0.00\n",
      "Epoch [192], train_loss: 5548.49 with loss1: 5548.49 and loss2: 0.00\n",
      "Epoch [193], train_loss: 5659.21 with loss1: 5659.21 and loss2: 0.00\n",
      "Epoch [194], train_loss: 5983.70 with loss1: 5983.70 and loss2: 0.00\n",
      "Epoch [195], train_loss: 6425.90 with loss1: 6425.90 and loss2: 0.00\n",
      "Epoch [196], train_loss: 5714.71 with loss1: 5714.71 and loss2: 0.00\n",
      "Epoch [197], train_loss: 5694.56 with loss1: 5694.56 and loss2: 0.00\n",
      "Epoch [198], train_loss: 5583.98 with loss1: 5583.98 and loss2: 0.00\n",
      "Epoch [199], train_loss: 5576.21 with loss1: 5576.21 and loss2: 0.00\n",
      "Epoch [200], train_loss: 6856.06 with loss1: 6856.06 and loss2: 0.00\n",
      "Epoch [201], train_loss: 6750.95 with loss1: 6750.95 and loss2: 0.00\n",
      "Epoch [202], train_loss: 5623.48 with loss1: 5623.48 and loss2: 0.00\n",
      "Epoch [203], train_loss: 6132.67 with loss1: 6132.67 and loss2: 0.00\n",
      "Epoch [204], train_loss: 5985.75 with loss1: 5985.75 and loss2: 0.00\n",
      "Epoch [205], train_loss: 5488.67 with loss1: 5488.67 and loss2: 0.00\n",
      "Epoch [206], train_loss: 5449.00 with loss1: 5449.00 and loss2: 0.00\n",
      "Epoch [207], train_loss: 5229.88 with loss1: 5229.88 and loss2: 0.00\n",
      "Epoch [208], train_loss: 6603.94 with loss1: 6603.94 and loss2: 0.00\n",
      "Epoch [209], train_loss: 5222.60 with loss1: 5222.60 and loss2: 0.00\n",
      "Epoch [210], train_loss: 5252.62 with loss1: 5252.62 and loss2: 0.00\n",
      "Epoch [211], train_loss: 5861.96 with loss1: 5861.96 and loss2: 0.00\n",
      "Epoch [212], train_loss: 6143.91 with loss1: 6143.91 and loss2: 0.00\n",
      "Epoch [213], train_loss: 5406.64 with loss1: 5406.64 and loss2: 0.00\n",
      "Epoch [214], train_loss: 5607.71 with loss1: 5607.71 and loss2: 0.00\n",
      "Epoch [215], train_loss: 5390.13 with loss1: 5390.13 and loss2: 0.00\n",
      "Epoch [216], train_loss: 5562.88 with loss1: 5562.88 and loss2: 0.00\n",
      "Epoch [217], train_loss: 5828.69 with loss1: 5828.69 and loss2: 0.00\n",
      "Epoch [218], train_loss: 6366.33 with loss1: 6366.33 and loss2: 0.00\n",
      "Epoch [219], train_loss: 5521.63 with loss1: 5521.63 and loss2: 0.00\n",
      "Epoch [220], train_loss: 6545.61 with loss1: 6545.61 and loss2: 0.00\n",
      "Epoch [221], train_loss: 5373.54 with loss1: 5373.54 and loss2: 0.00\n",
      "Epoch [222], train_loss: 5571.89 with loss1: 5571.89 and loss2: 0.00\n",
      "Epoch [223], train_loss: 5562.15 with loss1: 5562.15 and loss2: 0.00\n",
      "Epoch [224], train_loss: 6677.29 with loss1: 6677.29 and loss2: 0.00\n",
      "Epoch [225], train_loss: 5184.44 with loss1: 5184.44 and loss2: 0.00\n",
      "Epoch [226], train_loss: 5307.90 with loss1: 5307.90 and loss2: 0.00\n",
      "Epoch [227], train_loss: 5607.18 with loss1: 5607.18 and loss2: 0.00\n",
      "Epoch [228], train_loss: 5719.05 with loss1: 5719.05 and loss2: 0.00\n",
      "Epoch [229], train_loss: 5331.80 with loss1: 5331.80 and loss2: 0.00\n",
      "Epoch [230], train_loss: 5491.36 with loss1: 5491.36 and loss2: 0.00\n",
      "Epoch [231], train_loss: 5209.96 with loss1: 5209.96 and loss2: 0.00\n",
      "Epoch [232], train_loss: 5131.10 with loss1: 5131.10 and loss2: 0.00\n",
      "Epoch [233], train_loss: 5569.26 with loss1: 5569.26 and loss2: 0.00\n",
      "Epoch [234], train_loss: 5190.21 with loss1: 5190.21 and loss2: 0.00\n",
      "Epoch [235], train_loss: 5248.91 with loss1: 5248.91 and loss2: 0.00\n",
      "Epoch [236], train_loss: 5282.26 with loss1: 5282.26 and loss2: 0.00\n",
      "Epoch [237], train_loss: 5030.82 with loss1: 5030.82 and loss2: 0.00\n",
      "Epoch [238], train_loss: 5538.57 with loss1: 5538.57 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [239], train_loss: 5257.14 with loss1: 5257.14 and loss2: 0.00\n",
      "Epoch [240], train_loss: 5024.77 with loss1: 5024.77 and loss2: 0.00\n",
      "Epoch [241], train_loss: 6501.08 with loss1: 6501.08 and loss2: 0.00\n",
      "Epoch [242], train_loss: 5458.52 with loss1: 5458.52 and loss2: 0.00\n",
      "Epoch [243], train_loss: 5782.76 with loss1: 5782.76 and loss2: 0.00\n",
      "Epoch [244], train_loss: 6131.38 with loss1: 6131.38 and loss2: 0.00\n",
      "Epoch [245], train_loss: 5551.84 with loss1: 5551.84 and loss2: 0.00\n",
      "Epoch [246], train_loss: 6363.19 with loss1: 6363.19 and loss2: 0.00\n",
      "Epoch [247], train_loss: 6468.12 with loss1: 6468.12 and loss2: 0.00\n",
      "Epoch [248], train_loss: 5197.91 with loss1: 5197.91 and loss2: 0.00\n",
      "Epoch [249], train_loss: 5493.80 with loss1: 5493.80 and loss2: 0.00\n",
      "Epoch [250], train_loss: 5896.02 with loss1: 5896.02 and loss2: 0.00\n",
      "Epoch [251], train_loss: 6144.61 with loss1: 6144.61 and loss2: 0.00\n",
      "Epoch [252], train_loss: 5387.96 with loss1: 5387.96 and loss2: 0.00\n",
      "Epoch [253], train_loss: 5395.56 with loss1: 5395.56 and loss2: 0.00\n",
      "Epoch [254], train_loss: 6297.55 with loss1: 6297.55 and loss2: 0.00\n",
      "Epoch [255], train_loss: 6386.18 with loss1: 6386.18 and loss2: 0.00\n",
      "Epoch [256], train_loss: 5360.63 with loss1: 5360.63 and loss2: 0.00\n",
      "Epoch [257], train_loss: 5064.23 with loss1: 5064.23 and loss2: 0.00\n",
      "Epoch [258], train_loss: 4953.67 with loss1: 4953.67 and loss2: 0.00\n",
      "Epoch [259], train_loss: 4953.55 with loss1: 4953.55 and loss2: 0.00\n",
      "Epoch [260], train_loss: 5634.99 with loss1: 5634.99 and loss2: 0.00\n",
      "Epoch [261], train_loss: 5466.57 with loss1: 5466.57 and loss2: 0.00\n",
      "Epoch [262], train_loss: 5155.60 with loss1: 5155.60 and loss2: 0.00\n",
      "Epoch [263], train_loss: 5131.07 with loss1: 5131.07 and loss2: 0.00\n",
      "Epoch [264], train_loss: 6351.80 with loss1: 6351.80 and loss2: 0.00\n",
      "Epoch [265], train_loss: 4934.43 with loss1: 4934.43 and loss2: 0.00\n",
      "Epoch [266], train_loss: 5036.79 with loss1: 5036.79 and loss2: 0.00\n",
      "Epoch [267], train_loss: 4853.40 with loss1: 4853.40 and loss2: 0.00\n",
      "Epoch [268], train_loss: 5538.16 with loss1: 5538.16 and loss2: 0.00\n",
      "Epoch [269], train_loss: 6330.62 with loss1: 6330.62 and loss2: 0.00\n",
      "Epoch [270], train_loss: 5313.95 with loss1: 5313.95 and loss2: 0.00\n",
      "Epoch [271], train_loss: 5550.06 with loss1: 5550.06 and loss2: 0.00\n",
      "Epoch [272], train_loss: 5453.42 with loss1: 5453.42 and loss2: 0.00\n",
      "Epoch [273], train_loss: 5261.69 with loss1: 5261.69 and loss2: 0.00\n",
      "Epoch [274], train_loss: 4951.82 with loss1: 4951.82 and loss2: 0.00\n",
      "Epoch [275], train_loss: 5503.12 with loss1: 5503.12 and loss2: 0.00\n",
      "Epoch [276], train_loss: 4826.78 with loss1: 4826.78 and loss2: 0.00\n",
      "Epoch [277], train_loss: 4929.56 with loss1: 4929.56 and loss2: 0.00\n",
      "Epoch [278], train_loss: 4725.93 with loss1: 4725.93 and loss2: 0.00\n",
      "Epoch [279], train_loss: 4765.61 with loss1: 4765.61 and loss2: 0.00\n",
      "Epoch [280], train_loss: 4862.59 with loss1: 4862.59 and loss2: 0.00\n",
      "Epoch [281], train_loss: 5228.27 with loss1: 5228.27 and loss2: 0.00\n",
      "Epoch [282], train_loss: 5662.06 with loss1: 5662.06 and loss2: 0.00\n",
      "Epoch [283], train_loss: 5412.07 with loss1: 5412.07 and loss2: 0.00\n",
      "Epoch [284], train_loss: 6181.19 with loss1: 6181.19 and loss2: 0.00\n",
      "Epoch [285], train_loss: 5194.67 with loss1: 5194.67 and loss2: 0.00\n",
      "Epoch [286], train_loss: 4806.38 with loss1: 4806.38 and loss2: 0.00\n",
      "Epoch [287], train_loss: 5478.50 with loss1: 5478.50 and loss2: 0.00\n",
      "Epoch [288], train_loss: 4978.59 with loss1: 4978.59 and loss2: 0.00\n",
      "Epoch [289], train_loss: 5855.68 with loss1: 5855.68 and loss2: 0.00\n",
      "Epoch [290], train_loss: 5501.30 with loss1: 5501.30 and loss2: 0.00\n",
      "Epoch [291], train_loss: 4687.18 with loss1: 4687.18 and loss2: 0.00\n",
      "Epoch [292], train_loss: 4768.42 with loss1: 4768.42 and loss2: 0.00\n",
      "Epoch [293], train_loss: 4579.23 with loss1: 4579.23 and loss2: 0.00\n",
      "Epoch [294], train_loss: 4716.19 with loss1: 4716.19 and loss2: 0.00\n",
      "Epoch [295], train_loss: 5492.97 with loss1: 5492.97 and loss2: 0.00\n",
      "Epoch [296], train_loss: 6001.62 with loss1: 6001.62 and loss2: 0.00\n",
      "Epoch [297], train_loss: 6162.01 with loss1: 6162.01 and loss2: 0.00\n",
      "Epoch [298], train_loss: 4980.20 with loss1: 4980.20 and loss2: 0.00\n",
      "Epoch [299], train_loss: 5408.40 with loss1: 5408.40 and loss2: 0.00\n",
      "Epoch [300], train_loss: 5092.56 with loss1: 5092.56 and loss2: 0.00\n",
      "Epoch [301], train_loss: 4654.62 with loss1: 4654.62 and loss2: 0.00\n",
      "Epoch [302], train_loss: 4563.94 with loss1: 4563.94 and loss2: 0.00\n",
      "Epoch [303], train_loss: 5444.82 with loss1: 5444.82 and loss2: 0.00\n",
      "Epoch [304], train_loss: 5381.50 with loss1: 5381.50 and loss2: 0.00\n",
      "Epoch [305], train_loss: 5037.29 with loss1: 5037.29 and loss2: 0.00\n",
      "Epoch [306], train_loss: 5131.94 with loss1: 5131.94 and loss2: 0.00\n",
      "Epoch [307], train_loss: 5488.18 with loss1: 5488.18 and loss2: 0.00\n",
      "Epoch [308], train_loss: 5317.44 with loss1: 5317.44 and loss2: 0.00\n",
      "Epoch [309], train_loss: 5036.76 with loss1: 5036.76 and loss2: 0.00\n",
      "Epoch [310], train_loss: 4463.28 with loss1: 4463.28 and loss2: 0.00\n",
      "Epoch [311], train_loss: 4559.56 with loss1: 4559.56 and loss2: 0.00\n",
      "Epoch [312], train_loss: 5749.24 with loss1: 5749.24 and loss2: 0.00\n",
      "Epoch [313], train_loss: 5922.87 with loss1: 5922.87 and loss2: 0.00\n",
      "Epoch [314], train_loss: 4432.10 with loss1: 4432.10 and loss2: 0.00\n",
      "Epoch [315], train_loss: 5361.81 with loss1: 5361.81 and loss2: 0.00\n",
      "Epoch [316], train_loss: 4711.55 with loss1: 4711.55 and loss2: 0.00\n",
      "Epoch [317], train_loss: 5138.30 with loss1: 5138.30 and loss2: 0.00\n",
      "Epoch [318], train_loss: 5189.45 with loss1: 5189.45 and loss2: 0.00\n",
      "Epoch [319], train_loss: 4269.04 with loss1: 4269.04 and loss2: 0.00\n",
      "Epoch [320], train_loss: 4900.04 with loss1: 4900.04 and loss2: 0.00\n",
      "Epoch [321], train_loss: 5093.06 with loss1: 5093.06 and loss2: 0.00\n",
      "Epoch [322], train_loss: 5146.01 with loss1: 5146.01 and loss2: 0.00\n",
      "Epoch [323], train_loss: 5315.15 with loss1: 5315.15 and loss2: 0.00\n",
      "Epoch [324], train_loss: 5390.09 with loss1: 5390.09 and loss2: 0.00\n",
      "Epoch [325], train_loss: 5613.50 with loss1: 5613.50 and loss2: 0.00\n",
      "Epoch [326], train_loss: 4839.16 with loss1: 4839.16 and loss2: 0.00\n",
      "Epoch [327], train_loss: 4289.61 with loss1: 4289.61 and loss2: 0.00\n",
      "Epoch [328], train_loss: 4251.85 with loss1: 4251.85 and loss2: 0.00\n",
      "Epoch [329], train_loss: 4147.55 with loss1: 4147.55 and loss2: 0.00\n",
      "Epoch [330], train_loss: 4186.98 with loss1: 4186.98 and loss2: 0.00\n",
      "Epoch [331], train_loss: 5041.92 with loss1: 5041.92 and loss2: 0.00\n",
      "Epoch [332], train_loss: 5543.65 with loss1: 5543.65 and loss2: 0.00\n",
      "Epoch [333], train_loss: 5400.73 with loss1: 5400.73 and loss2: 0.00\n",
      "Epoch [334], train_loss: 5272.82 with loss1: 5272.82 and loss2: 0.00\n",
      "Epoch [335], train_loss: 5255.21 with loss1: 5255.21 and loss2: 0.00\n",
      "Epoch [336], train_loss: 4201.16 with loss1: 4201.16 and loss2: 0.00\n",
      "Epoch [337], train_loss: 4204.06 with loss1: 4204.06 and loss2: 0.00\n",
      "Epoch [338], train_loss: 5337.13 with loss1: 5337.13 and loss2: 0.00\n",
      "Epoch [339], train_loss: 4500.59 with loss1: 4500.59 and loss2: 0.00\n",
      "Epoch [340], train_loss: 4995.81 with loss1: 4995.81 and loss2: 0.00\n",
      "Epoch [341], train_loss: 5260.51 with loss1: 5260.51 and loss2: 0.00\n",
      "Epoch [342], train_loss: 5031.76 with loss1: 5031.76 and loss2: 0.00\n",
      "Epoch [343], train_loss: 4635.25 with loss1: 4635.25 and loss2: 0.00\n",
      "Epoch [344], train_loss: 4451.17 with loss1: 4451.17 and loss2: 0.00\n",
      "Epoch [345], train_loss: 4349.97 with loss1: 4349.97 and loss2: 0.00\n",
      "Epoch [346], train_loss: 4842.10 with loss1: 4842.10 and loss2: 0.00\n",
      "Epoch [347], train_loss: 5246.52 with loss1: 5246.52 and loss2: 0.00\n",
      "Epoch [348], train_loss: 4387.83 with loss1: 4387.83 and loss2: 0.00\n",
      "Epoch [349], train_loss: 4128.62 with loss1: 4128.62 and loss2: 0.00\n",
      "Epoch [350], train_loss: 4774.99 with loss1: 4774.99 and loss2: 0.00\n",
      "Epoch [351], train_loss: 4665.25 with loss1: 4665.25 and loss2: 0.00\n",
      "Epoch [352], train_loss: 5223.31 with loss1: 5223.31 and loss2: 0.00\n",
      "Epoch [353], train_loss: 4483.71 with loss1: 4483.71 and loss2: 0.00\n",
      "Epoch [354], train_loss: 4649.73 with loss1: 4649.73 and loss2: 0.00\n",
      "Epoch [355], train_loss: 4970.59 with loss1: 4970.59 and loss2: 0.00\n",
      "Epoch [356], train_loss: 3974.36 with loss1: 3974.36 and loss2: 0.00\n",
      "Epoch [357], train_loss: 4810.88 with loss1: 4810.88 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [358], train_loss: 4299.65 with loss1: 4299.65 and loss2: 0.00\n",
      "Epoch [359], train_loss: 5211.18 with loss1: 5211.18 and loss2: 0.00\n",
      "Epoch [360], train_loss: 3884.29 with loss1: 3884.29 and loss2: 0.00\n",
      "Epoch [361], train_loss: 4471.90 with loss1: 4471.90 and loss2: 0.00\n",
      "Epoch [362], train_loss: 4848.57 with loss1: 4848.57 and loss2: 0.00\n",
      "Epoch [363], train_loss: 4696.26 with loss1: 4696.26 and loss2: 0.00\n",
      "Epoch [364], train_loss: 5167.29 with loss1: 5167.29 and loss2: 0.00\n",
      "Epoch [365], train_loss: 4316.56 with loss1: 4316.56 and loss2: 0.00\n",
      "Epoch [366], train_loss: 5162.63 with loss1: 5162.63 and loss2: 0.00\n",
      "Epoch [367], train_loss: 5268.40 with loss1: 5268.40 and loss2: 0.00\n",
      "Epoch [368], train_loss: 4255.43 with loss1: 4255.43 and loss2: 0.00\n",
      "Epoch [369], train_loss: 4102.75 with loss1: 4102.75 and loss2: 0.00\n",
      "Epoch [370], train_loss: 3822.71 with loss1: 3822.71 and loss2: 0.00\n",
      "Epoch [371], train_loss: 5194.83 with loss1: 5194.83 and loss2: 0.00\n",
      "Epoch [372], train_loss: 5234.47 with loss1: 5234.47 and loss2: 0.00\n",
      "Epoch [373], train_loss: 5141.16 with loss1: 5141.16 and loss2: 0.00\n",
      "Epoch [374], train_loss: 4247.45 with loss1: 4247.45 and loss2: 0.00\n",
      "Epoch [375], train_loss: 3967.53 with loss1: 3967.53 and loss2: 0.00\n",
      "Epoch [376], train_loss: 3940.99 with loss1: 3940.99 and loss2: 0.00\n",
      "Epoch [377], train_loss: 4694.35 with loss1: 4694.35 and loss2: 0.00\n",
      "Epoch [378], train_loss: 4930.93 with loss1: 4930.93 and loss2: 0.00\n",
      "Epoch [379], train_loss: 3856.56 with loss1: 3856.56 and loss2: 0.00\n",
      "Epoch [380], train_loss: 5233.48 with loss1: 5233.48 and loss2: 0.00\n",
      "Epoch [381], train_loss: 5004.47 with loss1: 5004.47 and loss2: 0.00\n",
      "Epoch [382], train_loss: 4567.93 with loss1: 4567.93 and loss2: 0.00\n",
      "Epoch [383], train_loss: 4517.91 with loss1: 4517.91 and loss2: 0.00\n",
      "Epoch [384], train_loss: 4578.65 with loss1: 4578.65 and loss2: 0.00\n",
      "Epoch [385], train_loss: 5231.96 with loss1: 5231.96 and loss2: 0.00\n",
      "Epoch [386], train_loss: 4519.46 with loss1: 4519.46 and loss2: 0.00\n",
      "Epoch [387], train_loss: 4601.46 with loss1: 4601.46 and loss2: 0.00\n",
      "Epoch [388], train_loss: 4061.56 with loss1: 4061.56 and loss2: 0.00\n",
      "Epoch [389], train_loss: 4787.18 with loss1: 4787.18 and loss2: 0.00\n",
      "Epoch [390], train_loss: 4164.84 with loss1: 4164.84 and loss2: 0.00\n",
      "Epoch [391], train_loss: 3885.39 with loss1: 3885.39 and loss2: 0.00\n",
      "Epoch [392], train_loss: 3748.32 with loss1: 3748.32 and loss2: 0.00\n",
      "Epoch [393], train_loss: 4471.08 with loss1: 4471.08 and loss2: 0.00\n",
      "Epoch [394], train_loss: 4695.26 with loss1: 4695.26 and loss2: 0.00\n",
      "Epoch [395], train_loss: 4968.86 with loss1: 4968.86 and loss2: 0.00\n",
      "Epoch [396], train_loss: 3810.98 with loss1: 3810.98 and loss2: 0.00\n",
      "Epoch [397], train_loss: 4581.01 with loss1: 4581.01 and loss2: 0.00\n",
      "Epoch [398], train_loss: 3669.84 with loss1: 3669.84 and loss2: 0.00\n",
      "Epoch [399], train_loss: 3863.57 with loss1: 3863.57 and loss2: 0.00\n",
      "Epoch [400], train_loss: 3991.62 with loss1: 3991.62 and loss2: 0.00\n",
      "Epoch [401], train_loss: 4548.92 with loss1: 4548.92 and loss2: 0.00\n",
      "Epoch [402], train_loss: 3884.68 with loss1: 3884.68 and loss2: 0.00\n",
      "Epoch [403], train_loss: 5083.23 with loss1: 5083.23 and loss2: 0.00\n",
      "Epoch [404], train_loss: 3644.37 with loss1: 3644.37 and loss2: 0.00\n",
      "Epoch [405], train_loss: 4440.67 with loss1: 4440.67 and loss2: 0.00\n",
      "Epoch [406], train_loss: 4445.75 with loss1: 4445.75 and loss2: 0.00\n",
      "Epoch [407], train_loss: 4140.36 with loss1: 4140.36 and loss2: 0.00\n",
      "Epoch [408], train_loss: 4414.91 with loss1: 4414.91 and loss2: 0.00\n",
      "Epoch [409], train_loss: 5125.05 with loss1: 5125.05 and loss2: 0.00\n",
      "Epoch [410], train_loss: 3983.55 with loss1: 3983.55 and loss2: 0.00\n",
      "Epoch [411], train_loss: 5146.35 with loss1: 5146.35 and loss2: 0.00\n",
      "Epoch [412], train_loss: 5200.71 with loss1: 5200.71 and loss2: 0.00\n",
      "Epoch [413], train_loss: 4550.96 with loss1: 4550.96 and loss2: 0.00\n",
      "Epoch [414], train_loss: 4112.35 with loss1: 4112.35 and loss2: 0.00\n",
      "Epoch [415], train_loss: 4368.97 with loss1: 4368.97 and loss2: 0.00\n",
      "Epoch [416], train_loss: 4627.92 with loss1: 4627.92 and loss2: 0.00\n",
      "Epoch [417], train_loss: 4059.96 with loss1: 4059.96 and loss2: 0.00\n",
      "Epoch [418], train_loss: 3598.54 with loss1: 3598.54 and loss2: 0.00\n",
      "Epoch [419], train_loss: 4158.57 with loss1: 4158.57 and loss2: 0.00\n",
      "Epoch [420], train_loss: 4009.64 with loss1: 4009.64 and loss2: 0.00\n",
      "Epoch [421], train_loss: 4361.20 with loss1: 4361.20 and loss2: 0.00\n",
      "Epoch [422], train_loss: 5089.22 with loss1: 5089.22 and loss2: 0.00\n",
      "Epoch [423], train_loss: 4412.87 with loss1: 4412.87 and loss2: 0.00\n",
      "Epoch [424], train_loss: 3767.42 with loss1: 3767.42 and loss2: 0.00\n",
      "Epoch [425], train_loss: 3597.43 with loss1: 3597.43 and loss2: 0.00\n",
      "Epoch [426], train_loss: 3713.84 with loss1: 3713.84 and loss2: 0.00\n",
      "Epoch [427], train_loss: 3790.66 with loss1: 3790.66 and loss2: 0.00\n",
      "Epoch [428], train_loss: 4110.37 with loss1: 4110.37 and loss2: 0.00\n",
      "Epoch [429], train_loss: 5014.17 with loss1: 5014.17 and loss2: 0.00\n",
      "Epoch [430], train_loss: 4466.98 with loss1: 4466.98 and loss2: 0.00\n",
      "Epoch [431], train_loss: 3543.96 with loss1: 3543.96 and loss2: 0.00\n",
      "Epoch [432], train_loss: 4361.57 with loss1: 4361.57 and loss2: 0.00\n",
      "Epoch [433], train_loss: 4408.52 with loss1: 4408.52 and loss2: 0.00\n",
      "Epoch [434], train_loss: 3538.30 with loss1: 3538.30 and loss2: 0.00\n",
      "Epoch [435], train_loss: 5034.37 with loss1: 5034.37 and loss2: 0.00\n",
      "Epoch [436], train_loss: 4353.91 with loss1: 4353.91 and loss2: 0.00\n",
      "Epoch [437], train_loss: 4347.07 with loss1: 4347.07 and loss2: 0.00\n",
      "Epoch [438], train_loss: 4076.97 with loss1: 4076.97 and loss2: 0.00\n",
      "Epoch [439], train_loss: 4364.84 with loss1: 4364.84 and loss2: 0.00\n",
      "Epoch [440], train_loss: 4532.19 with loss1: 4532.19 and loss2: 0.00\n",
      "Epoch [441], train_loss: 4971.30 with loss1: 4971.30 and loss2: 0.00\n",
      "Epoch [442], train_loss: 4450.79 with loss1: 4450.79 and loss2: 0.00\n",
      "Epoch [443], train_loss: 4347.38 with loss1: 4347.38 and loss2: 0.00\n",
      "Epoch [444], train_loss: 3968.17 with loss1: 3968.17 and loss2: 0.00\n",
      "Epoch [445], train_loss: 3765.40 with loss1: 3765.40 and loss2: 0.00\n",
      "Epoch [446], train_loss: 3600.99 with loss1: 3600.99 and loss2: 0.00\n",
      "Epoch [447], train_loss: 4277.16 with loss1: 4277.16 and loss2: 0.00\n",
      "Epoch [448], train_loss: 5043.70 with loss1: 5043.70 and loss2: 0.00\n",
      "Epoch [449], train_loss: 3518.61 with loss1: 3518.61 and loss2: 0.00\n",
      "Epoch [450], train_loss: 5028.45 with loss1: 5028.45 and loss2: 0.00\n",
      "Epoch [451], train_loss: 3574.63 with loss1: 3574.63 and loss2: 0.00\n",
      "Epoch [452], train_loss: 3793.36 with loss1: 3793.36 and loss2: 0.00\n",
      "Epoch [453], train_loss: 3742.90 with loss1: 3742.90 and loss2: 0.00\n",
      "Epoch [454], train_loss: 3491.22 with loss1: 3491.22 and loss2: 0.00\n",
      "Epoch [455], train_loss: 4262.01 with loss1: 4262.01 and loss2: 0.00\n",
      "Epoch [456], train_loss: 4270.24 with loss1: 4270.24 and loss2: 0.00\n",
      "Epoch [457], train_loss: 3481.08 with loss1: 3481.08 and loss2: 0.00\n",
      "Epoch [458], train_loss: 4272.34 with loss1: 4272.34 and loss2: 0.00\n",
      "Epoch [459], train_loss: 4086.14 with loss1: 4086.14 and loss2: 0.00\n",
      "Epoch [460], train_loss: 4869.77 with loss1: 4869.77 and loss2: 0.00\n",
      "Epoch [461], train_loss: 4376.45 with loss1: 4376.45 and loss2: 0.00\n",
      "Epoch [462], train_loss: 3782.04 with loss1: 3782.04 and loss2: 0.00\n",
      "Epoch [463], train_loss: 4435.67 with loss1: 4435.67 and loss2: 0.00\n",
      "Epoch [464], train_loss: 4287.62 with loss1: 4287.62 and loss2: 0.00\n",
      "Epoch [465], train_loss: 3388.34 with loss1: 3388.34 and loss2: 0.00\n",
      "Epoch [466], train_loss: 4225.45 with loss1: 4225.45 and loss2: 0.00\n",
      "Epoch [467], train_loss: 3686.19 with loss1: 3686.19 and loss2: 0.00\n",
      "Epoch [468], train_loss: 3408.98 with loss1: 3408.98 and loss2: 0.00\n",
      "Epoch [469], train_loss: 4076.30 with loss1: 4076.30 and loss2: 0.00\n",
      "Epoch [470], train_loss: 4352.56 with loss1: 4352.56 and loss2: 0.00\n",
      "Epoch [471], train_loss: 3800.20 with loss1: 3800.20 and loss2: 0.00\n",
      "Epoch [472], train_loss: 3571.12 with loss1: 3571.12 and loss2: 0.00\n",
      "Epoch [473], train_loss: 3718.75 with loss1: 3718.75 and loss2: 0.00\n",
      "Epoch [474], train_loss: 4285.51 with loss1: 4285.51 and loss2: 0.00\n",
      "Epoch [475], train_loss: 4225.32 with loss1: 4225.32 and loss2: 0.00\n",
      "Epoch [476], train_loss: 3654.11 with loss1: 3654.11 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [477], train_loss: 4198.28 with loss1: 4198.28 and loss2: 0.00\n",
      "Epoch [478], train_loss: 4199.22 with loss1: 4199.22 and loss2: 0.00\n",
      "Epoch [479], train_loss: 4201.58 with loss1: 4201.58 and loss2: 0.00\n",
      "Epoch [480], train_loss: 4249.04 with loss1: 4249.04 and loss2: 0.00\n",
      "Epoch [481], train_loss: 5003.00 with loss1: 5003.00 and loss2: 0.00\n",
      "Epoch [482], train_loss: 3871.00 with loss1: 3871.00 and loss2: 0.00\n",
      "Epoch [483], train_loss: 4092.38 with loss1: 4092.38 and loss2: 0.00\n",
      "Epoch [484], train_loss: 4383.12 with loss1: 4383.12 and loss2: 0.00\n",
      "Epoch [485], train_loss: 4357.51 with loss1: 4357.51 and loss2: 0.00\n",
      "Epoch [486], train_loss: 4233.48 with loss1: 4233.48 and loss2: 0.00\n",
      "Epoch [487], train_loss: 4185.08 with loss1: 4185.08 and loss2: 0.00\n",
      "Epoch [488], train_loss: 4152.99 with loss1: 4152.99 and loss2: 0.00\n",
      "Epoch [489], train_loss: 4154.94 with loss1: 4154.94 and loss2: 0.00\n",
      "Epoch [490], train_loss: 4291.90 with loss1: 4291.90 and loss2: 0.00\n",
      "Epoch [491], train_loss: 4172.88 with loss1: 4172.88 and loss2: 0.00\n",
      "Epoch [492], train_loss: 3633.99 with loss1: 3633.99 and loss2: 0.00\n",
      "Epoch [493], train_loss: 3335.00 with loss1: 3335.00 and loss2: 0.00\n",
      "Epoch [494], train_loss: 4020.37 with loss1: 4020.37 and loss2: 0.00\n",
      "Epoch [495], train_loss: 3960.87 with loss1: 3960.87 and loss2: 0.00\n",
      "Epoch [496], train_loss: 3665.35 with loss1: 3665.35 and loss2: 0.00\n",
      "Epoch [497], train_loss: 4213.57 with loss1: 4213.57 and loss2: 0.00\n",
      "Epoch [498], train_loss: 3783.48 with loss1: 3783.48 and loss2: 0.00\n",
      "Epoch [499], train_loss: 3515.13 with loss1: 3515.13 and loss2: 0.00\n",
      "Epoch [500], train_loss: 3440.98 with loss1: 3440.98 and loss2: 0.00\n",
      "Epoch [501], train_loss: 3378.37 with loss1: 3378.37 and loss2: 0.00\n",
      "Epoch [502], train_loss: 3323.53 with loss1: 3323.53 and loss2: 0.00\n",
      "Epoch [503], train_loss: 4189.60 with loss1: 4189.60 and loss2: 0.00\n",
      "Epoch [504], train_loss: 3241.76 with loss1: 3241.76 and loss2: 0.00\n",
      "Epoch [505], train_loss: 4783.17 with loss1: 4783.17 and loss2: 0.00\n",
      "Epoch [506], train_loss: 4405.51 with loss1: 4405.51 and loss2: 0.00\n",
      "Epoch [507], train_loss: 3281.01 with loss1: 3281.01 and loss2: 0.00\n",
      "Epoch [508], train_loss: 4796.40 with loss1: 4796.40 and loss2: 0.00\n",
      "Epoch [509], train_loss: 4442.83 with loss1: 4442.83 and loss2: 0.00\n",
      "Epoch [510], train_loss: 3275.24 with loss1: 3275.24 and loss2: 0.00\n",
      "Epoch [511], train_loss: 4097.56 with loss1: 4097.56 and loss2: 0.00\n",
      "Epoch [512], train_loss: 4109.26 with loss1: 4109.26 and loss2: 0.00\n",
      "Epoch [513], train_loss: 3255.77 with loss1: 3255.77 and loss2: 0.00\n",
      "Epoch [514], train_loss: 4104.87 with loss1: 4104.87 and loss2: 0.00\n",
      "Epoch [515], train_loss: 3826.90 with loss1: 3826.90 and loss2: 0.00\n",
      "Epoch [516], train_loss: 3687.77 with loss1: 3687.77 and loss2: 0.00\n",
      "Epoch [517], train_loss: 3298.00 with loss1: 3298.00 and loss2: 0.00\n",
      "Epoch [518], train_loss: 4098.48 with loss1: 4098.48 and loss2: 0.00\n",
      "Epoch [519], train_loss: 3284.12 with loss1: 3284.12 and loss2: 0.00\n",
      "Epoch [520], train_loss: 4106.64 with loss1: 4106.64 and loss2: 0.00\n",
      "Epoch [521], train_loss: 3175.15 with loss1: 3175.15 and loss2: 0.00\n",
      "Epoch [522], train_loss: 3862.31 with loss1: 3862.31 and loss2: 0.00\n",
      "Epoch [523], train_loss: 4080.89 with loss1: 4080.89 and loss2: 0.00\n",
      "Epoch [524], train_loss: 4253.13 with loss1: 4253.13 and loss2: 0.00\n",
      "Epoch [525], train_loss: 3671.92 with loss1: 3671.92 and loss2: 0.00\n",
      "Epoch [526], train_loss: 3177.26 with loss1: 3177.26 and loss2: 0.00\n",
      "Epoch [527], train_loss: 4735.32 with loss1: 4735.32 and loss2: 0.00\n",
      "Epoch [528], train_loss: 4807.44 with loss1: 4807.44 and loss2: 0.00\n",
      "Epoch [529], train_loss: 4765.76 with loss1: 4765.76 and loss2: 0.00\n",
      "Epoch [530], train_loss: 4740.48 with loss1: 4740.48 and loss2: 0.00\n",
      "Epoch [531], train_loss: 4693.83 with loss1: 4693.83 and loss2: 0.00\n",
      "Epoch [532], train_loss: 4085.67 with loss1: 4085.67 and loss2: 0.00\n",
      "Epoch [533], train_loss: 4694.15 with loss1: 4694.15 and loss2: 0.00\n",
      "Epoch [534], train_loss: 4697.47 with loss1: 4697.47 and loss2: 0.00\n",
      "Epoch [535], train_loss: 4307.50 with loss1: 4307.50 and loss2: 0.00\n",
      "Epoch [536], train_loss: 4376.55 with loss1: 4376.55 and loss2: 0.00\n",
      "Epoch [537], train_loss: 4098.65 with loss1: 4098.65 and loss2: 0.00\n",
      "Epoch [538], train_loss: 3540.87 with loss1: 3540.87 and loss2: 0.00\n",
      "Epoch [539], train_loss: 4148.02 with loss1: 4148.02 and loss2: 0.00\n",
      "Epoch [540], train_loss: 3522.36 with loss1: 3522.36 and loss2: 0.00\n",
      "Epoch [541], train_loss: 3700.59 with loss1: 3700.59 and loss2: 0.00\n",
      "Epoch [542], train_loss: 4033.64 with loss1: 4033.64 and loss2: 0.00\n",
      "Epoch [543], train_loss: 3512.67 with loss1: 3512.67 and loss2: 0.00\n",
      "Epoch [544], train_loss: 3719.95 with loss1: 3719.95 and loss2: 0.00\n",
      "Epoch [545], train_loss: 4038.72 with loss1: 4038.72 and loss2: 0.00\n",
      "Epoch [546], train_loss: 4686.60 with loss1: 4686.60 and loss2: 0.00\n",
      "Epoch [547], train_loss: 4227.08 with loss1: 4227.08 and loss2: 0.00\n",
      "Epoch [548], train_loss: 4063.13 with loss1: 4063.13 and loss2: 0.00\n",
      "Epoch [549], train_loss: 3082.56 with loss1: 3082.56 and loss2: 0.00\n",
      "Epoch [550], train_loss: 3734.46 with loss1: 3734.46 and loss2: 0.00\n",
      "Epoch [551], train_loss: 4008.16 with loss1: 4008.16 and loss2: 0.00\n",
      "Epoch [552], train_loss: 4726.34 with loss1: 4726.34 and loss2: 0.00\n",
      "Epoch [553], train_loss: 3620.08 with loss1: 3620.08 and loss2: 0.00\n",
      "Epoch [554], train_loss: 3554.94 with loss1: 3554.94 and loss2: 0.00\n",
      "Epoch [555], train_loss: 4005.94 with loss1: 4005.94 and loss2: 0.00\n",
      "Epoch [556], train_loss: 3493.52 with loss1: 3493.52 and loss2: 0.00\n",
      "Epoch [557], train_loss: 3982.05 with loss1: 3982.05 and loss2: 0.00\n",
      "Epoch [558], train_loss: 4690.79 with loss1: 4690.79 and loss2: 0.00\n",
      "Epoch [559], train_loss: 3635.10 with loss1: 3635.10 and loss2: 0.00\n",
      "Epoch [560], train_loss: 4607.69 with loss1: 4607.69 and loss2: 0.00\n",
      "Epoch [561], train_loss: 3066.65 with loss1: 3066.65 and loss2: 0.00\n",
      "Epoch [562], train_loss: 4020.05 with loss1: 4020.05 and loss2: 0.00\n",
      "Epoch [563], train_loss: 3605.30 with loss1: 3605.30 and loss2: 0.00\n",
      "Epoch [564], train_loss: 3596.18 with loss1: 3596.18 and loss2: 0.00\n",
      "Epoch [565], train_loss: 4634.17 with loss1: 4634.17 and loss2: 0.00\n",
      "Epoch [566], train_loss: 3618.99 with loss1: 3618.99 and loss2: 0.00\n",
      "Epoch [567], train_loss: 3421.78 with loss1: 3421.78 and loss2: 0.00\n",
      "Epoch [568], train_loss: 3839.90 with loss1: 3839.90 and loss2: 0.00\n",
      "Epoch [569], train_loss: 4034.30 with loss1: 4034.30 and loss2: 0.00\n",
      "Epoch [570], train_loss: 3071.79 with loss1: 3071.79 and loss2: 0.00\n",
      "Epoch [571], train_loss: 3494.77 with loss1: 3494.77 and loss2: 0.00\n",
      "Epoch [572], train_loss: 3955.73 with loss1: 3955.73 and loss2: 0.00\n",
      "Epoch [573], train_loss: 4095.89 with loss1: 4095.89 and loss2: 0.00\n",
      "Epoch [574], train_loss: 3567.18 with loss1: 3567.18 and loss2: 0.00\n",
      "Epoch [575], train_loss: 3817.72 with loss1: 3817.72 and loss2: 0.00\n",
      "Epoch [576], train_loss: 3743.71 with loss1: 3743.71 and loss2: 0.00\n",
      "Epoch [577], train_loss: 3136.80 with loss1: 3136.80 and loss2: 0.00\n",
      "Epoch [578], train_loss: 3494.04 with loss1: 3494.04 and loss2: 0.00\n",
      "Epoch [579], train_loss: 3463.46 with loss1: 3463.46 and loss2: 0.00\n",
      "Epoch [580], train_loss: 3940.79 with loss1: 3940.79 and loss2: 0.00\n",
      "Epoch [581], train_loss: 4073.81 with loss1: 4073.81 and loss2: 0.00\n",
      "Epoch [582], train_loss: 4277.15 with loss1: 4277.15 and loss2: 0.00\n",
      "Epoch [583], train_loss: 3571.87 with loss1: 3571.87 and loss2: 0.00\n",
      "Epoch [584], train_loss: 3002.92 with loss1: 3002.92 and loss2: 0.00\n",
      "Epoch [585], train_loss: 3480.68 with loss1: 3480.68 and loss2: 0.00\n",
      "Epoch [586], train_loss: 4013.98 with loss1: 4013.98 and loss2: 0.00\n",
      "Epoch [587], train_loss: 4164.73 with loss1: 4164.73 and loss2: 0.00\n",
      "Epoch [588], train_loss: 4250.45 with loss1: 4250.45 and loss2: 0.00\n",
      "Epoch [589], train_loss: 3548.95 with loss1: 3548.95 and loss2: 0.00\n",
      "Epoch [590], train_loss: 3924.72 with loss1: 3924.72 and loss2: 0.00\n",
      "Epoch [591], train_loss: 4640.33 with loss1: 4640.33 and loss2: 0.00\n",
      "Epoch [592], train_loss: 2985.48 with loss1: 2985.48 and loss2: 0.00\n",
      "Epoch [593], train_loss: 3630.12 with loss1: 3630.12 and loss2: 0.00\n",
      "Epoch [594], train_loss: 3751.12 with loss1: 3751.12 and loss2: 0.00\n",
      "Epoch [595], train_loss: 3994.96 with loss1: 3994.96 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [596], train_loss: 3458.61 with loss1: 3458.61 and loss2: 0.00\n",
      "Epoch [597], train_loss: 3906.37 with loss1: 3906.37 and loss2: 0.00\n",
      "Epoch [598], train_loss: 4019.28 with loss1: 4019.28 and loss2: 0.00\n",
      "Epoch [599], train_loss: 2959.14 with loss1: 2959.14 and loss2: 0.00\n",
      "Epoch [600], train_loss: 3466.25 with loss1: 3466.25 and loss2: 0.00\n",
      "Epoch [601], train_loss: 4594.80 with loss1: 4594.80 and loss2: 0.00\n",
      "Epoch [602], train_loss: 3929.01 with loss1: 3929.01 and loss2: 0.00\n",
      "Epoch [603], train_loss: 2985.33 with loss1: 2985.33 and loss2: 0.00\n",
      "Epoch [604], train_loss: 4549.44 with loss1: 4549.44 and loss2: 0.00\n",
      "Epoch [605], train_loss: 3906.88 with loss1: 3906.88 and loss2: 0.00\n",
      "Epoch [606], train_loss: 3552.94 with loss1: 3552.94 and loss2: 0.00\n",
      "Epoch [607], train_loss: 3777.13 with loss1: 3777.13 and loss2: 0.00\n",
      "Epoch [608], train_loss: 4514.67 with loss1: 4514.67 and loss2: 0.00\n",
      "Epoch [609], train_loss: 3007.01 with loss1: 3007.01 and loss2: 0.00\n",
      "Epoch [610], train_loss: 4547.23 with loss1: 4547.23 and loss2: 0.00\n",
      "Epoch [611], train_loss: 4660.46 with loss1: 4660.46 and loss2: 0.00\n",
      "Epoch [612], train_loss: 2974.36 with loss1: 2974.36 and loss2: 0.00\n",
      "Epoch [613], train_loss: 3608.42 with loss1: 3608.42 and loss2: 0.00\n",
      "Epoch [614], train_loss: 3353.46 with loss1: 3353.46 and loss2: 0.00\n",
      "Epoch [615], train_loss: 3490.52 with loss1: 3490.52 and loss2: 0.00\n",
      "Epoch [616], train_loss: 3977.14 with loss1: 3977.14 and loss2: 0.00\n",
      "Epoch [617], train_loss: 3877.74 with loss1: 3877.74 and loss2: 0.00\n",
      "Epoch [618], train_loss: 3393.69 with loss1: 3393.69 and loss2: 0.00\n",
      "Epoch [619], train_loss: 4010.97 with loss1: 4010.97 and loss2: 0.00\n",
      "Epoch [620], train_loss: 4158.41 with loss1: 4158.41 and loss2: 0.00\n",
      "Epoch [621], train_loss: 3486.30 with loss1: 3486.30 and loss2: 0.00\n",
      "Epoch [622], train_loss: 3854.26 with loss1: 3854.26 and loss2: 0.00\n",
      "Epoch [623], train_loss: 2988.82 with loss1: 2988.82 and loss2: 0.00\n",
      "Epoch [624], train_loss: 3890.48 with loss1: 3890.48 and loss2: 0.00\n",
      "Epoch [625], train_loss: 3479.44 with loss1: 3479.44 and loss2: 0.00\n",
      "Epoch [626], train_loss: 3881.32 with loss1: 3881.32 and loss2: 0.00\n",
      "Epoch [627], train_loss: 3396.09 with loss1: 3396.09 and loss2: 0.00\n",
      "Epoch [628], train_loss: 4567.06 with loss1: 4567.06 and loss2: 0.00\n",
      "Epoch [629], train_loss: 2917.96 with loss1: 2917.96 and loss2: 0.00\n",
      "Epoch [630], train_loss: 3420.43 with loss1: 3420.43 and loss2: 0.00\n",
      "Epoch [631], train_loss: 3840.13 with loss1: 3840.13 and loss2: 0.00\n",
      "Epoch [632], train_loss: 2965.92 with loss1: 2965.92 and loss2: 0.00\n",
      "Epoch [633], train_loss: 4480.88 with loss1: 4480.88 and loss2: 0.00\n",
      "Epoch [634], train_loss: 2887.45 with loss1: 2887.45 and loss2: 0.00\n",
      "Epoch [635], train_loss: 3409.97 with loss1: 3409.97 and loss2: 0.00\n",
      "Epoch [636], train_loss: 3830.77 with loss1: 3830.77 and loss2: 0.00\n",
      "Epoch [637], train_loss: 3967.85 with loss1: 3967.85 and loss2: 0.00\n",
      "Epoch [638], train_loss: 3402.68 with loss1: 3402.68 and loss2: 0.00\n",
      "Epoch [639], train_loss: 3833.41 with loss1: 3833.41 and loss2: 0.00\n",
      "Epoch [640], train_loss: 3826.78 with loss1: 3826.78 and loss2: 0.00\n",
      "Epoch [641], train_loss: 2913.79 with loss1: 2913.79 and loss2: 0.00\n",
      "Epoch [642], train_loss: 4481.17 with loss1: 4481.17 and loss2: 0.00\n",
      "Epoch [643], train_loss: 4186.96 with loss1: 4186.96 and loss2: 0.00\n",
      "Epoch [644], train_loss: 4633.98 with loss1: 4633.98 and loss2: 0.00\n",
      "Epoch [645], train_loss: 3459.07 with loss1: 3459.07 and loss2: 0.00\n",
      "Epoch [646], train_loss: 3476.74 with loss1: 3476.74 and loss2: 0.00\n",
      "Epoch [647], train_loss: 3808.05 with loss1: 3808.05 and loss2: 0.00\n",
      "Epoch [648], train_loss: 3811.87 with loss1: 3811.87 and loss2: 0.00\n",
      "Epoch [649], train_loss: 3818.58 with loss1: 3818.58 and loss2: 0.00\n",
      "Epoch [650], train_loss: 3809.41 with loss1: 3809.41 and loss2: 0.00\n",
      "Epoch [651], train_loss: 3462.60 with loss1: 3462.60 and loss2: 0.00\n",
      "Epoch [652], train_loss: 4446.52 with loss1: 4446.52 and loss2: 0.00\n",
      "Epoch [653], train_loss: 3817.19 with loss1: 3817.19 and loss2: 0.00\n",
      "Epoch [654], train_loss: 2878.15 with loss1: 2878.15 and loss2: 0.00\n",
      "Epoch [655], train_loss: 3405.60 with loss1: 3405.60 and loss2: 0.00\n",
      "Epoch [656], train_loss: 4484.52 with loss1: 4484.52 and loss2: 0.00\n",
      "Epoch [657], train_loss: 3431.75 with loss1: 3431.75 and loss2: 0.00\n",
      "Epoch [658], train_loss: 3705.56 with loss1: 3705.56 and loss2: 0.00\n",
      "Epoch [659], train_loss: 3905.16 with loss1: 3905.16 and loss2: 0.00\n",
      "Epoch [660], train_loss: 4544.94 with loss1: 4544.94 and loss2: 0.00\n",
      "Epoch [661], train_loss: 3418.99 with loss1: 3418.99 and loss2: 0.00\n",
      "Epoch [662], train_loss: 3793.17 with loss1: 3793.17 and loss2: 0.00\n",
      "Epoch [663], train_loss: 4023.38 with loss1: 4023.38 and loss2: 0.00\n",
      "Epoch [664], train_loss: 3396.71 with loss1: 3396.71 and loss2: 0.00\n",
      "Epoch [665], train_loss: 3581.63 with loss1: 3581.63 and loss2: 0.00\n",
      "Epoch [666], train_loss: 3726.09 with loss1: 3726.09 and loss2: 0.00\n",
      "Epoch [667], train_loss: 4424.30 with loss1: 4424.30 and loss2: 0.00\n",
      "Epoch [668], train_loss: 3416.00 with loss1: 3416.00 and loss2: 0.00\n",
      "Epoch [669], train_loss: 4440.08 with loss1: 4440.08 and loss2: 0.00\n",
      "Epoch [670], train_loss: 4107.96 with loss1: 4107.96 and loss2: 0.00\n",
      "Epoch [671], train_loss: 4592.37 with loss1: 4592.37 and loss2: 0.00\n",
      "Epoch [672], train_loss: 3401.35 with loss1: 3401.35 and loss2: 0.00\n",
      "Epoch [673], train_loss: 3502.59 with loss1: 3502.59 and loss2: 0.00\n",
      "Epoch [674], train_loss: 3496.01 with loss1: 3496.01 and loss2: 0.00\n",
      "Epoch [675], train_loss: 3505.41 with loss1: 3505.41 and loss2: 0.00\n",
      "Epoch [676], train_loss: 4413.16 with loss1: 4413.16 and loss2: 0.00\n",
      "Epoch [677], train_loss: 2831.84 with loss1: 2831.84 and loss2: 0.00\n",
      "Epoch [678], train_loss: 3773.53 with loss1: 3773.53 and loss2: 0.00\n",
      "Epoch [679], train_loss: 4494.45 with loss1: 4494.45 and loss2: 0.00\n",
      "Epoch [680], train_loss: 2807.50 with loss1: 2807.50 and loss2: 0.00\n",
      "Epoch [681], train_loss: 2978.00 with loss1: 2978.00 and loss2: 0.00\n",
      "Epoch [682], train_loss: 3569.32 with loss1: 3569.32 and loss2: 0.00\n",
      "Epoch [683], train_loss: 3227.83 with loss1: 3227.83 and loss2: 0.00\n",
      "Epoch [684], train_loss: 4401.31 with loss1: 4401.31 and loss2: 0.00\n",
      "Epoch [685], train_loss: 3406.97 with loss1: 3406.97 and loss2: 0.00\n",
      "Epoch [686], train_loss: 4408.18 with loss1: 4408.18 and loss2: 0.00\n",
      "Epoch [687], train_loss: 3976.57 with loss1: 3976.57 and loss2: 0.00\n",
      "Epoch [688], train_loss: 4032.49 with loss1: 4032.49 and loss2: 0.00\n",
      "Epoch [689], train_loss: 3365.31 with loss1: 3365.31 and loss2: 0.00\n",
      "Epoch [690], train_loss: 3763.84 with loss1: 3763.84 and loss2: 0.00\n",
      "Epoch [691], train_loss: 2818.08 with loss1: 2818.08 and loss2: 0.00\n",
      "Epoch [692], train_loss: 3488.44 with loss1: 3488.44 and loss2: 0.00\n",
      "Epoch [693], train_loss: 3485.64 with loss1: 3485.64 and loss2: 0.00\n",
      "Epoch [694], train_loss: 4419.61 with loss1: 4419.61 and loss2: 0.00\n",
      "Epoch [695], train_loss: 2789.61 with loss1: 2789.61 and loss2: 0.00\n",
      "Epoch [696], train_loss: 3732.98 with loss1: 3732.98 and loss2: 0.00\n",
      "Epoch [697], train_loss: 3310.88 with loss1: 3310.88 and loss2: 0.00\n",
      "Epoch [698], train_loss: 3747.74 with loss1: 3747.74 and loss2: 0.00\n",
      "Epoch [699], train_loss: 3304.00 with loss1: 3304.00 and loss2: 0.00\n",
      "Epoch [700], train_loss: 3422.54 with loss1: 3422.54 and loss2: 0.00\n",
      "Epoch [701], train_loss: 3576.81 with loss1: 3576.81 and loss2: 0.00\n",
      "Epoch [702], train_loss: 3585.83 with loss1: 3585.83 and loss2: 0.00\n",
      "Epoch [703], train_loss: 3745.94 with loss1: 3745.94 and loss2: 0.00\n",
      "Epoch [704], train_loss: 4469.10 with loss1: 4469.10 and loss2: 0.00\n",
      "Epoch [705], train_loss: 4081.22 with loss1: 4081.22 and loss2: 0.00\n",
      "Epoch [706], train_loss: 4535.41 with loss1: 4535.41 and loss2: 0.00\n",
      "Epoch [707], train_loss: 3349.43 with loss1: 3349.43 and loss2: 0.00\n",
      "Epoch [708], train_loss: 3329.41 with loss1: 3329.41 and loss2: 0.00\n",
      "Epoch [709], train_loss: 2858.53 with loss1: 2858.53 and loss2: 0.00\n",
      "Epoch [710], train_loss: 3494.28 with loss1: 3494.28 and loss2: 0.00\n",
      "Epoch [711], train_loss: 3740.49 with loss1: 3740.49 and loss2: 0.00\n",
      "Epoch [712], train_loss: 3812.52 with loss1: 3812.52 and loss2: 0.00\n",
      "Epoch [713], train_loss: 4472.25 with loss1: 4472.25 and loss2: 0.00\n",
      "Epoch [714], train_loss: 3333.08 with loss1: 3333.08 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [715], train_loss: 3770.91 with loss1: 3770.91 and loss2: 0.00\n",
      "Epoch [716], train_loss: 3739.85 with loss1: 3739.85 and loss2: 0.00\n",
      "Epoch [717], train_loss: 4461.04 with loss1: 4461.04 and loss2: 0.00\n",
      "Epoch [718], train_loss: 4085.34 with loss1: 4085.34 and loss2: 0.00\n",
      "Epoch [719], train_loss: 4481.47 with loss1: 4481.47 and loss2: 0.00\n",
      "Epoch [720], train_loss: 4424.89 with loss1: 4424.89 and loss2: 0.00\n",
      "Epoch [721], train_loss: 3352.10 with loss1: 3352.10 and loss2: 0.00\n",
      "Epoch [722], train_loss: 4349.88 with loss1: 4349.88 and loss2: 0.00\n",
      "Epoch [723], train_loss: 4397.84 with loss1: 4397.84 and loss2: 0.00\n",
      "Epoch [724], train_loss: 4399.01 with loss1: 4399.01 and loss2: 0.00\n",
      "Epoch [725], train_loss: 3979.67 with loss1: 3979.67 and loss2: 0.00\n",
      "Epoch [726], train_loss: 4420.32 with loss1: 4420.32 and loss2: 0.00\n",
      "Epoch [727], train_loss: 3927.66 with loss1: 3927.66 and loss2: 0.00\n",
      "Epoch [728], train_loss: 3772.59 with loss1: 3772.59 and loss2: 0.00\n",
      "Epoch [729], train_loss: 4397.68 with loss1: 4397.68 and loss2: 0.00\n",
      "Epoch [730], train_loss: 2742.74 with loss1: 2742.74 and loss2: 0.00\n",
      "Epoch [731], train_loss: 3824.22 with loss1: 3824.22 and loss2: 0.00\n",
      "Epoch [732], train_loss: 3900.38 with loss1: 3900.38 and loss2: 0.00\n",
      "Epoch [733], train_loss: 3750.54 with loss1: 3750.54 and loss2: 0.00\n",
      "Epoch [734], train_loss: 3709.55 with loss1: 3709.55 and loss2: 0.00\n",
      "Epoch [735], train_loss: 2723.81 with loss1: 2723.81 and loss2: 0.00\n",
      "Epoch [736], train_loss: 2793.98 with loss1: 2793.98 and loss2: 0.00\n",
      "Epoch [737], train_loss: 3700.24 with loss1: 3700.24 and loss2: 0.00\n",
      "Epoch [738], train_loss: 4376.24 with loss1: 4376.24 and loss2: 0.00\n",
      "Epoch [739], train_loss: 3284.31 with loss1: 3284.31 and loss2: 0.00\n",
      "Epoch [740], train_loss: 3690.37 with loss1: 3690.37 and loss2: 0.00\n",
      "Epoch [741], train_loss: 3258.30 with loss1: 3258.30 and loss2: 0.00\n",
      "Epoch [742], train_loss: 3679.31 with loss1: 3679.31 and loss2: 0.00\n",
      "Epoch [743], train_loss: 3256.92 with loss1: 3256.92 and loss2: 0.00\n",
      "Epoch [744], train_loss: 3809.14 with loss1: 3809.14 and loss2: 0.00\n",
      "Epoch [745], train_loss: 3709.60 with loss1: 3709.60 and loss2: 0.00\n",
      "Epoch [746], train_loss: 2711.24 with loss1: 2711.24 and loss2: 0.00\n",
      "Epoch [747], train_loss: 4333.09 with loss1: 4333.09 and loss2: 0.00\n",
      "Epoch [748], train_loss: 3981.72 with loss1: 3981.72 and loss2: 0.00\n",
      "Epoch [749], train_loss: 4036.03 with loss1: 4036.03 and loss2: 0.00\n",
      "Epoch [750], train_loss: 3299.53 with loss1: 3299.53 and loss2: 0.00\n",
      "Epoch [751], train_loss: 3281.22 with loss1: 3281.22 and loss2: 0.00\n",
      "Epoch [752], train_loss: 4344.83 with loss1: 4344.83 and loss2: 0.00\n",
      "Epoch [753], train_loss: 3949.74 with loss1: 3949.74 and loss2: 0.00\n",
      "Epoch [754], train_loss: 3948.53 with loss1: 3948.53 and loss2: 0.00\n",
      "Epoch [755], train_loss: 3733.71 with loss1: 3733.71 and loss2: 0.00\n",
      "Epoch [756], train_loss: 3267.52 with loss1: 3267.52 and loss2: 0.00\n",
      "Epoch [757], train_loss: 3315.90 with loss1: 3315.90 and loss2: 0.00\n",
      "Epoch [758], train_loss: 4338.26 with loss1: 4338.26 and loss2: 0.00\n",
      "Epoch [759], train_loss: 2697.84 with loss1: 2697.84 and loss2: 0.00\n",
      "Epoch [760], train_loss: 2798.65 with loss1: 2798.65 and loss2: 0.00\n",
      "Epoch [761], train_loss: 3393.26 with loss1: 3393.26 and loss2: 0.00\n",
      "Epoch [762], train_loss: 3431.89 with loss1: 3431.89 and loss2: 0.00\n",
      "Epoch [763], train_loss: 3404.07 with loss1: 3404.07 and loss2: 0.00\n",
      "Epoch [764], train_loss: 3361.70 with loss1: 3361.70 and loss2: 0.00\n",
      "Epoch [765], train_loss: 3330.91 with loss1: 3330.91 and loss2: 0.00\n",
      "Epoch [766], train_loss: 4325.83 with loss1: 4325.83 and loss2: 0.00\n",
      "Epoch [767], train_loss: 3826.72 with loss1: 3826.72 and loss2: 0.00\n",
      "Epoch [768], train_loss: 2701.59 with loss1: 2701.59 and loss2: 0.00\n",
      "Epoch [769], train_loss: 3295.76 with loss1: 3295.76 and loss2: 0.00\n",
      "Epoch [770], train_loss: 2879.19 with loss1: 2879.19 and loss2: 0.00\n",
      "Epoch [771], train_loss: 4300.20 with loss1: 4300.20 and loss2: 0.00\n",
      "Epoch [772], train_loss: 3251.54 with loss1: 3251.54 and loss2: 0.00\n",
      "Epoch [773], train_loss: 3702.23 with loss1: 3702.23 and loss2: 0.00\n",
      "Epoch [774], train_loss: 4393.36 with loss1: 4393.36 and loss2: 0.00\n",
      "Epoch [775], train_loss: 3935.28 with loss1: 3935.28 and loss2: 0.00\n",
      "Epoch [776], train_loss: 3247.02 with loss1: 3247.02 and loss2: 0.00\n",
      "Epoch [777], train_loss: 3276.66 with loss1: 3276.66 and loss2: 0.00\n",
      "Epoch [778], train_loss: 3651.48 with loss1: 3651.48 and loss2: 0.00\n",
      "Epoch [779], train_loss: 2687.28 with loss1: 2687.28 and loss2: 0.00\n",
      "Epoch [780], train_loss: 3298.65 with loss1: 3298.65 and loss2: 0.00\n",
      "Epoch [781], train_loss: 3323.76 with loss1: 3323.76 and loss2: 0.00\n",
      "Epoch [782], train_loss: 2954.66 with loss1: 2954.66 and loss2: 0.00\n",
      "Epoch [783], train_loss: 2829.89 with loss1: 2829.89 and loss2: 0.00\n",
      "Epoch [784], train_loss: 3708.37 with loss1: 3708.37 and loss2: 0.00\n",
      "Epoch [785], train_loss: 3672.46 with loss1: 3672.46 and loss2: 0.00\n",
      "Epoch [786], train_loss: 3226.57 with loss1: 3226.57 and loss2: 0.00\n",
      "Epoch [787], train_loss: 3644.42 with loss1: 3644.42 and loss2: 0.00\n",
      "Epoch [788], train_loss: 3645.17 with loss1: 3645.17 and loss2: 0.00\n",
      "Epoch [789], train_loss: 3841.35 with loss1: 3841.35 and loss2: 0.00\n",
      "Epoch [790], train_loss: 3899.98 with loss1: 3899.98 and loss2: 0.00\n",
      "Epoch [791], train_loss: 3700.48 with loss1: 3700.48 and loss2: 0.00\n",
      "Epoch [792], train_loss: 3236.03 with loss1: 3236.03 and loss2: 0.00\n",
      "Epoch [793], train_loss: 2772.79 with loss1: 2772.79 and loss2: 0.00\n",
      "Epoch [794], train_loss: 3627.80 with loss1: 3627.80 and loss2: 0.00\n",
      "Epoch [795], train_loss: 4331.06 with loss1: 4331.06 and loss2: 0.00\n",
      "Epoch [796], train_loss: 3651.78 with loss1: 3651.78 and loss2: 0.00\n",
      "Epoch [797], train_loss: 3634.67 with loss1: 3634.67 and loss2: 0.00\n",
      "Epoch [798], train_loss: 3229.08 with loss1: 3229.08 and loss2: 0.00\n",
      "Epoch [799], train_loss: 4319.23 with loss1: 4319.23 and loss2: 0.00\n",
      "Epoch [800], train_loss: 3908.09 with loss1: 3908.09 and loss2: 0.00\n",
      "Epoch [801], train_loss: 4378.13 with loss1: 4378.13 and loss2: 0.00\n",
      "Epoch [802], train_loss: 2665.63 with loss1: 2665.63 and loss2: 0.00\n",
      "Epoch [803], train_loss: 3741.23 with loss1: 3741.23 and loss2: 0.00\n",
      "Epoch [804], train_loss: 3209.52 with loss1: 3209.52 and loss2: 0.00\n",
      "Epoch [805], train_loss: 3266.57 with loss1: 3266.57 and loss2: 0.00\n",
      "Epoch [806], train_loss: 3328.09 with loss1: 3328.09 and loss2: 0.00\n",
      "Epoch [807], train_loss: 2909.05 with loss1: 2909.05 and loss2: 0.00\n",
      "Epoch [808], train_loss: 3327.56 with loss1: 3327.56 and loss2: 0.00\n",
      "Epoch [809], train_loss: 3392.43 with loss1: 3392.43 and loss2: 0.00\n",
      "Epoch [810], train_loss: 3295.37 with loss1: 3295.37 and loss2: 0.00\n",
      "Epoch [811], train_loss: 2843.76 with loss1: 2843.76 and loss2: 0.00\n",
      "Epoch [812], train_loss: 4274.35 with loss1: 4274.35 and loss2: 0.00\n",
      "Epoch [813], train_loss: 3213.81 with loss1: 3213.81 and loss2: 0.00\n",
      "Epoch [814], train_loss: 3276.04 with loss1: 3276.04 and loss2: 0.00\n",
      "Epoch [815], train_loss: 3328.21 with loss1: 3328.21 and loss2: 0.00\n",
      "Epoch [816], train_loss: 3612.61 with loss1: 3612.61 and loss2: 0.00\n",
      "Epoch [817], train_loss: 2660.59 with loss1: 2660.59 and loss2: 0.00\n",
      "Epoch [818], train_loss: 3673.54 with loss1: 3673.54 and loss2: 0.00\n",
      "Epoch [819], train_loss: 3201.64 with loss1: 3201.64 and loss2: 0.00\n",
      "Epoch [820], train_loss: 3659.92 with loss1: 3659.92 and loss2: 0.00\n",
      "Epoch [821], train_loss: 2644.42 with loss1: 2644.42 and loss2: 0.00\n",
      "Epoch [822], train_loss: 4261.39 with loss1: 4261.39 and loss2: 0.00\n",
      "Epoch [823], train_loss: 3194.10 with loss1: 3194.10 and loss2: 0.00\n",
      "Epoch [824], train_loss: 3280.55 with loss1: 3280.55 and loss2: 0.00\n",
      "Epoch [825], train_loss: 4270.07 with loss1: 4270.07 and loss2: 0.00\n",
      "Epoch [826], train_loss: 3853.75 with loss1: 3853.75 and loss2: 0.00\n",
      "Epoch [827], train_loss: 4365.07 with loss1: 4365.07 and loss2: 0.00\n",
      "Epoch [828], train_loss: 3227.84 with loss1: 3227.84 and loss2: 0.00\n",
      "Epoch [829], train_loss: 3229.48 with loss1: 3229.48 and loss2: 0.00\n",
      "Epoch [830], train_loss: 3275.95 with loss1: 3275.95 and loss2: 0.00\n",
      "Epoch [831], train_loss: 3246.73 with loss1: 3246.73 and loss2: 0.00\n",
      "Epoch [832], train_loss: 3583.57 with loss1: 3583.57 and loss2: 0.00\n",
      "Epoch [833], train_loss: 3602.72 with loss1: 3602.72 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [834], train_loss: 4323.46 with loss1: 4323.46 and loss2: 0.00\n",
      "Epoch [835], train_loss: 3902.28 with loss1: 3902.28 and loss2: 0.00\n",
      "Epoch [836], train_loss: 4351.94 with loss1: 4351.94 and loss2: 0.00\n",
      "Epoch [837], train_loss: 4285.20 with loss1: 4285.20 and loss2: 0.00\n",
      "Epoch [838], train_loss: 3635.16 with loss1: 3635.16 and loss2: 0.00\n",
      "Epoch [839], train_loss: 3202.06 with loss1: 3202.06 and loss2: 0.00\n",
      "Epoch [840], train_loss: 2711.59 with loss1: 2711.59 and loss2: 0.00\n",
      "Epoch [841], train_loss: 2659.58 with loss1: 2659.58 and loss2: 0.00\n",
      "Epoch [842], train_loss: 2630.59 with loss1: 2630.59 and loss2: 0.00\n",
      "Epoch [843], train_loss: 3275.29 with loss1: 3275.29 and loss2: 0.00\n",
      "Epoch [844], train_loss: 3310.98 with loss1: 3310.98 and loss2: 0.00\n",
      "Epoch [845], train_loss: 3208.11 with loss1: 3208.11 and loss2: 0.00\n",
      "Epoch [846], train_loss: 3264.45 with loss1: 3264.45 and loss2: 0.00\n",
      "Epoch [847], train_loss: 4220.92 with loss1: 4220.92 and loss2: 0.00\n",
      "Epoch [848], train_loss: 2621.07 with loss1: 2621.07 and loss2: 0.00\n",
      "Epoch [849], train_loss: 3213.83 with loss1: 3213.83 and loss2: 0.00\n",
      "Epoch [850], train_loss: 3196.71 with loss1: 3196.71 and loss2: 0.00\n",
      "Epoch [851], train_loss: 2630.81 with loss1: 2630.81 and loss2: 0.00\n",
      "Epoch [852], train_loss: 3309.14 with loss1: 3309.14 and loss2: 0.00\n",
      "Epoch [853], train_loss: 3582.65 with loss1: 3582.65 and loss2: 0.00\n",
      "Epoch [854], train_loss: 3200.29 with loss1: 3200.29 and loss2: 0.00\n",
      "Epoch [855], train_loss: 3556.32 with loss1: 3556.32 and loss2: 0.00\n",
      "Epoch [856], train_loss: 2600.02 with loss1: 2600.02 and loss2: 0.00\n",
      "Epoch [857], train_loss: 4217.23 with loss1: 4217.23 and loss2: 0.00\n",
      "Epoch [858], train_loss: 2587.87 with loss1: 2587.87 and loss2: 0.00\n",
      "Epoch [859], train_loss: 4224.06 with loss1: 4224.06 and loss2: 0.00\n",
      "Epoch [860], train_loss: 3198.82 with loss1: 3198.82 and loss2: 0.00\n",
      "Epoch [861], train_loss: 3723.55 with loss1: 3723.55 and loss2: 0.00\n",
      "Epoch [862], train_loss: 3192.46 with loss1: 3192.46 and loss2: 0.00\n",
      "Epoch [863], train_loss: 3214.32 with loss1: 3214.32 and loss2: 0.00\n",
      "Epoch [864], train_loss: 3312.28 with loss1: 3312.28 and loss2: 0.00\n",
      "Epoch [865], train_loss: 3332.15 with loss1: 3332.15 and loss2: 0.00\n",
      "Epoch [866], train_loss: 3299.22 with loss1: 3299.22 and loss2: 0.00\n",
      "Epoch [867], train_loss: 4218.00 with loss1: 4218.00 and loss2: 0.00\n",
      "Epoch [868], train_loss: 4272.38 with loss1: 4272.38 and loss2: 0.00\n",
      "Epoch [869], train_loss: 2588.16 with loss1: 2588.16 and loss2: 0.00\n",
      "Epoch [870], train_loss: 2634.08 with loss1: 2634.08 and loss2: 0.00\n",
      "Epoch [871], train_loss: 2684.07 with loss1: 2684.07 and loss2: 0.00\n",
      "Epoch [872], train_loss: 2659.11 with loss1: 2659.11 and loss2: 0.00\n",
      "Epoch [873], train_loss: 3239.28 with loss1: 3239.28 and loss2: 0.00\n",
      "Epoch [874], train_loss: 4206.59 with loss1: 4206.59 and loss2: 0.00\n",
      "Epoch [875], train_loss: 3161.21 with loss1: 3161.21 and loss2: 0.00\n",
      "Epoch [876], train_loss: 3668.72 with loss1: 3668.72 and loss2: 0.00\n",
      "Epoch [877], train_loss: 3156.23 with loss1: 3156.23 and loss2: 0.00\n",
      "Epoch [878], train_loss: 3549.94 with loss1: 3549.94 and loss2: 0.00\n",
      "Epoch [879], train_loss: 3170.59 with loss1: 3170.59 and loss2: 0.00\n",
      "Epoch [880], train_loss: 3613.62 with loss1: 3613.62 and loss2: 0.00\n",
      "Epoch [881], train_loss: 3766.66 with loss1: 3766.66 and loss2: 0.00\n",
      "Epoch [882], train_loss: 3845.85 with loss1: 3845.85 and loss2: 0.00\n",
      "Epoch [883], train_loss: 3627.72 with loss1: 3627.72 and loss2: 0.00\n",
      "Epoch [884], train_loss: 3146.23 with loss1: 3146.23 and loss2: 0.00\n",
      "Epoch [885], train_loss: 3541.14 with loss1: 3541.14 and loss2: 0.00\n",
      "Epoch [886], train_loss: 3677.70 with loss1: 3677.70 and loss2: 0.00\n",
      "Epoch [887], train_loss: 4292.77 with loss1: 4292.77 and loss2: 0.00\n",
      "Epoch [888], train_loss: 3586.69 with loss1: 3586.69 and loss2: 0.00\n",
      "Epoch [889], train_loss: 3776.97 with loss1: 3776.97 and loss2: 0.00\n",
      "Epoch [890], train_loss: 3170.75 with loss1: 3170.75 and loss2: 0.00\n",
      "Epoch [891], train_loss: 3170.94 with loss1: 3170.94 and loss2: 0.00\n",
      "Epoch [892], train_loss: 2612.88 with loss1: 2612.88 and loss2: 0.00\n",
      "Epoch [893], train_loss: 3617.66 with loss1: 3617.66 and loss2: 0.00\n",
      "Epoch [894], train_loss: 3556.98 with loss1: 3556.98 and loss2: 0.00\n",
      "Epoch [895], train_loss: 2547.53 with loss1: 2547.53 and loss2: 0.00\n",
      "Epoch [896], train_loss: 4207.13 with loss1: 4207.13 and loss2: 0.00\n",
      "Epoch [897], train_loss: 3162.46 with loss1: 3162.46 and loss2: 0.00\n",
      "Epoch [898], train_loss: 4222.86 with loss1: 4222.86 and loss2: 0.00\n",
      "Epoch [899], train_loss: 3162.27 with loss1: 3162.27 and loss2: 0.00\n",
      "Epoch [900], train_loss: 3183.24 with loss1: 3183.24 and loss2: 0.00\n",
      "Epoch [901], train_loss: 3294.65 with loss1: 3294.65 and loss2: 0.00\n",
      "Epoch [902], train_loss: 3525.32 with loss1: 3525.32 and loss2: 0.00\n",
      "Epoch [903], train_loss: 3144.51 with loss1: 3144.51 and loss2: 0.00\n",
      "Epoch [904], train_loss: 3204.83 with loss1: 3204.83 and loss2: 0.00\n",
      "Epoch [905], train_loss: 4179.48 with loss1: 4179.48 and loss2: 0.00\n",
      "Epoch [906], train_loss: 3723.27 with loss1: 3723.27 and loss2: 0.00\n",
      "Epoch [907], train_loss: 2545.94 with loss1: 2545.94 and loss2: 0.00\n",
      "Epoch [908], train_loss: 3516.61 with loss1: 3516.61 and loss2: 0.00\n",
      "Epoch [909], train_loss: 3136.87 with loss1: 3136.87 and loss2: 0.00\n",
      "Epoch [910], train_loss: 3169.04 with loss1: 3169.04 and loss2: 0.00\n",
      "Epoch [911], train_loss: 3513.86 with loss1: 3513.86 and loss2: 0.00\n",
      "Epoch [912], train_loss: 4268.63 with loss1: 4268.63 and loss2: 0.00\n",
      "Epoch [913], train_loss: 3123.65 with loss1: 3123.65 and loss2: 0.00\n",
      "Epoch [914], train_loss: 4178.93 with loss1: 4178.93 and loss2: 0.00\n",
      "Epoch [915], train_loss: 2538.15 with loss1: 2538.15 and loss2: 0.00\n",
      "Epoch [916], train_loss: 2693.11 with loss1: 2693.11 and loss2: 0.00\n",
      "Epoch [917], train_loss: 3233.78 with loss1: 3233.78 and loss2: 0.00\n",
      "Epoch [918], train_loss: 3641.17 with loss1: 3641.17 and loss2: 0.00\n",
      "Epoch [919], train_loss: 3537.97 with loss1: 3537.97 and loss2: 0.00\n",
      "Epoch [920], train_loss: 3151.40 with loss1: 3151.40 and loss2: 0.00\n",
      "Epoch [921], train_loss: 3511.75 with loss1: 3511.75 and loss2: 0.00\n",
      "Epoch [922], train_loss: 3173.85 with loss1: 3173.85 and loss2: 0.00\n",
      "Epoch [923], train_loss: 3590.42 with loss1: 3590.42 and loss2: 0.00\n",
      "Epoch [924], train_loss: 3730.30 with loss1: 3730.30 and loss2: 0.00\n",
      "Epoch [925], train_loss: 3834.43 with loss1: 3834.43 and loss2: 0.00\n",
      "Epoch [926], train_loss: 3104.53 with loss1: 3104.53 and loss2: 0.00\n",
      "Epoch [927], train_loss: 4201.99 with loss1: 4201.99 and loss2: 0.00\n",
      "Epoch [928], train_loss: 3143.85 with loss1: 3143.85 and loss2: 0.00\n",
      "Epoch [929], train_loss: 3151.07 with loss1: 3151.07 and loss2: 0.00\n",
      "Epoch [930], train_loss: 3610.00 with loss1: 3610.00 and loss2: 0.00\n",
      "Epoch [931], train_loss: 2531.48 with loss1: 2531.48 and loss2: 0.00\n",
      "Epoch [932], train_loss: 2639.28 with loss1: 2639.28 and loss2: 0.00\n",
      "Epoch [933], train_loss: 3233.92 with loss1: 3233.92 and loss2: 0.00\n",
      "Epoch [934], train_loss: 4160.91 with loss1: 4160.91 and loss2: 0.00\n",
      "Epoch [935], train_loss: 3523.59 with loss1: 3523.59 and loss2: 0.00\n",
      "Epoch [936], train_loss: 3120.80 with loss1: 3120.80 and loss2: 0.00\n",
      "Epoch [937], train_loss: 3496.13 with loss1: 3496.13 and loss2: 0.00\n",
      "Epoch [938], train_loss: 2527.85 with loss1: 2527.85 and loss2: 0.00\n",
      "Epoch [939], train_loss: 3217.08 with loss1: 3217.08 and loss2: 0.00\n",
      "Epoch [940], train_loss: 3497.53 with loss1: 3497.53 and loss2: 0.00\n",
      "Epoch [941], train_loss: 3154.72 with loss1: 3154.72 and loss2: 0.00\n",
      "Epoch [942], train_loss: 3484.21 with loss1: 3484.21 and loss2: 0.00\n",
      "Epoch [943], train_loss: 2538.62 with loss1: 2538.62 and loss2: 0.00\n",
      "Epoch [944], train_loss: 3581.53 with loss1: 3581.53 and loss2: 0.00\n",
      "Epoch [945], train_loss: 3531.65 with loss1: 3531.65 and loss2: 0.00\n",
      "Epoch [946], train_loss: 4276.15 with loss1: 4276.15 and loss2: 0.00\n",
      "Epoch [947], train_loss: 3152.91 with loss1: 3152.91 and loss2: 0.00\n",
      "Epoch [948], train_loss: 3138.16 with loss1: 3138.16 and loss2: 0.00\n",
      "Epoch [949], train_loss: 2726.45 with loss1: 2726.45 and loss2: 0.00\n",
      "Epoch [950], train_loss: 3481.03 with loss1: 3481.03 and loss2: 0.00\n",
      "Epoch [951], train_loss: 3115.32 with loss1: 3115.32 and loss2: 0.00\n",
      "Epoch [952], train_loss: 3175.23 with loss1: 3175.23 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [953], train_loss: 3296.86 with loss1: 3296.86 and loss2: 0.00\n",
      "Epoch [954], train_loss: 2601.20 with loss1: 2601.20 and loss2: 0.00\n",
      "Epoch [955], train_loss: 3473.50 with loss1: 3473.50 and loss2: 0.00\n",
      "Epoch [956], train_loss: 3123.09 with loss1: 3123.09 and loss2: 0.00\n",
      "Epoch [957], train_loss: 3230.76 with loss1: 3230.76 and loss2: 0.00\n",
      "Epoch [958], train_loss: 2781.26 with loss1: 2781.26 and loss2: 0.00\n",
      "Epoch [959], train_loss: 3486.60 with loss1: 3486.60 and loss2: 0.00\n",
      "Epoch [960], train_loss: 3644.07 with loss1: 3644.07 and loss2: 0.00\n",
      "Epoch [961], train_loss: 3679.50 with loss1: 3679.50 and loss2: 0.00\n",
      "Epoch [962], train_loss: 4220.29 with loss1: 4220.29 and loss2: 0.00\n",
      "Epoch [963], train_loss: 3559.68 with loss1: 3559.68 and loss2: 0.00\n",
      "Epoch [964], train_loss: 3740.39 with loss1: 3740.39 and loss2: 0.00\n",
      "Epoch [965], train_loss: 3552.41 with loss1: 3552.41 and loss2: 0.00\n",
      "Epoch [966], train_loss: 3083.21 with loss1: 3083.21 and loss2: 0.00\n",
      "Epoch [967], train_loss: 3476.91 with loss1: 3476.91 and loss2: 0.00\n",
      "Epoch [968], train_loss: 2521.66 with loss1: 2521.66 and loss2: 0.00\n",
      "Epoch [969], train_loss: 4142.95 with loss1: 4142.95 and loss2: 0.00\n",
      "Epoch [970], train_loss: 3704.90 with loss1: 3704.90 and loss2: 0.00\n",
      "Epoch [971], train_loss: 3736.50 with loss1: 3736.50 and loss2: 0.00\n",
      "Epoch [972], train_loss: 3107.83 with loss1: 3107.83 and loss2: 0.00\n",
      "Epoch [973], train_loss: 2531.06 with loss1: 2531.06 and loss2: 0.00\n",
      "Epoch [974], train_loss: 3182.58 with loss1: 3182.58 and loss2: 0.00\n",
      "Epoch [975], train_loss: 3470.32 with loss1: 3470.32 and loss2: 0.00\n",
      "Epoch [976], train_loss: 2504.02 with loss1: 2504.02 and loss2: 0.00\n",
      "Epoch [977], train_loss: 4145.69 with loss1: 4145.69 and loss2: 0.00\n",
      "Epoch [978], train_loss: 3507.58 with loss1: 3507.58 and loss2: 0.00\n",
      "Epoch [979], train_loss: 3489.82 with loss1: 3489.82 and loss2: 0.00\n",
      "Epoch [980], train_loss: 3482.30 with loss1: 3482.30 and loss2: 0.00\n",
      "Epoch [981], train_loss: 2493.35 with loss1: 2493.35 and loss2: 0.00\n",
      "Epoch [982], train_loss: 3136.30 with loss1: 3136.30 and loss2: 0.00\n",
      "Epoch [983], train_loss: 3457.51 with loss1: 3457.51 and loss2: 0.00\n",
      "Epoch [984], train_loss: 3092.22 with loss1: 3092.22 and loss2: 0.00\n",
      "Epoch [985], train_loss: 3620.19 with loss1: 3620.19 and loss2: 0.00\n",
      "Epoch [986], train_loss: 3066.71 with loss1: 3066.71 and loss2: 0.00\n",
      "Epoch [987], train_loss: 3461.47 with loss1: 3461.47 and loss2: 0.00\n",
      "Epoch [988], train_loss: 3110.53 with loss1: 3110.53 and loss2: 0.00\n",
      "Epoch [989], train_loss: 4126.51 with loss1: 4126.51 and loss2: 0.00\n",
      "Epoch [990], train_loss: 3685.74 with loss1: 3685.74 and loss2: 0.00\n",
      "Epoch [991], train_loss: 3100.52 with loss1: 3100.52 and loss2: 0.00\n",
      "Epoch [992], train_loss: 4168.88 with loss1: 4168.88 and loss2: 0.00\n",
      "Epoch [993], train_loss: 3101.08 with loss1: 3101.08 and loss2: 0.00\n",
      "Epoch [994], train_loss: 3591.52 with loss1: 3591.52 and loss2: 0.00\n",
      "Epoch [995], train_loss: 3668.74 with loss1: 3668.74 and loss2: 0.00\n",
      "Epoch [996], train_loss: 3690.30 with loss1: 3690.30 and loss2: 0.00\n",
      "Epoch [997], train_loss: 4230.57 with loss1: 4230.57 and loss2: 0.00\n",
      "Epoch [998], train_loss: 3093.38 with loss1: 3093.38 and loss2: 0.00\n",
      "Epoch [999], train_loss: 3483.61 with loss1: 3483.61 and loss2: 0.00\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# new model\n",
    "# delete loss2\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history = fit(epochs=1000, lr=5e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aabba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 82938.25 with loss1: 82938.25 and loss2: 0.00\n",
      "Epoch [1], train_loss: 59790.07 with loss1: 59790.07 and loss2: 0.00\n",
      "Epoch [2], train_loss: 56011.70 with loss1: 56011.70 and loss2: 0.00\n",
      "Epoch [3], train_loss: 49071.40 with loss1: 49071.40 and loss2: 0.00\n",
      "Epoch [4], train_loss: 32797.59 with loss1: 32797.59 and loss2: 0.00\n",
      "Epoch [5], train_loss: 24859.16 with loss1: 24859.16 and loss2: 0.00\n",
      "Epoch [6], train_loss: 22937.26 with loss1: 22937.26 and loss2: 0.00\n",
      "Epoch [7], train_loss: 12778.61 with loss1: 12778.61 and loss2: 0.00\n",
      "Epoch [8], train_loss: 12235.31 with loss1: 12235.31 and loss2: 0.00\n",
      "Epoch [9], train_loss: 11393.92 with loss1: 11393.92 and loss2: 0.00\n",
      "Epoch [10], train_loss: 10219.99 with loss1: 10219.99 and loss2: 0.00\n",
      "Epoch [11], train_loss: 10087.34 with loss1: 10087.34 and loss2: 0.00\n",
      "Epoch [12], train_loss: 9793.68 with loss1: 9793.68 and loss2: 0.00\n",
      "Epoch [13], train_loss: 9259.64 with loss1: 9259.64 and loss2: 0.00\n",
      "Epoch [14], train_loss: 9932.75 with loss1: 9932.75 and loss2: 0.00\n",
      "Epoch [15], train_loss: 8898.26 with loss1: 8898.26 and loss2: 0.00\n",
      "Epoch [16], train_loss: 9095.55 with loss1: 9095.55 and loss2: 0.00\n",
      "Epoch [17], train_loss: 8991.37 with loss1: 8991.37 and loss2: 0.00\n",
      "Epoch [18], train_loss: 9474.89 with loss1: 9474.89 and loss2: 0.00\n",
      "Epoch [19], train_loss: 7769.38 with loss1: 7769.38 and loss2: 0.00\n",
      "Epoch [20], train_loss: 8320.27 with loss1: 8320.27 and loss2: 0.00\n",
      "Epoch [21], train_loss: 8128.15 with loss1: 8128.15 and loss2: 0.00\n",
      "Epoch [22], train_loss: 8079.66 with loss1: 8079.66 and loss2: 0.00\n",
      "Epoch [23], train_loss: 7503.25 with loss1: 7503.25 and loss2: 0.00\n",
      "Epoch [24], train_loss: 7962.19 with loss1: 7962.19 and loss2: 0.00\n",
      "Epoch [25], train_loss: 8174.75 with loss1: 8174.75 and loss2: 0.00\n",
      "Epoch [26], train_loss: 8844.16 with loss1: 8844.16 and loss2: 0.00\n",
      "Epoch [27], train_loss: 7582.35 with loss1: 7582.35 and loss2: 0.00\n",
      "Epoch [28], train_loss: 8839.19 with loss1: 8839.19 and loss2: 0.00\n",
      "Epoch [29], train_loss: 7155.32 with loss1: 7155.32 and loss2: 0.00\n",
      "Epoch [30], train_loss: 6974.31 with loss1: 6974.31 and loss2: 0.00\n",
      "Epoch [31], train_loss: 7019.07 with loss1: 7019.07 and loss2: 0.00\n",
      "Epoch [32], train_loss: 6976.02 with loss1: 6976.02 and loss2: 0.00\n",
      "Epoch [33], train_loss: 7352.53 with loss1: 7352.53 and loss2: 0.00\n",
      "Epoch [34], train_loss: 6948.13 with loss1: 6948.13 and loss2: 0.00\n",
      "Epoch [35], train_loss: 7746.31 with loss1: 7746.31 and loss2: 0.00\n",
      "Epoch [36], train_loss: 7192.32 with loss1: 7192.32 and loss2: 0.00\n",
      "Epoch [37], train_loss: 7195.46 with loss1: 7195.46 and loss2: 0.00\n",
      "Epoch [38], train_loss: 6767.66 with loss1: 6767.66 and loss2: 0.00\n",
      "Epoch [39], train_loss: 7477.37 with loss1: 7477.37 and loss2: 0.00\n",
      "Epoch [40], train_loss: 6728.91 with loss1: 6728.91 and loss2: 0.00\n",
      "Epoch [41], train_loss: 7209.86 with loss1: 7209.86 and loss2: 0.00\n",
      "Epoch [42], train_loss: 7268.17 with loss1: 7268.17 and loss2: 0.00\n",
      "Epoch [43], train_loss: 7152.83 with loss1: 7152.83 and loss2: 0.00\n",
      "Epoch [44], train_loss: 6944.23 with loss1: 6944.23 and loss2: 0.00\n",
      "Epoch [45], train_loss: 6927.39 with loss1: 6927.39 and loss2: 0.00\n",
      "Epoch [46], train_loss: 6930.11 with loss1: 6930.11 and loss2: 0.00\n",
      "Epoch [47], train_loss: 6822.74 with loss1: 6822.74 and loss2: 0.00\n",
      "Epoch [48], train_loss: 7821.93 with loss1: 7821.93 and loss2: 0.00\n",
      "Epoch [49], train_loss: 7602.76 with loss1: 7602.76 and loss2: 0.00\n",
      "Epoch [50], train_loss: 6475.44 with loss1: 6475.44 and loss2: 0.00\n",
      "Epoch [51], train_loss: 7033.36 with loss1: 7033.36 and loss2: 0.00\n",
      "Epoch [52], train_loss: 6778.90 with loss1: 6778.90 and loss2: 0.00\n",
      "Epoch [53], train_loss: 6517.44 with loss1: 6517.44 and loss2: 0.00\n",
      "Epoch [54], train_loss: 6457.18 with loss1: 6457.18 and loss2: 0.00\n",
      "Epoch [55], train_loss: 6620.42 with loss1: 6620.42 and loss2: 0.00\n",
      "Epoch [56], train_loss: 6748.50 with loss1: 6748.50 and loss2: 0.00\n",
      "Epoch [57], train_loss: 7703.89 with loss1: 7703.89 and loss2: 0.00\n",
      "Epoch [58], train_loss: 8062.80 with loss1: 8062.80 and loss2: 0.00\n",
      "Epoch [59], train_loss: 6787.17 with loss1: 6787.17 and loss2: 0.00\n",
      "Epoch [60], train_loss: 6741.33 with loss1: 6741.33 and loss2: 0.00\n",
      "Epoch [61], train_loss: 7411.85 with loss1: 7411.85 and loss2: 0.00\n",
      "Epoch [62], train_loss: 7168.68 with loss1: 7168.68 and loss2: 0.00\n",
      "Epoch [63], train_loss: 7933.79 with loss1: 7933.79 and loss2: 0.00\n",
      "Epoch [64], train_loss: 7554.57 with loss1: 7554.57 and loss2: 0.00\n",
      "Epoch [65], train_loss: 6253.68 with loss1: 6253.68 and loss2: 0.00\n",
      "Epoch [66], train_loss: 6643.68 with loss1: 6643.68 and loss2: 0.00\n",
      "Epoch [67], train_loss: 6633.41 with loss1: 6633.41 and loss2: 0.00\n",
      "Epoch [68], train_loss: 7090.44 with loss1: 7090.44 and loss2: 0.00\n",
      "Epoch [69], train_loss: 6825.33 with loss1: 6825.33 and loss2: 0.00\n",
      "Epoch [70], train_loss: 6252.93 with loss1: 6252.93 and loss2: 0.00\n",
      "Epoch [71], train_loss: 6630.39 with loss1: 6630.39 and loss2: 0.00\n",
      "Epoch [72], train_loss: 6512.16 with loss1: 6512.16 and loss2: 0.00\n",
      "Epoch [73], train_loss: 6339.94 with loss1: 6339.94 and loss2: 0.00\n",
      "Epoch [74], train_loss: 7687.89 with loss1: 7687.89 and loss2: 0.00\n",
      "Epoch [75], train_loss: 6405.78 with loss1: 6405.78 and loss2: 0.00\n",
      "Epoch [76], train_loss: 6692.56 with loss1: 6692.56 and loss2: 0.00\n",
      "Epoch [77], train_loss: 6321.96 with loss1: 6321.96 and loss2: 0.00\n",
      "Epoch [78], train_loss: 6883.64 with loss1: 6883.64 and loss2: 0.00\n",
      "Epoch [79], train_loss: 6074.85 with loss1: 6074.85 and loss2: 0.00\n",
      "Epoch [80], train_loss: 7485.64 with loss1: 7485.64 and loss2: 0.00\n",
      "Epoch [81], train_loss: 6037.41 with loss1: 6037.41 and loss2: 0.00\n",
      "Epoch [82], train_loss: 6716.42 with loss1: 6716.42 and loss2: 0.00\n",
      "Epoch [83], train_loss: 6304.92 with loss1: 6304.92 and loss2: 0.00\n",
      "Epoch [84], train_loss: 7556.00 with loss1: 7556.00 and loss2: 0.00\n",
      "Epoch [85], train_loss: 7372.69 with loss1: 7372.69 and loss2: 0.00\n",
      "Epoch [86], train_loss: 6293.31 with loss1: 6293.31 and loss2: 0.00\n",
      "Epoch [87], train_loss: 6429.23 with loss1: 6429.23 and loss2: 0.00\n",
      "Epoch [88], train_loss: 6021.33 with loss1: 6021.33 and loss2: 0.00\n",
      "Epoch [89], train_loss: 6004.18 with loss1: 6004.18 and loss2: 0.00\n",
      "Epoch [90], train_loss: 6298.13 with loss1: 6298.13 and loss2: 0.00\n",
      "Epoch [91], train_loss: 6336.64 with loss1: 6336.64 and loss2: 0.00\n",
      "Epoch [92], train_loss: 6160.01 with loss1: 6160.01 and loss2: 0.00\n",
      "Epoch [93], train_loss: 6266.96 with loss1: 6266.96 and loss2: 0.00\n",
      "Epoch [94], train_loss: 6225.64 with loss1: 6225.64 and loss2: 0.00\n",
      "Epoch [95], train_loss: 6121.15 with loss1: 6121.15 and loss2: 0.00\n",
      "Epoch [96], train_loss: 6218.33 with loss1: 6218.33 and loss2: 0.00\n",
      "Epoch [97], train_loss: 6187.43 with loss1: 6187.43 and loss2: 0.00\n",
      "Epoch [98], train_loss: 6220.28 with loss1: 6220.28 and loss2: 0.00\n",
      "Epoch [99], train_loss: 6094.50 with loss1: 6094.50 and loss2: 0.00\n",
      "Epoch [100], train_loss: 6137.89 with loss1: 6137.89 and loss2: 0.00\n",
      "Epoch [101], train_loss: 5896.70 with loss1: 5896.70 and loss2: 0.00\n",
      "Epoch [102], train_loss: 6170.36 with loss1: 6170.36 and loss2: 0.00\n",
      "Epoch [103], train_loss: 6121.34 with loss1: 6121.34 and loss2: 0.00\n",
      "Epoch [104], train_loss: 7529.64 with loss1: 7529.64 and loss2: 0.00\n",
      "Epoch [105], train_loss: 6234.67 with loss1: 6234.67 and loss2: 0.00\n",
      "Epoch [106], train_loss: 7348.85 with loss1: 7348.85 and loss2: 0.00\n",
      "Epoch [107], train_loss: 5923.75 with loss1: 5923.75 and loss2: 0.00\n",
      "Epoch [108], train_loss: 5836.17 with loss1: 5836.17 and loss2: 0.00\n",
      "Epoch [109], train_loss: 6171.40 with loss1: 6171.40 and loss2: 0.00\n",
      "Epoch [110], train_loss: 6708.66 with loss1: 6708.66 and loss2: 0.00\n",
      "Epoch [111], train_loss: 7308.46 with loss1: 7308.46 and loss2: 0.00\n",
      "Epoch [112], train_loss: 6166.51 with loss1: 6166.51 and loss2: 0.00\n",
      "Epoch [113], train_loss: 6061.15 with loss1: 6061.15 and loss2: 0.00\n",
      "Epoch [114], train_loss: 5897.58 with loss1: 5897.58 and loss2: 0.00\n",
      "Epoch [115], train_loss: 6057.06 with loss1: 6057.06 and loss2: 0.00\n",
      "Epoch [116], train_loss: 5815.41 with loss1: 5815.41 and loss2: 0.00\n",
      "Epoch [117], train_loss: 6149.53 with loss1: 6149.53 and loss2: 0.00\n",
      "Epoch [118], train_loss: 7346.39 with loss1: 7346.39 and loss2: 0.00\n",
      "Epoch [119], train_loss: 6061.20 with loss1: 6061.20 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120], train_loss: 5817.48 with loss1: 5817.48 and loss2: 0.00\n",
      "Epoch [121], train_loss: 5825.54 with loss1: 5825.54 and loss2: 0.00\n",
      "Epoch [122], train_loss: 6092.05 with loss1: 6092.05 and loss2: 0.00\n",
      "Epoch [123], train_loss: 5954.06 with loss1: 5954.06 and loss2: 0.00\n",
      "Epoch [124], train_loss: 7226.69 with loss1: 7226.69 and loss2: 0.00\n",
      "Epoch [125], train_loss: 5900.05 with loss1: 5900.05 and loss2: 0.00\n",
      "Epoch [126], train_loss: 5713.09 with loss1: 5713.09 and loss2: 0.00\n",
      "Epoch [127], train_loss: 5800.07 with loss1: 5800.07 and loss2: 0.00\n",
      "Epoch [128], train_loss: 5622.43 with loss1: 5622.43 and loss2: 0.00\n",
      "Epoch [129], train_loss: 5689.13 with loss1: 5689.13 and loss2: 0.00\n",
      "Epoch [130], train_loss: 7212.95 with loss1: 7212.95 and loss2: 0.00\n",
      "Epoch [131], train_loss: 5988.74 with loss1: 5988.74 and loss2: 0.00\n",
      "Epoch [132], train_loss: 5752.96 with loss1: 5752.96 and loss2: 0.00\n",
      "Epoch [133], train_loss: 5707.03 with loss1: 5707.03 and loss2: 0.00\n",
      "Epoch [134], train_loss: 6967.09 with loss1: 6967.09 and loss2: 0.00\n",
      "Epoch [135], train_loss: 7153.12 with loss1: 7153.12 and loss2: 0.00\n",
      "Epoch [136], train_loss: 5950.75 with loss1: 5950.75 and loss2: 0.00\n",
      "Epoch [137], train_loss: 6660.69 with loss1: 6660.69 and loss2: 0.00\n",
      "Epoch [138], train_loss: 5849.61 with loss1: 5849.61 and loss2: 0.00\n",
      "Epoch [139], train_loss: 5852.69 with loss1: 5852.69 and loss2: 0.00\n",
      "Epoch [140], train_loss: 5885.20 with loss1: 5885.20 and loss2: 0.00\n",
      "Epoch [141], train_loss: 7167.02 with loss1: 7167.02 and loss2: 0.00\n",
      "Epoch [142], train_loss: 5946.53 with loss1: 5946.53 and loss2: 0.00\n",
      "Epoch [143], train_loss: 5953.37 with loss1: 5953.37 and loss2: 0.00\n",
      "Epoch [144], train_loss: 6072.40 with loss1: 6072.40 and loss2: 0.00\n",
      "Epoch [145], train_loss: 6874.35 with loss1: 6874.35 and loss2: 0.00\n",
      "Epoch [146], train_loss: 5812.91 with loss1: 5812.91 and loss2: 0.00\n",
      "Epoch [147], train_loss: 5697.31 with loss1: 5697.31 and loss2: 0.00\n",
      "Epoch [148], train_loss: 5619.00 with loss1: 5619.00 and loss2: 0.00\n",
      "Epoch [149], train_loss: 5629.23 with loss1: 5629.23 and loss2: 0.00\n",
      "Epoch [150], train_loss: 5943.67 with loss1: 5943.67 and loss2: 0.00\n",
      "Epoch [151], train_loss: 5958.37 with loss1: 5958.37 and loss2: 0.00\n",
      "Epoch [152], train_loss: 5859.84 with loss1: 5859.84 and loss2: 0.00\n",
      "Epoch [153], train_loss: 5798.87 with loss1: 5798.87 and loss2: 0.00\n",
      "Epoch [154], train_loss: 5770.61 with loss1: 5770.61 and loss2: 0.00\n",
      "Epoch [155], train_loss: 5741.15 with loss1: 5741.15 and loss2: 0.00\n",
      "Epoch [156], train_loss: 5885.05 with loss1: 5885.05 and loss2: 0.00\n",
      "Epoch [157], train_loss: 5931.05 with loss1: 5931.05 and loss2: 0.00\n",
      "Epoch [158], train_loss: 5611.33 with loss1: 5611.33 and loss2: 0.00\n",
      "Epoch [159], train_loss: 5909.27 with loss1: 5909.27 and loss2: 0.00\n",
      "Epoch [160], train_loss: 5747.92 with loss1: 5747.92 and loss2: 0.00\n",
      "Epoch [161], train_loss: 5719.81 with loss1: 5719.81 and loss2: 0.00\n",
      "Epoch [162], train_loss: 5845.09 with loss1: 5845.09 and loss2: 0.00\n",
      "Epoch [163], train_loss: 5704.70 with loss1: 5704.70 and loss2: 0.00\n",
      "Epoch [164], train_loss: 5820.16 with loss1: 5820.16 and loss2: 0.00\n",
      "Epoch [165], train_loss: 7085.40 with loss1: 7085.40 and loss2: 0.00\n",
      "Epoch [166], train_loss: 5737.51 with loss1: 5737.51 and loss2: 0.00\n",
      "Epoch [167], train_loss: 5800.69 with loss1: 5800.69 and loss2: 0.00\n",
      "Epoch [168], train_loss: 5762.31 with loss1: 5762.31 and loss2: 0.00\n",
      "Epoch [169], train_loss: 7018.14 with loss1: 7018.14 and loss2: 0.00\n",
      "Epoch [170], train_loss: 5768.48 with loss1: 5768.48 and loss2: 0.00\n",
      "Epoch [171], train_loss: 5754.35 with loss1: 5754.35 and loss2: 0.00\n",
      "Epoch [172], train_loss: 5590.93 with loss1: 5590.93 and loss2: 0.00\n",
      "Epoch [173], train_loss: 5481.02 with loss1: 5481.02 and loss2: 0.00\n",
      "Epoch [174], train_loss: 6812.82 with loss1: 6812.82 and loss2: 0.00\n",
      "Epoch [175], train_loss: 5423.19 with loss1: 5423.19 and loss2: 0.00\n",
      "Epoch [176], train_loss: 6812.62 with loss1: 6812.62 and loss2: 0.00\n",
      "Epoch [177], train_loss: 5730.88 with loss1: 5730.88 and loss2: 0.00\n",
      "Epoch [178], train_loss: 6772.41 with loss1: 6772.41 and loss2: 0.00\n",
      "Epoch [179], train_loss: 5669.21 with loss1: 5669.21 and loss2: 0.00\n",
      "Epoch [180], train_loss: 6536.29 with loss1: 6536.29 and loss2: 0.00\n",
      "Epoch [181], train_loss: 5787.15 with loss1: 5787.15 and loss2: 0.00\n",
      "Epoch [182], train_loss: 5768.80 with loss1: 5768.80 and loss2: 0.00\n",
      "Epoch [183], train_loss: 6523.47 with loss1: 6523.47 and loss2: 0.00\n",
      "Epoch [184], train_loss: 5649.69 with loss1: 5649.69 and loss2: 0.00\n",
      "Epoch [185], train_loss: 5512.81 with loss1: 5512.81 and loss2: 0.00\n",
      "Epoch [186], train_loss: 6040.37 with loss1: 6040.37 and loss2: 0.00\n",
      "Epoch [187], train_loss: 5637.67 with loss1: 5637.67 and loss2: 0.00\n",
      "Epoch [188], train_loss: 5741.49 with loss1: 5741.49 and loss2: 0.00\n",
      "Epoch [189], train_loss: 5853.32 with loss1: 5853.32 and loss2: 0.00\n",
      "Epoch [190], train_loss: 5641.74 with loss1: 5641.74 and loss2: 0.00\n",
      "Epoch [191], train_loss: 6903.03 with loss1: 6903.03 and loss2: 0.00\n",
      "Epoch [192], train_loss: 5645.31 with loss1: 5645.31 and loss2: 0.00\n",
      "Epoch [193], train_loss: 5550.46 with loss1: 5550.46 and loss2: 0.00\n",
      "Epoch [194], train_loss: 5566.11 with loss1: 5566.11 and loss2: 0.00\n",
      "Epoch [195], train_loss: 6802.69 with loss1: 6802.69 and loss2: 0.00\n",
      "Epoch [196], train_loss: 5399.07 with loss1: 5399.07 and loss2: 0.00\n",
      "Epoch [197], train_loss: 5722.56 with loss1: 5722.56 and loss2: 0.00\n",
      "Epoch [198], train_loss: 5611.99 with loss1: 5611.99 and loss2: 0.00\n",
      "Epoch [199], train_loss: 6723.49 with loss1: 6723.49 and loss2: 0.00\n",
      "Epoch [200], train_loss: 5718.30 with loss1: 5718.30 and loss2: 0.00\n",
      "Epoch [201], train_loss: 5533.75 with loss1: 5533.75 and loss2: 0.00\n",
      "Epoch [202], train_loss: 5710.52 with loss1: 5710.52 and loss2: 0.00\n",
      "Epoch [203], train_loss: 6225.41 with loss1: 6225.41 and loss2: 0.00\n",
      "Epoch [204], train_loss: 6517.23 with loss1: 6517.23 and loss2: 0.00\n",
      "Epoch [205], train_loss: 5384.34 with loss1: 5384.34 and loss2: 0.00\n",
      "Epoch [206], train_loss: 6646.97 with loss1: 6646.97 and loss2: 0.00\n",
      "Epoch [207], train_loss: 5693.75 with loss1: 5693.75 and loss2: 0.00\n",
      "Epoch [208], train_loss: 6712.36 with loss1: 6712.36 and loss2: 0.00\n",
      "Epoch [209], train_loss: 5467.75 with loss1: 5467.75 and loss2: 0.00\n",
      "Epoch [210], train_loss: 5392.46 with loss1: 5392.46 and loss2: 0.00\n",
      "Epoch [211], train_loss: 5853.80 with loss1: 5853.80 and loss2: 0.00\n",
      "Epoch [212], train_loss: 6430.03 with loss1: 6430.03 and loss2: 0.00\n",
      "Epoch [213], train_loss: 6673.49 with loss1: 6673.49 and loss2: 0.00\n",
      "Epoch [214], train_loss: 6568.84 with loss1: 6568.84 and loss2: 0.00\n",
      "Epoch [215], train_loss: 5643.43 with loss1: 5643.43 and loss2: 0.00\n",
      "Epoch [216], train_loss: 6588.35 with loss1: 6588.35 and loss2: 0.00\n",
      "Epoch [217], train_loss: 5623.86 with loss1: 5623.86 and loss2: 0.00\n",
      "Epoch [218], train_loss: 5466.20 with loss1: 5466.20 and loss2: 0.00\n",
      "Epoch [219], train_loss: 5382.08 with loss1: 5382.08 and loss2: 0.00\n",
      "Epoch [220], train_loss: 5398.47 with loss1: 5398.47 and loss2: 0.00\n",
      "Epoch [221], train_loss: 5620.45 with loss1: 5620.45 and loss2: 0.00\n",
      "Epoch [222], train_loss: 5599.40 with loss1: 5599.40 and loss2: 0.00\n",
      "Epoch [223], train_loss: 5538.92 with loss1: 5538.92 and loss2: 0.00\n",
      "Epoch [224], train_loss: 5412.28 with loss1: 5412.28 and loss2: 0.00\n",
      "Epoch [225], train_loss: 5597.51 with loss1: 5597.51 and loss2: 0.00\n",
      "Epoch [226], train_loss: 5399.61 with loss1: 5399.61 and loss2: 0.00\n",
      "Epoch [227], train_loss: 5320.92 with loss1: 5320.92 and loss2: 0.00\n",
      "Epoch [228], train_loss: 5616.16 with loss1: 5616.16 and loss2: 0.00\n",
      "Epoch [229], train_loss: 6785.19 with loss1: 6785.19 and loss2: 0.00\n",
      "Epoch [230], train_loss: 5546.62 with loss1: 5546.62 and loss2: 0.00\n",
      "Epoch [231], train_loss: 5777.41 with loss1: 5777.41 and loss2: 0.00\n",
      "Epoch [232], train_loss: 5501.67 with loss1: 5501.67 and loss2: 0.00\n",
      "Epoch [233], train_loss: 6715.54 with loss1: 6715.54 and loss2: 0.00\n",
      "Epoch [234], train_loss: 6597.70 with loss1: 6597.70 and loss2: 0.00\n",
      "Epoch [235], train_loss: 5564.18 with loss1: 5564.18 and loss2: 0.00\n",
      "Epoch [236], train_loss: 5539.08 with loss1: 5539.08 and loss2: 0.00\n",
      "Epoch [237], train_loss: 5577.46 with loss1: 5577.46 and loss2: 0.00\n",
      "Epoch [238], train_loss: 5925.94 with loss1: 5925.94 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [239], train_loss: 5779.59 with loss1: 5779.59 and loss2: 0.00\n",
      "Epoch [240], train_loss: 5479.03 with loss1: 5479.03 and loss2: 0.00\n",
      "Epoch [241], train_loss: 5530.39 with loss1: 5530.39 and loss2: 0.00\n",
      "Epoch [242], train_loss: 5447.96 with loss1: 5447.96 and loss2: 0.00\n",
      "Epoch [243], train_loss: 5281.57 with loss1: 5281.57 and loss2: 0.00\n",
      "Epoch [244], train_loss: 5723.83 with loss1: 5723.83 and loss2: 0.00\n",
      "Epoch [245], train_loss: 6392.44 with loss1: 6392.44 and loss2: 0.00\n",
      "Epoch [246], train_loss: 5572.52 with loss1: 5572.52 and loss2: 0.00\n",
      "Epoch [247], train_loss: 5473.90 with loss1: 5473.90 and loss2: 0.00\n",
      "Epoch [248], train_loss: 5440.16 with loss1: 5440.16 and loss2: 0.00\n",
      "Epoch [249], train_loss: 5581.73 with loss1: 5581.73 and loss2: 0.00\n",
      "Epoch [250], train_loss: 6383.41 with loss1: 6383.41 and loss2: 0.00\n",
      "Epoch [251], train_loss: 6544.18 with loss1: 6544.18 and loss2: 0.00\n",
      "Epoch [252], train_loss: 5538.41 with loss1: 5538.41 and loss2: 0.00\n",
      "Epoch [253], train_loss: 5625.55 with loss1: 5625.55 and loss2: 0.00\n",
      "Epoch [254], train_loss: 5360.12 with loss1: 5360.12 and loss2: 0.00\n",
      "Epoch [255], train_loss: 5489.78 with loss1: 5489.78 and loss2: 0.00\n",
      "Epoch [256], train_loss: 5498.12 with loss1: 5498.12 and loss2: 0.00\n",
      "Epoch [257], train_loss: 5503.96 with loss1: 5503.96 and loss2: 0.00\n",
      "Epoch [258], train_loss: 6415.46 with loss1: 6415.46 and loss2: 0.00\n",
      "Epoch [259], train_loss: 6522.83 with loss1: 6522.83 and loss2: 0.00\n",
      "Epoch [260], train_loss: 6400.93 with loss1: 6400.93 and loss2: 0.00\n",
      "Epoch [261], train_loss: 5277.93 with loss1: 5277.93 and loss2: 0.00\n",
      "Epoch [262], train_loss: 5507.11 with loss1: 5507.11 and loss2: 0.00\n",
      "Epoch [263], train_loss: 5376.26 with loss1: 5376.26 and loss2: 0.00\n",
      "Epoch [264], train_loss: 5540.19 with loss1: 5540.19 and loss2: 0.00\n",
      "Epoch [265], train_loss: 5807.89 with loss1: 5807.89 and loss2: 0.00\n",
      "Epoch [266], train_loss: 5474.45 with loss1: 5474.45 and loss2: 0.00\n",
      "Epoch [267], train_loss: 5340.24 with loss1: 5340.24 and loss2: 0.00\n",
      "Epoch [268], train_loss: 5600.81 with loss1: 5600.81 and loss2: 0.00\n",
      "Epoch [269], train_loss: 5339.17 with loss1: 5339.17 and loss2: 0.00\n",
      "Epoch [270], train_loss: 6489.79 with loss1: 6489.79 and loss2: 0.00\n",
      "Epoch [271], train_loss: 6373.89 with loss1: 6373.89 and loss2: 0.00\n",
      "Epoch [272], train_loss: 5512.93 with loss1: 5512.93 and loss2: 0.00\n",
      "Epoch [273], train_loss: 5348.82 with loss1: 5348.82 and loss2: 0.00\n",
      "Epoch [274], train_loss: 5448.42 with loss1: 5448.42 and loss2: 0.00\n",
      "Epoch [275], train_loss: 5472.22 with loss1: 5472.22 and loss2: 0.00\n",
      "Epoch [276], train_loss: 5363.76 with loss1: 5363.76 and loss2: 0.00\n",
      "Epoch [277], train_loss: 5456.62 with loss1: 5456.62 and loss2: 0.00\n",
      "Epoch [278], train_loss: 6494.21 with loss1: 6494.21 and loss2: 0.00\n",
      "Epoch [279], train_loss: 6410.35 with loss1: 6410.35 and loss2: 0.00\n",
      "Epoch [280], train_loss: 5464.06 with loss1: 5464.06 and loss2: 0.00\n",
      "Epoch [281], train_loss: 5670.45 with loss1: 5670.45 and loss2: 0.00\n",
      "Epoch [282], train_loss: 5321.19 with loss1: 5321.19 and loss2: 0.00\n",
      "Epoch [283], train_loss: 5447.40 with loss1: 5447.40 and loss2: 0.00\n",
      "Epoch [284], train_loss: 5372.95 with loss1: 5372.95 and loss2: 0.00\n",
      "Epoch [285], train_loss: 5530.76 with loss1: 5530.76 and loss2: 0.00\n",
      "Epoch [286], train_loss: 5299.40 with loss1: 5299.40 and loss2: 0.00\n",
      "Epoch [287], train_loss: 5296.47 with loss1: 5296.47 and loss2: 0.00\n",
      "Epoch [288], train_loss: 5476.62 with loss1: 5476.62 and loss2: 0.00\n",
      "Epoch [289], train_loss: 5687.23 with loss1: 5687.23 and loss2: 0.00\n",
      "Epoch [290], train_loss: 5303.95 with loss1: 5303.95 and loss2: 0.00\n",
      "Epoch [291], train_loss: 5224.26 with loss1: 5224.26 and loss2: 0.00\n",
      "Epoch [292], train_loss: 5427.87 with loss1: 5427.87 and loss2: 0.00\n",
      "Epoch [293], train_loss: 6454.49 with loss1: 6454.49 and loss2: 0.00\n",
      "Epoch [294], train_loss: 5206.90 with loss1: 5206.90 and loss2: 0.00\n",
      "Epoch [295], train_loss: 5198.61 with loss1: 5198.61 and loss2: 0.00\n",
      "Epoch [296], train_loss: 5514.81 with loss1: 5514.81 and loss2: 0.00\n",
      "Epoch [297], train_loss: 5366.64 with loss1: 5366.64 and loss2: 0.00\n",
      "Epoch [298], train_loss: 6476.90 with loss1: 6476.90 and loss2: 0.00\n",
      "Epoch [299], train_loss: 5403.53 with loss1: 5403.53 and loss2: 0.00\n",
      "Epoch [300], train_loss: 5657.52 with loss1: 5657.52 and loss2: 0.00\n",
      "Epoch [301], train_loss: 6344.28 with loss1: 6344.28 and loss2: 0.00\n",
      "Epoch [302], train_loss: 5374.95 with loss1: 5374.95 and loss2: 0.00\n",
      "Epoch [303], train_loss: 5596.73 with loss1: 5596.73 and loss2: 0.00\n",
      "Epoch [304], train_loss: 5301.81 with loss1: 5301.81 and loss2: 0.00\n",
      "Epoch [305], train_loss: 5201.25 with loss1: 5201.25 and loss2: 0.00\n",
      "Epoch [306], train_loss: 5262.80 with loss1: 5262.80 and loss2: 0.00\n",
      "Epoch [307], train_loss: 5241.84 with loss1: 5241.84 and loss2: 0.00\n",
      "Epoch [308], train_loss: 5165.81 with loss1: 5165.81 and loss2: 0.00\n",
      "Epoch [309], train_loss: 6380.20 with loss1: 6380.20 and loss2: 0.00\n",
      "Epoch [310], train_loss: 5219.91 with loss1: 5219.91 and loss2: 0.00\n",
      "Epoch [311], train_loss: 5445.77 with loss1: 5445.77 and loss2: 0.00\n",
      "Epoch [312], train_loss: 5367.71 with loss1: 5367.71 and loss2: 0.00\n",
      "Epoch [313], train_loss: 5246.56 with loss1: 5246.56 and loss2: 0.00\n",
      "Epoch [314], train_loss: 6420.05 with loss1: 6420.05 and loss2: 0.00\n",
      "Epoch [315], train_loss: 6305.14 with loss1: 6305.14 and loss2: 0.00\n",
      "Epoch [316], train_loss: 5165.27 with loss1: 5165.27 and loss2: 0.00\n",
      "Epoch [317], train_loss: 5383.00 with loss1: 5383.00 and loss2: 0.00\n",
      "Epoch [318], train_loss: 5421.69 with loss1: 5421.69 and loss2: 0.00\n",
      "Epoch [319], train_loss: 5350.82 with loss1: 5350.82 and loss2: 0.00\n",
      "Epoch [320], train_loss: 5221.25 with loss1: 5221.25 and loss2: 0.00\n",
      "Epoch [321], train_loss: 5255.10 with loss1: 5255.10 and loss2: 0.00\n",
      "Epoch [322], train_loss: 6407.88 with loss1: 6407.88 and loss2: 0.00\n",
      "Epoch [323], train_loss: 5391.01 with loss1: 5391.01 and loss2: 0.00\n",
      "Epoch [324], train_loss: 6382.33 with loss1: 6382.33 and loss2: 0.00\n",
      "Epoch [325], train_loss: 5173.56 with loss1: 5173.56 and loss2: 0.00\n",
      "Epoch [326], train_loss: 5367.40 with loss1: 5367.40 and loss2: 0.00\n",
      "Epoch [327], train_loss: 5385.79 with loss1: 5385.79 and loss2: 0.00\n",
      "Epoch [328], train_loss: 6443.82 with loss1: 6443.82 and loss2: 0.00\n",
      "Epoch [329], train_loss: 5349.36 with loss1: 5349.36 and loss2: 0.00\n",
      "Epoch [330], train_loss: 6366.14 with loss1: 6366.14 and loss2: 0.00\n",
      "Epoch [331], train_loss: 5390.04 with loss1: 5390.04 and loss2: 0.00\n",
      "Epoch [332], train_loss: 5542.53 with loss1: 5542.53 and loss2: 0.00\n",
      "Epoch [333], train_loss: 6305.63 with loss1: 6305.63 and loss2: 0.00\n",
      "Epoch [334], train_loss: 5331.35 with loss1: 5331.35 and loss2: 0.00\n",
      "Epoch [335], train_loss: 5254.50 with loss1: 5254.50 and loss2: 0.00\n",
      "Epoch [336], train_loss: 5380.71 with loss1: 5380.71 and loss2: 0.00\n",
      "Epoch [337], train_loss: 5253.48 with loss1: 5253.48 and loss2: 0.00\n",
      "Epoch [338], train_loss: 5389.11 with loss1: 5389.11 and loss2: 0.00\n",
      "Epoch [339], train_loss: 5324.50 with loss1: 5324.50 and loss2: 0.00\n",
      "Epoch [340], train_loss: 5292.59 with loss1: 5292.59 and loss2: 0.00\n",
      "Epoch [341], train_loss: 5340.51 with loss1: 5340.51 and loss2: 0.00\n",
      "Epoch [342], train_loss: 5330.62 with loss1: 5330.62 and loss2: 0.00\n",
      "Epoch [343], train_loss: 5284.44 with loss1: 5284.44 and loss2: 0.00\n",
      "Epoch [344], train_loss: 5138.60 with loss1: 5138.60 and loss2: 0.00\n",
      "Epoch [345], train_loss: 5099.83 with loss1: 5099.83 and loss2: 0.00\n",
      "Epoch [346], train_loss: 6353.09 with loss1: 6353.09 and loss2: 0.00\n",
      "Epoch [347], train_loss: 5381.97 with loss1: 5381.97 and loss2: 0.00\n",
      "Epoch [348], train_loss: 5273.86 with loss1: 5273.86 and loss2: 0.00\n",
      "Epoch [349], train_loss: 5163.47 with loss1: 5163.47 and loss2: 0.00\n",
      "Epoch [350], train_loss: 6363.67 with loss1: 6363.67 and loss2: 0.00\n",
      "Epoch [351], train_loss: 5344.20 with loss1: 5344.20 and loss2: 0.00\n",
      "Epoch [352], train_loss: 5285.54 with loss1: 5285.54 and loss2: 0.00\n",
      "Epoch [353], train_loss: 5253.85 with loss1: 5253.85 and loss2: 0.00\n",
      "Epoch [354], train_loss: 5361.32 with loss1: 5361.32 and loss2: 0.00\n",
      "Epoch [355], train_loss: 5218.41 with loss1: 5218.41 and loss2: 0.00\n",
      "Epoch [356], train_loss: 5104.69 with loss1: 5104.69 and loss2: 0.00\n",
      "Epoch [357], train_loss: 5193.80 with loss1: 5193.80 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [358], train_loss: 5055.15 with loss1: 5055.15 and loss2: 0.00\n",
      "Epoch [359], train_loss: 5334.35 with loss1: 5334.35 and loss2: 0.00\n",
      "Epoch [360], train_loss: 5472.51 with loss1: 5472.51 and loss2: 0.00\n",
      "Epoch [361], train_loss: 5281.90 with loss1: 5281.90 and loss2: 0.00\n",
      "Epoch [362], train_loss: 5225.04 with loss1: 5225.04 and loss2: 0.00\n",
      "Epoch [363], train_loss: 5149.39 with loss1: 5149.39 and loss2: 0.00\n",
      "Epoch [364], train_loss: 5468.02 with loss1: 5468.02 and loss2: 0.00\n",
      "Epoch [365], train_loss: 5195.13 with loss1: 5195.13 and loss2: 0.00\n",
      "Epoch [366], train_loss: 5335.48 with loss1: 5335.48 and loss2: 0.00\n",
      "Epoch [367], train_loss: 5250.24 with loss1: 5250.24 and loss2: 0.00\n",
      "Epoch [368], train_loss: 5439.14 with loss1: 5439.14 and loss2: 0.00\n",
      "Epoch [369], train_loss: 5255.84 with loss1: 5255.84 and loss2: 0.00\n",
      "Epoch [370], train_loss: 6424.66 with loss1: 6424.66 and loss2: 0.00\n",
      "Epoch [371], train_loss: 5266.40 with loss1: 5266.40 and loss2: 0.00\n",
      "Epoch [372], train_loss: 5233.40 with loss1: 5233.40 and loss2: 0.00\n",
      "Epoch [373], train_loss: 5267.65 with loss1: 5267.65 and loss2: 0.00\n",
      "Epoch [374], train_loss: 5131.11 with loss1: 5131.11 and loss2: 0.00\n",
      "Epoch [375], train_loss: 5183.00 with loss1: 5183.00 and loss2: 0.00\n",
      "Epoch [376], train_loss: 5164.30 with loss1: 5164.30 and loss2: 0.00\n",
      "Epoch [377], train_loss: 5363.22 with loss1: 5363.22 and loss2: 0.00\n",
      "Epoch [378], train_loss: 5619.38 with loss1: 5619.38 and loss2: 0.00\n",
      "Epoch [379], train_loss: 5181.15 with loss1: 5181.15 and loss2: 0.00\n",
      "Epoch [380], train_loss: 6304.19 with loss1: 6304.19 and loss2: 0.00\n",
      "Epoch [381], train_loss: 5280.85 with loss1: 5280.85 and loss2: 0.00\n",
      "Epoch [382], train_loss: 5094.14 with loss1: 5094.14 and loss2: 0.00\n",
      "Epoch [383], train_loss: 5348.31 with loss1: 5348.31 and loss2: 0.00\n",
      "Epoch [384], train_loss: 5268.21 with loss1: 5268.21 and loss2: 0.00\n",
      "Epoch [385], train_loss: 5301.57 with loss1: 5301.57 and loss2: 0.00\n",
      "Epoch [386], train_loss: 5177.26 with loss1: 5177.26 and loss2: 0.00\n",
      "Epoch [387], train_loss: 5152.07 with loss1: 5152.07 and loss2: 0.00\n",
      "Epoch [388], train_loss: 5037.80 with loss1: 5037.80 and loss2: 0.00\n",
      "Epoch [389], train_loss: 6315.90 with loss1: 6315.90 and loss2: 0.00\n",
      "Epoch [390], train_loss: 5110.65 with loss1: 5110.65 and loss2: 0.00\n",
      "Epoch [391], train_loss: 5090.01 with loss1: 5090.01 and loss2: 0.00\n",
      "Epoch [392], train_loss: 6248.39 with loss1: 6248.39 and loss2: 0.00\n",
      "Epoch [393], train_loss: 5299.77 with loss1: 5299.77 and loss2: 0.00\n",
      "Epoch [394], train_loss: 5227.75 with loss1: 5227.75 and loss2: 0.00\n",
      "Epoch [395], train_loss: 5099.50 with loss1: 5099.50 and loss2: 0.00\n",
      "Epoch [396], train_loss: 5151.88 with loss1: 5151.88 and loss2: 0.00\n",
      "Epoch [397], train_loss: 5117.24 with loss1: 5117.24 and loss2: 0.00\n",
      "Epoch [398], train_loss: 5324.68 with loss1: 5324.68 and loss2: 0.00\n",
      "Epoch [399], train_loss: 5224.17 with loss1: 5224.17 and loss2: 0.00\n",
      "Epoch [400], train_loss: 5173.43 with loss1: 5173.43 and loss2: 0.00\n",
      "Epoch [401], train_loss: 5263.50 with loss1: 5263.50 and loss2: 0.00\n",
      "Epoch [402], train_loss: 6335.98 with loss1: 6335.98 and loss2: 0.00\n",
      "Epoch [403], train_loss: 5252.90 with loss1: 5252.90 and loss2: 0.00\n",
      "Epoch [404], train_loss: 6309.48 with loss1: 6309.48 and loss2: 0.00\n",
      "Epoch [405], train_loss: 5101.56 with loss1: 5101.56 and loss2: 0.00\n",
      "Epoch [406], train_loss: 5083.00 with loss1: 5083.00 and loss2: 0.00\n",
      "Epoch [407], train_loss: 5069.30 with loss1: 5069.30 and loss2: 0.00\n",
      "Epoch [408], train_loss: 5280.46 with loss1: 5280.46 and loss2: 0.00\n",
      "Epoch [409], train_loss: 5224.16 with loss1: 5224.16 and loss2: 0.00\n",
      "Epoch [410], train_loss: 5175.83 with loss1: 5175.83 and loss2: 0.00\n",
      "Epoch [411], train_loss: 5300.55 with loss1: 5300.55 and loss2: 0.00\n",
      "Epoch [412], train_loss: 5147.42 with loss1: 5147.42 and loss2: 0.00\n",
      "Epoch [413], train_loss: 5271.54 with loss1: 5271.54 and loss2: 0.00\n",
      "Epoch [414], train_loss: 5214.89 with loss1: 5214.89 and loss2: 0.00\n",
      "Epoch [415], train_loss: 5196.94 with loss1: 5196.94 and loss2: 0.00\n",
      "Epoch [416], train_loss: 5068.63 with loss1: 5068.63 and loss2: 0.00\n",
      "Epoch [417], train_loss: 6335.74 with loss1: 6335.74 and loss2: 0.00\n",
      "Epoch [418], train_loss: 5238.09 with loss1: 5238.09 and loss2: 0.00\n",
      "Epoch [419], train_loss: 5201.65 with loss1: 5201.65 and loss2: 0.00\n",
      "Epoch [420], train_loss: 5374.56 with loss1: 5374.56 and loss2: 0.00\n",
      "Epoch [421], train_loss: 6215.71 with loss1: 6215.71 and loss2: 0.00\n",
      "Epoch [422], train_loss: 6205.18 with loss1: 6205.18 and loss2: 0.00\n",
      "Epoch [423], train_loss: 5228.65 with loss1: 5228.65 and loss2: 0.00\n",
      "Epoch [424], train_loss: 5035.05 with loss1: 5035.05 and loss2: 0.00\n",
      "Epoch [425], train_loss: 5261.49 with loss1: 5261.49 and loss2: 0.00\n",
      "Epoch [426], train_loss: 5182.33 with loss1: 5182.33 and loss2: 0.00\n",
      "Epoch [427], train_loss: 6321.56 with loss1: 6321.56 and loss2: 0.00\n",
      "Epoch [428], train_loss: 5099.84 with loss1: 5099.84 and loss2: 0.00\n",
      "Epoch [429], train_loss: 5229.87 with loss1: 5229.87 and loss2: 0.00\n",
      "Epoch [430], train_loss: 5140.22 with loss1: 5140.22 and loss2: 0.00\n",
      "Epoch [431], train_loss: 5196.80 with loss1: 5196.80 and loss2: 0.00\n",
      "Epoch [432], train_loss: 5020.10 with loss1: 5020.10 and loss2: 0.00\n",
      "Epoch [433], train_loss: 5269.14 with loss1: 5269.14 and loss2: 0.00\n",
      "Epoch [434], train_loss: 5155.63 with loss1: 5155.63 and loss2: 0.00\n",
      "Epoch [435], train_loss: 5379.21 with loss1: 5379.21 and loss2: 0.00\n",
      "Epoch [436], train_loss: 5444.65 with loss1: 5444.65 and loss2: 0.00\n",
      "Epoch [437], train_loss: 6223.76 with loss1: 6223.76 and loss2: 0.00\n",
      "Epoch [438], train_loss: 5002.60 with loss1: 5002.60 and loss2: 0.00\n",
      "Epoch [439], train_loss: 4952.27 with loss1: 4952.27 and loss2: 0.00\n",
      "Epoch [440], train_loss: 5278.12 with loss1: 5278.12 and loss2: 0.00\n",
      "Epoch [441], train_loss: 4977.16 with loss1: 4977.16 and loss2: 0.00\n",
      "Epoch [442], train_loss: 5266.01 with loss1: 5266.01 and loss2: 0.00\n",
      "Epoch [443], train_loss: 5140.76 with loss1: 5140.76 and loss2: 0.00\n",
      "Epoch [444], train_loss: 6262.30 with loss1: 6262.30 and loss2: 0.00\n",
      "Epoch [445], train_loss: 5219.61 with loss1: 5219.61 and loss2: 0.00\n",
      "Epoch [446], train_loss: 5130.63 with loss1: 5130.63 and loss2: 0.00\n",
      "Epoch [447], train_loss: 6246.83 with loss1: 6246.83 and loss2: 0.00\n",
      "Epoch [448], train_loss: 4952.74 with loss1: 4952.74 and loss2: 0.00\n",
      "Epoch [449], train_loss: 4926.43 with loss1: 4926.43 and loss2: 0.00\n",
      "Epoch [450], train_loss: 5206.29 with loss1: 5206.29 and loss2: 0.00\n",
      "Epoch [451], train_loss: 6257.34 with loss1: 6257.34 and loss2: 0.00\n",
      "Epoch [452], train_loss: 4928.05 with loss1: 4928.05 and loss2: 0.00\n",
      "Epoch [453], train_loss: 5080.19 with loss1: 5080.19 and loss2: 0.00\n",
      "Epoch [454], train_loss: 5261.23 with loss1: 5261.23 and loss2: 0.00\n",
      "Epoch [455], train_loss: 6277.73 with loss1: 6277.73 and loss2: 0.00\n",
      "Epoch [456], train_loss: 6093.03 with loss1: 6093.03 and loss2: 0.00\n",
      "Epoch [457], train_loss: 5019.46 with loss1: 5019.46 and loss2: 0.00\n",
      "Epoch [458], train_loss: 6125.77 with loss1: 6125.77 and loss2: 0.00\n",
      "Epoch [459], train_loss: 4941.39 with loss1: 4941.39 and loss2: 0.00\n",
      "Epoch [460], train_loss: 5046.58 with loss1: 5046.58 and loss2: 0.00\n",
      "Epoch [461], train_loss: 4913.22 with loss1: 4913.22 and loss2: 0.00\n",
      "Epoch [462], train_loss: 5267.23 with loss1: 5267.23 and loss2: 0.00\n",
      "Epoch [463], train_loss: 5174.96 with loss1: 5174.96 and loss2: 0.00\n",
      "Epoch [464], train_loss: 5142.77 with loss1: 5142.77 and loss2: 0.00\n",
      "Epoch [465], train_loss: 5119.36 with loss1: 5119.36 and loss2: 0.00\n",
      "Epoch [466], train_loss: 5108.22 with loss1: 5108.22 and loss2: 0.00\n",
      "Epoch [467], train_loss: 6306.12 with loss1: 6306.12 and loss2: 0.00\n",
      "Epoch [468], train_loss: 5203.24 with loss1: 5203.24 and loss2: 0.00\n",
      "Epoch [469], train_loss: 5179.55 with loss1: 5179.55 and loss2: 0.00\n",
      "Epoch [470], train_loss: 6241.11 with loss1: 6241.11 and loss2: 0.00\n",
      "Epoch [471], train_loss: 5133.69 with loss1: 5133.69 and loss2: 0.00\n",
      "Epoch [472], train_loss: 5117.44 with loss1: 5117.44 and loss2: 0.00\n",
      "Epoch [473], train_loss: 5130.86 with loss1: 5130.86 and loss2: 0.00\n",
      "Epoch [474], train_loss: 4950.18 with loss1: 4950.18 and loss2: 0.00\n",
      "Epoch [475], train_loss: 4892.58 with loss1: 4892.58 and loss2: 0.00\n",
      "Epoch [476], train_loss: 5057.09 with loss1: 5057.09 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [477], train_loss: 4853.61 with loss1: 4853.61 and loss2: 0.00\n",
      "Epoch [478], train_loss: 6230.73 with loss1: 6230.73 and loss2: 0.00\n",
      "Epoch [479], train_loss: 5167.80 with loss1: 5167.80 and loss2: 0.00\n",
      "Epoch [480], train_loss: 5276.36 with loss1: 5276.36 and loss2: 0.00\n",
      "Epoch [481], train_loss: 5362.32 with loss1: 5362.32 and loss2: 0.00\n",
      "Epoch [482], train_loss: 4986.26 with loss1: 4986.26 and loss2: 0.00\n",
      "Epoch [483], train_loss: 5107.26 with loss1: 5107.26 and loss2: 0.00\n",
      "Epoch [484], train_loss: 6219.76 with loss1: 6219.76 and loss2: 0.00\n",
      "Epoch [485], train_loss: 6052.40 with loss1: 6052.40 and loss2: 0.00\n",
      "Epoch [486], train_loss: 4897.01 with loss1: 4897.01 and loss2: 0.00\n",
      "Epoch [487], train_loss: 5025.70 with loss1: 5025.70 and loss2: 0.00\n",
      "Epoch [488], train_loss: 4993.82 with loss1: 4993.82 and loss2: 0.00\n",
      "Epoch [489], train_loss: 6127.57 with loss1: 6127.57 and loss2: 0.00\n",
      "Epoch [490], train_loss: 4882.76 with loss1: 4882.76 and loss2: 0.00\n",
      "Epoch [491], train_loss: 5003.16 with loss1: 5003.16 and loss2: 0.00\n",
      "Epoch [492], train_loss: 6117.80 with loss1: 6117.80 and loss2: 0.00\n",
      "Epoch [493], train_loss: 5193.96 with loss1: 5193.96 and loss2: 0.00\n",
      "Epoch [494], train_loss: 6061.29 with loss1: 6061.29 and loss2: 0.00\n",
      "Epoch [495], train_loss: 4873.89 with loss1: 4873.89 and loss2: 0.00\n",
      "Epoch [496], train_loss: 5010.99 with loss1: 5010.99 and loss2: 0.00\n",
      "Epoch [497], train_loss: 5175.71 with loss1: 5175.71 and loss2: 0.00\n",
      "Epoch [498], train_loss: 6049.66 with loss1: 6049.66 and loss2: 0.00\n",
      "Epoch [499], train_loss: 5109.24 with loss1: 5109.24 and loss2: 0.00\n",
      "Epoch [500], train_loss: 5066.31 with loss1: 5066.31 and loss2: 0.00\n",
      "Epoch [501], train_loss: 5009.89 with loss1: 5009.89 and loss2: 0.00\n",
      "Epoch [502], train_loss: 5115.93 with loss1: 5115.93 and loss2: 0.00\n",
      "Epoch [503], train_loss: 5046.44 with loss1: 5046.44 and loss2: 0.00\n",
      "Epoch [504], train_loss: 4848.25 with loss1: 4848.25 and loss2: 0.00\n",
      "Epoch [505], train_loss: 5262.58 with loss1: 5262.58 and loss2: 0.00\n",
      "Epoch [506], train_loss: 5111.06 with loss1: 5111.06 and loss2: 0.00\n",
      "Epoch [507], train_loss: 5011.85 with loss1: 5011.85 and loss2: 0.00\n",
      "Epoch [508], train_loss: 6174.84 with loss1: 6174.84 and loss2: 0.00\n",
      "Epoch [509], train_loss: 6009.34 with loss1: 6009.34 and loss2: 0.00\n",
      "Epoch [510], train_loss: 5964.05 with loss1: 5964.05 and loss2: 0.00\n",
      "Epoch [511], train_loss: 4860.55 with loss1: 4860.55 and loss2: 0.00\n",
      "Epoch [512], train_loss: 4830.01 with loss1: 4830.01 and loss2: 0.00\n",
      "Epoch [513], train_loss: 4806.60 with loss1: 4806.60 and loss2: 0.00\n",
      "Epoch [514], train_loss: 4794.54 with loss1: 4794.54 and loss2: 0.00\n",
      "Epoch [515], train_loss: 6115.61 with loss1: 6115.61 and loss2: 0.00\n",
      "Epoch [516], train_loss: 5112.81 with loss1: 5112.81 and loss2: 0.00\n",
      "Epoch [517], train_loss: 6123.05 with loss1: 6123.05 and loss2: 0.00\n",
      "Epoch [518], train_loss: 5156.51 with loss1: 5156.51 and loss2: 0.00\n",
      "Epoch [519], train_loss: 5040.47 with loss1: 5040.47 and loss2: 0.00\n",
      "Epoch [520], train_loss: 5137.76 with loss1: 5137.76 and loss2: 0.00\n",
      "Epoch [521], train_loss: 4919.57 with loss1: 4919.57 and loss2: 0.00\n",
      "Epoch [522], train_loss: 5041.84 with loss1: 5041.84 and loss2: 0.00\n",
      "Epoch [523], train_loss: 5135.40 with loss1: 5135.40 and loss2: 0.00\n",
      "Epoch [524], train_loss: 5039.30 with loss1: 5039.30 and loss2: 0.00\n",
      "Epoch [525], train_loss: 6088.20 with loss1: 6088.20 and loss2: 0.00\n",
      "Epoch [526], train_loss: 4812.43 with loss1: 4812.43 and loss2: 0.00\n",
      "Epoch [527], train_loss: 5155.79 with loss1: 5155.79 and loss2: 0.00\n",
      "Epoch [528], train_loss: 4987.78 with loss1: 4987.78 and loss2: 0.00\n",
      "Epoch [529], train_loss: 5092.89 with loss1: 5092.89 and loss2: 0.00\n",
      "Epoch [530], train_loss: 5128.71 with loss1: 5128.71 and loss2: 0.00\n",
      "Epoch [531], train_loss: 5028.18 with loss1: 5028.18 and loss2: 0.00\n",
      "Epoch [532], train_loss: 6041.45 with loss1: 6041.45 and loss2: 0.00\n",
      "Epoch [533], train_loss: 5090.55 with loss1: 5090.55 and loss2: 0.00\n",
      "Epoch [534], train_loss: 5983.20 with loss1: 5983.20 and loss2: 0.00\n",
      "Epoch [535], train_loss: 4928.71 with loss1: 4928.71 and loss2: 0.00\n",
      "Epoch [536], train_loss: 5163.04 with loss1: 5163.04 and loss2: 0.00\n",
      "Epoch [537], train_loss: 5002.95 with loss1: 5002.95 and loss2: 0.00\n",
      "Epoch [538], train_loss: 6082.69 with loss1: 6082.69 and loss2: 0.00\n",
      "Epoch [539], train_loss: 4924.26 with loss1: 4924.26 and loss2: 0.00\n",
      "Epoch [540], train_loss: 5052.62 with loss1: 5052.62 and loss2: 0.00\n",
      "Epoch [541], train_loss: 5080.12 with loss1: 5080.12 and loss2: 0.00\n",
      "Epoch [542], train_loss: 4854.19 with loss1: 4854.19 and loss2: 0.00\n",
      "Epoch [543], train_loss: 4954.32 with loss1: 4954.32 and loss2: 0.00\n",
      "Epoch [544], train_loss: 5035.08 with loss1: 5035.08 and loss2: 0.00\n",
      "Epoch [545], train_loss: 4983.92 with loss1: 4983.92 and loss2: 0.00\n",
      "Epoch [546], train_loss: 4739.87 with loss1: 4739.87 and loss2: 0.00\n",
      "Epoch [547], train_loss: 5176.98 with loss1: 5176.98 and loss2: 0.00\n",
      "Epoch [548], train_loss: 6090.82 with loss1: 6090.82 and loss2: 0.00\n",
      "Epoch [549], train_loss: 5048.08 with loss1: 5048.08 and loss2: 0.00\n",
      "Epoch [550], train_loss: 5176.96 with loss1: 5176.96 and loss2: 0.00\n",
      "Epoch [551], train_loss: 4975.99 with loss1: 4975.99 and loss2: 0.00\n",
      "Epoch [552], train_loss: 6037.52 with loss1: 6037.52 and loss2: 0.00\n",
      "Epoch [553], train_loss: 5003.89 with loss1: 5003.89 and loss2: 0.00\n",
      "Epoch [554], train_loss: 4970.33 with loss1: 4970.33 and loss2: 0.00\n",
      "Epoch [555], train_loss: 4985.85 with loss1: 4985.85 and loss2: 0.00\n",
      "Epoch [556], train_loss: 4713.66 with loss1: 4713.66 and loss2: 0.00\n",
      "Epoch [557], train_loss: 4975.62 with loss1: 4975.62 and loss2: 0.00\n",
      "Epoch [558], train_loss: 5987.02 with loss1: 5987.02 and loss2: 0.00\n",
      "Epoch [559], train_loss: 5867.65 with loss1: 5867.65 and loss2: 0.00\n",
      "Epoch [560], train_loss: 5144.85 with loss1: 5144.85 and loss2: 0.00\n",
      "Epoch [561], train_loss: 4924.21 with loss1: 4924.21 and loss2: 0.00\n",
      "Epoch [562], train_loss: 5123.97 with loss1: 5123.97 and loss2: 0.00\n",
      "Epoch [563], train_loss: 4945.24 with loss1: 4945.24 and loss2: 0.00\n",
      "Epoch [564], train_loss: 4776.42 with loss1: 4776.42 and loss2: 0.00\n",
      "Epoch [565], train_loss: 5946.85 with loss1: 5946.85 and loss2: 0.00\n",
      "Epoch [566], train_loss: 4852.54 with loss1: 4852.54 and loss2: 0.00\n",
      "Epoch [567], train_loss: 5131.61 with loss1: 5131.61 and loss2: 0.00\n",
      "Epoch [568], train_loss: 4899.38 with loss1: 4899.38 and loss2: 0.00\n",
      "Epoch [569], train_loss: 5111.89 with loss1: 5111.89 and loss2: 0.00\n",
      "Epoch [570], train_loss: 5959.28 with loss1: 5959.28 and loss2: 0.00\n",
      "Epoch [571], train_loss: 5828.25 with loss1: 5828.25 and loss2: 0.00\n",
      "Epoch [572], train_loss: 4821.50 with loss1: 4821.50 and loss2: 0.00\n",
      "Epoch [573], train_loss: 5117.51 with loss1: 5117.51 and loss2: 0.00\n",
      "Epoch [574], train_loss: 4871.82 with loss1: 4871.82 and loss2: 0.00\n",
      "Epoch [575], train_loss: 5095.52 with loss1: 5095.52 and loss2: 0.00\n",
      "Epoch [576], train_loss: 4895.96 with loss1: 4895.96 and loss2: 0.00\n",
      "Epoch [577], train_loss: 4604.43 with loss1: 4604.43 and loss2: 0.00\n",
      "Epoch [578], train_loss: 4551.95 with loss1: 4551.95 and loss2: 0.00\n",
      "Epoch [579], train_loss: 5130.57 with loss1: 5130.57 and loss2: 0.00\n",
      "Epoch [580], train_loss: 4592.58 with loss1: 4592.58 and loss2: 0.00\n",
      "Epoch [581], train_loss: 5107.67 with loss1: 5107.67 and loss2: 0.00\n",
      "Epoch [582], train_loss: 5029.69 with loss1: 5029.69 and loss2: 0.00\n",
      "Epoch [583], train_loss: 4606.87 with loss1: 4606.87 and loss2: 0.00\n",
      "Epoch [584], train_loss: 5095.56 with loss1: 5095.56 and loss2: 0.00\n",
      "Epoch [585], train_loss: 4852.57 with loss1: 4852.57 and loss2: 0.00\n",
      "Epoch [586], train_loss: 4876.87 with loss1: 4876.87 and loss2: 0.00\n",
      "Epoch [587], train_loss: 4569.65 with loss1: 4569.65 and loss2: 0.00\n",
      "Epoch [588], train_loss: 4941.71 with loss1: 4941.71 and loss2: 0.00\n",
      "Epoch [589], train_loss: 4971.15 with loss1: 4971.15 and loss2: 0.00\n",
      "Epoch [590], train_loss: 5805.64 with loss1: 5805.64 and loss2: 0.00\n",
      "Epoch [591], train_loss: 5802.05 with loss1: 5802.05 and loss2: 0.00\n",
      "Epoch [592], train_loss: 5752.34 with loss1: 5752.34 and loss2: 0.00\n",
      "Epoch [593], train_loss: 4758.99 with loss1: 4758.99 and loss2: 0.00\n",
      "Epoch [594], train_loss: 5093.29 with loss1: 5093.29 and loss2: 0.00\n",
      "Epoch [595], train_loss: 4781.24 with loss1: 4781.24 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [596], train_loss: 4843.66 with loss1: 4843.66 and loss2: 0.00\n",
      "Epoch [597], train_loss: 4849.54 with loss1: 4849.54 and loss2: 0.00\n",
      "Epoch [598], train_loss: 4721.96 with loss1: 4721.96 and loss2: 0.00\n",
      "Epoch [599], train_loss: 5081.71 with loss1: 5081.71 and loss2: 0.00\n",
      "Epoch [600], train_loss: 4978.00 with loss1: 4978.00 and loss2: 0.00\n",
      "Epoch [601], train_loss: 4960.11 with loss1: 4960.11 and loss2: 0.00\n",
      "Epoch [602], train_loss: 4748.07 with loss1: 4748.07 and loss2: 0.00\n",
      "Epoch [603], train_loss: 4694.94 with loss1: 4694.94 and loss2: 0.00\n",
      "Epoch [604], train_loss: 5741.23 with loss1: 5741.23 and loss2: 0.00\n",
      "Epoch [605], train_loss: 5033.15 with loss1: 5033.15 and loss2: 0.00\n",
      "Epoch [606], train_loss: 4958.68 with loss1: 4958.68 and loss2: 0.00\n",
      "Epoch [607], train_loss: 4733.02 with loss1: 4733.02 and loss2: 0.00\n",
      "Epoch [608], train_loss: 4647.46 with loss1: 4647.46 and loss2: 0.00\n",
      "Epoch [609], train_loss: 4698.31 with loss1: 4698.31 and loss2: 0.00\n",
      "Epoch [610], train_loss: 4716.01 with loss1: 4716.01 and loss2: 0.00\n",
      "Epoch [611], train_loss: 5516.65 with loss1: 5516.65 and loss2: 0.00\n",
      "Epoch [612], train_loss: 4663.55 with loss1: 4663.55 and loss2: 0.00\n",
      "Epoch [613], train_loss: 5505.60 with loss1: 5505.60 and loss2: 0.00\n",
      "Epoch [614], train_loss: 5690.88 with loss1: 5690.88 and loss2: 0.00\n",
      "Epoch [615], train_loss: 4682.31 with loss1: 4682.31 and loss2: 0.00\n",
      "Epoch [616], train_loss: 4586.66 with loss1: 4586.66 and loss2: 0.00\n",
      "Epoch [617], train_loss: 4578.67 with loss1: 4578.67 and loss2: 0.00\n",
      "Epoch [618], train_loss: 4188.30 with loss1: 4188.30 and loss2: 0.00\n",
      "Epoch [619], train_loss: 5032.71 with loss1: 5032.71 and loss2: 0.00\n",
      "Epoch [620], train_loss: 5500.61 with loss1: 5500.61 and loss2: 0.00\n",
      "Epoch [621], train_loss: 4625.90 with loss1: 4625.90 and loss2: 0.00\n",
      "Epoch [622], train_loss: 5349.73 with loss1: 5349.73 and loss2: 0.00\n",
      "Epoch [623], train_loss: 4604.07 with loss1: 4604.07 and loss2: 0.00\n",
      "Epoch [624], train_loss: 4143.94 with loss1: 4143.94 and loss2: 0.00\n",
      "Epoch [625], train_loss: 4482.47 with loss1: 4482.47 and loss2: 0.00\n",
      "Epoch [626], train_loss: 4761.54 with loss1: 4761.54 and loss2: 0.00\n",
      "Epoch [627], train_loss: 4402.24 with loss1: 4402.24 and loss2: 0.00\n",
      "Epoch [628], train_loss: 4069.74 with loss1: 4069.74 and loss2: 0.00\n",
      "Epoch [629], train_loss: 4593.23 with loss1: 4593.23 and loss2: 0.00\n",
      "Epoch [630], train_loss: 4346.51 with loss1: 4346.51 and loss2: 0.00\n",
      "Epoch [631], train_loss: 4521.39 with loss1: 4521.39 and loss2: 0.00\n",
      "Epoch [632], train_loss: 4647.53 with loss1: 4647.53 and loss2: 0.00\n",
      "Epoch [633], train_loss: 5099.48 with loss1: 5099.48 and loss2: 0.00\n",
      "Epoch [634], train_loss: 4474.70 with loss1: 4474.70 and loss2: 0.00\n",
      "Epoch [635], train_loss: 4954.73 with loss1: 4954.73 and loss2: 0.00\n",
      "Epoch [636], train_loss: 4456.40 with loss1: 4456.40 and loss2: 0.00\n",
      "Epoch [637], train_loss: 4513.62 with loss1: 4513.62 and loss2: 0.00\n",
      "Epoch [638], train_loss: 3946.33 with loss1: 3946.33 and loss2: 0.00\n",
      "Epoch [639], train_loss: 4333.25 with loss1: 4333.25 and loss2: 0.00\n",
      "Epoch [640], train_loss: 4929.52 with loss1: 4929.52 and loss2: 0.00\n",
      "Epoch [641], train_loss: 5047.78 with loss1: 5047.78 and loss2: 0.00\n",
      "Epoch [642], train_loss: 4905.31 with loss1: 4905.31 and loss2: 0.00\n",
      "Epoch [643], train_loss: 5053.39 with loss1: 5053.39 and loss2: 0.00\n",
      "Epoch [644], train_loss: 4876.78 with loss1: 4876.78 and loss2: 0.00\n",
      "Epoch [645], train_loss: 4833.67 with loss1: 4833.67 and loss2: 0.00\n",
      "Epoch [646], train_loss: 4140.52 with loss1: 4140.52 and loss2: 0.00\n",
      "Epoch [647], train_loss: 4311.75 with loss1: 4311.75 and loss2: 0.00\n",
      "Epoch [648], train_loss: 3764.98 with loss1: 3764.98 and loss2: 0.00\n",
      "Epoch [649], train_loss: 4366.70 with loss1: 4366.70 and loss2: 0.00\n",
      "Epoch [650], train_loss: 4738.67 with loss1: 4738.67 and loss2: 0.00\n",
      "Epoch [651], train_loss: 5207.38 with loss1: 5207.38 and loss2: 0.00\n",
      "Epoch [652], train_loss: 3989.01 with loss1: 3989.01 and loss2: 0.00\n",
      "Epoch [653], train_loss: 4876.57 with loss1: 4876.57 and loss2: 0.00\n",
      "Epoch [654], train_loss: 4757.05 with loss1: 4757.05 and loss2: 0.00\n",
      "Epoch [655], train_loss: 5116.66 with loss1: 5116.66 and loss2: 0.00\n",
      "Epoch [656], train_loss: 3725.12 with loss1: 3725.12 and loss2: 0.00\n",
      "Epoch [657], train_loss: 4004.85 with loss1: 4004.85 and loss2: 0.00\n",
      "Epoch [658], train_loss: 3970.04 with loss1: 3970.04 and loss2: 0.00\n",
      "Epoch [659], train_loss: 4056.39 with loss1: 4056.39 and loss2: 0.00\n",
      "Epoch [660], train_loss: 4981.84 with loss1: 4981.84 and loss2: 0.00\n",
      "Epoch [661], train_loss: 5091.83 with loss1: 5091.83 and loss2: 0.00\n",
      "Epoch [662], train_loss: 4934.38 with loss1: 4934.38 and loss2: 0.00\n",
      "Epoch [663], train_loss: 4249.26 with loss1: 4249.26 and loss2: 0.00\n",
      "Epoch [664], train_loss: 4830.55 with loss1: 4830.55 and loss2: 0.00\n",
      "Epoch [665], train_loss: 4220.03 with loss1: 4220.03 and loss2: 0.00\n",
      "Epoch [666], train_loss: 4528.45 with loss1: 4528.45 and loss2: 0.00\n",
      "Epoch [667], train_loss: 4157.81 with loss1: 4157.81 and loss2: 0.00\n",
      "Epoch [668], train_loss: 4187.11 with loss1: 4187.11 and loss2: 0.00\n",
      "Epoch [669], train_loss: 4470.44 with loss1: 4470.44 and loss2: 0.00\n",
      "Epoch [670], train_loss: 3955.49 with loss1: 3955.49 and loss2: 0.00\n",
      "Epoch [671], train_loss: 4735.92 with loss1: 4735.92 and loss2: 0.00\n",
      "Epoch [672], train_loss: 4754.19 with loss1: 4754.19 and loss2: 0.00\n",
      "Epoch [673], train_loss: 3830.56 with loss1: 3830.56 and loss2: 0.00\n",
      "Epoch [674], train_loss: 3809.78 with loss1: 3809.78 and loss2: 0.00\n",
      "Epoch [675], train_loss: 3401.15 with loss1: 3401.15 and loss2: 0.00\n",
      "Epoch [676], train_loss: 3824.49 with loss1: 3824.49 and loss2: 0.00\n",
      "Epoch [677], train_loss: 3400.13 with loss1: 3400.13 and loss2: 0.00\n",
      "Epoch [678], train_loss: 4081.39 with loss1: 4081.39 and loss2: 0.00\n",
      "Epoch [679], train_loss: 3766.08 with loss1: 3766.08 and loss2: 0.00\n",
      "Epoch [680], train_loss: 3343.33 with loss1: 3343.33 and loss2: 0.00\n",
      "Epoch [681], train_loss: 3774.52 with loss1: 3774.52 and loss2: 0.00\n",
      "Epoch [682], train_loss: 4047.34 with loss1: 4047.34 and loss2: 0.00\n",
      "Epoch [683], train_loss: 4661.67 with loss1: 4661.67 and loss2: 0.00\n",
      "Epoch [684], train_loss: 4436.42 with loss1: 4436.42 and loss2: 0.00\n",
      "Epoch [685], train_loss: 3717.07 with loss1: 3717.07 and loss2: 0.00\n",
      "Epoch [686], train_loss: 4146.03 with loss1: 4146.03 and loss2: 0.00\n",
      "Epoch [687], train_loss: 4080.39 with loss1: 4080.39 and loss2: 0.00\n",
      "Epoch [688], train_loss: 3923.56 with loss1: 3923.56 and loss2: 0.00\n",
      "Epoch [689], train_loss: 4102.06 with loss1: 4102.06 and loss2: 0.00\n",
      "Epoch [690], train_loss: 4649.94 with loss1: 4649.94 and loss2: 0.00\n",
      "Epoch [691], train_loss: 3293.17 with loss1: 3293.17 and loss2: 0.00\n",
      "Epoch [692], train_loss: 4286.78 with loss1: 4286.78 and loss2: 0.00\n",
      "Epoch [693], train_loss: 4691.61 with loss1: 4691.61 and loss2: 0.00\n",
      "Epoch [694], train_loss: 4677.01 with loss1: 4677.01 and loss2: 0.00\n",
      "Epoch [695], train_loss: 3252.34 with loss1: 3252.34 and loss2: 0.00\n",
      "Epoch [696], train_loss: 4376.38 with loss1: 4376.38 and loss2: 0.00\n",
      "Epoch [697], train_loss: 3848.59 with loss1: 3848.59 and loss2: 0.00\n",
      "Epoch [698], train_loss: 4682.60 with loss1: 4682.60 and loss2: 0.00\n",
      "Epoch [699], train_loss: 4550.02 with loss1: 4550.02 and loss2: 0.00\n",
      "Epoch [700], train_loss: 3352.29 with loss1: 3352.29 and loss2: 0.00\n",
      "Epoch [701], train_loss: 4416.39 with loss1: 4416.39 and loss2: 0.00\n",
      "Epoch [702], train_loss: 4385.60 with loss1: 4385.60 and loss2: 0.00\n",
      "Epoch [703], train_loss: 3989.88 with loss1: 3989.88 and loss2: 0.00\n",
      "Epoch [704], train_loss: 3156.57 with loss1: 3156.57 and loss2: 0.00\n",
      "Epoch [705], train_loss: 3177.03 with loss1: 3177.03 and loss2: 0.00\n",
      "Epoch [706], train_loss: 4011.11 with loss1: 4011.11 and loss2: 0.00\n",
      "Epoch [707], train_loss: 4035.82 with loss1: 4035.82 and loss2: 0.00\n",
      "Epoch [708], train_loss: 3272.64 with loss1: 3272.64 and loss2: 0.00\n",
      "Epoch [709], train_loss: 3612.89 with loss1: 3612.89 and loss2: 0.00\n",
      "Epoch [710], train_loss: 3127.92 with loss1: 3127.92 and loss2: 0.00\n",
      "Epoch [711], train_loss: 4223.88 with loss1: 4223.88 and loss2: 0.00\n",
      "Epoch [712], train_loss: 3611.93 with loss1: 3611.93 and loss2: 0.00\n",
      "Epoch [713], train_loss: 3180.36 with loss1: 3180.36 and loss2: 0.00\n",
      "Epoch [714], train_loss: 3580.86 with loss1: 3580.86 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [715], train_loss: 4551.65 with loss1: 4551.65 and loss2: 0.00\n",
      "Epoch [716], train_loss: 3932.81 with loss1: 3932.81 and loss2: 0.00\n",
      "Epoch [717], train_loss: 3558.44 with loss1: 3558.44 and loss2: 0.00\n",
      "Epoch [718], train_loss: 3611.64 with loss1: 3611.64 and loss2: 0.00\n",
      "Epoch [719], train_loss: 3903.30 with loss1: 3903.30 and loss2: 0.00\n",
      "Epoch [720], train_loss: 4213.75 with loss1: 4213.75 and loss2: 0.00\n",
      "Epoch [721], train_loss: 3103.26 with loss1: 3103.26 and loss2: 0.00\n",
      "Epoch [722], train_loss: 3904.70 with loss1: 3904.70 and loss2: 0.00\n",
      "Epoch [723], train_loss: 3680.35 with loss1: 3680.35 and loss2: 0.00\n",
      "Epoch [724], train_loss: 4229.09 with loss1: 4229.09 and loss2: 0.00\n",
      "Epoch [725], train_loss: 4388.76 with loss1: 4388.76 and loss2: 0.00\n",
      "Epoch [726], train_loss: 4786.82 with loss1: 4786.82 and loss2: 0.00\n",
      "Epoch [727], train_loss: 7092.87 with loss1: 7092.87 and loss2: 0.00\n",
      "Epoch [728], train_loss: 4319.81 with loss1: 4319.81 and loss2: 0.00\n",
      "Epoch [729], train_loss: 4102.03 with loss1: 4102.03 and loss2: 0.00\n",
      "Epoch [730], train_loss: 3636.07 with loss1: 3636.07 and loss2: 0.00\n",
      "Epoch [731], train_loss: 4577.62 with loss1: 4577.62 and loss2: 0.00\n",
      "Epoch [732], train_loss: 3671.35 with loss1: 3671.35 and loss2: 0.00\n",
      "Epoch [733], train_loss: 3639.49 with loss1: 3639.49 and loss2: 0.00\n",
      "Epoch [734], train_loss: 3728.79 with loss1: 3728.79 and loss2: 0.00\n",
      "Epoch [735], train_loss: 4497.02 with loss1: 4497.02 and loss2: 0.00\n",
      "Epoch [736], train_loss: 3030.57 with loss1: 3030.57 and loss2: 0.00\n",
      "Epoch [737], train_loss: 3631.18 with loss1: 3631.18 and loss2: 0.00\n",
      "Epoch [738], train_loss: 3913.90 with loss1: 3913.90 and loss2: 0.00\n",
      "Epoch [739], train_loss: 3624.56 with loss1: 3624.56 and loss2: 0.00\n",
      "Epoch [740], train_loss: 4478.13 with loss1: 4478.13 and loss2: 0.00\n",
      "Epoch [741], train_loss: 3543.99 with loss1: 3543.99 and loss2: 0.00\n",
      "Epoch [742], train_loss: 3108.52 with loss1: 3108.52 and loss2: 0.00\n",
      "Epoch [743], train_loss: 3868.49 with loss1: 3868.49 and loss2: 0.00\n",
      "Epoch [744], train_loss: 3573.83 with loss1: 3573.83 and loss2: 0.00\n",
      "Epoch [745], train_loss: 3529.56 with loss1: 3529.56 and loss2: 0.00\n",
      "Epoch [746], train_loss: 3482.25 with loss1: 3482.25 and loss2: 0.00\n",
      "Epoch [747], train_loss: 2980.64 with loss1: 2980.64 and loss2: 0.00\n",
      "Epoch [748], train_loss: 4465.90 with loss1: 4465.90 and loss2: 0.00\n",
      "Epoch [749], train_loss: 4507.21 with loss1: 4507.21 and loss2: 0.00\n",
      "Epoch [750], train_loss: 4493.51 with loss1: 4493.51 and loss2: 0.00\n",
      "Epoch [751], train_loss: 3496.47 with loss1: 3496.47 and loss2: 0.00\n",
      "Epoch [752], train_loss: 2963.83 with loss1: 2963.83 and loss2: 0.00\n",
      "Epoch [753], train_loss: 3463.44 with loss1: 3463.44 and loss2: 0.00\n",
      "Epoch [754], train_loss: 3529.33 with loss1: 3529.33 and loss2: 0.00\n",
      "Epoch [755], train_loss: 4437.89 with loss1: 4437.89 and loss2: 0.00\n",
      "Epoch [756], train_loss: 4467.95 with loss1: 4467.95 and loss2: 0.00\n",
      "Epoch [757], train_loss: 4492.08 with loss1: 4492.08 and loss2: 0.00\n",
      "Epoch [758], train_loss: 3484.26 with loss1: 3484.26 and loss2: 0.00\n",
      "Epoch [759], train_loss: 4451.45 with loss1: 4451.45 and loss2: 0.00\n",
      "Epoch [760], train_loss: 4170.10 with loss1: 4170.10 and loss2: 0.00\n",
      "Epoch [761], train_loss: 2933.81 with loss1: 2933.81 and loss2: 0.00\n",
      "Epoch [762], train_loss: 2917.93 with loss1: 2917.93 and loss2: 0.00\n",
      "Epoch [763], train_loss: 3990.35 with loss1: 3990.35 and loss2: 0.00\n",
      "Epoch [764], train_loss: 4125.23 with loss1: 4125.23 and loss2: 0.00\n",
      "Epoch [765], train_loss: 3459.38 with loss1: 3459.38 and loss2: 0.00\n",
      "Epoch [766], train_loss: 2973.63 with loss1: 2973.63 and loss2: 0.00\n",
      "Epoch [767], train_loss: 2958.55 with loss1: 2958.55 and loss2: 0.00\n",
      "Epoch [768], train_loss: 3446.06 with loss1: 3446.06 and loss2: 0.00\n",
      "Epoch [769], train_loss: 3787.12 with loss1: 3787.12 and loss2: 0.00\n",
      "Epoch [770], train_loss: 2891.75 with loss1: 2891.75 and loss2: 0.00\n",
      "Epoch [771], train_loss: 3986.65 with loss1: 3986.65 and loss2: 0.00\n",
      "Epoch [772], train_loss: 4462.21 with loss1: 4462.21 and loss2: 0.00\n",
      "Epoch [773], train_loss: 3781.39 with loss1: 3781.39 and loss2: 0.00\n",
      "Epoch [774], train_loss: 3423.82 with loss1: 3423.82 and loss2: 0.00\n",
      "Epoch [775], train_loss: 4408.19 with loss1: 4408.19 and loss2: 0.00\n",
      "Epoch [776], train_loss: 4434.65 with loss1: 4434.65 and loss2: 0.00\n",
      "Epoch [777], train_loss: 4397.92 with loss1: 4397.92 and loss2: 0.00\n",
      "Epoch [778], train_loss: 3412.15 with loss1: 3412.15 and loss2: 0.00\n",
      "Epoch [779], train_loss: 2863.66 with loss1: 2863.66 and loss2: 0.00\n",
      "Epoch [780], train_loss: 4376.30 with loss1: 4376.30 and loss2: 0.00\n",
      "Epoch [781], train_loss: 3759.99 with loss1: 3759.99 and loss2: 0.00\n",
      "Epoch [782], train_loss: 3393.58 with loss1: 3393.58 and loss2: 0.00\n",
      "Epoch [783], train_loss: 4051.89 with loss1: 4051.89 and loss2: 0.00\n",
      "Epoch [784], train_loss: 3414.96 with loss1: 3414.96 and loss2: 0.00\n",
      "Epoch [785], train_loss: 2906.36 with loss1: 2906.36 and loss2: 0.00\n",
      "Epoch [786], train_loss: 3933.15 with loss1: 3933.15 and loss2: 0.00\n",
      "Epoch [787], train_loss: 4090.68 with loss1: 4090.68 and loss2: 0.00\n",
      "Epoch [788], train_loss: 3407.63 with loss1: 3407.63 and loss2: 0.00\n",
      "Epoch [789], train_loss: 2879.73 with loss1: 2879.73 and loss2: 0.00\n",
      "Epoch [790], train_loss: 3911.78 with loss1: 3911.78 and loss2: 0.00\n",
      "Epoch [791], train_loss: 3402.98 with loss1: 3402.98 and loss2: 0.00\n",
      "Epoch [792], train_loss: 3414.98 with loss1: 3414.98 and loss2: 0.00\n",
      "Epoch [793], train_loss: 3429.15 with loss1: 3429.15 and loss2: 0.00\n",
      "Epoch [794], train_loss: 3371.49 with loss1: 3371.49 and loss2: 0.00\n",
      "Epoch [795], train_loss: 4396.06 with loss1: 4396.06 and loss2: 0.00\n",
      "Epoch [796], train_loss: 4178.26 with loss1: 4178.26 and loss2: 0.00\n",
      "Epoch [797], train_loss: 4439.77 with loss1: 4439.77 and loss2: 0.00\n",
      "Epoch [798], train_loss: 3491.90 with loss1: 3491.90 and loss2: 0.00\n",
      "Epoch [799], train_loss: 3387.42 with loss1: 3387.42 and loss2: 0.00\n",
      "Epoch [800], train_loss: 3735.96 with loss1: 3735.96 and loss2: 0.00\n",
      "Epoch [801], train_loss: 3982.81 with loss1: 3982.81 and loss2: 0.00\n",
      "Epoch [802], train_loss: 4116.54 with loss1: 4116.54 and loss2: 0.00\n",
      "Epoch [803], train_loss: 2844.87 with loss1: 2844.87 and loss2: 0.00\n",
      "Epoch [804], train_loss: 2802.53 with loss1: 2802.53 and loss2: 0.00\n",
      "Epoch [805], train_loss: 3365.76 with loss1: 3365.76 and loss2: 0.00\n",
      "Epoch [806], train_loss: 3405.87 with loss1: 3405.87 and loss2: 0.00\n",
      "Epoch [807], train_loss: 3000.54 with loss1: 3000.54 and loss2: 0.00\n",
      "Epoch [808], train_loss: 3858.78 with loss1: 3858.78 and loss2: 0.00\n",
      "Epoch [809], train_loss: 3384.26 with loss1: 3384.26 and loss2: 0.00\n",
      "Epoch [810], train_loss: 4348.35 with loss1: 4348.35 and loss2: 0.00\n",
      "Epoch [811], train_loss: 4384.34 with loss1: 4384.34 and loss2: 0.00\n",
      "Epoch [812], train_loss: 3357.64 with loss1: 3357.64 and loss2: 0.00\n",
      "Epoch [813], train_loss: 2911.15 with loss1: 2911.15 and loss2: 0.00\n",
      "Epoch [814], train_loss: 3851.36 with loss1: 3851.36 and loss2: 0.00\n",
      "Epoch [815], train_loss: 3361.88 with loss1: 3361.88 and loss2: 0.00\n",
      "Epoch [816], train_loss: 3554.18 with loss1: 3554.18 and loss2: 0.00\n",
      "Epoch [817], train_loss: 3670.18 with loss1: 3670.18 and loss2: 0.00\n",
      "Epoch [818], train_loss: 4006.70 with loss1: 4006.70 and loss2: 0.00\n",
      "Epoch [819], train_loss: 4390.80 with loss1: 4390.80 and loss2: 0.00\n",
      "Epoch [820], train_loss: 4377.00 with loss1: 4377.00 and loss2: 0.00\n",
      "Epoch [821], train_loss: 3367.79 with loss1: 3367.79 and loss2: 0.00\n",
      "Epoch [822], train_loss: 3392.23 with loss1: 3392.23 and loss2: 0.00\n",
      "Epoch [823], train_loss: 3829.04 with loss1: 3829.04 and loss2: 0.00\n",
      "Epoch [824], train_loss: 2749.95 with loss1: 2749.95 and loss2: 0.00\n",
      "Epoch [825], train_loss: 3409.81 with loss1: 3409.81 and loss2: 0.00\n",
      "Epoch [826], train_loss: 4331.57 with loss1: 4331.57 and loss2: 0.00\n",
      "Epoch [827], train_loss: 3927.01 with loss1: 3927.01 and loss2: 0.00\n",
      "Epoch [828], train_loss: 4096.38 with loss1: 4096.38 and loss2: 0.00\n",
      "Epoch [829], train_loss: 4454.91 with loss1: 4454.91 and loss2: 0.00\n",
      "Epoch [830], train_loss: 3715.34 with loss1: 3715.34 and loss2: 0.00\n",
      "Epoch [831], train_loss: 4357.80 with loss1: 4357.80 and loss2: 0.00\n",
      "Epoch [832], train_loss: 3330.47 with loss1: 3330.47 and loss2: 0.00\n",
      "Epoch [833], train_loss: 3883.12 with loss1: 3883.12 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [834], train_loss: 2744.56 with loss1: 2744.56 and loss2: 0.00\n",
      "Epoch [835], train_loss: 3677.58 with loss1: 3677.58 and loss2: 0.00\n",
      "Epoch [836], train_loss: 4348.55 with loss1: 4348.55 and loss2: 0.00\n",
      "Epoch [837], train_loss: 3320.57 with loss1: 3320.57 and loss2: 0.00\n",
      "Epoch [838], train_loss: 4322.13 with loss1: 4322.13 and loss2: 0.00\n",
      "Epoch [839], train_loss: 3317.23 with loss1: 3317.23 and loss2: 0.00\n",
      "Epoch [840], train_loss: 2733.92 with loss1: 2733.92 and loss2: 0.00\n",
      "Epoch [841], train_loss: 3312.64 with loss1: 3312.64 and loss2: 0.00\n",
      "Epoch [842], train_loss: 3660.29 with loss1: 3660.29 and loss2: 0.00\n",
      "Epoch [843], train_loss: 3327.05 with loss1: 3327.05 and loss2: 0.00\n",
      "Epoch [844], train_loss: 3791.86 with loss1: 3791.86 and loss2: 0.00\n",
      "Epoch [845], train_loss: 2718.38 with loss1: 2718.38 and loss2: 0.00\n",
      "Epoch [846], train_loss: 3814.08 with loss1: 3814.08 and loss2: 0.00\n",
      "Epoch [847], train_loss: 3292.42 with loss1: 3292.42 and loss2: 0.00\n",
      "Epoch [848], train_loss: 2859.11 with loss1: 2859.11 and loss2: 0.00\n",
      "Epoch [849], train_loss: 4299.35 with loss1: 4299.35 and loss2: 0.00\n",
      "Epoch [850], train_loss: 3648.96 with loss1: 3648.96 and loss2: 0.00\n",
      "Epoch [851], train_loss: 3321.15 with loss1: 3321.15 and loss2: 0.00\n",
      "Epoch [852], train_loss: 3393.80 with loss1: 3393.80 and loss2: 0.00\n",
      "Epoch [853], train_loss: 3279.46 with loss1: 3279.46 and loss2: 0.00\n",
      "Epoch [854], train_loss: 3282.37 with loss1: 3282.37 and loss2: 0.00\n",
      "Epoch [855], train_loss: 3636.16 with loss1: 3636.16 and loss2: 0.00\n",
      "Epoch [856], train_loss: 3878.79 with loss1: 3878.79 and loss2: 0.00\n",
      "Epoch [857], train_loss: 4066.54 with loss1: 4066.54 and loss2: 0.00\n",
      "Epoch [858], train_loss: 3669.12 with loss1: 3669.12 and loss2: 0.00\n",
      "Epoch [859], train_loss: 3324.00 with loss1: 3324.00 and loss2: 0.00\n",
      "Epoch [860], train_loss: 3298.85 with loss1: 3298.85 and loss2: 0.00\n",
      "Epoch [861], train_loss: 3321.08 with loss1: 3321.08 and loss2: 0.00\n",
      "Epoch [862], train_loss: 2814.83 with loss1: 2814.83 and loss2: 0.00\n",
      "Epoch [863], train_loss: 3652.67 with loss1: 3652.67 and loss2: 0.00\n",
      "Epoch [864], train_loss: 3627.13 with loss1: 3627.13 and loss2: 0.00\n",
      "Epoch [865], train_loss: 3288.71 with loss1: 3288.71 and loss2: 0.00\n",
      "Epoch [866], train_loss: 3338.71 with loss1: 3338.71 and loss2: 0.00\n",
      "Epoch [867], train_loss: 4304.29 with loss1: 4304.29 and loss2: 0.00\n",
      "Epoch [868], train_loss: 4348.88 with loss1: 4348.88 and loss2: 0.00\n",
      "Epoch [869], train_loss: 4332.45 with loss1: 4332.45 and loss2: 0.00\n",
      "Epoch [870], train_loss: 3633.86 with loss1: 3633.86 and loss2: 0.00\n",
      "Epoch [871], train_loss: 4304.58 with loss1: 4304.58 and loss2: 0.00\n",
      "Epoch [872], train_loss: 4301.15 with loss1: 4301.15 and loss2: 0.00\n",
      "Epoch [873], train_loss: 3917.93 with loss1: 3917.93 and loss2: 0.00\n",
      "Epoch [874], train_loss: 3988.66 with loss1: 3988.66 and loss2: 0.00\n",
      "Epoch [875], train_loss: 2729.27 with loss1: 2729.27 and loss2: 0.00\n",
      "Epoch [876], train_loss: 3632.42 with loss1: 3632.42 and loss2: 0.00\n",
      "Epoch [877], train_loss: 3271.62 with loss1: 3271.62 and loss2: 0.00\n",
      "Epoch [878], train_loss: 4296.41 with loss1: 4296.41 and loss2: 0.00\n",
      "Epoch [879], train_loss: 3250.39 with loss1: 3250.39 and loss2: 0.00\n",
      "Epoch [880], train_loss: 3745.51 with loss1: 3745.51 and loss2: 0.00\n",
      "Epoch [881], train_loss: 2668.34 with loss1: 2668.34 and loss2: 0.00\n",
      "Epoch [882], train_loss: 3622.37 with loss1: 3622.37 and loss2: 0.00\n",
      "Epoch [883], train_loss: 2671.83 with loss1: 2671.83 and loss2: 0.00\n",
      "Epoch [884], train_loss: 4254.63 with loss1: 4254.63 and loss2: 0.00\n",
      "Epoch [885], train_loss: 2657.66 with loss1: 2657.66 and loss2: 0.00\n",
      "Epoch [886], train_loss: 4271.42 with loss1: 4271.42 and loss2: 0.00\n",
      "Epoch [887], train_loss: 3337.99 with loss1: 3337.99 and loss2: 0.00\n",
      "Epoch [888], train_loss: 4046.41 with loss1: 4046.41 and loss2: 0.00\n",
      "Epoch [889], train_loss: 3428.18 with loss1: 3428.18 and loss2: 0.00\n",
      "Epoch [890], train_loss: 2699.06 with loss1: 2699.06 and loss2: 0.00\n",
      "Epoch [891], train_loss: 3808.21 with loss1: 3808.21 and loss2: 0.00\n",
      "Epoch [892], train_loss: 2632.60 with loss1: 2632.60 and loss2: 0.00\n",
      "Epoch [893], train_loss: 3257.27 with loss1: 3257.27 and loss2: 0.00\n",
      "Epoch [894], train_loss: 3236.23 with loss1: 3236.23 and loss2: 0.00\n",
      "Epoch [895], train_loss: 4264.40 with loss1: 4264.40 and loss2: 0.00\n",
      "Epoch [896], train_loss: 3598.33 with loss1: 3598.33 and loss2: 0.00\n",
      "Epoch [897], train_loss: 3235.38 with loss1: 3235.38 and loss2: 0.00\n",
      "Epoch [898], train_loss: 3832.33 with loss1: 3832.33 and loss2: 0.00\n",
      "Epoch [899], train_loss: 2622.13 with loss1: 2622.13 and loss2: 0.00\n",
      "Epoch [900], train_loss: 2678.08 with loss1: 2678.08 and loss2: 0.00\n",
      "Epoch [901], train_loss: 2740.45 with loss1: 2740.45 and loss2: 0.00\n",
      "Epoch [902], train_loss: 3604.53 with loss1: 3604.53 and loss2: 0.00\n",
      "Epoch [903], train_loss: 3579.02 with loss1: 3579.02 and loss2: 0.00\n",
      "Epoch [904], train_loss: 3811.26 with loss1: 3811.26 and loss2: 0.00\n",
      "Epoch [905], train_loss: 3608.79 with loss1: 3608.79 and loss2: 0.00\n",
      "Epoch [906], train_loss: 3236.88 with loss1: 3236.88 and loss2: 0.00\n",
      "Epoch [907], train_loss: 4226.52 with loss1: 4226.52 and loss2: 0.00\n",
      "Epoch [908], train_loss: 3209.90 with loss1: 3209.90 and loss2: 0.00\n",
      "Epoch [909], train_loss: 3254.64 with loss1: 3254.64 and loss2: 0.00\n",
      "Epoch [910], train_loss: 2836.85 with loss1: 2836.85 and loss2: 0.00\n",
      "Epoch [911], train_loss: 3348.51 with loss1: 3348.51 and loss2: 0.00\n",
      "Epoch [912], train_loss: 3780.93 with loss1: 3780.93 and loss2: 0.00\n",
      "Epoch [913], train_loss: 3262.96 with loss1: 3262.96 and loss2: 0.00\n",
      "Epoch [914], train_loss: 2604.33 with loss1: 2604.33 and loss2: 0.00\n",
      "Epoch [915], train_loss: 3572.43 with loss1: 3572.43 and loss2: 0.00\n",
      "Epoch [916], train_loss: 3222.75 with loss1: 3222.75 and loss2: 0.00\n",
      "Epoch [917], train_loss: 3799.24 with loss1: 3799.24 and loss2: 0.00\n",
      "Epoch [918], train_loss: 3242.99 with loss1: 3242.99 and loss2: 0.00\n",
      "Epoch [919], train_loss: 3587.18 with loss1: 3587.18 and loss2: 0.00\n",
      "Epoch [920], train_loss: 4299.58 with loss1: 4299.58 and loss2: 0.00\n",
      "Epoch [921], train_loss: 3969.47 with loss1: 3969.47 and loss2: 0.00\n",
      "Epoch [922], train_loss: 3312.06 with loss1: 3312.06 and loss2: 0.00\n",
      "Epoch [923], train_loss: 2612.14 with loss1: 2612.14 and loss2: 0.00\n",
      "Epoch [924], train_loss: 3567.97 with loss1: 3567.97 and loss2: 0.00\n",
      "Epoch [925], train_loss: 3202.11 with loss1: 3202.11 and loss2: 0.00\n",
      "Epoch [926], train_loss: 4249.36 with loss1: 4249.36 and loss2: 0.00\n",
      "Epoch [927], train_loss: 3902.15 with loss1: 3902.15 and loss2: 0.00\n",
      "Epoch [928], train_loss: 3613.88 with loss1: 3613.88 and loss2: 0.00\n",
      "Epoch [929], train_loss: 3559.26 with loss1: 3559.26 and loss2: 0.00\n",
      "Epoch [930], train_loss: 3555.71 with loss1: 3555.71 and loss2: 0.00\n",
      "Epoch [931], train_loss: 4282.96 with loss1: 4282.96 and loss2: 0.00\n",
      "Epoch [932], train_loss: 3216.83 with loss1: 3216.83 and loss2: 0.00\n",
      "Epoch [933], train_loss: 3220.96 with loss1: 3220.96 and loss2: 0.00\n",
      "Epoch [934], train_loss: 3684.35 with loss1: 3684.35 and loss2: 0.00\n",
      "Epoch [935], train_loss: 3195.30 with loss1: 3195.30 and loss2: 0.00\n",
      "Epoch [936], train_loss: 4247.63 with loss1: 4247.63 and loss2: 0.00\n",
      "Epoch [937], train_loss: 2618.77 with loss1: 2618.77 and loss2: 0.00\n",
      "Epoch [938], train_loss: 3588.57 with loss1: 3588.57 and loss2: 0.00\n",
      "Epoch [939], train_loss: 2620.27 with loss1: 2620.27 and loss2: 0.00\n",
      "Epoch [940], train_loss: 4229.97 with loss1: 4229.97 and loss2: 0.00\n",
      "Epoch [941], train_loss: 3197.12 with loss1: 3197.12 and loss2: 0.00\n",
      "Epoch [942], train_loss: 3686.37 with loss1: 3686.37 and loss2: 0.00\n",
      "Epoch [943], train_loss: 2591.43 with loss1: 2591.43 and loss2: 0.00\n",
      "Epoch [944], train_loss: 2656.01 with loss1: 2656.01 and loss2: 0.00\n",
      "Epoch [945], train_loss: 4234.13 with loss1: 4234.13 and loss2: 0.00\n",
      "Epoch [946], train_loss: 3852.96 with loss1: 3852.96 and loss2: 0.00\n",
      "Epoch [947], train_loss: 2615.57 with loss1: 2615.57 and loss2: 0.00\n",
      "Epoch [948], train_loss: 3742.98 with loss1: 3742.98 and loss2: 0.00\n",
      "Epoch [949], train_loss: 3192.20 with loss1: 3192.20 and loss2: 0.00\n",
      "Epoch [950], train_loss: 3675.36 with loss1: 3675.36 and loss2: 0.00\n",
      "Epoch [951], train_loss: 3197.19 with loss1: 3197.19 and loss2: 0.00\n",
      "Epoch [952], train_loss: 3669.17 with loss1: 3669.17 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [953], train_loss: 3187.57 with loss1: 3187.57 and loss2: 0.00\n",
      "Epoch [954], train_loss: 3534.75 with loss1: 3534.75 and loss2: 0.00\n",
      "Epoch [955], train_loss: 3212.89 with loss1: 3212.89 and loss2: 0.00\n",
      "Epoch [956], train_loss: 3652.05 with loss1: 3652.05 and loss2: 0.00\n",
      "Epoch [957], train_loss: 2582.51 with loss1: 2582.51 and loss2: 0.00\n",
      "Epoch [958], train_loss: 4189.23 with loss1: 4189.23 and loss2: 0.00\n",
      "Epoch [959], train_loss: 3537.07 with loss1: 3537.07 and loss2: 0.00\n",
      "Epoch [960], train_loss: 4278.96 with loss1: 4278.96 and loss2: 0.00\n",
      "Epoch [961], train_loss: 2545.75 with loss1: 2545.75 and loss2: 0.00\n",
      "Epoch [962], train_loss: 3193.21 with loss1: 3193.21 and loss2: 0.00\n",
      "Epoch [963], train_loss: 3172.55 with loss1: 3172.55 and loss2: 0.00\n",
      "Epoch [964], train_loss: 2587.41 with loss1: 2587.41 and loss2: 0.00\n",
      "Epoch [965], train_loss: 4184.86 with loss1: 4184.86 and loss2: 0.00\n",
      "Epoch [966], train_loss: 3896.04 with loss1: 3896.04 and loss2: 0.00\n",
      "Epoch [967], train_loss: 3207.47 with loss1: 3207.47 and loss2: 0.00\n",
      "Epoch [968], train_loss: 3548.28 with loss1: 3548.28 and loss2: 0.00\n",
      "Epoch [969], train_loss: 3210.96 with loss1: 3210.96 and loss2: 0.00\n",
      "Epoch [970], train_loss: 3322.22 with loss1: 3322.22 and loss2: 0.00\n",
      "Epoch [971], train_loss: 2625.75 with loss1: 2625.75 and loss2: 0.00\n",
      "Epoch [972], train_loss: 3350.36 with loss1: 3350.36 and loss2: 0.00\n",
      "Epoch [973], train_loss: 2987.22 with loss1: 2987.22 and loss2: 0.00\n",
      "Epoch [974], train_loss: 3681.63 with loss1: 3681.63 and loss2: 0.00\n",
      "Epoch [975], train_loss: 3183.94 with loss1: 3183.94 and loss2: 0.00\n",
      "Epoch [976], train_loss: 2678.48 with loss1: 2678.48 and loss2: 0.00\n",
      "Epoch [977], train_loss: 3668.19 with loss1: 3668.19 and loss2: 0.00\n",
      "Epoch [978], train_loss: 3817.28 with loss1: 3817.28 and loss2: 0.00\n",
      "Epoch [979], train_loss: 3903.75 with loss1: 3903.75 and loss2: 0.00\n",
      "Epoch [980], train_loss: 4238.64 with loss1: 4238.64 and loss2: 0.00\n",
      "Epoch [981], train_loss: 3747.85 with loss1: 3747.85 and loss2: 0.00\n",
      "Epoch [982], train_loss: 3740.03 with loss1: 3740.03 and loss2: 0.00\n",
      "Epoch [983], train_loss: 3705.92 with loss1: 3705.92 and loss2: 0.00\n",
      "Epoch [984], train_loss: 3686.01 with loss1: 3686.01 and loss2: 0.00\n",
      "Epoch [985], train_loss: 3144.72 with loss1: 3144.72 and loss2: 0.00\n",
      "Epoch [986], train_loss: 4189.58 with loss1: 4189.58 and loss2: 0.00\n",
      "Epoch [987], train_loss: 3566.93 with loss1: 3566.93 and loss2: 0.00\n",
      "Epoch [988], train_loss: 3136.72 with loss1: 3136.72 and loss2: 0.00\n",
      "Epoch [989], train_loss: 3220.12 with loss1: 3220.12 and loss2: 0.00\n",
      "Epoch [990], train_loss: 2737.65 with loss1: 2737.65 and loss2: 0.00\n",
      "Epoch [991], train_loss: 3237.73 with loss1: 3237.73 and loss2: 0.00\n",
      "Epoch [992], train_loss: 2649.36 with loss1: 2649.36 and loss2: 0.00\n",
      "Epoch [993], train_loss: 3275.26 with loss1: 3275.26 and loss2: 0.00\n",
      "Epoch [994], train_loss: 3659.47 with loss1: 3659.47 and loss2: 0.00\n",
      "Epoch [995], train_loss: 2517.03 with loss1: 2517.03 and loss2: 0.00\n",
      "Epoch [996], train_loss: 3161.65 with loss1: 3161.65 and loss2: 0.00\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=100, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# new model\n",
    "# delete loss2\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history = fit(epochs=1000, lr=5e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8958b178",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 4270.23 with loss1: 4270.23 and loss2: 0.00\n",
      "Epoch [1], train_loss: 2890.31 with loss1: 2890.31 and loss2: 0.00\n",
      "Epoch [2], train_loss: 3517.77 with loss1: 3517.77 and loss2: 0.00\n",
      "Epoch [3], train_loss: 4440.01 with loss1: 4440.01 and loss2: 0.00\n",
      "Epoch [4], train_loss: 3445.12 with loss1: 3445.12 and loss2: 0.00\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# original model\n",
    "# delete loss2\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history, mu_history = fit(epochs=5, lr=5e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81bd7d8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 306755.75 with loss1: 306755.75 and loss2: 0.00\n",
      "Epoch [1], train_loss: 306134.22 with loss1: 306134.22 and loss2: 0.00\n",
      "Epoch [2], train_loss: 306399.94 with loss1: 306399.94 and loss2: 0.00\n",
      "Epoch [3], train_loss: 309384.38 with loss1: 309384.38 and loss2: 0.00\n",
      "Epoch [4], train_loss: 305904.59 with loss1: 305904.59 and loss2: 0.00\n",
      "Epoch [5], train_loss: 309634.50 with loss1: 309634.50 and loss2: 0.00\n",
      "Epoch [6], train_loss: 309598.00 with loss1: 309598.00 and loss2: 0.00\n",
      "Epoch [7], train_loss: 303805.41 with loss1: 303805.41 and loss2: 0.00\n",
      "Epoch [8], train_loss: 305740.41 with loss1: 305740.41 and loss2: 0.00\n",
      "Epoch [9], train_loss: 305711.44 with loss1: 305711.44 and loss2: 0.00\n",
      "Epoch [10], train_loss: 309469.06 with loss1: 309469.06 and loss2: 0.00\n",
      "Epoch [11], train_loss: 306000.38 with loss1: 306000.38 and loss2: 0.00\n",
      "Epoch [12], train_loss: 309057.03 with loss1: 309057.03 and loss2: 0.00\n",
      "Epoch [13], train_loss: 305610.69 with loss1: 305610.69 and loss2: 0.00\n",
      "Epoch [14], train_loss: 305944.09 with loss1: 305944.09 and loss2: 0.00\n",
      "Epoch [15], train_loss: 309354.03 with loss1: 309354.03 and loss2: 0.00\n",
      "Epoch [16], train_loss: 311304.00 with loss1: 311304.00 and loss2: 0.00\n",
      "Epoch [17], train_loss: 303575.97 with loss1: 303575.97 and loss2: 0.00\n",
      "Epoch [18], train_loss: 308951.78 with loss1: 308951.78 and loss2: 0.00\n",
      "Epoch [19], train_loss: 309291.56 with loss1: 309291.56 and loss2: 0.00\n",
      "Epoch [20], train_loss: 303529.28 with loss1: 303529.28 and loss2: 0.00\n",
      "Epoch [21], train_loss: 303519.16 with loss1: 303519.16 and loss2: 0.00\n",
      "Epoch [22], train_loss: 311214.12 with loss1: 311214.12 and loss2: 0.00\n",
      "Epoch [23], train_loss: 309240.94 with loss1: 309240.94 and loss2: 0.00\n",
      "Epoch [24], train_loss: 303483.81 with loss1: 303483.81 and loss2: 0.00\n",
      "Epoch [25], train_loss: 305437.34 with loss1: 305437.34 and loss2: 0.00\n",
      "Epoch [26], train_loss: 305780.28 with loss1: 305780.28 and loss2: 0.00\n",
      "Epoch [27], train_loss: 303445.47 with loss1: 303445.47 and loss2: 0.00\n",
      "Epoch [28], train_loss: 305760.34 with loss1: 305760.34 and loss2: 0.00\n",
      "Epoch [29], train_loss: 311138.16 with loss1: 311138.16 and loss2: 0.00\n",
      "Epoch [30], train_loss: 305379.94 with loss1: 305379.94 and loss2: 0.00\n",
      "Epoch [31], train_loss: 305368.53 with loss1: 305368.53 and loss2: 0.00\n",
      "Epoch [32], train_loss: 311103.22 with loss1: 311103.22 and loss2: 0.00\n",
      "Epoch [33], train_loss: 308780.31 with loss1: 308780.31 and loss2: 0.00\n",
      "Epoch [34], train_loss: 308770.41 with loss1: 308770.41 and loss2: 0.00\n",
      "Epoch [35], train_loss: 311080.25 with loss1: 311080.25 and loss2: 0.00\n",
      "Epoch [36], train_loss: 305324.22 with loss1: 305324.22 and loss2: 0.00\n",
      "Epoch [37], train_loss: 309099.69 with loss1: 309099.69 and loss2: 0.00\n",
      "Epoch [38], train_loss: 303345.12 with loss1: 303345.12 and loss2: 0.00\n",
      "Epoch [39], train_loss: 309081.69 with loss1: 309081.69 and loss2: 0.00\n",
      "Epoch [40], train_loss: 311034.25 with loss1: 311034.25 and loss2: 0.00\n",
      "Epoch [41], train_loss: 305280.16 with loss1: 305280.16 and loss2: 0.00\n",
      "Epoch [42], train_loss: 311020.28 with loss1: 311020.28 and loss2: 0.00\n",
      "Epoch [43], train_loss: 309050.31 with loss1: 309050.31 and loss2: 0.00\n",
      "Epoch [44], train_loss: 309043.75 with loss1: 309043.75 and loss2: 0.00\n",
      "Epoch [45], train_loss: 305608.53 with loss1: 305608.53 and loss2: 0.00\n",
      "Epoch [46], train_loss: 308669.91 with loss1: 308669.91 and loss2: 0.00\n",
      "Epoch [47], train_loss: 303271.91 with loss1: 303271.91 and loss2: 0.00\n",
      "Epoch [48], train_loss: 309011.94 with loss1: 309011.94 and loss2: 0.00\n",
      "Epoch [49], train_loss: 305577.59 with loss1: 305577.59 and loss2: 0.00\n",
      "Epoch [50], train_loss: 305569.69 with loss1: 305569.69 and loss2: 0.00\n",
      "Epoch [51], train_loss: 305205.16 with loss1: 305205.16 and loss2: 0.00\n",
      "Epoch [52], train_loss: 303236.78 with loss1: 303236.78 and loss2: 0.00\n",
      "Epoch [53], train_loss: 305548.31 with loss1: 305548.31 and loss2: 0.00\n",
      "Epoch [54], train_loss: 303220.59 with loss1: 303220.59 and loss2: 0.00\n",
      "Epoch [55], train_loss: 303215.66 with loss1: 303215.66 and loss2: 0.00\n",
      "Epoch [56], train_loss: 305168.53 with loss1: 305168.53 and loss2: 0.00\n",
      "Epoch [57], train_loss: 308946.19 with loss1: 308946.19 and loss2: 0.00\n",
      "Epoch [58], train_loss: 308582.09 with loss1: 308582.09 and loss2: 0.00\n",
      "Epoch [59], train_loss: 303186.00 with loss1: 303186.00 and loss2: 0.00\n",
      "Epoch [60], train_loss: 305496.75 with loss1: 305496.75 and loss2: 0.00\n",
      "Epoch [61], train_loss: 308916.22 with loss1: 308916.22 and loss2: 0.00\n",
      "Epoch [62], train_loss: 303167.12 with loss1: 303167.12 and loss2: 0.00\n",
      "Epoch [63], train_loss: 308903.22 with loss1: 308903.22 and loss2: 0.00\n",
      "Epoch [64], train_loss: 310858.31 with loss1: 310858.31 and loss2: 0.00\n",
      "Epoch [65], train_loss: 310849.59 with loss1: 310849.59 and loss2: 0.00\n",
      "Epoch [66], train_loss: 305456.62 with loss1: 305456.62 and loss2: 0.00\n",
      "Epoch [67], train_loss: 310838.97 with loss1: 310838.97 and loss2: 0.00\n",
      "Epoch [68], train_loss: 308869.19 with loss1: 308869.19 and loss2: 0.00\n",
      "Epoch [69], train_loss: 305435.81 with loss1: 305435.81 and loss2: 0.00\n",
      "Epoch [70], train_loss: 305071.31 with loss1: 305071.31 and loss2: 0.00\n",
      "Epoch [71], train_loss: 308849.44 with loss1: 308849.44 and loss2: 0.00\n",
      "Epoch [72], train_loss: 305056.78 with loss1: 305056.78 and loss2: 0.00\n",
      "Epoch [73], train_loss: 305408.62 with loss1: 305408.62 and loss2: 0.00\n",
      "Epoch [74], train_loss: 305402.47 with loss1: 305402.47 and loss2: 0.00\n",
      "Epoch [75], train_loss: 310781.56 with loss1: 310781.56 and loss2: 0.00\n",
      "Epoch [76], train_loss: 308814.66 with loss1: 308814.66 and loss2: 0.00\n",
      "Epoch [77], train_loss: 305381.53 with loss1: 305381.53 and loss2: 0.00\n",
      "Epoch [78], train_loss: 308801.06 with loss1: 308801.06 and loss2: 0.00\n",
      "Epoch [79], train_loss: 308795.12 with loss1: 308795.12 and loss2: 0.00\n",
      "Epoch [80], train_loss: 305361.75 with loss1: 305361.75 and loss2: 0.00\n",
      "Epoch [81], train_loss: 303037.41 with loss1: 303037.41 and loss2: 0.00\n",
      "Epoch [82], train_loss: 310736.88 with loss1: 310736.88 and loss2: 0.00\n",
      "Epoch [83], train_loss: 304984.78 with loss1: 304984.78 and loss2: 0.00\n",
      "Epoch [84], train_loss: 303016.00 with loss1: 303016.00 and loss2: 0.00\n",
      "Epoch [85], train_loss: 303010.66 with loss1: 303010.66 and loss2: 0.00\n",
      "Epoch [86], train_loss: 308392.81 with loss1: 308392.81 and loss2: 0.00\n",
      "Epoch [87], train_loss: 304957.66 with loss1: 304957.66 and loss2: 0.00\n",
      "Epoch [88], train_loss: 308378.44 with loss1: 308378.44 and loss2: 0.00\n",
      "Epoch [89], train_loss: 302985.06 with loss1: 302985.06 and loss2: 0.00\n",
      "Epoch [90], train_loss: 302979.09 with loss1: 302979.09 and loss2: 0.00\n",
      "Epoch [91], train_loss: 308715.78 with loss1: 308715.78 and loss2: 0.00\n",
      "Epoch [92], train_loss: 305283.47 with loss1: 305283.47 and loss2: 0.00\n",
      "Epoch [93], train_loss: 305276.25 with loss1: 305276.25 and loss2: 0.00\n",
      "Epoch [94], train_loss: 304912.88 with loss1: 304912.88 and loss2: 0.00\n",
      "Epoch [95], train_loss: 310651.62 with loss1: 310651.62 and loss2: 0.00\n",
      "Epoch [96], train_loss: 308683.81 with loss1: 308683.81 and loss2: 0.00\n",
      "Epoch [97], train_loss: 310637.75 with loss1: 310637.75 and loss2: 0.00\n",
      "Epoch [98], train_loss: 304887.81 with loss1: 304887.81 and loss2: 0.00\n",
      "Epoch [99], train_loss: 302920.34 with loss1: 302920.34 and loss2: 0.00\n",
      "Epoch [100], train_loss: 304874.06 with loss1: 304874.06 and loss2: 0.00\n",
      "Epoch [101], train_loss: 304868.31 with loss1: 304868.31 and loss2: 0.00\n",
      "Epoch [102], train_loss: 308287.81 with loss1: 308287.81 and loss2: 0.00\n",
      "Epoch [103], train_loss: 308637.50 with loss1: 308637.50 and loss2: 0.00\n",
      "Epoch [104], train_loss: 308273.44 with loss1: 308273.44 and loss2: 0.00\n",
      "Epoch [105], train_loss: 302881.66 with loss1: 302881.66 and loss2: 0.00\n",
      "Epoch [106], train_loss: 308617.38 with loss1: 308617.38 and loss2: 0.00\n",
      "Epoch [107], train_loss: 305186.28 with loss1: 305186.28 and loss2: 0.00\n",
      "Epoch [108], train_loss: 308250.41 with loss1: 308250.41 and loss2: 0.00\n",
      "Epoch [109], train_loss: 308242.69 with loss1: 308242.69 and loss2: 0.00\n",
      "Epoch [110], train_loss: 310552.66 with loss1: 310552.66 and loss2: 0.00\n",
      "Epoch [111], train_loss: 308229.28 with loss1: 308229.28 and loss2: 0.00\n",
      "Epoch [112], train_loss: 302835.91 with loss1: 302835.91 and loss2: 0.00\n",
      "Epoch [113], train_loss: 310532.94 with loss1: 310532.94 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [114], train_loss: 304786.16 with loss1: 304786.16 and loss2: 0.00\n",
      "Epoch [115], train_loss: 308560.62 with loss1: 308560.62 and loss2: 0.00\n",
      "Epoch [116], train_loss: 302810.41 with loss1: 302810.41 and loss2: 0.00\n",
      "Epoch [117], train_loss: 304764.16 with loss1: 304764.16 and loss2: 0.00\n",
      "Epoch [118], train_loss: 305115.22 with loss1: 305115.22 and loss2: 0.00\n",
      "Epoch [119], train_loss: 308534.56 with loss1: 308534.56 and loss2: 0.00\n",
      "Epoch [120], train_loss: 304745.53 with loss1: 304745.53 and loss2: 0.00\n",
      "Epoch [121], train_loss: 304739.88 with loss1: 304739.88 and loss2: 0.00\n",
      "Epoch [122], train_loss: 305089.41 with loss1: 305089.41 and loss2: 0.00\n",
      "Epoch [123], train_loss: 310468.12 with loss1: 310468.12 and loss2: 0.00\n",
      "Epoch [124], train_loss: 308145.59 with loss1: 308145.59 and loss2: 0.00\n",
      "Epoch [125], train_loss: 310455.97 with loss1: 310455.97 and loss2: 0.00\n",
      "Epoch [126], train_loss: 302747.62 with loss1: 302747.62 and loss2: 0.00\n",
      "Epoch [127], train_loss: 308483.25 with loss1: 308483.25 and loss2: 0.00\n",
      "Epoch [128], train_loss: 308475.97 with loss1: 308475.97 and loss2: 0.00\n",
      "Epoch [129], train_loss: 308112.91 with loss1: 308112.91 and loss2: 0.00\n",
      "Epoch [130], train_loss: 304681.84 with loss1: 304681.84 and loss2: 0.00\n",
      "Epoch [131], train_loss: 308457.34 with loss1: 308457.34 and loss2: 0.00\n",
      "Epoch [132], train_loss: 302708.84 with loss1: 302708.84 and loss2: 0.00\n",
      "Epoch [133], train_loss: 302702.00 with loss1: 302702.00 and loss2: 0.00\n",
      "Epoch [134], train_loss: 304656.28 with loss1: 304656.28 and loss2: 0.00\n",
      "Epoch [135], train_loss: 304650.59 with loss1: 304650.59 and loss2: 0.00\n",
      "Epoch [136], train_loss: 305000.41 with loss1: 305000.41 and loss2: 0.00\n",
      "Epoch [137], train_loss: 304637.44 with loss1: 304637.44 and loss2: 0.00\n",
      "Epoch [138], train_loss: 310371.81 with loss1: 310371.81 and loss2: 0.00\n",
      "Epoch [139], train_loss: 302665.19 with loss1: 302665.19 and loss2: 0.00\n",
      "Epoch [140], train_loss: 308042.28 with loss1: 308042.28 and loss2: 0.00\n",
      "Epoch [141], train_loss: 308035.81 with loss1: 308035.81 and loss2: 0.00\n",
      "Epoch [142], train_loss: 304961.84 with loss1: 304961.84 and loss2: 0.00\n",
      "Epoch [143], train_loss: 304955.56 with loss1: 304955.56 and loss2: 0.00\n",
      "Epoch [144], train_loss: 310333.69 with loss1: 310333.69 and loss2: 0.00\n",
      "Epoch [145], train_loss: 302625.75 with loss1: 302625.75 and loss2: 0.00\n",
      "Epoch [146], train_loss: 308004.34 with loss1: 308004.34 and loss2: 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/39609888/ipykernel_28281/4256931304.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# delete loss2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# dataset: 4 images (IMCL not included)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/local/39609888/ipykernel_28281/2695912267.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, lr, h0, model, train_loader, val_loader, opt_func)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Training Phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;31m#print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/local/39609888/ipykernel_28281/286729573.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;34m\"\"\"Yield a batch of data after moving it to device\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.10/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.10/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/pytorch/1.10/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/apps/pytorch/1.10/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# new model\n",
    "# delete loss2\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history, mu_history = fit(epochs=500, lr=5e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f651e9bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 250011.30 with loss1: 250011.30 and loss2: 0.00\n",
      "Epoch [1], train_loss: 113181.12 with loss1: 113181.12 and loss2: 0.00\n",
      "Epoch [2], train_loss: 66515.07 with loss1: 66515.07 and loss2: 0.00\n",
      "Epoch [3], train_loss: 50718.62 with loss1: 50718.62 and loss2: 0.00\n",
      "Epoch [4], train_loss: 43674.10 with loss1: 43674.10 and loss2: 0.00\n",
      "Epoch [5], train_loss: 38498.85 with loss1: 38498.85 and loss2: 0.00\n",
      "Epoch [6], train_loss: 35532.88 with loss1: 35532.88 and loss2: 0.00\n",
      "Epoch [7], train_loss: 29453.29 with loss1: 29453.29 and loss2: 0.00\n",
      "Epoch [8], train_loss: 24550.56 with loss1: 24550.56 and loss2: 0.00\n",
      "Epoch [9], train_loss: 23573.29 with loss1: 23573.29 and loss2: 0.00\n",
      "Epoch [10], train_loss: 23536.45 with loss1: 23536.45 and loss2: 0.00\n",
      "Epoch [11], train_loss: 22324.34 with loss1: 22324.34 and loss2: 0.00\n",
      "Epoch [12], train_loss: 17646.66 with loss1: 17646.66 and loss2: 0.00\n",
      "Epoch [13], train_loss: 16546.18 with loss1: 16546.18 and loss2: 0.00\n",
      "Epoch [14], train_loss: 14874.89 with loss1: 14874.89 and loss2: 0.00\n",
      "Epoch [15], train_loss: 15192.13 with loss1: 15192.13 and loss2: 0.00\n",
      "Epoch [16], train_loss: 12887.44 with loss1: 12887.44 and loss2: 0.00\n",
      "Epoch [17], train_loss: 13008.01 with loss1: 13008.01 and loss2: 0.00\n",
      "Epoch [18], train_loss: 13937.83 with loss1: 13937.83 and loss2: 0.00\n",
      "Epoch [19], train_loss: 12294.21 with loss1: 12294.21 and loss2: 0.00\n",
      "Epoch [20], train_loss: 10870.88 with loss1: 10870.88 and loss2: 0.00\n",
      "Epoch [21], train_loss: 10304.20 with loss1: 10304.20 and loss2: 0.00\n",
      "Epoch [22], train_loss: 10960.51 with loss1: 10960.51 and loss2: 0.00\n",
      "Epoch [23], train_loss: 10177.29 with loss1: 10177.29 and loss2: 0.00\n",
      "Epoch [24], train_loss: 10845.84 with loss1: 10845.84 and loss2: 0.00\n",
      "Epoch [25], train_loss: 8925.47 with loss1: 8925.47 and loss2: 0.00\n",
      "Epoch [26], train_loss: 8422.78 with loss1: 8422.78 and loss2: 0.00\n",
      "Epoch [27], train_loss: 8279.20 with loss1: 8279.20 and loss2: 0.00\n",
      "Epoch [28], train_loss: 8633.06 with loss1: 8633.06 and loss2: 0.00\n",
      "Epoch [29], train_loss: 9132.19 with loss1: 9132.19 and loss2: 0.00\n",
      "Epoch [30], train_loss: 8259.33 with loss1: 8259.33 and loss2: 0.00\n",
      "Epoch [31], train_loss: 9836.19 with loss1: 9836.19 and loss2: 0.00\n",
      "Epoch [32], train_loss: 8046.30 with loss1: 8046.30 and loss2: 0.00\n",
      "Epoch [33], train_loss: 7648.90 with loss1: 7648.90 and loss2: 0.00\n",
      "Epoch [34], train_loss: 7896.17 with loss1: 7896.17 and loss2: 0.00\n",
      "Epoch [35], train_loss: 8285.97 with loss1: 8285.97 and loss2: 0.00\n",
      "Epoch [36], train_loss: 7222.43 with loss1: 7222.43 and loss2: 0.00\n",
      "Epoch [37], train_loss: 6954.44 with loss1: 6954.44 and loss2: 0.00\n",
      "Epoch [38], train_loss: 7144.56 with loss1: 7144.56 and loss2: 0.00\n",
      "Epoch [39], train_loss: 7652.80 with loss1: 7652.80 and loss2: 0.00\n",
      "Epoch [40], train_loss: 6538.95 with loss1: 6538.95 and loss2: 0.00\n",
      "Epoch [41], train_loss: 6939.91 with loss1: 6939.91 and loss2: 0.00\n",
      "Epoch [42], train_loss: 7783.75 with loss1: 7783.75 and loss2: 0.00\n",
      "Epoch [43], train_loss: 7128.68 with loss1: 7128.68 and loss2: 0.00\n",
      "Epoch [44], train_loss: 6988.56 with loss1: 6988.56 and loss2: 0.00\n",
      "Epoch [45], train_loss: 6864.73 with loss1: 6864.73 and loss2: 0.00\n",
      "Epoch [46], train_loss: 6659.92 with loss1: 6659.92 and loss2: 0.00\n",
      "Epoch [47], train_loss: 7480.24 with loss1: 7480.24 and loss2: 0.00\n",
      "Epoch [48], train_loss: 7050.65 with loss1: 7050.65 and loss2: 0.00\n",
      "Epoch [49], train_loss: 7277.78 with loss1: 7277.78 and loss2: 0.00\n",
      "Epoch [50], train_loss: 7204.86 with loss1: 7204.86 and loss2: 0.00\n",
      "Epoch [51], train_loss: 8231.71 with loss1: 8231.71 and loss2: 0.00\n",
      "Epoch [52], train_loss: 7071.01 with loss1: 7071.01 and loss2: 0.00\n",
      "Epoch [53], train_loss: 5711.21 with loss1: 5711.21 and loss2: 0.00\n",
      "Epoch [54], train_loss: 5777.78 with loss1: 5777.78 and loss2: 0.00\n",
      "Epoch [55], train_loss: 6156.13 with loss1: 6156.13 and loss2: 0.00\n",
      "Epoch [56], train_loss: 6163.62 with loss1: 6163.62 and loss2: 0.00\n",
      "Epoch [57], train_loss: 6055.43 with loss1: 6055.43 and loss2: 0.00\n",
      "Epoch [58], train_loss: 5707.00 with loss1: 5707.00 and loss2: 0.00\n",
      "Epoch [59], train_loss: 6646.28 with loss1: 6646.28 and loss2: 0.00\n",
      "Epoch [60], train_loss: 5989.24 with loss1: 5989.24 and loss2: 0.00\n",
      "Epoch [61], train_loss: 6205.11 with loss1: 6205.11 and loss2: 0.00\n",
      "Epoch [62], train_loss: 5924.43 with loss1: 5924.43 and loss2: 0.00\n",
      "Epoch [63], train_loss: 7609.66 with loss1: 7609.66 and loss2: 0.00\n",
      "Epoch [64], train_loss: 5647.35 with loss1: 5647.35 and loss2: 0.00\n",
      "Epoch [65], train_loss: 5397.79 with loss1: 5397.79 and loss2: 0.00\n",
      "Epoch [66], train_loss: 7127.29 with loss1: 7127.29 and loss2: 0.00\n",
      "Epoch [67], train_loss: 5790.54 with loss1: 5790.54 and loss2: 0.00\n",
      "Epoch [68], train_loss: 6723.89 with loss1: 6723.89 and loss2: 0.00\n",
      "Epoch [69], train_loss: 5792.16 with loss1: 5792.16 and loss2: 0.00\n",
      "Epoch [70], train_loss: 5813.12 with loss1: 5813.12 and loss2: 0.00\n",
      "Epoch [71], train_loss: 7298.97 with loss1: 7298.97 and loss2: 0.00\n",
      "Epoch [72], train_loss: 5416.72 with loss1: 5416.72 and loss2: 0.00\n",
      "Epoch [73], train_loss: 6123.16 with loss1: 6123.16 and loss2: 0.00\n",
      "Epoch [74], train_loss: 5604.90 with loss1: 5604.90 and loss2: 0.00\n",
      "Epoch [75], train_loss: 5944.97 with loss1: 5944.97 and loss2: 0.00\n",
      "Epoch [76], train_loss: 6218.24 with loss1: 6218.24 and loss2: 0.00\n",
      "Epoch [77], train_loss: 6069.21 with loss1: 6069.21 and loss2: 0.00\n",
      "Epoch [78], train_loss: 5068.91 with loss1: 5068.91 and loss2: 0.00\n",
      "Epoch [79], train_loss: 6721.09 with loss1: 6721.09 and loss2: 0.00\n",
      "Epoch [80], train_loss: 5112.60 with loss1: 5112.60 and loss2: 0.00\n",
      "Epoch [81], train_loss: 4943.23 with loss1: 4943.23 and loss2: 0.00\n",
      "Epoch [82], train_loss: 5564.07 with loss1: 5564.07 and loss2: 0.00\n",
      "Epoch [83], train_loss: 5378.50 with loss1: 5378.50 and loss2: 0.00\n",
      "Epoch [84], train_loss: 5320.44 with loss1: 5320.44 and loss2: 0.00\n",
      "Epoch [85], train_loss: 6286.26 with loss1: 6286.26 and loss2: 0.00\n",
      "Epoch [86], train_loss: 5421.33 with loss1: 5421.33 and loss2: 0.00\n",
      "Epoch [87], train_loss: 5483.47 with loss1: 5483.47 and loss2: 0.00\n",
      "Epoch [88], train_loss: 4858.11 with loss1: 4858.11 and loss2: 0.00\n",
      "Epoch [89], train_loss: 6560.40 with loss1: 6560.40 and loss2: 0.00\n",
      "Epoch [90], train_loss: 4971.07 with loss1: 4971.07 and loss2: 0.00\n",
      "Epoch [91], train_loss: 4788.37 with loss1: 4788.37 and loss2: 0.00\n",
      "Epoch [92], train_loss: 5193.71 with loss1: 5193.71 and loss2: 0.00\n",
      "Epoch [93], train_loss: 5309.90 with loss1: 5309.90 and loss2: 0.00\n",
      "Epoch [94], train_loss: 5289.58 with loss1: 5289.58 and loss2: 0.00\n",
      "Epoch [95], train_loss: 6600.70 with loss1: 6600.70 and loss2: 0.00\n",
      "Epoch [96], train_loss: 5307.86 with loss1: 5307.86 and loss2: 0.00\n",
      "Epoch [97], train_loss: 5132.86 with loss1: 5132.86 and loss2: 0.00\n",
      "Epoch [98], train_loss: 6187.98 with loss1: 6187.98 and loss2: 0.00\n",
      "Epoch [99], train_loss: 5942.38 with loss1: 5942.38 and loss2: 0.00\n",
      "Epoch [100], train_loss: 5829.51 with loss1: 5829.51 and loss2: 0.00\n",
      "Epoch [101], train_loss: 5883.82 with loss1: 5883.82 and loss2: 0.00\n",
      "Epoch [102], train_loss: 4827.07 with loss1: 4827.07 and loss2: 0.00\n",
      "Epoch [103], train_loss: 5464.09 with loss1: 5464.09 and loss2: 0.00\n",
      "Epoch [104], train_loss: 4927.26 with loss1: 4927.26 and loss2: 0.00\n",
      "Epoch [105], train_loss: 4572.62 with loss1: 4572.62 and loss2: 0.00\n",
      "Epoch [106], train_loss: 5180.94 with loss1: 5180.94 and loss2: 0.00\n",
      "Epoch [107], train_loss: 4730.67 with loss1: 4730.67 and loss2: 0.00\n",
      "Epoch [108], train_loss: 5406.07 with loss1: 5406.07 and loss2: 0.00\n",
      "Epoch [109], train_loss: 5442.69 with loss1: 5442.69 and loss2: 0.00\n",
      "Epoch [110], train_loss: 6343.37 with loss1: 6343.37 and loss2: 0.00\n",
      "Epoch [111], train_loss: 5735.14 with loss1: 5735.14 and loss2: 0.00\n",
      "Epoch [112], train_loss: 6126.03 with loss1: 6126.03 and loss2: 0.00\n",
      "Epoch [113], train_loss: 4583.74 with loss1: 4583.74 and loss2: 0.00\n",
      "Epoch [114], train_loss: 4548.27 with loss1: 4548.27 and loss2: 0.00\n",
      "Epoch [115], train_loss: 5901.59 with loss1: 5901.59 and loss2: 0.00\n",
      "Epoch [116], train_loss: 5005.39 with loss1: 5005.39 and loss2: 0.00\n",
      "Epoch [117], train_loss: 5044.20 with loss1: 5044.20 and loss2: 0.00\n",
      "Epoch [118], train_loss: 6035.49 with loss1: 6035.49 and loss2: 0.00\n",
      "Epoch [119], train_loss: 5972.89 with loss1: 5972.89 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120], train_loss: 5095.67 with loss1: 5095.67 and loss2: 0.00\n",
      "Epoch [121], train_loss: 5715.13 with loss1: 5715.13 and loss2: 0.00\n",
      "Epoch [122], train_loss: 5749.95 with loss1: 5749.95 and loss2: 0.00\n",
      "Epoch [123], train_loss: 4935.41 with loss1: 4935.41 and loss2: 0.00\n",
      "Epoch [124], train_loss: 5997.08 with loss1: 5997.08 and loss2: 0.00\n",
      "Epoch [125], train_loss: 5864.33 with loss1: 5864.33 and loss2: 0.00\n",
      "Epoch [126], train_loss: 5805.13 with loss1: 5805.13 and loss2: 0.00\n",
      "Epoch [127], train_loss: 5032.26 with loss1: 5032.26 and loss2: 0.00\n",
      "Epoch [128], train_loss: 4370.32 with loss1: 4370.32 and loss2: 0.00\n",
      "Epoch [129], train_loss: 4729.99 with loss1: 4729.99 and loss2: 0.00\n",
      "Epoch [130], train_loss: 4303.10 with loss1: 4303.10 and loss2: 0.00\n",
      "Epoch [131], train_loss: 4653.32 with loss1: 4653.32 and loss2: 0.00\n",
      "Epoch [132], train_loss: 4919.67 with loss1: 4919.67 and loss2: 0.00\n",
      "Epoch [133], train_loss: 5038.60 with loss1: 5038.60 and loss2: 0.00\n",
      "Epoch [134], train_loss: 5623.49 with loss1: 5623.49 and loss2: 0.00\n",
      "Epoch [135], train_loss: 5595.30 with loss1: 5595.30 and loss2: 0.00\n",
      "Epoch [136], train_loss: 4419.61 with loss1: 4419.61 and loss2: 0.00\n",
      "Epoch [137], train_loss: 4707.25 with loss1: 4707.25 and loss2: 0.00\n",
      "Epoch [138], train_loss: 5768.67 with loss1: 5768.67 and loss2: 0.00\n",
      "Epoch [139], train_loss: 5597.28 with loss1: 5597.28 and loss2: 0.00\n",
      "Epoch [140], train_loss: 5461.30 with loss1: 5461.30 and loss2: 0.00\n",
      "Epoch [141], train_loss: 5441.91 with loss1: 5441.91 and loss2: 0.00\n",
      "Epoch [142], train_loss: 5483.51 with loss1: 5483.51 and loss2: 0.00\n",
      "Epoch [143], train_loss: 5602.99 with loss1: 5602.99 and loss2: 0.00\n",
      "Epoch [144], train_loss: 4288.35 with loss1: 4288.35 and loss2: 0.00\n",
      "Epoch [145], train_loss: 4721.66 with loss1: 4721.66 and loss2: 0.00\n",
      "Epoch [146], train_loss: 4868.05 with loss1: 4868.05 and loss2: 0.00\n",
      "Epoch [147], train_loss: 5663.14 with loss1: 5663.14 and loss2: 0.00\n",
      "Epoch [148], train_loss: 5404.42 with loss1: 5404.42 and loss2: 0.00\n",
      "Epoch [149], train_loss: 4825.46 with loss1: 4825.46 and loss2: 0.00\n",
      "Epoch [150], train_loss: 4720.40 with loss1: 4720.40 and loss2: 0.00\n",
      "Epoch [151], train_loss: 4128.49 with loss1: 4128.49 and loss2: 0.00\n",
      "Epoch [152], train_loss: 4533.44 with loss1: 4533.44 and loss2: 0.00\n",
      "Epoch [153], train_loss: 4692.71 with loss1: 4692.71 and loss2: 0.00\n",
      "Epoch [154], train_loss: 5438.81 with loss1: 5438.81 and loss2: 0.00\n",
      "Epoch [155], train_loss: 4292.73 with loss1: 4292.73 and loss2: 0.00\n",
      "Epoch [156], train_loss: 4502.60 with loss1: 4502.60 and loss2: 0.00\n",
      "Epoch [157], train_loss: 4434.28 with loss1: 4434.28 and loss2: 0.00\n",
      "Epoch [158], train_loss: 4668.39 with loss1: 4668.39 and loss2: 0.00\n",
      "Epoch [159], train_loss: 5576.94 with loss1: 5576.94 and loss2: 0.00\n",
      "Epoch [160], train_loss: 5284.29 with loss1: 5284.29 and loss2: 0.00\n",
      "Epoch [161], train_loss: 4685.85 with loss1: 4685.85 and loss2: 0.00\n",
      "Epoch [162], train_loss: 4704.99 with loss1: 4704.99 and loss2: 0.00\n",
      "Epoch [163], train_loss: 5458.45 with loss1: 5458.45 and loss2: 0.00\n",
      "Epoch [164], train_loss: 5164.42 with loss1: 5164.42 and loss2: 0.00\n",
      "Epoch [165], train_loss: 5158.52 with loss1: 5158.52 and loss2: 0.00\n",
      "Epoch [166], train_loss: 4674.09 with loss1: 4674.09 and loss2: 0.00\n",
      "Epoch [167], train_loss: 4629.64 with loss1: 4629.64 and loss2: 0.00\n",
      "Epoch [168], train_loss: 5238.83 with loss1: 5238.83 and loss2: 0.00\n",
      "Epoch [169], train_loss: 4492.25 with loss1: 4492.25 and loss2: 0.00\n",
      "Epoch [170], train_loss: 4524.48 with loss1: 4524.48 and loss2: 0.00\n",
      "Epoch [171], train_loss: 4523.94 with loss1: 4523.94 and loss2: 0.00\n",
      "Epoch [172], train_loss: 4484.84 with loss1: 4484.84 and loss2: 0.00\n",
      "Epoch [173], train_loss: 4370.06 with loss1: 4370.06 and loss2: 0.00\n",
      "Epoch [174], train_loss: 3978.48 with loss1: 3978.48 and loss2: 0.00\n",
      "Epoch [175], train_loss: 5155.84 with loss1: 5155.84 and loss2: 0.00\n",
      "Epoch [176], train_loss: 3947.80 with loss1: 3947.80 and loss2: 0.00\n",
      "Epoch [177], train_loss: 4264.29 with loss1: 4264.29 and loss2: 0.00\n",
      "Epoch [178], train_loss: 4223.78 with loss1: 4223.78 and loss2: 0.00\n",
      "Epoch [179], train_loss: 5257.41 with loss1: 5257.41 and loss2: 0.00\n",
      "Epoch [180], train_loss: 3897.40 with loss1: 3897.40 and loss2: 0.00\n",
      "Epoch [181], train_loss: 5053.10 with loss1: 5053.10 and loss2: 0.00\n",
      "Epoch [182], train_loss: 4450.16 with loss1: 4450.16 and loss2: 0.00\n",
      "Epoch [183], train_loss: 4202.04 with loss1: 4202.04 and loss2: 0.00\n",
      "Epoch [184], train_loss: 5178.35 with loss1: 5178.35 and loss2: 0.00\n",
      "Epoch [185], train_loss: 4307.26 with loss1: 4307.26 and loss2: 0.00\n",
      "Epoch [186], train_loss: 5053.31 with loss1: 5053.31 and loss2: 0.00\n",
      "Epoch [187], train_loss: 4404.96 with loss1: 4404.96 and loss2: 0.00\n",
      "Epoch [188], train_loss: 4351.92 with loss1: 4351.92 and loss2: 0.00\n",
      "Epoch [189], train_loss: 4974.60 with loss1: 4974.60 and loss2: 0.00\n",
      "Epoch [190], train_loss: 4946.50 with loss1: 4946.50 and loss2: 0.00\n",
      "Epoch [191], train_loss: 4845.57 with loss1: 4845.57 and loss2: 0.00\n",
      "Epoch [192], train_loss: 4376.72 with loss1: 4376.72 and loss2: 0.00\n",
      "Epoch [193], train_loss: 4119.77 with loss1: 4119.77 and loss2: 0.00\n",
      "Epoch [194], train_loss: 3978.71 with loss1: 3978.71 and loss2: 0.00\n",
      "Epoch [195], train_loss: 4237.48 with loss1: 4237.48 and loss2: 0.00\n",
      "Epoch [196], train_loss: 4204.62 with loss1: 4204.62 and loss2: 0.00\n",
      "Epoch [197], train_loss: 4799.02 with loss1: 4799.02 and loss2: 0.00\n",
      "Epoch [198], train_loss: 4802.22 with loss1: 4802.22 and loss2: 0.00\n",
      "Epoch [199], train_loss: 3708.00 with loss1: 3708.00 and loss2: 0.00\n",
      "Epoch [200], train_loss: 4719.67 with loss1: 4719.67 and loss2: 0.00\n",
      "Epoch [201], train_loss: 4182.99 with loss1: 4182.99 and loss2: 0.00\n",
      "Epoch [202], train_loss: 4011.82 with loss1: 4011.82 and loss2: 0.00\n",
      "Epoch [203], train_loss: 3575.35 with loss1: 3575.35 and loss2: 0.00\n",
      "Epoch [204], train_loss: 3943.80 with loss1: 3943.80 and loss2: 0.00\n",
      "Epoch [205], train_loss: 4655.78 with loss1: 4655.78 and loss2: 0.00\n",
      "Epoch [206], train_loss: 3828.10 with loss1: 3828.10 and loss2: 0.00\n",
      "Epoch [207], train_loss: 3649.12 with loss1: 3649.12 and loss2: 0.00\n",
      "Epoch [208], train_loss: 3584.83 with loss1: 3584.83 and loss2: 0.00\n",
      "Epoch [209], train_loss: 4622.56 with loss1: 4622.56 and loss2: 0.00\n",
      "Epoch [210], train_loss: 4136.39 with loss1: 4136.39 and loss2: 0.00\n",
      "Epoch [211], train_loss: 3993.61 with loss1: 3993.61 and loss2: 0.00\n",
      "Epoch [212], train_loss: 4561.66 with loss1: 4561.66 and loss2: 0.00\n",
      "Epoch [213], train_loss: 3969.80 with loss1: 3969.80 and loss2: 0.00\n",
      "Epoch [214], train_loss: 3526.39 with loss1: 3526.39 and loss2: 0.00\n",
      "Epoch [215], train_loss: 3670.42 with loss1: 3670.42 and loss2: 0.00\n",
      "Epoch [216], train_loss: 3967.67 with loss1: 3967.67 and loss2: 0.00\n",
      "Epoch [217], train_loss: 4352.71 with loss1: 4352.71 and loss2: 0.00\n",
      "Epoch [218], train_loss: 4418.92 with loss1: 4418.92 and loss2: 0.00\n",
      "Epoch [219], train_loss: 3923.21 with loss1: 3923.21 and loss2: 0.00\n",
      "Epoch [220], train_loss: 4276.75 with loss1: 4276.75 and loss2: 0.00\n",
      "Epoch [221], train_loss: 3849.43 with loss1: 3849.43 and loss2: 0.00\n",
      "Epoch [222], train_loss: 3443.13 with loss1: 3443.13 and loss2: 0.00\n",
      "Epoch [223], train_loss: 4357.49 with loss1: 4357.49 and loss2: 0.00\n",
      "Epoch [224], train_loss: 3445.75 with loss1: 3445.75 and loss2: 0.00\n",
      "Epoch [225], train_loss: 3860.12 with loss1: 3860.12 and loss2: 0.00\n",
      "Epoch [226], train_loss: 3784.94 with loss1: 3784.94 and loss2: 0.00\n",
      "Epoch [227], train_loss: 3744.39 with loss1: 3744.39 and loss2: 0.00\n",
      "Epoch [228], train_loss: 3411.33 with loss1: 3411.33 and loss2: 0.00\n",
      "Epoch [229], train_loss: 4133.33 with loss1: 4133.33 and loss2: 0.00\n",
      "Epoch [230], train_loss: 3729.81 with loss1: 3729.81 and loss2: 0.00\n",
      "Epoch [231], train_loss: 4098.17 with loss1: 4098.17 and loss2: 0.00\n",
      "Epoch [232], train_loss: 3290.89 with loss1: 3290.89 and loss2: 0.00\n",
      "Epoch [233], train_loss: 3848.13 with loss1: 3848.13 and loss2: 0.00\n",
      "Epoch [234], train_loss: 3673.23 with loss1: 3673.23 and loss2: 0.00\n",
      "Epoch [235], train_loss: 3673.34 with loss1: 3673.34 and loss2: 0.00\n",
      "Epoch [236], train_loss: 3671.11 with loss1: 3671.11 and loss2: 0.00\n",
      "Epoch [237], train_loss: 3754.42 with loss1: 3754.42 and loss2: 0.00\n",
      "Epoch [238], train_loss: 3867.89 with loss1: 3867.89 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [239], train_loss: 3220.87 with loss1: 3220.87 and loss2: 0.00\n",
      "Epoch [240], train_loss: 3639.11 with loss1: 3639.11 and loss2: 0.00\n",
      "Epoch [241], train_loss: 3834.82 with loss1: 3834.82 and loss2: 0.00\n",
      "Epoch [242], train_loss: 3907.32 with loss1: 3907.32 and loss2: 0.00\n",
      "Epoch [243], train_loss: 3867.67 with loss1: 3867.67 and loss2: 0.00\n",
      "Epoch [244], train_loss: 3052.97 with loss1: 3052.97 and loss2: 0.00\n",
      "Epoch [245], train_loss: 3840.29 with loss1: 3840.29 and loss2: 0.00\n",
      "Epoch [246], train_loss: 3107.24 with loss1: 3107.24 and loss2: 0.00\n",
      "Epoch [247], train_loss: 3693.08 with loss1: 3693.08 and loss2: 0.00\n",
      "Epoch [248], train_loss: 3480.24 with loss1: 3480.24 and loss2: 0.00\n",
      "Epoch [249], train_loss: 3157.13 with loss1: 3157.13 and loss2: 0.00\n",
      "Epoch [250], train_loss: 3020.33 with loss1: 3020.33 and loss2: 0.00\n",
      "Epoch [251], train_loss: 2962.47 with loss1: 2962.47 and loss2: 0.00\n",
      "Epoch [252], train_loss: 3736.32 with loss1: 3736.32 and loss2: 0.00\n",
      "Epoch [253], train_loss: 3412.07 with loss1: 3412.07 and loss2: 0.00\n",
      "Epoch [254], train_loss: 3543.28 with loss1: 3543.28 and loss2: 0.00\n",
      "Epoch [255], train_loss: 3859.16 with loss1: 3859.16 and loss2: 0.00\n",
      "Epoch [256], train_loss: 3132.74 with loss1: 3132.74 and loss2: 0.00\n",
      "Epoch [257], train_loss: 3050.47 with loss1: 3050.47 and loss2: 0.00\n",
      "Epoch [258], train_loss: 3524.87 with loss1: 3524.87 and loss2: 0.00\n",
      "Epoch [259], train_loss: 3707.29 with loss1: 3707.29 and loss2: 0.00\n",
      "Epoch [260], train_loss: 3068.10 with loss1: 3068.10 and loss2: 0.00\n",
      "Epoch [261], train_loss: 2931.71 with loss1: 2931.71 and loss2: 0.00\n",
      "Epoch [262], train_loss: 3041.48 with loss1: 3041.48 and loss2: 0.00\n",
      "Epoch [263], train_loss: 3566.21 with loss1: 3566.21 and loss2: 0.00\n",
      "Epoch [264], train_loss: 3602.98 with loss1: 3602.98 and loss2: 0.00\n",
      "Epoch [265], train_loss: 2857.51 with loss1: 2857.51 and loss2: 0.00\n",
      "Epoch [266], train_loss: 3525.88 with loss1: 3525.88 and loss2: 0.00\n",
      "Epoch [267], train_loss: 3521.80 with loss1: 3521.80 and loss2: 0.00\n",
      "Epoch [268], train_loss: 2799.94 with loss1: 2799.94 and loss2: 0.00\n",
      "Epoch [269], train_loss: 3355.54 with loss1: 3355.54 and loss2: 0.00\n",
      "Epoch [270], train_loss: 3436.74 with loss1: 3436.74 and loss2: 0.00\n",
      "Epoch [271], train_loss: 3553.12 with loss1: 3553.12 and loss2: 0.00\n",
      "Epoch [272], train_loss: 3304.37 with loss1: 3304.37 and loss2: 0.00\n",
      "Epoch [273], train_loss: 3493.00 with loss1: 3493.00 and loss2: 0.00\n",
      "Epoch [274], train_loss: 3181.24 with loss1: 3181.24 and loss2: 0.00\n",
      "Epoch [275], train_loss: 3638.63 with loss1: 3638.63 and loss2: 0.00\n",
      "Epoch [276], train_loss: 3471.77 with loss1: 3471.77 and loss2: 0.00\n",
      "Epoch [277], train_loss: 3420.53 with loss1: 3420.53 and loss2: 0.00\n",
      "Epoch [278], train_loss: 2718.89 with loss1: 2718.89 and loss2: 0.00\n",
      "Epoch [279], train_loss: 3448.80 with loss1: 3448.80 and loss2: 0.00\n",
      "Epoch [280], train_loss: 2659.48 with loss1: 2659.48 and loss2: 0.00\n",
      "Epoch [281], train_loss: 3198.07 with loss1: 3198.07 and loss2: 0.00\n",
      "Epoch [282], train_loss: 3214.70 with loss1: 3214.70 and loss2: 0.00\n",
      "Epoch [283], train_loss: 2674.16 with loss1: 2674.16 and loss2: 0.00\n",
      "Epoch [284], train_loss: 3168.86 with loss1: 3168.86 and loss2: 0.00\n",
      "Epoch [285], train_loss: 3286.72 with loss1: 3286.72 and loss2: 0.00\n",
      "Epoch [286], train_loss: 3247.27 with loss1: 3247.27 and loss2: 0.00\n",
      "Epoch [287], train_loss: 3133.53 with loss1: 3133.53 and loss2: 0.00\n",
      "Epoch [288], train_loss: 3053.33 with loss1: 3053.33 and loss2: 0.00\n",
      "Epoch [289], train_loss: 3209.01 with loss1: 3209.01 and loss2: 0.00\n",
      "Epoch [290], train_loss: 2570.71 with loss1: 2570.71 and loss2: 0.00\n",
      "Epoch [291], train_loss: 2822.92 with loss1: 2822.92 and loss2: 0.00\n",
      "Epoch [292], train_loss: 3343.95 with loss1: 3343.95 and loss2: 0.00\n",
      "Epoch [293], train_loss: 3041.48 with loss1: 3041.48 and loss2: 0.00\n",
      "Epoch [294], train_loss: 3233.82 with loss1: 3233.82 and loss2: 0.00\n",
      "Epoch [295], train_loss: 3294.91 with loss1: 3294.91 and loss2: 0.00\n",
      "Epoch [296], train_loss: 3203.40 with loss1: 3203.40 and loss2: 0.00\n",
      "Epoch [297], train_loss: 2534.72 with loss1: 2534.72 and loss2: 0.00\n",
      "Epoch [298], train_loss: 3293.50 with loss1: 3293.50 and loss2: 0.00\n",
      "Epoch [299], train_loss: 3127.60 with loss1: 3127.60 and loss2: 0.00\n",
      "Epoch [300], train_loss: 3054.03 with loss1: 3054.03 and loss2: 0.00\n",
      "Epoch [301], train_loss: 3167.29 with loss1: 3167.29 and loss2: 0.00\n",
      "Epoch [302], train_loss: 3346.16 with loss1: 3346.16 and loss2: 0.00\n",
      "Epoch [303], train_loss: 3214.40 with loss1: 3214.40 and loss2: 0.00\n",
      "Epoch [304], train_loss: 3275.33 with loss1: 3275.33 and loss2: 0.00\n",
      "Epoch [305], train_loss: 3177.87 with loss1: 3177.87 and loss2: 0.00\n",
      "Epoch [306], train_loss: 3065.56 with loss1: 3065.56 and loss2: 0.00\n",
      "Epoch [307], train_loss: 2758.43 with loss1: 2758.43 and loss2: 0.00\n",
      "Epoch [308], train_loss: 2980.42 with loss1: 2980.42 and loss2: 0.00\n",
      "Epoch [309], train_loss: 3231.75 with loss1: 3231.75 and loss2: 0.00\n",
      "Epoch [310], train_loss: 2980.71 with loss1: 2980.71 and loss2: 0.00\n",
      "Epoch [311], train_loss: 3245.28 with loss1: 3245.28 and loss2: 0.00\n",
      "Epoch [312], train_loss: 3183.32 with loss1: 3183.32 and loss2: 0.00\n",
      "Epoch [313], train_loss: 2452.89 with loss1: 2452.89 and loss2: 0.00\n",
      "Epoch [314], train_loss: 2957.45 with loss1: 2957.45 and loss2: 0.00\n",
      "Epoch [315], train_loss: 2422.28 with loss1: 2422.28 and loss2: 0.00\n",
      "Epoch [316], train_loss: 2996.41 with loss1: 2996.41 and loss2: 0.00\n",
      "Epoch [317], train_loss: 2439.05 with loss1: 2439.05 and loss2: 0.00\n",
      "Epoch [318], train_loss: 3002.79 with loss1: 3002.79 and loss2: 0.00\n",
      "Epoch [319], train_loss: 2648.67 with loss1: 2648.67 and loss2: 0.00\n",
      "Epoch [320], train_loss: 2447.48 with loss1: 2447.48 and loss2: 0.00\n",
      "Epoch [321], train_loss: 2379.79 with loss1: 2379.79 and loss2: 0.00\n",
      "Epoch [322], train_loss: 2926.18 with loss1: 2926.18 and loss2: 0.00\n",
      "Epoch [323], train_loss: 3168.95 with loss1: 3168.95 and loss2: 0.00\n",
      "Epoch [324], train_loss: 3139.65 with loss1: 3139.65 and loss2: 0.00\n",
      "Epoch [325], train_loss: 3023.51 with loss1: 3023.51 and loss2: 0.00\n",
      "Epoch [326], train_loss: 2968.40 with loss1: 2968.40 and loss2: 0.00\n",
      "Epoch [327], train_loss: 3133.82 with loss1: 3133.82 and loss2: 0.00\n",
      "Epoch [328], train_loss: 2375.53 with loss1: 2375.53 and loss2: 0.00\n",
      "Epoch [329], train_loss: 2680.33 with loss1: 2680.33 and loss2: 0.00\n",
      "Epoch [330], train_loss: 2874.66 with loss1: 2874.66 and loss2: 0.00\n",
      "Epoch [331], train_loss: 2356.39 with loss1: 2356.39 and loss2: 0.00\n",
      "Epoch [332], train_loss: 2826.83 with loss1: 2826.83 and loss2: 0.00\n",
      "Epoch [333], train_loss: 2598.31 with loss1: 2598.31 and loss2: 0.00\n",
      "Epoch [334], train_loss: 2796.27 with loss1: 2796.27 and loss2: 0.00\n",
      "Epoch [335], train_loss: 2872.68 with loss1: 2872.68 and loss2: 0.00\n",
      "Epoch [336], train_loss: 2748.68 with loss1: 2748.68 and loss2: 0.00\n",
      "Epoch [337], train_loss: 2567.99 with loss1: 2567.99 and loss2: 0.00\n",
      "Epoch [338], train_loss: 3056.42 with loss1: 3056.42 and loss2: 0.00\n",
      "Epoch [339], train_loss: 2737.19 with loss1: 2737.19 and loss2: 0.00\n",
      "Epoch [340], train_loss: 2790.25 with loss1: 2790.25 and loss2: 0.00\n",
      "Epoch [341], train_loss: 2881.71 with loss1: 2881.71 and loss2: 0.00\n",
      "Epoch [342], train_loss: 2738.57 with loss1: 2738.57 and loss2: 0.00\n",
      "Epoch [343], train_loss: 2959.08 with loss1: 2959.08 and loss2: 0.00\n",
      "Epoch [344], train_loss: 2873.45 with loss1: 2873.45 and loss2: 0.00\n",
      "Epoch [345], train_loss: 2609.32 with loss1: 2609.32 and loss2: 0.00\n",
      "Epoch [346], train_loss: 2942.25 with loss1: 2942.25 and loss2: 0.00\n",
      "Epoch [347], train_loss: 2953.93 with loss1: 2953.93 and loss2: 0.00\n",
      "Epoch [348], train_loss: 2523.82 with loss1: 2523.82 and loss2: 0.00\n",
      "Epoch [349], train_loss: 2487.03 with loss1: 2487.03 and loss2: 0.00\n",
      "Epoch [350], train_loss: 2871.50 with loss1: 2871.50 and loss2: 0.00\n",
      "Epoch [351], train_loss: 2269.51 with loss1: 2269.51 and loss2: 0.00\n",
      "Epoch [352], train_loss: 2709.66 with loss1: 2709.66 and loss2: 0.00\n",
      "Epoch [353], train_loss: 2501.05 with loss1: 2501.05 and loss2: 0.00\n",
      "Epoch [354], train_loss: 2237.69 with loss1: 2237.69 and loss2: 0.00\n",
      "Epoch [355], train_loss: 2717.62 with loss1: 2717.62 and loss2: 0.00\n",
      "Epoch [356], train_loss: 2916.36 with loss1: 2916.36 and loss2: 0.00\n",
      "Epoch [357], train_loss: 2796.53 with loss1: 2796.53 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [358], train_loss: 2212.80 with loss1: 2212.80 and loss2: 0.00\n",
      "Epoch [359], train_loss: 2493.83 with loss1: 2493.83 and loss2: 0.00\n",
      "Epoch [360], train_loss: 2730.97 with loss1: 2730.97 and loss2: 0.00\n",
      "Epoch [361], train_loss: 2878.26 with loss1: 2878.26 and loss2: 0.00\n",
      "Epoch [362], train_loss: 2466.51 with loss1: 2466.51 and loss2: 0.00\n",
      "Epoch [363], train_loss: 2658.09 with loss1: 2658.09 and loss2: 0.00\n",
      "Epoch [364], train_loss: 2754.61 with loss1: 2754.61 and loss2: 0.00\n",
      "Epoch [365], train_loss: 2885.03 with loss1: 2885.03 and loss2: 0.00\n",
      "Epoch [366], train_loss: 2174.73 with loss1: 2174.73 and loss2: 0.00\n",
      "Epoch [367], train_loss: 2152.87 with loss1: 2152.87 and loss2: 0.00\n",
      "Epoch [368], train_loss: 2663.71 with loss1: 2663.71 and loss2: 0.00\n",
      "Epoch [369], train_loss: 2722.20 with loss1: 2722.20 and loss2: 0.00\n",
      "Epoch [370], train_loss: 2880.64 with loss1: 2880.64 and loss2: 0.00\n",
      "Epoch [371], train_loss: 2560.75 with loss1: 2560.75 and loss2: 0.00\n",
      "Epoch [372], train_loss: 2734.05 with loss1: 2734.05 and loss2: 0.00\n",
      "Epoch [373], train_loss: 2555.34 with loss1: 2555.34 and loss2: 0.00\n",
      "Epoch [374], train_loss: 2950.74 with loss1: 2950.74 and loss2: 0.00\n",
      "Epoch [375], train_loss: 2729.07 with loss1: 2729.07 and loss2: 0.00\n",
      "Epoch [376], train_loss: 2712.62 with loss1: 2712.62 and loss2: 0.00\n",
      "Epoch [377], train_loss: 2423.69 with loss1: 2423.69 and loss2: 0.00\n",
      "Epoch [378], train_loss: 2731.02 with loss1: 2731.02 and loss2: 0.00\n",
      "Epoch [379], train_loss: 2672.34 with loss1: 2672.34 and loss2: 0.00\n",
      "Epoch [380], train_loss: 2137.85 with loss1: 2137.85 and loss2: 0.00\n",
      "Epoch [381], train_loss: 2098.64 with loss1: 2098.64 and loss2: 0.00\n",
      "Epoch [382], train_loss: 2589.30 with loss1: 2589.30 and loss2: 0.00\n",
      "Epoch [383], train_loss: 2124.78 with loss1: 2124.78 and loss2: 0.00\n",
      "Epoch [384], train_loss: 2706.29 with loss1: 2706.29 and loss2: 0.00\n",
      "Epoch [385], train_loss: 2550.07 with loss1: 2550.07 and loss2: 0.00\n",
      "Epoch [386], train_loss: 2620.67 with loss1: 2620.67 and loss2: 0.00\n",
      "Epoch [387], train_loss: 2408.80 with loss1: 2408.80 and loss2: 0.00\n",
      "Epoch [388], train_loss: 2527.13 with loss1: 2527.13 and loss2: 0.00\n",
      "Epoch [389], train_loss: 2903.11 with loss1: 2903.11 and loss2: 0.00\n",
      "Epoch [390], train_loss: 2597.55 with loss1: 2597.55 and loss2: 0.00\n",
      "Epoch [391], train_loss: 2351.37 with loss1: 2351.37 and loss2: 0.00\n",
      "Epoch [392], train_loss: 2536.77 with loss1: 2536.77 and loss2: 0.00\n",
      "Epoch [393], train_loss: 2710.28 with loss1: 2710.28 and loss2: 0.00\n",
      "Epoch [394], train_loss: 2622.81 with loss1: 2622.81 and loss2: 0.00\n",
      "Epoch [395], train_loss: 2758.43 with loss1: 2758.43 and loss2: 0.00\n",
      "Epoch [396], train_loss: 2604.28 with loss1: 2604.28 and loss2: 0.00\n",
      "Epoch [397], train_loss: 2815.34 with loss1: 2815.34 and loss2: 0.00\n",
      "Epoch [398], train_loss: 2579.42 with loss1: 2579.42 and loss2: 0.00\n",
      "Epoch [399], train_loss: 2683.79 with loss1: 2683.79 and loss2: 0.00\n",
      "Epoch [400], train_loss: 2723.61 with loss1: 2723.61 and loss2: 0.00\n",
      "Epoch [401], train_loss: 2067.00 with loss1: 2067.00 and loss2: 0.00\n",
      "Epoch [402], train_loss: 2030.74 with loss1: 2030.74 and loss2: 0.00\n",
      "Epoch [403], train_loss: 2027.60 with loss1: 2027.60 and loss2: 0.00\n",
      "Epoch [404], train_loss: 2488.20 with loss1: 2488.20 and loss2: 0.00\n",
      "Epoch [405], train_loss: 2468.69 with loss1: 2468.69 and loss2: 0.00\n",
      "Epoch [406], train_loss: 2029.35 with loss1: 2029.35 and loss2: 0.00\n",
      "Epoch [407], train_loss: 2550.58 with loss1: 2550.58 and loss2: 0.00\n",
      "Epoch [408], train_loss: 2614.05 with loss1: 2614.05 and loss2: 0.00\n",
      "Epoch [409], train_loss: 2669.99 with loss1: 2669.99 and loss2: 0.00\n",
      "Epoch [410], train_loss: 2402.44 with loss1: 2402.44 and loss2: 0.00\n",
      "Epoch [411], train_loss: 2619.43 with loss1: 2619.43 and loss2: 0.00\n",
      "Epoch [412], train_loss: 2417.82 with loss1: 2417.82 and loss2: 0.00\n",
      "Epoch [413], train_loss: 2529.80 with loss1: 2529.80 and loss2: 0.00\n",
      "Epoch [414], train_loss: 2431.91 with loss1: 2431.91 and loss2: 0.00\n",
      "Epoch [415], train_loss: 2083.42 with loss1: 2083.42 and loss2: 0.00\n",
      "Epoch [416], train_loss: 2689.57 with loss1: 2689.57 and loss2: 0.00\n",
      "Epoch [417], train_loss: 2458.49 with loss1: 2458.49 and loss2: 0.00\n",
      "Epoch [418], train_loss: 2364.92 with loss1: 2364.92 and loss2: 0.00\n",
      "Epoch [419], train_loss: 2785.47 with loss1: 2785.47 and loss2: 0.00\n",
      "Epoch [420], train_loss: 2535.35 with loss1: 2535.35 and loss2: 0.00\n",
      "Epoch [421], train_loss: 2738.89 with loss1: 2738.89 and loss2: 0.00\n",
      "Epoch [422], train_loss: 2716.68 with loss1: 2716.68 and loss2: 0.00\n",
      "Epoch [423], train_loss: 2366.06 with loss1: 2366.06 and loss2: 0.00\n",
      "Epoch [424], train_loss: 2347.62 with loss1: 2347.62 and loss2: 0.00\n",
      "Epoch [425], train_loss: 2423.21 with loss1: 2423.21 and loss2: 0.00\n",
      "Epoch [426], train_loss: 2511.41 with loss1: 2511.41 and loss2: 0.00\n",
      "Epoch [427], train_loss: 2006.49 with loss1: 2006.49 and loss2: 0.00\n",
      "Epoch [428], train_loss: 2426.35 with loss1: 2426.35 and loss2: 0.00\n",
      "Epoch [429], train_loss: 2588.26 with loss1: 2588.26 and loss2: 0.00\n",
      "Epoch [430], train_loss: 2674.25 with loss1: 2674.25 and loss2: 0.00\n",
      "Epoch [431], train_loss: 2349.42 with loss1: 2349.42 and loss2: 0.00\n",
      "Epoch [432], train_loss: 2336.55 with loss1: 2336.55 and loss2: 0.00\n",
      "Epoch [433], train_loss: 2486.93 with loss1: 2486.93 and loss2: 0.00\n",
      "Epoch [434], train_loss: 2640.63 with loss1: 2640.63 and loss2: 0.00\n",
      "Epoch [435], train_loss: 2446.29 with loss1: 2446.29 and loss2: 0.00\n",
      "Epoch [436], train_loss: 2441.05 with loss1: 2441.05 and loss2: 0.00\n",
      "Epoch [437], train_loss: 2447.88 with loss1: 2447.88 and loss2: 0.00\n",
      "Epoch [438], train_loss: 2399.76 with loss1: 2399.76 and loss2: 0.00\n",
      "Epoch [439], train_loss: 2229.93 with loss1: 2229.93 and loss2: 0.00\n",
      "Epoch [440], train_loss: 1965.83 with loss1: 1965.83 and loss2: 0.00\n",
      "Epoch [441], train_loss: 2412.96 with loss1: 2412.96 and loss2: 0.00\n",
      "Epoch [442], train_loss: 2260.27 with loss1: 2260.27 and loss2: 0.00\n",
      "Epoch [443], train_loss: 1932.85 with loss1: 1932.85 and loss2: 0.00\n",
      "Epoch [444], train_loss: 2374.37 with loss1: 2374.37 and loss2: 0.00\n",
      "Epoch [445], train_loss: 2401.29 with loss1: 2401.29 and loss2: 0.00\n",
      "Epoch [446], train_loss: 2184.37 with loss1: 2184.37 and loss2: 0.00\n",
      "Epoch [447], train_loss: 2302.52 with loss1: 2302.52 and loss2: 0.00\n",
      "Epoch [448], train_loss: 2405.35 with loss1: 2405.35 and loss2: 0.00\n",
      "Epoch [449], train_loss: 2484.69 with loss1: 2484.69 and loss2: 0.00\n",
      "Epoch [450], train_loss: 1948.06 with loss1: 1948.06 and loss2: 0.00\n",
      "Epoch [451], train_loss: 2173.66 with loss1: 2173.66 and loss2: 0.00\n",
      "Epoch [452], train_loss: 2141.24 with loss1: 2141.24 and loss2: 0.00\n",
      "Epoch [453], train_loss: 2303.09 with loss1: 2303.09 and loss2: 0.00\n",
      "Epoch [454], train_loss: 2190.33 with loss1: 2190.33 and loss2: 0.00\n",
      "Epoch [455], train_loss: 1910.23 with loss1: 1910.23 and loss2: 0.00\n",
      "Epoch [456], train_loss: 2551.88 with loss1: 2551.88 and loss2: 0.00\n",
      "Epoch [457], train_loss: 2298.18 with loss1: 2298.18 and loss2: 0.00\n",
      "Epoch [458], train_loss: 2503.21 with loss1: 2503.21 and loss2: 0.00\n",
      "Epoch [459], train_loss: 1921.70 with loss1: 1921.70 and loss2: 0.00\n",
      "Epoch [460], train_loss: 2153.75 with loss1: 2153.75 and loss2: 0.00\n",
      "Epoch [461], train_loss: 2547.82 with loss1: 2547.82 and loss2: 0.00\n",
      "Epoch [462], train_loss: 2240.28 with loss1: 2240.28 and loss2: 0.00\n",
      "Epoch [463], train_loss: 2145.29 with loss1: 2145.29 and loss2: 0.00\n",
      "Epoch [464], train_loss: 1923.34 with loss1: 1923.34 and loss2: 0.00\n",
      "Epoch [465], train_loss: 1874.14 with loss1: 1874.14 and loss2: 0.00\n",
      "Epoch [466], train_loss: 1900.16 with loss1: 1900.16 and loss2: 0.00\n",
      "Epoch [467], train_loss: 1889.31 with loss1: 1889.31 and loss2: 0.00\n",
      "Epoch [468], train_loss: 2142.22 with loss1: 2142.22 and loss2: 0.00\n",
      "Epoch [469], train_loss: 1859.03 with loss1: 1859.03 and loss2: 0.00\n",
      "Epoch [470], train_loss: 2501.38 with loss1: 2501.38 and loss2: 0.00\n",
      "Epoch [471], train_loss: 2211.58 with loss1: 2211.58 and loss2: 0.00\n",
      "Epoch [472], train_loss: 2418.35 with loss1: 2418.35 and loss2: 0.00\n",
      "Epoch [473], train_loss: 1898.77 with loss1: 1898.77 and loss2: 0.00\n",
      "Epoch [474], train_loss: 2269.19 with loss1: 2269.19 and loss2: 0.00\n",
      "Epoch [475], train_loss: 2207.69 with loss1: 2207.69 and loss2: 0.00\n",
      "Epoch [476], train_loss: 1857.27 with loss1: 1857.27 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [477], train_loss: 2254.31 with loss1: 2254.31 and loss2: 0.00\n",
      "Epoch [478], train_loss: 2203.90 with loss1: 2203.90 and loss2: 0.00\n",
      "Epoch [479], train_loss: 2358.45 with loss1: 2358.45 and loss2: 0.00\n",
      "Epoch [480], train_loss: 2361.07 with loss1: 2361.07 and loss2: 0.00\n",
      "Epoch [481], train_loss: 2092.31 with loss1: 2092.31 and loss2: 0.00\n",
      "Epoch [482], train_loss: 2487.65 with loss1: 2487.65 and loss2: 0.00\n",
      "Epoch [483], train_loss: 2294.98 with loss1: 2294.98 and loss2: 0.00\n",
      "Epoch [484], train_loss: 2523.07 with loss1: 2523.07 and loss2: 0.00\n",
      "Epoch [485], train_loss: 2306.03 with loss1: 2306.03 and loss2: 0.00\n",
      "Epoch [486], train_loss: 2070.56 with loss1: 2070.56 and loss2: 0.00\n",
      "Epoch [487], train_loss: 1838.69 with loss1: 1838.69 and loss2: 0.00\n",
      "Epoch [488], train_loss: 2448.99 with loss1: 2448.99 and loss2: 0.00\n",
      "Epoch [489], train_loss: 2309.43 with loss1: 2309.43 and loss2: 0.00\n",
      "Epoch [490], train_loss: 1821.79 with loss1: 1821.79 and loss2: 0.00\n",
      "Epoch [491], train_loss: 2243.01 with loss1: 2243.01 and loss2: 0.00\n",
      "Epoch [492], train_loss: 2382.63 with loss1: 2382.63 and loss2: 0.00\n",
      "Epoch [493], train_loss: 2248.69 with loss1: 2248.69 and loss2: 0.00\n",
      "Epoch [494], train_loss: 2164.74 with loss1: 2164.74 and loss2: 0.00\n",
      "Epoch [495], train_loss: 2286.98 with loss1: 2286.98 and loss2: 0.00\n",
      "Epoch [496], train_loss: 2303.72 with loss1: 2303.72 and loss2: 0.00\n",
      "Epoch [497], train_loss: 1828.65 with loss1: 1828.65 and loss2: 0.00\n",
      "Epoch [498], train_loss: 2411.83 with loss1: 2411.83 and loss2: 0.00\n",
      "Epoch [499], train_loss: 1809.16 with loss1: 1809.16 and loss2: 0.00\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# model version 3 \n",
    "# delete loss2\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history, mu_history = fit(epochs=500, lr=1e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4917f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 2837.28 with loss1: 2837.28 and loss2: 0.00\n",
      "Epoch [1], train_loss: 3926.54 with loss1: 3926.54 and loss2: 0.00\n",
      "Epoch [2], train_loss: 2714.78 with loss1: 2714.78 and loss2: 0.00\n",
      "Epoch [3], train_loss: 1974.15 with loss1: 1974.15 and loss2: 0.00\n",
      "Epoch [4], train_loss: 2070.35 with loss1: 2070.35 and loss2: 0.00\n",
      "Epoch [5], train_loss: 3129.99 with loss1: 3129.99 and loss2: 0.00\n",
      "Epoch [6], train_loss: 2387.63 with loss1: 2387.63 and loss2: 0.00\n",
      "Epoch [7], train_loss: 2321.27 with loss1: 2321.27 and loss2: 0.00\n",
      "Epoch [8], train_loss: 2493.09 with loss1: 2493.09 and loss2: 0.00\n",
      "Epoch [9], train_loss: 2083.81 with loss1: 2083.81 and loss2: 0.00\n",
      "Epoch [10], train_loss: 2485.81 with loss1: 2485.81 and loss2: 0.00\n",
      "Epoch [11], train_loss: 2484.39 with loss1: 2484.39 and loss2: 0.00\n",
      "Epoch [12], train_loss: 2020.65 with loss1: 2020.65 and loss2: 0.00\n",
      "Epoch [13], train_loss: 2239.99 with loss1: 2239.99 and loss2: 0.00\n",
      "Epoch [14], train_loss: 2261.72 with loss1: 2261.72 and loss2: 0.00\n",
      "Epoch [15], train_loss: 2079.23 with loss1: 2079.23 and loss2: 0.00\n",
      "Epoch [16], train_loss: 2527.40 with loss1: 2527.40 and loss2: 0.00\n",
      "Epoch [17], train_loss: 3264.37 with loss1: 3264.37 and loss2: 0.00\n",
      "Epoch [18], train_loss: 5762.97 with loss1: 5762.97 and loss2: 0.00\n",
      "Epoch [19], train_loss: 7417.58 with loss1: 7417.58 and loss2: 0.00\n",
      "Epoch [20], train_loss: 2150.72 with loss1: 2150.72 and loss2: 0.00\n",
      "Epoch [21], train_loss: 2649.53 with loss1: 2649.53 and loss2: 0.00\n",
      "Epoch [22], train_loss: 2108.73 with loss1: 2108.73 and loss2: 0.00\n",
      "Epoch [23], train_loss: 2372.81 with loss1: 2372.81 and loss2: 0.00\n",
      "Epoch [24], train_loss: 2130.37 with loss1: 2130.37 and loss2: 0.00\n",
      "Epoch [25], train_loss: 2287.11 with loss1: 2287.11 and loss2: 0.00\n",
      "Epoch [26], train_loss: 2210.73 with loss1: 2210.73 and loss2: 0.00\n",
      "Epoch [27], train_loss: 2243.96 with loss1: 2243.96 and loss2: 0.00\n",
      "Epoch [28], train_loss: 2137.04 with loss1: 2137.04 and loss2: 0.00\n",
      "Epoch [29], train_loss: 3175.49 with loss1: 3175.49 and loss2: 0.00\n",
      "Epoch [30], train_loss: 4282.86 with loss1: 4282.86 and loss2: 0.00\n",
      "Epoch [31], train_loss: 4402.56 with loss1: 4402.56 and loss2: 0.00\n",
      "Epoch [32], train_loss: 2061.71 with loss1: 2061.71 and loss2: 0.00\n",
      "Epoch [33], train_loss: 2622.48 with loss1: 2622.48 and loss2: 0.00\n",
      "Epoch [34], train_loss: 1973.70 with loss1: 1973.70 and loss2: 0.00\n",
      "Epoch [35], train_loss: 2877.84 with loss1: 2877.84 and loss2: 0.00\n",
      "Epoch [36], train_loss: 2271.04 with loss1: 2271.04 and loss2: 0.00\n",
      "Epoch [37], train_loss: 1835.38 with loss1: 1835.38 and loss2: 0.00\n",
      "Epoch [38], train_loss: 2197.34 with loss1: 2197.34 and loss2: 0.00\n",
      "Epoch [39], train_loss: 2158.22 with loss1: 2158.22 and loss2: 0.00\n",
      "Epoch [40], train_loss: 2598.46 with loss1: 2598.46 and loss2: 0.00\n",
      "Epoch [41], train_loss: 2029.89 with loss1: 2029.89 and loss2: 0.00\n",
      "Epoch [42], train_loss: 2672.57 with loss1: 2672.57 and loss2: 0.00\n",
      "Epoch [43], train_loss: 3327.18 with loss1: 3327.18 and loss2: 0.00\n",
      "Epoch [44], train_loss: 3409.11 with loss1: 3409.11 and loss2: 0.00\n",
      "Epoch [45], train_loss: 3201.37 with loss1: 3201.37 and loss2: 0.00\n",
      "Epoch [46], train_loss: 2170.48 with loss1: 2170.48 and loss2: 0.00\n",
      "Epoch [47], train_loss: 1800.20 with loss1: 1800.20 and loss2: 0.00\n",
      "Epoch [48], train_loss: 1968.44 with loss1: 1968.44 and loss2: 0.00\n",
      "Epoch [49], train_loss: 2578.30 with loss1: 2578.30 and loss2: 0.00\n",
      "Epoch [50], train_loss: 3141.97 with loss1: 3141.97 and loss2: 0.00\n",
      "Epoch [51], train_loss: 1939.42 with loss1: 1939.42 and loss2: 0.00\n",
      "Epoch [52], train_loss: 2274.50 with loss1: 2274.50 and loss2: 0.00\n",
      "Epoch [53], train_loss: 2603.45 with loss1: 2603.45 and loss2: 0.00\n",
      "Epoch [54], train_loss: 2955.92 with loss1: 2955.92 and loss2: 0.00\n",
      "Epoch [55], train_loss: 1915.00 with loss1: 1915.00 and loss2: 0.00\n",
      "Epoch [56], train_loss: 1758.61 with loss1: 1758.61 and loss2: 0.00\n",
      "Epoch [57], train_loss: 1749.27 with loss1: 1749.27 and loss2: 0.00\n",
      "Epoch [58], train_loss: 2270.76 with loss1: 2270.76 and loss2: 0.00\n",
      "Epoch [59], train_loss: 2092.33 with loss1: 2092.33 and loss2: 0.00\n",
      "Epoch [60], train_loss: 2407.90 with loss1: 2407.90 and loss2: 0.00\n",
      "Epoch [61], train_loss: 1942.95 with loss1: 1942.95 and loss2: 0.00\n",
      "Epoch [62], train_loss: 1937.94 with loss1: 1937.94 and loss2: 0.00\n",
      "Epoch [63], train_loss: 1692.33 with loss1: 1692.33 and loss2: 0.00\n",
      "Epoch [64], train_loss: 2182.13 with loss1: 2182.13 and loss2: 0.00\n",
      "Epoch [65], train_loss: 3044.40 with loss1: 3044.40 and loss2: 0.00\n",
      "Epoch [66], train_loss: 3087.58 with loss1: 3087.58 and loss2: 0.00\n",
      "Epoch [67], train_loss: 2945.22 with loss1: 2945.22 and loss2: 0.00\n",
      "Epoch [68], train_loss: 1801.13 with loss1: 1801.13 and loss2: 0.00\n",
      "Epoch [69], train_loss: 2080.58 with loss1: 2080.58 and loss2: 0.00\n",
      "Epoch [70], train_loss: 1889.28 with loss1: 1889.28 and loss2: 0.00\n",
      "Epoch [71], train_loss: 2284.00 with loss1: 2284.00 and loss2: 0.00\n",
      "Epoch [72], train_loss: 1917.20 with loss1: 1917.20 and loss2: 0.00\n",
      "Epoch [73], train_loss: 1728.50 with loss1: 1728.50 and loss2: 0.00\n",
      "Epoch [74], train_loss: 1948.90 with loss1: 1948.90 and loss2: 0.00\n",
      "Epoch [75], train_loss: 1986.17 with loss1: 1986.17 and loss2: 0.00\n",
      "Epoch [76], train_loss: 1988.15 with loss1: 1988.15 and loss2: 0.00\n",
      "Epoch [77], train_loss: 1886.97 with loss1: 1886.97 and loss2: 0.00\n",
      "Epoch [78], train_loss: 1839.77 with loss1: 1839.77 and loss2: 0.00\n",
      "Epoch [79], train_loss: 1595.81 with loss1: 1595.81 and loss2: 0.00\n",
      "Epoch [80], train_loss: 1815.79 with loss1: 1815.79 and loss2: 0.00\n",
      "Epoch [81], train_loss: 1592.76 with loss1: 1592.76 and loss2: 0.00\n",
      "Epoch [82], train_loss: 2196.35 with loss1: 2196.35 and loss2: 0.00\n",
      "Epoch [83], train_loss: 1885.77 with loss1: 1885.77 and loss2: 0.00\n",
      "Epoch [84], train_loss: 2423.91 with loss1: 2423.91 and loss2: 0.00\n",
      "Epoch [85], train_loss: 1914.04 with loss1: 1914.04 and loss2: 0.00\n",
      "Epoch [86], train_loss: 2324.00 with loss1: 2324.00 and loss2: 0.00\n",
      "Epoch [87], train_loss: 2670.06 with loss1: 2670.06 and loss2: 0.00\n",
      "Epoch [88], train_loss: 1787.67 with loss1: 1787.67 and loss2: 0.00\n",
      "Epoch [89], train_loss: 1836.76 with loss1: 1836.76 and loss2: 0.00\n",
      "Epoch [90], train_loss: 2041.38 with loss1: 2041.38 and loss2: 0.00\n",
      "Epoch [91], train_loss: 2575.48 with loss1: 2575.48 and loss2: 0.00\n",
      "Epoch [92], train_loss: 1931.56 with loss1: 1931.56 and loss2: 0.00\n",
      "Epoch [93], train_loss: 1782.73 with loss1: 1782.73 and loss2: 0.00\n",
      "Epoch [94], train_loss: 1802.41 with loss1: 1802.41 and loss2: 0.00\n",
      "Epoch [95], train_loss: 1577.32 with loss1: 1577.32 and loss2: 0.00\n",
      "Epoch [96], train_loss: 2024.95 with loss1: 2024.95 and loss2: 0.00\n",
      "Epoch [97], train_loss: 1858.43 with loss1: 1858.43 and loss2: 0.00\n",
      "Epoch [98], train_loss: 1770.98 with loss1: 1770.98 and loss2: 0.00\n",
      "Epoch [99], train_loss: 2284.87 with loss1: 2284.87 and loss2: 0.00\n",
      "Epoch [100], train_loss: 1902.48 with loss1: 1902.48 and loss2: 0.00\n",
      "Epoch [101], train_loss: 1890.91 with loss1: 1890.91 and loss2: 0.00\n",
      "Epoch [102], train_loss: 2479.00 with loss1: 2479.00 and loss2: 0.00\n",
      "Epoch [103], train_loss: 1887.57 with loss1: 1887.57 and loss2: 0.00\n",
      "Epoch [104], train_loss: 1867.80 with loss1: 1867.80 and loss2: 0.00\n",
      "Epoch [105], train_loss: 1765.00 with loss1: 1765.00 and loss2: 0.00\n",
      "Epoch [106], train_loss: 1681.41 with loss1: 1681.41 and loss2: 0.00\n",
      "Epoch [107], train_loss: 2115.51 with loss1: 2115.51 and loss2: 0.00\n",
      "Epoch [108], train_loss: 1609.30 with loss1: 1609.30 and loss2: 0.00\n",
      "Epoch [109], train_loss: 1929.80 with loss1: 1929.80 and loss2: 0.00\n",
      "Epoch [110], train_loss: 2515.79 with loss1: 2515.79 and loss2: 0.00\n",
      "Epoch [111], train_loss: 2668.53 with loss1: 2668.53 and loss2: 0.00\n",
      "Epoch [112], train_loss: 2401.77 with loss1: 2401.77 and loss2: 0.00\n",
      "Epoch [113], train_loss: 1864.64 with loss1: 1864.64 and loss2: 0.00\n",
      "Epoch [114], train_loss: 2067.25 with loss1: 2067.25 and loss2: 0.00\n",
      "Epoch [115], train_loss: 1692.90 with loss1: 1692.90 and loss2: 0.00\n",
      "Epoch [116], train_loss: 2075.94 with loss1: 2075.94 and loss2: 0.00\n",
      "Epoch [117], train_loss: 1770.48 with loss1: 1770.48 and loss2: 0.00\n",
      "Epoch [118], train_loss: 1879.41 with loss1: 1879.41 and loss2: 0.00\n",
      "Epoch [119], train_loss: 1895.05 with loss1: 1895.05 and loss2: 0.00\n",
      "Epoch [120], train_loss: 1841.80 with loss1: 1841.80 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [121], train_loss: 1719.73 with loss1: 1719.73 and loss2: 0.00\n",
      "Epoch [122], train_loss: 1997.47 with loss1: 1997.47 and loss2: 0.00\n",
      "Epoch [123], train_loss: 1735.01 with loss1: 1735.01 and loss2: 0.00\n",
      "Epoch [124], train_loss: 1887.80 with loss1: 1887.80 and loss2: 0.00\n",
      "Epoch [125], train_loss: 1635.03 with loss1: 1635.03 and loss2: 0.00\n",
      "Epoch [126], train_loss: 1705.54 with loss1: 1705.54 and loss2: 0.00\n",
      "Epoch [127], train_loss: 1925.92 with loss1: 1925.92 and loss2: 0.00\n",
      "Epoch [128], train_loss: 1688.17 with loss1: 1688.17 and loss2: 0.00\n",
      "Epoch [129], train_loss: 1737.19 with loss1: 1737.19 and loss2: 0.00\n",
      "Epoch [130], train_loss: 2497.80 with loss1: 2497.80 and loss2: 0.00\n",
      "Epoch [131], train_loss: 1596.17 with loss1: 1596.17 and loss2: 0.00\n",
      "Epoch [132], train_loss: 1786.45 with loss1: 1786.45 and loss2: 0.00\n",
      "Epoch [133], train_loss: 1683.14 with loss1: 1683.14 and loss2: 0.00\n",
      "Epoch [134], train_loss: 2153.34 with loss1: 2153.34 and loss2: 0.00\n",
      "Epoch [135], train_loss: 2632.32 with loss1: 2632.32 and loss2: 0.00\n",
      "Epoch [136], train_loss: 1810.11 with loss1: 1810.11 and loss2: 0.00\n",
      "Epoch [137], train_loss: 1828.50 with loss1: 1828.50 and loss2: 0.00\n",
      "Epoch [138], train_loss: 1678.23 with loss1: 1678.23 and loss2: 0.00\n",
      "Epoch [139], train_loss: 1666.85 with loss1: 1666.85 and loss2: 0.00\n",
      "Epoch [140], train_loss: 1808.71 with loss1: 1808.71 and loss2: 0.00\n",
      "Epoch [141], train_loss: 1838.50 with loss1: 1838.50 and loss2: 0.00\n",
      "Epoch [142], train_loss: 1455.05 with loss1: 1455.05 and loss2: 0.00\n",
      "Epoch [143], train_loss: 1834.18 with loss1: 1834.18 and loss2: 0.00\n",
      "Epoch [144], train_loss: 2453.38 with loss1: 2453.38 and loss2: 0.00\n",
      "Epoch [145], train_loss: 1789.86 with loss1: 1789.86 and loss2: 0.00\n",
      "Epoch [146], train_loss: 1496.02 with loss1: 1496.02 and loss2: 0.00\n",
      "Epoch [147], train_loss: 1923.40 with loss1: 1923.40 and loss2: 0.00\n",
      "Epoch [148], train_loss: 2350.56 with loss1: 2350.56 and loss2: 0.00\n",
      "Epoch [149], train_loss: 2696.71 with loss1: 2696.71 and loss2: 0.00\n",
      "Epoch [150], train_loss: 2356.52 with loss1: 2356.52 and loss2: 0.00\n",
      "Epoch [151], train_loss: 1659.82 with loss1: 1659.82 and loss2: 0.00\n",
      "Epoch [152], train_loss: 1451.32 with loss1: 1451.32 and loss2: 0.00\n",
      "Epoch [153], train_loss: 1984.90 with loss1: 1984.90 and loss2: 0.00\n",
      "Epoch [154], train_loss: 1594.52 with loss1: 1594.52 and loss2: 0.00\n",
      "Epoch [155], train_loss: 1531.89 with loss1: 1531.89 and loss2: 0.00\n",
      "Epoch [156], train_loss: 1920.87 with loss1: 1920.87 and loss2: 0.00\n",
      "Epoch [157], train_loss: 1442.60 with loss1: 1442.60 and loss2: 0.00\n",
      "Epoch [158], train_loss: 1985.73 with loss1: 1985.73 and loss2: 0.00\n",
      "Epoch [159], train_loss: 2183.02 with loss1: 2183.02 and loss2: 0.00\n",
      "Epoch [160], train_loss: 1528.60 with loss1: 1528.60 and loss2: 0.00\n",
      "Epoch [161], train_loss: 1687.60 with loss1: 1687.60 and loss2: 0.00\n",
      "Epoch [162], train_loss: 1424.46 with loss1: 1424.46 and loss2: 0.00\n",
      "Epoch [163], train_loss: 1609.11 with loss1: 1609.11 and loss2: 0.00\n",
      "Epoch [164], train_loss: 1654.32 with loss1: 1654.32 and loss2: 0.00\n",
      "Epoch [165], train_loss: 2009.23 with loss1: 2009.23 and loss2: 0.00\n",
      "Epoch [166], train_loss: 1465.01 with loss1: 1465.01 and loss2: 0.00\n",
      "Epoch [167], train_loss: 2099.94 with loss1: 2099.94 and loss2: 0.00\n",
      "Epoch [168], train_loss: 2384.20 with loss1: 2384.20 and loss2: 0.00\n",
      "Epoch [169], train_loss: 2365.87 with loss1: 2365.87 and loss2: 0.00\n",
      "Epoch [170], train_loss: 2310.58 with loss1: 2310.58 and loss2: 0.00\n",
      "Epoch [171], train_loss: 2165.51 with loss1: 2165.51 and loss2: 0.00\n",
      "Epoch [172], train_loss: 1569.88 with loss1: 1569.88 and loss2: 0.00\n",
      "Epoch [173], train_loss: 1832.23 with loss1: 1832.23 and loss2: 0.00\n",
      "Epoch [174], train_loss: 1998.67 with loss1: 1998.67 and loss2: 0.00\n",
      "Epoch [175], train_loss: 2067.08 with loss1: 2067.08 and loss2: 0.00\n",
      "Epoch [176], train_loss: 2068.77 with loss1: 2068.77 and loss2: 0.00\n",
      "Epoch [177], train_loss: 2043.38 with loss1: 2043.38 and loss2: 0.00\n",
      "Epoch [178], train_loss: 1647.31 with loss1: 1647.31 and loss2: 0.00\n",
      "Epoch [179], train_loss: 1651.93 with loss1: 1651.93 and loss2: 0.00\n",
      "Epoch [180], train_loss: 1399.17 with loss1: 1399.17 and loss2: 0.00\n",
      "Epoch [181], train_loss: 1913.98 with loss1: 1913.98 and loss2: 0.00\n",
      "Epoch [182], train_loss: 1585.15 with loss1: 1585.15 and loss2: 0.00\n",
      "Epoch [183], train_loss: 1904.92 with loss1: 1904.92 and loss2: 0.00\n",
      "Epoch [184], train_loss: 1719.27 with loss1: 1719.27 and loss2: 0.00\n",
      "Epoch [185], train_loss: 1388.01 with loss1: 1388.01 and loss2: 0.00\n",
      "Epoch [186], train_loss: 1672.83 with loss1: 1672.83 and loss2: 0.00\n",
      "Epoch [187], train_loss: 1702.68 with loss1: 1702.68 and loss2: 0.00\n",
      "Epoch [188], train_loss: 1627.03 with loss1: 1627.03 and loss2: 0.00\n",
      "Epoch [189], train_loss: 2043.07 with loss1: 2043.07 and loss2: 0.00\n",
      "Epoch [190], train_loss: 1464.19 with loss1: 1464.19 and loss2: 0.00\n",
      "Epoch [191], train_loss: 1817.60 with loss1: 1817.60 and loss2: 0.00\n",
      "Epoch [192], train_loss: 1526.56 with loss1: 1526.56 and loss2: 0.00\n",
      "Epoch [193], train_loss: 1918.02 with loss1: 1918.02 and loss2: 0.00\n",
      "Epoch [194], train_loss: 1422.80 with loss1: 1422.80 and loss2: 0.00\n",
      "Epoch [195], train_loss: 1918.04 with loss1: 1918.04 and loss2: 0.00\n",
      "Epoch [196], train_loss: 1537.38 with loss1: 1537.38 and loss2: 0.00\n",
      "Epoch [197], train_loss: 1740.46 with loss1: 1740.46 and loss2: 0.00\n",
      "Epoch [198], train_loss: 1867.31 with loss1: 1867.31 and loss2: 0.00\n",
      "Epoch [199], train_loss: 1798.56 with loss1: 1798.56 and loss2: 0.00\n",
      "Epoch [200], train_loss: 1504.69 with loss1: 1504.69 and loss2: 0.00\n",
      "Epoch [201], train_loss: 1740.51 with loss1: 1740.51 and loss2: 0.00\n",
      "Epoch [202], train_loss: 1896.75 with loss1: 1896.75 and loss2: 0.00\n",
      "Epoch [203], train_loss: 1353.30 with loss1: 1353.30 and loss2: 0.00\n",
      "Epoch [204], train_loss: 1351.50 with loss1: 1351.50 and loss2: 0.00\n",
      "Epoch [205], train_loss: 1765.49 with loss1: 1765.49 and loss2: 0.00\n",
      "Epoch [206], train_loss: 1651.85 with loss1: 1651.85 and loss2: 0.00\n",
      "Epoch [207], train_loss: 1341.39 with loss1: 1341.39 and loss2: 0.00\n",
      "Epoch [208], train_loss: 1685.88 with loss1: 1685.88 and loss2: 0.00\n",
      "Epoch [209], train_loss: 2073.44 with loss1: 2073.44 and loss2: 0.00\n",
      "Epoch [210], train_loss: 1510.01 with loss1: 1510.01 and loss2: 0.00\n",
      "Epoch [211], train_loss: 1519.43 with loss1: 1519.43 and loss2: 0.00\n",
      "Epoch [212], train_loss: 1329.40 with loss1: 1329.40 and loss2: 0.00\n",
      "Epoch [213], train_loss: 1777.87 with loss1: 1777.87 and loss2: 0.00\n",
      "Epoch [214], train_loss: 1545.24 with loss1: 1545.24 and loss2: 0.00\n",
      "Epoch [215], train_loss: 1728.37 with loss1: 1728.37 and loss2: 0.00\n",
      "Epoch [216], train_loss: 2009.14 with loss1: 2009.14 and loss2: 0.00\n",
      "Epoch [217], train_loss: 2110.48 with loss1: 2110.48 and loss2: 0.00\n",
      "Epoch [218], train_loss: 1630.02 with loss1: 1630.02 and loss2: 0.00\n",
      "Epoch [219], train_loss: 1533.62 with loss1: 1533.62 and loss2: 0.00\n",
      "Epoch [220], train_loss: 1776.24 with loss1: 1776.24 and loss2: 0.00\n",
      "Epoch [221], train_loss: 2029.00 with loss1: 2029.00 and loss2: 0.00\n",
      "Epoch [222], train_loss: 1537.66 with loss1: 1537.66 and loss2: 0.00\n",
      "Epoch [223], train_loss: 1632.31 with loss1: 1632.31 and loss2: 0.00\n",
      "Epoch [224], train_loss: 1475.46 with loss1: 1475.46 and loss2: 0.00\n",
      "Epoch [225], train_loss: 1563.59 with loss1: 1563.59 and loss2: 0.00\n",
      "Epoch [226], train_loss: 1791.77 with loss1: 1791.77 and loss2: 0.00\n",
      "Epoch [227], train_loss: 1614.37 with loss1: 1614.37 and loss2: 0.00\n",
      "Epoch [228], train_loss: 1571.01 with loss1: 1571.01 and loss2: 0.00\n",
      "Epoch [229], train_loss: 1560.28 with loss1: 1560.28 and loss2: 0.00\n",
      "Epoch [230], train_loss: 1805.32 with loss1: 1805.32 and loss2: 0.00\n",
      "Epoch [231], train_loss: 1650.05 with loss1: 1650.05 and loss2: 0.00\n",
      "Epoch [232], train_loss: 2005.27 with loss1: 2005.27 and loss2: 0.00\n",
      "Epoch [233], train_loss: 2088.60 with loss1: 2088.60 and loss2: 0.00\n",
      "Epoch [234], train_loss: 1461.91 with loss1: 1461.91 and loss2: 0.00\n",
      "Epoch [235], train_loss: 1469.87 with loss1: 1469.87 and loss2: 0.00\n",
      "Epoch [236], train_loss: 1280.72 with loss1: 1280.72 and loss2: 0.00\n",
      "Epoch [237], train_loss: 1704.52 with loss1: 1704.52 and loss2: 0.00\n",
      "Epoch [238], train_loss: 1369.83 with loss1: 1369.83 and loss2: 0.00\n",
      "Epoch [239], train_loss: 1475.94 with loss1: 1475.94 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [240], train_loss: 1961.88 with loss1: 1961.88 and loss2: 0.00\n",
      "Epoch [241], train_loss: 2007.90 with loss1: 2007.90 and loss2: 0.00\n",
      "Epoch [242], train_loss: 1913.66 with loss1: 1913.66 and loss2: 0.00\n",
      "Epoch [243], train_loss: 1465.31 with loss1: 1465.31 and loss2: 0.00\n",
      "Epoch [244], train_loss: 1562.87 with loss1: 1562.87 and loss2: 0.00\n",
      "Epoch [245], train_loss: 1458.80 with loss1: 1458.80 and loss2: 0.00\n",
      "Epoch [246], train_loss: 1816.93 with loss1: 1816.93 and loss2: 0.00\n",
      "Epoch [247], train_loss: 1334.03 with loss1: 1334.03 and loss2: 0.00\n",
      "Epoch [248], train_loss: 1575.48 with loss1: 1575.48 and loss2: 0.00\n",
      "Epoch [249], train_loss: 1800.08 with loss1: 1800.08 and loss2: 0.00\n",
      "Epoch [250], train_loss: 1448.57 with loss1: 1448.57 and loss2: 0.00\n",
      "Epoch [251], train_loss: 1541.79 with loss1: 1541.79 and loss2: 0.00\n",
      "Epoch [252], train_loss: 1714.36 with loss1: 1714.36 and loss2: 0.00\n",
      "Epoch [253], train_loss: 2011.07 with loss1: 2011.07 and loss2: 0.00\n",
      "Epoch [254], train_loss: 1656.81 with loss1: 1656.81 and loss2: 0.00\n",
      "Epoch [255], train_loss: 1488.91 with loss1: 1488.91 and loss2: 0.00\n",
      "Epoch [256], train_loss: 1777.62 with loss1: 1777.62 and loss2: 0.00\n",
      "Epoch [257], train_loss: 1289.14 with loss1: 1289.14 and loss2: 0.00\n",
      "Epoch [258], train_loss: 1551.50 with loss1: 1551.50 and loss2: 0.00\n",
      "Epoch [259], train_loss: 1630.78 with loss1: 1630.78 and loss2: 0.00\n",
      "Epoch [260], train_loss: 1526.82 with loss1: 1526.82 and loss2: 0.00\n",
      "Epoch [261], train_loss: 1641.24 with loss1: 1641.24 and loss2: 0.00\n",
      "Epoch [262], train_loss: 1897.54 with loss1: 1897.54 and loss2: 0.00\n",
      "Epoch [263], train_loss: 1282.74 with loss1: 1282.74 and loss2: 0.00\n",
      "Epoch [264], train_loss: 1626.68 with loss1: 1626.68 and loss2: 0.00\n",
      "Epoch [265], train_loss: 1445.53 with loss1: 1445.53 and loss2: 0.00\n",
      "Epoch [266], train_loss: 1442.29 with loss1: 1442.29 and loss2: 0.00\n",
      "Epoch [267], train_loss: 1795.12 with loss1: 1795.12 and loss2: 0.00\n",
      "Epoch [268], train_loss: 1279.62 with loss1: 1279.62 and loss2: 0.00\n",
      "Epoch [269], train_loss: 1360.73 with loss1: 1360.73 and loss2: 0.00\n",
      "Epoch [270], train_loss: 1491.06 with loss1: 1491.06 and loss2: 0.00\n",
      "Epoch [271], train_loss: 1219.60 with loss1: 1219.60 and loss2: 0.00\n",
      "Epoch [272], train_loss: 1384.15 with loss1: 1384.15 and loss2: 0.00\n",
      "Epoch [273], train_loss: 1210.19 with loss1: 1210.19 and loss2: 0.00\n",
      "Epoch [274], train_loss: 1427.62 with loss1: 1427.62 and loss2: 0.00\n",
      "Epoch [275], train_loss: 1385.07 with loss1: 1385.07 and loss2: 0.00\n",
      "Epoch [276], train_loss: 1394.58 with loss1: 1394.58 and loss2: 0.00\n",
      "Epoch [277], train_loss: 1463.95 with loss1: 1463.95 and loss2: 0.00\n",
      "Epoch [278], train_loss: 1454.27 with loss1: 1454.27 and loss2: 0.00\n",
      "Epoch [279], train_loss: 1376.02 with loss1: 1376.02 and loss2: 0.00\n",
      "Epoch [280], train_loss: 1532.12 with loss1: 1532.12 and loss2: 0.00\n",
      "Epoch [281], train_loss: 1411.29 with loss1: 1411.29 and loss2: 0.00\n",
      "Epoch [282], train_loss: 1659.83 with loss1: 1659.83 and loss2: 0.00\n",
      "Epoch [283], train_loss: 1720.49 with loss1: 1720.49 and loss2: 0.00\n",
      "Epoch [284], train_loss: 1575.76 with loss1: 1575.76 and loss2: 0.00\n",
      "Epoch [285], train_loss: 1409.38 with loss1: 1409.38 and loss2: 0.00\n",
      "Epoch [286], train_loss: 1797.50 with loss1: 1797.50 and loss2: 0.00\n",
      "Epoch [287], train_loss: 1837.85 with loss1: 1837.85 and loss2: 0.00\n",
      "Epoch [288], train_loss: 1296.17 with loss1: 1296.17 and loss2: 0.00\n",
      "Epoch [289], train_loss: 1379.14 with loss1: 1379.14 and loss2: 0.00\n",
      "Epoch [290], train_loss: 1483.79 with loss1: 1483.79 and loss2: 0.00\n",
      "Epoch [291], train_loss: 1452.16 with loss1: 1452.16 and loss2: 0.00\n",
      "Epoch [292], train_loss: 1644.88 with loss1: 1644.88 and loss2: 0.00\n",
      "Epoch [293], train_loss: 1221.28 with loss1: 1221.28 and loss2: 0.00\n",
      "Epoch [294], train_loss: 1500.32 with loss1: 1500.32 and loss2: 0.00\n",
      "Epoch [295], train_loss: 1837.47 with loss1: 1837.47 and loss2: 0.00\n",
      "Epoch [296], train_loss: 1436.28 with loss1: 1436.28 and loss2: 0.00\n",
      "Epoch [297], train_loss: 1338.99 with loss1: 1338.99 and loss2: 0.00\n",
      "Epoch [298], train_loss: 1352.32 with loss1: 1352.32 and loss2: 0.00\n",
      "Epoch [299], train_loss: 1407.16 with loss1: 1407.16 and loss2: 0.00\n",
      "Epoch [300], train_loss: 1182.04 with loss1: 1182.04 and loss2: 0.00\n",
      "Epoch [301], train_loss: 1572.18 with loss1: 1572.18 and loss2: 0.00\n",
      "Epoch [302], train_loss: 1339.55 with loss1: 1339.55 and loss2: 0.00\n",
      "Epoch [303], train_loss: 1338.46 with loss1: 1338.46 and loss2: 0.00\n",
      "Epoch [304], train_loss: 1545.02 with loss1: 1545.02 and loss2: 0.00\n",
      "Epoch [305], train_loss: 1874.85 with loss1: 1874.85 and loss2: 0.00\n",
      "Epoch [306], train_loss: 1599.42 with loss1: 1599.42 and loss2: 0.00\n",
      "Epoch [307], train_loss: 1273.44 with loss1: 1273.44 and loss2: 0.00\n",
      "Epoch [308], train_loss: 1475.01 with loss1: 1475.01 and loss2: 0.00\n",
      "Epoch [309], train_loss: 1339.69 with loss1: 1339.69 and loss2: 0.00\n",
      "Epoch [310], train_loss: 1457.47 with loss1: 1457.47 and loss2: 0.00\n",
      "Epoch [311], train_loss: 1353.38 with loss1: 1353.38 and loss2: 0.00\n",
      "Epoch [312], train_loss: 1144.83 with loss1: 1144.83 and loss2: 0.00\n",
      "Epoch [313], train_loss: 1340.90 with loss1: 1340.90 and loss2: 0.00\n",
      "Epoch [314], train_loss: 1406.88 with loss1: 1406.88 and loss2: 0.00\n",
      "Epoch [315], train_loss: 1659.33 with loss1: 1659.33 and loss2: 0.00\n",
      "Epoch [316], train_loss: 1449.32 with loss1: 1449.32 and loss2: 0.00\n",
      "Epoch [317], train_loss: 1759.14 with loss1: 1759.14 and loss2: 0.00\n",
      "Epoch [318], train_loss: 1836.42 with loss1: 1836.42 and loss2: 0.00\n",
      "Epoch [319], train_loss: 1411.98 with loss1: 1411.98 and loss2: 0.00\n",
      "Epoch [320], train_loss: 1240.88 with loss1: 1240.88 and loss2: 0.00\n",
      "Epoch [321], train_loss: 1427.04 with loss1: 1427.04 and loss2: 0.00\n",
      "Epoch [322], train_loss: 1383.99 with loss1: 1383.99 and loss2: 0.00\n",
      "Epoch [323], train_loss: 1336.98 with loss1: 1336.98 and loss2: 0.00\n",
      "Epoch [324], train_loss: 1356.06 with loss1: 1356.06 and loss2: 0.00\n",
      "Epoch [325], train_loss: 1776.25 with loss1: 1776.25 and loss2: 0.00\n",
      "Epoch [326], train_loss: 1402.04 with loss1: 1402.04 and loss2: 0.00\n",
      "Epoch [327], train_loss: 1135.88 with loss1: 1135.88 and loss2: 0.00\n",
      "Epoch [328], train_loss: 1134.38 with loss1: 1134.38 and loss2: 0.00\n",
      "Epoch [329], train_loss: 1438.92 with loss1: 1438.92 and loss2: 0.00\n",
      "Epoch [330], train_loss: 1293.38 with loss1: 1293.38 and loss2: 0.00\n",
      "Epoch [331], train_loss: 1302.26 with loss1: 1302.26 and loss2: 0.00\n",
      "Epoch [332], train_loss: 1135.98 with loss1: 1135.98 and loss2: 0.00\n",
      "Epoch [333], train_loss: 1526.65 with loss1: 1526.65 and loss2: 0.00\n",
      "Epoch [334], train_loss: 1273.14 with loss1: 1273.14 and loss2: 0.00\n",
      "Epoch [335], train_loss: 1129.80 with loss1: 1129.80 and loss2: 0.00\n",
      "Epoch [336], train_loss: 1480.41 with loss1: 1480.41 and loss2: 0.00\n",
      "Epoch [337], train_loss: 1872.31 with loss1: 1872.31 and loss2: 0.00\n",
      "Epoch [338], train_loss: 1215.59 with loss1: 1215.59 and loss2: 0.00\n",
      "Epoch [339], train_loss: 1306.86 with loss1: 1306.86 and loss2: 0.00\n",
      "Epoch [340], train_loss: 1625.74 with loss1: 1625.74 and loss2: 0.00\n",
      "Epoch [341], train_loss: 1764.27 with loss1: 1764.27 and loss2: 0.00\n",
      "Epoch [342], train_loss: 1188.36 with loss1: 1188.36 and loss2: 0.00\n",
      "Epoch [343], train_loss: 1313.04 with loss1: 1313.04 and loss2: 0.00\n",
      "Epoch [344], train_loss: 1103.31 with loss1: 1103.31 and loss2: 0.00\n",
      "Epoch [345], train_loss: 1263.90 with loss1: 1263.90 and loss2: 0.00\n",
      "Epoch [346], train_loss: 1597.76 with loss1: 1597.76 and loss2: 0.00\n",
      "Epoch [347], train_loss: 1178.43 with loss1: 1178.43 and loss2: 0.00\n",
      "Epoch [348], train_loss: 1413.13 with loss1: 1413.13 and loss2: 0.00\n",
      "Epoch [349], train_loss: 1315.00 with loss1: 1315.00 and loss2: 0.00\n",
      "Epoch [350], train_loss: 1644.10 with loss1: 1644.10 and loss2: 0.00\n",
      "Epoch [351], train_loss: 1247.60 with loss1: 1247.60 and loss2: 0.00\n",
      "Epoch [352], train_loss: 1164.64 with loss1: 1164.64 and loss2: 0.00\n",
      "Epoch [353], train_loss: 1359.68 with loss1: 1359.68 and loss2: 0.00\n",
      "Epoch [354], train_loss: 1499.59 with loss1: 1499.59 and loss2: 0.00\n",
      "Epoch [355], train_loss: 1274.46 with loss1: 1274.46 and loss2: 0.00\n",
      "Epoch [356], train_loss: 1437.80 with loss1: 1437.80 and loss2: 0.00\n",
      "Epoch [357], train_loss: 1092.45 with loss1: 1092.45 and loss2: 0.00\n",
      "Epoch [358], train_loss: 1439.09 with loss1: 1439.09 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [359], train_loss: 1256.90 with loss1: 1256.90 and loss2: 0.00\n",
      "Epoch [360], train_loss: 1261.01 with loss1: 1261.01 and loss2: 0.00\n",
      "Epoch [361], train_loss: 1409.40 with loss1: 1409.40 and loss2: 0.00\n",
      "Epoch [362], train_loss: 1255.33 with loss1: 1255.33 and loss2: 0.00\n",
      "Epoch [363], train_loss: 1141.09 with loss1: 1141.09 and loss2: 0.00\n",
      "Epoch [364], train_loss: 1578.92 with loss1: 1578.92 and loss2: 0.00\n",
      "Epoch [365], train_loss: 1244.03 with loss1: 1244.03 and loss2: 0.00\n",
      "Epoch [366], train_loss: 1087.24 with loss1: 1087.24 and loss2: 0.00\n",
      "Epoch [367], train_loss: 1126.16 with loss1: 1126.16 and loss2: 0.00\n",
      "Epoch [368], train_loss: 1508.63 with loss1: 1508.63 and loss2: 0.00\n",
      "Epoch [369], train_loss: 1240.21 with loss1: 1240.21 and loss2: 0.00\n",
      "Epoch [370], train_loss: 1616.47 with loss1: 1616.47 and loss2: 0.00\n",
      "Epoch [371], train_loss: 1224.93 with loss1: 1224.93 and loss2: 0.00\n",
      "Epoch [372], train_loss: 1413.18 with loss1: 1413.18 and loss2: 0.00\n",
      "Epoch [373], train_loss: 1825.20 with loss1: 1825.20 and loss2: 0.00\n",
      "Epoch [374], train_loss: 1359.62 with loss1: 1359.62 and loss2: 0.00\n",
      "Epoch [375], train_loss: 1249.05 with loss1: 1249.05 and loss2: 0.00\n",
      "Epoch [376], train_loss: 1183.11 with loss1: 1183.11 and loss2: 0.00\n",
      "Epoch [377], train_loss: 1459.36 with loss1: 1459.36 and loss2: 0.00\n",
      "Epoch [378], train_loss: 1233.02 with loss1: 1233.02 and loss2: 0.00\n",
      "Epoch [379], train_loss: 1607.47 with loss1: 1607.47 and loss2: 0.00\n",
      "Epoch [380], train_loss: 1215.31 with loss1: 1215.31 and loss2: 0.00\n",
      "Epoch [381], train_loss: 1226.45 with loss1: 1226.45 and loss2: 0.00\n",
      "Epoch [382], train_loss: 1200.67 with loss1: 1200.67 and loss2: 0.00\n",
      "Epoch [383], train_loss: 1203.67 with loss1: 1203.67 and loss2: 0.00\n",
      "Epoch [384], train_loss: 1397.43 with loss1: 1397.43 and loss2: 0.00\n",
      "Epoch [385], train_loss: 1125.48 with loss1: 1125.48 and loss2: 0.00\n",
      "Epoch [386], train_loss: 1337.06 with loss1: 1337.06 and loss2: 0.00\n",
      "Epoch [387], train_loss: 1095.66 with loss1: 1095.66 and loss2: 0.00\n",
      "Epoch [388], train_loss: 1143.03 with loss1: 1143.03 and loss2: 0.00\n",
      "Epoch [389], train_loss: 1543.63 with loss1: 1543.63 and loss2: 0.00\n",
      "Epoch [390], train_loss: 1206.26 with loss1: 1206.26 and loss2: 0.00\n",
      "Epoch [391], train_loss: 1044.66 with loss1: 1044.66 and loss2: 0.00\n",
      "Epoch [392], train_loss: 1070.70 with loss1: 1070.70 and loss2: 0.00\n",
      "Epoch [393], train_loss: 1444.42 with loss1: 1444.42 and loss2: 0.00\n",
      "Epoch [394], train_loss: 1283.73 with loss1: 1283.73 and loss2: 0.00\n",
      "Epoch [395], train_loss: 1156.24 with loss1: 1156.24 and loss2: 0.00\n",
      "Epoch [396], train_loss: 1217.00 with loss1: 1217.00 and loss2: 0.00\n",
      "Epoch [397], train_loss: 1301.57 with loss1: 1301.57 and loss2: 0.00\n",
      "Epoch [398], train_loss: 1058.98 with loss1: 1058.98 and loss2: 0.00\n",
      "Epoch [399], train_loss: 1414.11 with loss1: 1414.11 and loss2: 0.00\n",
      "Epoch [400], train_loss: 1794.23 with loss1: 1794.23 and loss2: 0.00\n",
      "Epoch [401], train_loss: 1754.30 with loss1: 1754.30 and loss2: 0.00\n",
      "Epoch [402], train_loss: 1609.10 with loss1: 1609.10 and loss2: 0.00\n",
      "Epoch [403], train_loss: 1114.01 with loss1: 1114.01 and loss2: 0.00\n",
      "Epoch [404], train_loss: 1321.12 with loss1: 1321.12 and loss2: 0.00\n",
      "Epoch [405], train_loss: 1487.39 with loss1: 1487.39 and loss2: 0.00\n",
      "Epoch [406], train_loss: 1083.47 with loss1: 1083.47 and loss2: 0.00\n",
      "Epoch [407], train_loss: 1187.22 with loss1: 1187.22 and loss2: 0.00\n",
      "Epoch [408], train_loss: 1422.41 with loss1: 1422.41 and loss2: 0.00\n",
      "Epoch [409], train_loss: 1648.90 with loss1: 1648.90 and loss2: 0.00\n",
      "Epoch [410], train_loss: 1104.07 with loss1: 1104.07 and loss2: 0.00\n",
      "Epoch [411], train_loss: 1356.76 with loss1: 1356.76 and loss2: 0.00\n",
      "Epoch [412], train_loss: 1189.56 with loss1: 1189.56 and loss2: 0.00\n",
      "Epoch [413], train_loss: 1164.29 with loss1: 1164.29 and loss2: 0.00\n",
      "Epoch [414], train_loss: 1146.70 with loss1: 1146.70 and loss2: 0.00\n",
      "Epoch [415], train_loss: 1241.47 with loss1: 1241.47 and loss2: 0.00\n",
      "Epoch [416], train_loss: 1020.92 with loss1: 1020.92 and loss2: 0.00\n",
      "Epoch [417], train_loss: 1057.41 with loss1: 1057.41 and loss2: 0.00\n",
      "Epoch [418], train_loss: 1287.59 with loss1: 1287.59 and loss2: 0.00\n",
      "Epoch [419], train_loss: 1643.59 with loss1: 1643.59 and loss2: 0.00\n",
      "Epoch [420], train_loss: 1152.31 with loss1: 1152.31 and loss2: 0.00\n",
      "Epoch [421], train_loss: 1262.32 with loss1: 1262.32 and loss2: 0.00\n",
      "Epoch [422], train_loss: 1247.16 with loss1: 1247.16 and loss2: 0.00\n",
      "Epoch [423], train_loss: 1654.68 with loss1: 1654.68 and loss2: 0.00\n",
      "Epoch [424], train_loss: 1392.21 with loss1: 1392.21 and loss2: 0.00\n",
      "Epoch [425], train_loss: 1176.63 with loss1: 1176.63 and loss2: 0.00\n",
      "Epoch [426], train_loss: 1137.94 with loss1: 1137.94 and loss2: 0.00\n",
      "Epoch [427], train_loss: 1289.25 with loss1: 1289.25 and loss2: 0.00\n",
      "Epoch [428], train_loss: 1241.39 with loss1: 1241.39 and loss2: 0.00\n",
      "Epoch [429], train_loss: 1210.29 with loss1: 1210.29 and loss2: 0.00\n",
      "Epoch [430], train_loss: 1125.48 with loss1: 1125.48 and loss2: 0.00\n",
      "Epoch [431], train_loss: 1311.35 with loss1: 1311.35 and loss2: 0.00\n",
      "Epoch [432], train_loss: 1577.61 with loss1: 1577.61 and loss2: 0.00\n",
      "Epoch [433], train_loss: 1762.15 with loss1: 1762.15 and loss2: 0.00\n",
      "Epoch [434], train_loss: 1809.53 with loss1: 1809.53 and loss2: 0.00\n",
      "Epoch [435], train_loss: 1080.95 with loss1: 1080.95 and loss2: 0.00\n",
      "Epoch [436], train_loss: 1207.09 with loss1: 1207.09 and loss2: 0.00\n",
      "Epoch [437], train_loss: 1445.20 with loss1: 1445.20 and loss2: 0.00\n",
      "Epoch [438], train_loss: 1035.71 with loss1: 1035.71 and loss2: 0.00\n",
      "Epoch [439], train_loss: 1254.01 with loss1: 1254.01 and loss2: 0.00\n",
      "Epoch [440], train_loss: 1419.94 with loss1: 1419.94 and loss2: 0.00\n",
      "Epoch [441], train_loss: 1189.01 with loss1: 1189.01 and loss2: 0.00\n",
      "Epoch [442], train_loss: 1186.44 with loss1: 1186.44 and loss2: 0.00\n",
      "Epoch [443], train_loss: 1372.66 with loss1: 1372.66 and loss2: 0.00\n",
      "Epoch [444], train_loss: 1222.35 with loss1: 1222.35 and loss2: 0.00\n",
      "Epoch [445], train_loss: 1163.48 with loss1: 1163.48 and loss2: 0.00\n",
      "Epoch [446], train_loss: 983.08 with loss1: 983.08 and loss2: 0.00\n",
      "Epoch [447], train_loss: 1359.98 with loss1: 1359.98 and loss2: 0.00\n",
      "Epoch [448], train_loss: 1228.03 with loss1: 1228.03 and loss2: 0.00\n",
      "Epoch [449], train_loss: 991.98 with loss1: 991.98 and loss2: 0.00\n",
      "Epoch [450], train_loss: 1249.04 with loss1: 1249.04 and loss2: 0.00\n",
      "Epoch [451], train_loss: 1114.75 with loss1: 1114.75 and loss2: 0.00\n",
      "Epoch [452], train_loss: 1158.82 with loss1: 1158.82 and loss2: 0.00\n",
      "Epoch [453], train_loss: 1118.51 with loss1: 1118.51 and loss2: 0.00\n",
      "Epoch [454], train_loss: 1136.35 with loss1: 1136.35 and loss2: 0.00\n",
      "Epoch [455], train_loss: 1104.71 with loss1: 1104.71 and loss2: 0.00\n",
      "Epoch [456], train_loss: 1112.09 with loss1: 1112.09 and loss2: 0.00\n",
      "Epoch [457], train_loss: 1294.92 with loss1: 1294.92 and loss2: 0.00\n",
      "Epoch [458], train_loss: 1103.38 with loss1: 1103.38 and loss2: 0.00\n",
      "Epoch [459], train_loss: 1101.22 with loss1: 1101.22 and loss2: 0.00\n",
      "Epoch [460], train_loss: 1208.88 with loss1: 1208.88 and loss2: 0.00\n",
      "Epoch [461], train_loss: 1144.04 with loss1: 1144.04 and loss2: 0.00\n",
      "Epoch [462], train_loss: 1242.54 with loss1: 1242.54 and loss2: 0.00\n",
      "Epoch [463], train_loss: 1527.31 with loss1: 1527.31 and loss2: 0.00\n",
      "Epoch [464], train_loss: 1660.91 with loss1: 1660.91 and loss2: 0.00\n",
      "Epoch [465], train_loss: 1197.10 with loss1: 1197.10 and loss2: 0.00\n",
      "Epoch [466], train_loss: 1239.01 with loss1: 1239.01 and loss2: 0.00\n",
      "Epoch [467], train_loss: 1361.26 with loss1: 1361.26 and loss2: 0.00\n",
      "Epoch [468], train_loss: 1093.79 with loss1: 1093.79 and loss2: 0.00\n",
      "Epoch [469], train_loss: 1317.97 with loss1: 1317.97 and loss2: 0.00\n",
      "Epoch [470], train_loss: 1124.04 with loss1: 1124.04 and loss2: 0.00\n",
      "Epoch [471], train_loss: 1150.78 with loss1: 1150.78 and loss2: 0.00\n",
      "Epoch [472], train_loss: 1218.27 with loss1: 1218.27 and loss2: 0.00\n",
      "Epoch [473], train_loss: 1390.49 with loss1: 1390.49 and loss2: 0.00\n",
      "Epoch [474], train_loss: 1130.10 with loss1: 1130.10 and loss2: 0.00\n",
      "Epoch [475], train_loss: 1321.38 with loss1: 1321.38 and loss2: 0.00\n",
      "Epoch [476], train_loss: 1234.51 with loss1: 1234.51 and loss2: 0.00\n",
      "Epoch [477], train_loss: 952.87 with loss1: 952.87 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [478], train_loss: 1139.26 with loss1: 1139.26 and loss2: 0.00\n",
      "Epoch [479], train_loss: 1098.65 with loss1: 1098.65 and loss2: 0.00\n",
      "Epoch [480], train_loss: 1114.46 with loss1: 1114.46 and loss2: 0.00\n",
      "Epoch [481], train_loss: 1087.11 with loss1: 1087.11 and loss2: 0.00\n",
      "Epoch [482], train_loss: 1379.45 with loss1: 1379.45 and loss2: 0.00\n",
      "Epoch [483], train_loss: 1116.12 with loss1: 1116.12 and loss2: 0.00\n",
      "Epoch [484], train_loss: 1110.35 with loss1: 1110.35 and loss2: 0.00\n",
      "Epoch [485], train_loss: 1113.93 with loss1: 1113.93 and loss2: 0.00\n",
      "Epoch [486], train_loss: 953.38 with loss1: 953.38 and loss2: 0.00\n",
      "Epoch [487], train_loss: 1345.01 with loss1: 1345.01 and loss2: 0.00\n",
      "Epoch [488], train_loss: 1076.66 with loss1: 1076.66 and loss2: 0.00\n",
      "Epoch [489], train_loss: 1017.13 with loss1: 1017.13 and loss2: 0.00\n",
      "Epoch [490], train_loss: 1064.71 with loss1: 1064.71 and loss2: 0.00\n",
      "Epoch [491], train_loss: 1263.54 with loss1: 1263.54 and loss2: 0.00\n",
      "Epoch [492], train_loss: 1031.86 with loss1: 1031.86 and loss2: 0.00\n",
      "Epoch [493], train_loss: 1052.11 with loss1: 1052.11 and loss2: 0.00\n",
      "Epoch [494], train_loss: 1077.39 with loss1: 1077.39 and loss2: 0.00\n",
      "Epoch [495], train_loss: 1099.13 with loss1: 1099.13 and loss2: 0.00\n",
      "Epoch [496], train_loss: 1428.42 with loss1: 1428.42 and loss2: 0.00\n",
      "Epoch [497], train_loss: 1447.11 with loss1: 1447.11 and loss2: 0.00\n",
      "Epoch [498], train_loss: 1594.50 with loss1: 1594.50 and loss2: 0.00\n",
      "Epoch [499], train_loss: 1096.32 with loss1: 1096.32 and loss2: 0.00\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# model version 3 \n",
    "# delete loss2\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history, mu_history = fit(epochs=500, lr=2e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2eef559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 935.64 with loss1: 935.64 and loss2: 0.00\n",
      "Epoch [1], train_loss: 1203.69 with loss1: 1203.69 and loss2: 0.00\n",
      "Epoch [2], train_loss: 1012.59 with loss1: 1012.59 and loss2: 0.00\n",
      "Epoch [3], train_loss: 1007.32 with loss1: 1007.32 and loss2: 0.00\n",
      "Epoch [4], train_loss: 1237.06 with loss1: 1237.06 and loss2: 0.00\n",
      "Epoch [5], train_loss: 1477.44 with loss1: 1477.44 and loss2: 0.00\n",
      "Epoch [6], train_loss: 1053.03 with loss1: 1053.03 and loss2: 0.00\n",
      "Epoch [7], train_loss: 1044.75 with loss1: 1044.75 and loss2: 0.00\n",
      "Epoch [8], train_loss: 1243.70 with loss1: 1243.70 and loss2: 0.00\n",
      "Epoch [9], train_loss: 1510.00 with loss1: 1510.00 and loss2: 0.00\n",
      "Epoch [10], train_loss: 1036.81 with loss1: 1036.81 and loss2: 0.00\n",
      "Epoch [11], train_loss: 1283.10 with loss1: 1283.10 and loss2: 0.00\n",
      "Epoch [12], train_loss: 1173.04 with loss1: 1173.04 and loss2: 0.00\n",
      "Epoch [13], train_loss: 909.06 with loss1: 909.06 and loss2: 0.00\n",
      "Epoch [14], train_loss: 1162.23 with loss1: 1162.23 and loss2: 0.00\n",
      "Epoch [15], train_loss: 907.00 with loss1: 907.00 and loss2: 0.00\n",
      "Epoch [16], train_loss: 1099.84 with loss1: 1099.84 and loss2: 0.00\n",
      "Epoch [17], train_loss: 1102.10 with loss1: 1102.10 and loss2: 0.00\n",
      "Epoch [18], train_loss: 1098.30 with loss1: 1098.30 and loss2: 0.00\n",
      "Epoch [19], train_loss: 1056.84 with loss1: 1056.84 and loss2: 0.00\n",
      "Epoch [20], train_loss: 1056.09 with loss1: 1056.09 and loss2: 0.00\n",
      "Epoch [21], train_loss: 918.77 with loss1: 918.77 and loss2: 0.00\n",
      "Epoch [22], train_loss: 1026.82 with loss1: 1026.82 and loss2: 0.00\n",
      "Epoch [23], train_loss: 1335.96 with loss1: 1335.96 and loss2: 0.00\n",
      "Epoch [24], train_loss: 967.97 with loss1: 967.97 and loss2: 0.00\n",
      "Epoch [25], train_loss: 993.40 with loss1: 993.40 and loss2: 0.00\n",
      "Epoch [26], train_loss: 1139.98 with loss1: 1139.98 and loss2: 0.00\n",
      "Epoch [27], train_loss: 1517.49 with loss1: 1517.49 and loss2: 0.00\n",
      "Epoch [28], train_loss: 947.72 with loss1: 947.72 and loss2: 0.00\n",
      "Epoch [29], train_loss: 1239.94 with loss1: 1239.94 and loss2: 0.00\n",
      "Epoch [30], train_loss: 1484.78 with loss1: 1484.78 and loss2: 0.00\n",
      "Epoch [31], train_loss: 1273.22 with loss1: 1273.22 and loss2: 0.00\n",
      "Epoch [32], train_loss: 1018.17 with loss1: 1018.17 and loss2: 0.00\n",
      "Epoch [33], train_loss: 1021.01 with loss1: 1021.01 and loss2: 0.00\n",
      "Epoch [34], train_loss: 902.68 with loss1: 902.68 and loss2: 0.00\n",
      "Epoch [35], train_loss: 1135.25 with loss1: 1135.25 and loss2: 0.00\n",
      "Epoch [36], train_loss: 1331.83 with loss1: 1331.83 and loss2: 0.00\n",
      "Epoch [37], train_loss: 1039.71 with loss1: 1039.71 and loss2: 0.00\n",
      "Epoch [38], train_loss: 995.10 with loss1: 995.10 and loss2: 0.00\n",
      "Epoch [39], train_loss: 1321.32 with loss1: 1321.32 and loss2: 0.00\n",
      "Epoch [40], train_loss: 1037.76 with loss1: 1037.76 and loss2: 0.00\n",
      "Epoch [41], train_loss: 1153.45 with loss1: 1153.45 and loss2: 0.00\n",
      "Epoch [42], train_loss: 1053.98 with loss1: 1053.98 and loss2: 0.00\n",
      "Epoch [43], train_loss: 1016.29 with loss1: 1016.29 and loss2: 0.00\n",
      "Epoch [44], train_loss: 1123.64 with loss1: 1123.64 and loss2: 0.00\n",
      "Epoch [45], train_loss: 1076.66 with loss1: 1076.66 and loss2: 0.00\n",
      "Epoch [46], train_loss: 992.07 with loss1: 992.07 and loss2: 0.00\n",
      "Epoch [47], train_loss: 1240.66 with loss1: 1240.66 and loss2: 0.00\n",
      "Epoch [48], train_loss: 1391.52 with loss1: 1391.52 and loss2: 0.00\n",
      "Epoch [49], train_loss: 1174.76 with loss1: 1174.76 and loss2: 0.00\n",
      "Epoch [50], train_loss: 1051.71 with loss1: 1051.71 and loss2: 0.00\n",
      "Epoch [51], train_loss: 1149.06 with loss1: 1149.06 and loss2: 0.00\n",
      "Epoch [52], train_loss: 1038.81 with loss1: 1038.81 and loss2: 0.00\n",
      "Epoch [53], train_loss: 1298.04 with loss1: 1298.04 and loss2: 0.00\n",
      "Epoch [54], train_loss: 1575.96 with loss1: 1575.96 and loss2: 0.00\n",
      "Epoch [55], train_loss: 1554.47 with loss1: 1554.47 and loss2: 0.00\n",
      "Epoch [56], train_loss: 978.00 with loss1: 978.00 and loss2: 0.00\n",
      "Epoch [57], train_loss: 1214.21 with loss1: 1214.21 and loss2: 0.00\n",
      "Epoch [58], train_loss: 1445.64 with loss1: 1445.64 and loss2: 0.00\n",
      "Epoch [59], train_loss: 953.47 with loss1: 953.47 and loss2: 0.00\n",
      "Epoch [60], train_loss: 1104.93 with loss1: 1104.93 and loss2: 0.00\n",
      "Epoch [61], train_loss: 1088.91 with loss1: 1088.91 and loss2: 0.00\n",
      "Epoch [62], train_loss: 1047.96 with loss1: 1047.96 and loss2: 0.00\n",
      "Epoch [63], train_loss: 1325.66 with loss1: 1325.66 and loss2: 0.00\n",
      "Epoch [64], train_loss: 1420.09 with loss1: 1420.09 and loss2: 0.00\n",
      "Epoch [65], train_loss: 1452.35 with loss1: 1452.35 and loss2: 0.00\n",
      "Epoch [66], train_loss: 944.67 with loss1: 944.67 and loss2: 0.00\n",
      "Epoch [67], train_loss: 912.19 with loss1: 912.19 and loss2: 0.00\n",
      "Epoch [68], train_loss: 1057.53 with loss1: 1057.53 and loss2: 0.00\n",
      "Epoch [69], train_loss: 1002.24 with loss1: 1002.24 and loss2: 0.00\n",
      "Epoch [70], train_loss: 1165.16 with loss1: 1165.16 and loss2: 0.00\n",
      "Epoch [71], train_loss: 1003.95 with loss1: 1003.95 and loss2: 0.00\n",
      "Epoch [72], train_loss: 1090.91 with loss1: 1090.91 and loss2: 0.00\n",
      "Epoch [73], train_loss: 1328.95 with loss1: 1328.95 and loss2: 0.00\n",
      "Epoch [74], train_loss: 1030.26 with loss1: 1030.26 and loss2: 0.00\n",
      "Epoch [75], train_loss: 1243.84 with loss1: 1243.84 and loss2: 0.00\n",
      "Epoch [76], train_loss: 997.35 with loss1: 997.35 and loss2: 0.00\n",
      "Epoch [77], train_loss: 989.16 with loss1: 989.16 and loss2: 0.00\n",
      "Epoch [78], train_loss: 1071.56 with loss1: 1071.56 and loss2: 0.00\n",
      "Epoch [79], train_loss: 997.68 with loss1: 997.68 and loss2: 0.00\n",
      "Epoch [80], train_loss: 849.14 with loss1: 849.14 and loss2: 0.00\n",
      "Epoch [81], train_loss: 1025.87 with loss1: 1025.87 and loss2: 0.00\n",
      "Epoch [82], train_loss: 1310.41 with loss1: 1310.41 and loss2: 0.00\n",
      "Epoch [83], train_loss: 1076.83 with loss1: 1076.83 and loss2: 0.00\n",
      "Epoch [84], train_loss: 870.53 with loss1: 870.53 and loss2: 0.00\n",
      "Epoch [85], train_loss: 1128.49 with loss1: 1128.49 and loss2: 0.00\n",
      "Epoch [86], train_loss: 989.50 with loss1: 989.50 and loss2: 0.00\n",
      "Epoch [87], train_loss: 842.65 with loss1: 842.65 and loss2: 0.00\n",
      "Epoch [88], train_loss: 1113.40 with loss1: 1113.40 and loss2: 0.00\n",
      "Epoch [89], train_loss: 1043.80 with loss1: 1043.80 and loss2: 0.00\n",
      "Epoch [90], train_loss: 922.53 with loss1: 922.53 and loss2: 0.00\n",
      "Epoch [91], train_loss: 1174.78 with loss1: 1174.78 and loss2: 0.00\n",
      "Epoch [92], train_loss: 1400.99 with loss1: 1400.99 and loss2: 0.00\n",
      "Epoch [93], train_loss: 1092.21 with loss1: 1092.21 and loss2: 0.00\n",
      "Epoch [94], train_loss: 1313.12 with loss1: 1313.12 and loss2: 0.00\n",
      "Epoch [95], train_loss: 1402.56 with loss1: 1402.56 and loss2: 0.00\n",
      "Epoch [96], train_loss: 1144.27 with loss1: 1144.27 and loss2: 0.00\n",
      "Epoch [97], train_loss: 885.15 with loss1: 885.15 and loss2: 0.00\n",
      "Epoch [98], train_loss: 1205.46 with loss1: 1205.46 and loss2: 0.00\n",
      "Epoch [99], train_loss: 1334.17 with loss1: 1334.17 and loss2: 0.00\n",
      "Epoch [100], train_loss: 1105.98 with loss1: 1105.98 and loss2: 0.00\n",
      "Epoch [101], train_loss: 1010.23 with loss1: 1010.23 and loss2: 0.00\n",
      "Epoch [102], train_loss: 984.68 with loss1: 984.68 and loss2: 0.00\n",
      "Epoch [103], train_loss: 1017.52 with loss1: 1017.52 and loss2: 0.00\n",
      "Epoch [104], train_loss: 933.44 with loss1: 933.44 and loss2: 0.00\n",
      "Epoch [105], train_loss: 1069.29 with loss1: 1069.29 and loss2: 0.00\n",
      "Epoch [106], train_loss: 996.34 with loss1: 996.34 and loss2: 0.00\n",
      "Epoch [107], train_loss: 928.19 with loss1: 928.19 and loss2: 0.00\n",
      "Epoch [108], train_loss: 1066.82 with loss1: 1066.82 and loss2: 0.00\n",
      "Epoch [109], train_loss: 837.34 with loss1: 837.34 and loss2: 0.00\n",
      "Epoch [110], train_loss: 837.43 with loss1: 837.43 and loss2: 0.00\n",
      "Epoch [111], train_loss: 1017.91 with loss1: 1017.91 and loss2: 0.00\n",
      "Epoch [112], train_loss: 1136.94 with loss1: 1136.94 and loss2: 0.00\n",
      "Epoch [113], train_loss: 1125.45 with loss1: 1125.45 and loss2: 0.00\n",
      "Epoch [114], train_loss: 987.62 with loss1: 987.62 and loss2: 0.00\n",
      "Epoch [115], train_loss: 1111.84 with loss1: 1111.84 and loss2: 0.00\n",
      "Epoch [116], train_loss: 1016.19 with loss1: 1016.19 and loss2: 0.00\n",
      "Epoch [117], train_loss: 1073.51 with loss1: 1073.51 and loss2: 0.00\n",
      "Epoch [118], train_loss: 1066.86 with loss1: 1066.86 and loss2: 0.00\n",
      "Epoch [119], train_loss: 1000.57 with loss1: 1000.57 and loss2: 0.00\n",
      "Epoch [120], train_loss: 1008.23 with loss1: 1008.23 and loss2: 0.00\n",
      "Epoch [121], train_loss: 949.25 with loss1: 949.25 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [122], train_loss: 901.07 with loss1: 901.07 and loss2: 0.00\n",
      "Epoch [123], train_loss: 1202.64 with loss1: 1202.64 and loss2: 0.00\n",
      "Epoch [124], train_loss: 992.93 with loss1: 992.93 and loss2: 0.00\n",
      "Epoch [125], train_loss: 1095.53 with loss1: 1095.53 and loss2: 0.00\n",
      "Epoch [126], train_loss: 991.76 with loss1: 991.76 and loss2: 0.00\n",
      "Epoch [127], train_loss: 969.83 with loss1: 969.83 and loss2: 0.00\n",
      "Epoch [128], train_loss: 1037.13 with loss1: 1037.13 and loss2: 0.00\n",
      "Epoch [129], train_loss: 1332.06 with loss1: 1332.06 and loss2: 0.00\n",
      "Epoch [130], train_loss: 987.61 with loss1: 987.61 and loss2: 0.00\n",
      "Epoch [131], train_loss: 973.70 with loss1: 973.70 and loss2: 0.00\n",
      "Epoch [132], train_loss: 935.26 with loss1: 935.26 and loss2: 0.00\n",
      "Epoch [133], train_loss: 1016.82 with loss1: 1016.82 and loss2: 0.00\n",
      "Epoch [134], train_loss: 934.94 with loss1: 934.94 and loss2: 0.00\n",
      "Epoch [135], train_loss: 1077.53 with loss1: 1077.53 and loss2: 0.00\n",
      "Epoch [136], train_loss: 830.66 with loss1: 830.66 and loss2: 0.00\n",
      "Epoch [137], train_loss: 1084.56 with loss1: 1084.56 and loss2: 0.00\n",
      "Epoch [138], train_loss: 946.97 with loss1: 946.97 and loss2: 0.00\n",
      "Epoch [139], train_loss: 828.56 with loss1: 828.56 and loss2: 0.00\n",
      "Epoch [140], train_loss: 1073.48 with loss1: 1073.48 and loss2: 0.00\n",
      "Epoch [141], train_loss: 816.54 with loss1: 816.54 and loss2: 0.00\n",
      "Epoch [142], train_loss: 1092.72 with loss1: 1092.72 and loss2: 0.00\n",
      "Epoch [143], train_loss: 1266.43 with loss1: 1266.43 and loss2: 0.00\n",
      "Epoch [144], train_loss: 863.28 with loss1: 863.28 and loss2: 0.00\n",
      "Epoch [145], train_loss: 1056.79 with loss1: 1056.79 and loss2: 0.00\n",
      "Epoch [146], train_loss: 992.81 with loss1: 992.81 and loss2: 0.00\n",
      "Epoch [147], train_loss: 1254.00 with loss1: 1254.00 and loss2: 0.00\n",
      "Epoch [148], train_loss: 867.38 with loss1: 867.38 and loss2: 0.00\n",
      "Epoch [149], train_loss: 1013.54 with loss1: 1013.54 and loss2: 0.00\n",
      "Epoch [150], train_loss: 922.25 with loss1: 922.25 and loss2: 0.00\n",
      "Epoch [151], train_loss: 1128.39 with loss1: 1128.39 and loss2: 0.00\n",
      "Epoch [152], train_loss: 1013.81 with loss1: 1013.81 and loss2: 0.00\n",
      "Epoch [153], train_loss: 963.17 with loss1: 963.17 and loss2: 0.00\n",
      "Epoch [154], train_loss: 912.11 with loss1: 912.11 and loss2: 0.00\n",
      "Epoch [155], train_loss: 1031.14 with loss1: 1031.14 and loss2: 0.00\n",
      "Epoch [156], train_loss: 950.58 with loss1: 950.58 and loss2: 0.00\n",
      "Epoch [157], train_loss: 1181.80 with loss1: 1181.80 and loss2: 0.00\n",
      "Epoch [158], train_loss: 950.23 with loss1: 950.23 and loss2: 0.00\n",
      "Epoch [159], train_loss: 1241.77 with loss1: 1241.77 and loss2: 0.00\n",
      "Epoch [160], train_loss: 995.51 with loss1: 995.51 and loss2: 0.00\n",
      "Epoch [161], train_loss: 1240.40 with loss1: 1240.40 and loss2: 0.00\n",
      "Epoch [162], train_loss: 842.19 with loss1: 842.19 and loss2: 0.00\n",
      "Epoch [163], train_loss: 797.18 with loss1: 797.18 and loss2: 0.00\n",
      "Epoch [164], train_loss: 1022.14 with loss1: 1022.14 and loss2: 0.00\n",
      "Epoch [165], train_loss: 883.20 with loss1: 883.20 and loss2: 0.00\n",
      "Epoch [166], train_loss: 908.75 with loss1: 908.75 and loss2: 0.00\n",
      "Epoch [167], train_loss: 1144.66 with loss1: 1144.66 and loss2: 0.00\n",
      "Epoch [168], train_loss: 929.99 with loss1: 929.99 and loss2: 0.00\n",
      "Epoch [169], train_loss: 1024.63 with loss1: 1024.63 and loss2: 0.00\n",
      "Epoch [170], train_loss: 1212.94 with loss1: 1212.94 and loss2: 0.00\n",
      "Epoch [171], train_loss: 926.27 with loss1: 926.27 and loss2: 0.00\n",
      "Epoch [172], train_loss: 908.99 with loss1: 908.99 and loss2: 0.00\n",
      "Epoch [173], train_loss: 1164.58 with loss1: 1164.58 and loss2: 0.00\n",
      "Epoch [174], train_loss: 811.94 with loss1: 811.94 and loss2: 0.00\n",
      "Epoch [175], train_loss: 1024.24 with loss1: 1024.24 and loss2: 0.00\n",
      "Epoch [176], train_loss: 1006.88 with loss1: 1006.88 and loss2: 0.00\n",
      "Epoch [177], train_loss: 967.65 with loss1: 967.65 and loss2: 0.00\n",
      "Epoch [178], train_loss: 891.16 with loss1: 891.16 and loss2: 0.00\n",
      "Epoch [179], train_loss: 900.32 with loss1: 900.32 and loss2: 0.00\n",
      "Epoch [180], train_loss: 943.11 with loss1: 943.11 and loss2: 0.00\n",
      "Epoch [181], train_loss: 975.86 with loss1: 975.86 and loss2: 0.00\n",
      "Epoch [182], train_loss: 948.74 with loss1: 948.74 and loss2: 0.00\n",
      "Epoch [183], train_loss: 995.12 with loss1: 995.12 and loss2: 0.00\n",
      "Epoch [184], train_loss: 1285.78 with loss1: 1285.78 and loss2: 0.00\n",
      "Epoch [185], train_loss: 821.60 with loss1: 821.60 and loss2: 0.00\n",
      "Epoch [186], train_loss: 940.34 with loss1: 940.34 and loss2: 0.00\n",
      "Epoch [187], train_loss: 1181.95 with loss1: 1181.95 and loss2: 0.00\n",
      "Epoch [188], train_loss: 792.55 with loss1: 792.55 and loss2: 0.00\n",
      "Epoch [189], train_loss: 890.09 with loss1: 890.09 and loss2: 0.00\n",
      "Epoch [190], train_loss: 889.89 with loss1: 889.89 and loss2: 0.00\n",
      "Epoch [191], train_loss: 1080.45 with loss1: 1080.45 and loss2: 0.00\n",
      "Epoch [192], train_loss: 891.46 with loss1: 891.46 and loss2: 0.00\n",
      "Epoch [193], train_loss: 849.97 with loss1: 849.97 and loss2: 0.00\n",
      "Epoch [194], train_loss: 977.34 with loss1: 977.34 and loss2: 0.00\n",
      "Epoch [195], train_loss: 914.01 with loss1: 914.01 and loss2: 0.00\n",
      "Epoch [196], train_loss: 888.03 with loss1: 888.03 and loss2: 0.00\n",
      "Epoch [197], train_loss: 1035.10 with loss1: 1035.10 and loss2: 0.00\n",
      "Epoch [198], train_loss: 785.12 with loss1: 785.12 and loss2: 0.00\n",
      "Epoch [199], train_loss: 1024.49 with loss1: 1024.49 and loss2: 0.00\n",
      "Epoch [200], train_loss: 1207.76 with loss1: 1207.76 and loss2: 0.00\n",
      "Epoch [201], train_loss: 979.71 with loss1: 979.71 and loss2: 0.00\n",
      "Epoch [202], train_loss: 1065.30 with loss1: 1065.30 and loss2: 0.00\n",
      "Epoch [203], train_loss: 1314.93 with loss1: 1314.93 and loss2: 0.00\n",
      "Epoch [204], train_loss: 1376.00 with loss1: 1376.00 and loss2: 0.00\n",
      "Epoch [205], train_loss: 872.89 with loss1: 872.89 and loss2: 0.00\n",
      "Epoch [206], train_loss: 1054.35 with loss1: 1054.35 and loss2: 0.00\n",
      "Epoch [207], train_loss: 1253.80 with loss1: 1253.80 and loss2: 0.00\n",
      "Epoch [208], train_loss: 1031.32 with loss1: 1031.32 and loss2: 0.00\n",
      "Epoch [209], train_loss: 783.50 with loss1: 783.50 and loss2: 0.00\n",
      "Epoch [210], train_loss: 906.15 with loss1: 906.15 and loss2: 0.00\n",
      "Epoch [211], train_loss: 782.54 with loss1: 782.54 and loss2: 0.00\n",
      "Epoch [212], train_loss: 881.82 with loss1: 881.82 and loss2: 0.00\n",
      "Epoch [213], train_loss: 950.95 with loss1: 950.95 and loss2: 0.00\n",
      "Epoch [214], train_loss: 877.60 with loss1: 877.60 and loss2: 0.00\n",
      "Epoch [215], train_loss: 883.41 with loss1: 883.41 and loss2: 0.00\n",
      "Epoch [216], train_loss: 1136.06 with loss1: 1136.06 and loss2: 0.00\n",
      "Epoch [217], train_loss: 882.18 with loss1: 882.18 and loss2: 0.00\n",
      "Epoch [218], train_loss: 975.32 with loss1: 975.32 and loss2: 0.00\n",
      "Epoch [219], train_loss: 748.98 with loss1: 748.98 and loss2: 0.00\n",
      "Epoch [220], train_loss: 904.48 with loss1: 904.48 and loss2: 0.00\n",
      "Epoch [221], train_loss: 942.45 with loss1: 942.45 and loss2: 0.00\n",
      "Epoch [222], train_loss: 931.55 with loss1: 931.55 and loss2: 0.00\n",
      "Epoch [223], train_loss: 895.61 with loss1: 895.61 and loss2: 0.00\n",
      "Epoch [224], train_loss: 966.48 with loss1: 966.48 and loss2: 0.00\n",
      "Epoch [225], train_loss: 836.74 with loss1: 836.74 and loss2: 0.00\n",
      "Epoch [226], train_loss: 863.94 with loss1: 863.94 and loss2: 0.00\n",
      "Epoch [227], train_loss: 1071.23 with loss1: 1071.23 and loss2: 0.00\n",
      "Epoch [228], train_loss: 1282.02 with loss1: 1282.02 and loss2: 0.00\n",
      "Epoch [229], train_loss: 1382.48 with loss1: 1382.48 and loss2: 0.00\n",
      "Epoch [230], train_loss: 852.45 with loss1: 852.45 and loss2: 0.00\n",
      "Epoch [231], train_loss: 889.98 with loss1: 889.98 and loss2: 0.00\n",
      "Epoch [232], train_loss: 1199.88 with loss1: 1199.88 and loss2: 0.00\n",
      "Epoch [233], train_loss: 1296.18 with loss1: 1296.18 and loss2: 0.00\n",
      "Epoch [234], train_loss: 1260.12 with loss1: 1260.12 and loss2: 0.00\n",
      "Epoch [235], train_loss: 935.92 with loss1: 935.92 and loss2: 0.00\n",
      "Epoch [236], train_loss: 762.33 with loss1: 762.33 and loss2: 0.00\n",
      "Epoch [237], train_loss: 940.04 with loss1: 940.04 and loss2: 0.00\n",
      "Epoch [238], train_loss: 860.43 with loss1: 860.43 and loss2: 0.00\n",
      "Epoch [239], train_loss: 866.32 with loss1: 866.32 and loss2: 0.00\n",
      "Epoch [240], train_loss: 1103.70 with loss1: 1103.70 and loss2: 0.00\n",
      "Epoch [241], train_loss: 1188.11 with loss1: 1188.11 and loss2: 0.00\n",
      "Epoch [242], train_loss: 1183.45 with loss1: 1183.45 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [243], train_loss: 940.05 with loss1: 940.05 and loss2: 0.00\n",
      "Epoch [244], train_loss: 919.76 with loss1: 919.76 and loss2: 0.00\n",
      "Epoch [245], train_loss: 1056.52 with loss1: 1056.52 and loss2: 0.00\n",
      "Epoch [246], train_loss: 966.10 with loss1: 966.10 and loss2: 0.00\n",
      "Epoch [247], train_loss: 1155.17 with loss1: 1155.17 and loss2: 0.00\n",
      "Epoch [248], train_loss: 1012.15 with loss1: 1012.15 and loss2: 0.00\n",
      "Epoch [249], train_loss: 1166.26 with loss1: 1166.26 and loss2: 0.00\n",
      "Epoch [250], train_loss: 1244.88 with loss1: 1244.88 and loss2: 0.00\n",
      "Epoch [251], train_loss: 1163.69 with loss1: 1163.69 and loss2: 0.00\n",
      "Epoch [252], train_loss: 866.56 with loss1: 866.56 and loss2: 0.00\n",
      "Epoch [253], train_loss: 928.86 with loss1: 928.86 and loss2: 0.00\n",
      "Epoch [254], train_loss: 739.66 with loss1: 739.66 and loss2: 0.00\n",
      "Epoch [255], train_loss: 783.56 with loss1: 783.56 and loss2: 0.00\n",
      "Epoch [256], train_loss: 865.16 with loss1: 865.16 and loss2: 0.00\n",
      "Epoch [257], train_loss: 896.14 with loss1: 896.14 and loss2: 0.00\n",
      "Epoch [258], train_loss: 1147.95 with loss1: 1147.95 and loss2: 0.00\n",
      "Epoch [259], train_loss: 950.80 with loss1: 950.80 and loss2: 0.00\n",
      "Epoch [260], train_loss: 819.10 with loss1: 819.10 and loss2: 0.00\n",
      "Epoch [261], train_loss: 942.01 with loss1: 942.01 and loss2: 0.00\n",
      "Epoch [262], train_loss: 901.74 with loss1: 901.74 and loss2: 0.00\n",
      "Epoch [263], train_loss: 742.59 with loss1: 742.59 and loss2: 0.00\n",
      "Epoch [264], train_loss: 1026.60 with loss1: 1026.60 and loss2: 0.00\n",
      "Epoch [265], train_loss: 745.57 with loss1: 745.57 and loss2: 0.00\n",
      "Epoch [266], train_loss: 1019.17 with loss1: 1019.17 and loss2: 0.00\n",
      "Epoch [267], train_loss: 866.57 with loss1: 866.57 and loss2: 0.00\n",
      "Epoch [268], train_loss: 1061.31 with loss1: 1061.31 and loss2: 0.00\n",
      "Epoch [269], train_loss: 863.17 with loss1: 863.17 and loss2: 0.00\n",
      "Epoch [270], train_loss: 870.24 with loss1: 870.24 and loss2: 0.00\n",
      "Epoch [271], train_loss: 839.81 with loss1: 839.81 and loss2: 0.00\n",
      "Epoch [272], train_loss: 896.01 with loss1: 896.01 and loss2: 0.00\n",
      "Epoch [273], train_loss: 840.23 with loss1: 840.23 and loss2: 0.00\n",
      "Epoch [274], train_loss: 739.75 with loss1: 739.75 and loss2: 0.00\n",
      "Epoch [275], train_loss: 925.77 with loss1: 925.77 and loss2: 0.00\n",
      "Epoch [276], train_loss: 908.93 with loss1: 908.93 and loss2: 0.00\n",
      "Epoch [277], train_loss: 1080.08 with loss1: 1080.08 and loss2: 0.00\n",
      "Epoch [278], train_loss: 1156.09 with loss1: 1156.09 and loss2: 0.00\n",
      "Epoch [279], train_loss: 1004.24 with loss1: 1004.24 and loss2: 0.00\n",
      "Epoch [280], train_loss: 926.02 with loss1: 926.02 and loss2: 0.00\n",
      "Epoch [281], train_loss: 878.46 with loss1: 878.46 and loss2: 0.00\n",
      "Epoch [282], train_loss: 732.72 with loss1: 732.72 and loss2: 0.00\n",
      "Epoch [283], train_loss: 845.07 with loss1: 845.07 and loss2: 0.00\n",
      "Epoch [284], train_loss: 884.88 with loss1: 884.88 and loss2: 0.00\n",
      "Epoch [285], train_loss: 863.95 with loss1: 863.95 and loss2: 0.00\n",
      "Epoch [286], train_loss: 827.39 with loss1: 827.39 and loss2: 0.00\n",
      "Epoch [287], train_loss: 995.18 with loss1: 995.18 and loss2: 0.00\n",
      "Epoch [288], train_loss: 734.06 with loss1: 734.06 and loss2: 0.00\n",
      "Epoch [289], train_loss: 840.88 with loss1: 840.88 and loss2: 0.00\n",
      "Epoch [290], train_loss: 810.15 with loss1: 810.15 and loss2: 0.00\n",
      "Epoch [291], train_loss: 902.35 with loss1: 902.35 and loss2: 0.00\n",
      "Epoch [292], train_loss: 1115.96 with loss1: 1115.96 and loss2: 0.00\n",
      "Epoch [293], train_loss: 1176.68 with loss1: 1176.68 and loss2: 0.00\n",
      "Epoch [294], train_loss: 985.52 with loss1: 985.52 and loss2: 0.00\n",
      "Epoch [295], train_loss: 762.08 with loss1: 762.08 and loss2: 0.00\n",
      "Epoch [296], train_loss: 760.89 with loss1: 760.89 and loss2: 0.00\n",
      "Epoch [297], train_loss: 1011.35 with loss1: 1011.35 and loss2: 0.00\n",
      "Epoch [298], train_loss: 851.26 with loss1: 851.26 and loss2: 0.00\n",
      "Epoch [299], train_loss: 973.28 with loss1: 973.28 and loss2: 0.00\n",
      "Epoch [300], train_loss: 905.27 with loss1: 905.27 and loss2: 0.00\n",
      "Epoch [301], train_loss: 1089.33 with loss1: 1089.33 and loss2: 0.00\n",
      "Epoch [302], train_loss: 759.25 with loss1: 759.25 and loss2: 0.00\n",
      "Epoch [303], train_loss: 945.08 with loss1: 945.08 and loss2: 0.00\n",
      "Epoch [304], train_loss: 922.05 with loss1: 922.05 and loss2: 0.00\n",
      "Epoch [305], train_loss: 774.01 with loss1: 774.01 and loss2: 0.00\n",
      "Epoch [306], train_loss: 923.97 with loss1: 923.97 and loss2: 0.00\n",
      "Epoch [307], train_loss: 1105.95 with loss1: 1105.95 and loss2: 0.00\n",
      "Epoch [308], train_loss: 1130.25 with loss1: 1130.25 and loss2: 0.00\n",
      "Epoch [309], train_loss: 759.30 with loss1: 759.30 and loss2: 0.00\n",
      "Epoch [310], train_loss: 996.40 with loss1: 996.40 and loss2: 0.00\n",
      "Epoch [311], train_loss: 877.91 with loss1: 877.91 and loss2: 0.00\n",
      "Epoch [312], train_loss: 873.58 with loss1: 873.58 and loss2: 0.00\n",
      "Epoch [313], train_loss: 770.39 with loss1: 770.39 and loss2: 0.00\n",
      "Epoch [314], train_loss: 1105.09 with loss1: 1105.09 and loss2: 0.00\n",
      "Epoch [315], train_loss: 840.05 with loss1: 840.05 and loss2: 0.00\n",
      "Epoch [316], train_loss: 1063.35 with loss1: 1063.35 and loss2: 0.00\n",
      "Epoch [317], train_loss: 826.86 with loss1: 826.86 and loss2: 0.00\n",
      "Epoch [318], train_loss: 1156.28 with loss1: 1156.28 and loss2: 0.00\n",
      "Epoch [319], train_loss: 1296.71 with loss1: 1296.71 and loss2: 0.00\n",
      "Epoch [320], train_loss: 870.61 with loss1: 870.61 and loss2: 0.00\n",
      "Epoch [321], train_loss: 705.24 with loss1: 705.24 and loss2: 0.00\n",
      "Epoch [322], train_loss: 946.04 with loss1: 946.04 and loss2: 0.00\n",
      "Epoch [323], train_loss: 1070.88 with loss1: 1070.88 and loss2: 0.00\n",
      "Epoch [324], train_loss: 714.15 with loss1: 714.15 and loss2: 0.00\n",
      "Epoch [325], train_loss: 789.92 with loss1: 789.92 and loss2: 0.00\n",
      "Epoch [326], train_loss: 801.09 with loss1: 801.09 and loss2: 0.00\n",
      "Epoch [327], train_loss: 821.65 with loss1: 821.65 and loss2: 0.00\n",
      "Epoch [328], train_loss: 710.98 with loss1: 710.98 and loss2: 0.00\n",
      "Epoch [329], train_loss: 730.12 with loss1: 730.12 and loss2: 0.00\n",
      "Epoch [330], train_loss: 810.31 with loss1: 810.31 and loss2: 0.00\n",
      "Epoch [331], train_loss: 703.17 with loss1: 703.17 and loss2: 0.00\n",
      "Epoch [332], train_loss: 803.12 with loss1: 803.12 and loss2: 0.00\n",
      "Epoch [333], train_loss: 795.80 with loss1: 795.80 and loss2: 0.00\n",
      "Epoch [334], train_loss: 929.23 with loss1: 929.23 and loss2: 0.00\n",
      "Epoch [335], train_loss: 1189.46 with loss1: 1189.46 and loss2: 0.00\n",
      "Epoch [336], train_loss: 883.92 with loss1: 883.92 and loss2: 0.00\n",
      "Epoch [337], train_loss: 827.96 with loss1: 827.96 and loss2: 0.00\n",
      "Epoch [338], train_loss: 1030.45 with loss1: 1030.45 and loss2: 0.00\n",
      "Epoch [339], train_loss: 754.99 with loss1: 754.99 and loss2: 0.00\n",
      "Epoch [340], train_loss: 910.90 with loss1: 910.90 and loss2: 0.00\n",
      "Epoch [341], train_loss: 1014.34 with loss1: 1014.34 and loss2: 0.00\n",
      "Epoch [342], train_loss: 934.07 with loss1: 934.07 and loss2: 0.00\n",
      "Epoch [343], train_loss: 861.36 with loss1: 861.36 and loss2: 0.00\n",
      "Epoch [344], train_loss: 746.69 with loss1: 746.69 and loss2: 0.00\n",
      "Epoch [345], train_loss: 880.88 with loss1: 880.88 and loss2: 0.00\n",
      "Epoch [346], train_loss: 749.93 with loss1: 749.93 and loss2: 0.00\n",
      "Epoch [347], train_loss: 1054.44 with loss1: 1054.44 and loss2: 0.00\n",
      "Epoch [348], train_loss: 917.64 with loss1: 917.64 and loss2: 0.00\n",
      "Epoch [349], train_loss: 1104.13 with loss1: 1104.13 and loss2: 0.00\n",
      "Epoch [350], train_loss: 1259.67 with loss1: 1259.67 and loss2: 0.00\n",
      "Epoch [351], train_loss: 1008.55 with loss1: 1008.55 and loss2: 0.00\n",
      "Epoch [352], train_loss: 899.69 with loss1: 899.69 and loss2: 0.00\n",
      "Epoch [353], train_loss: 1087.31 with loss1: 1087.31 and loss2: 0.00\n",
      "Epoch [354], train_loss: 706.12 with loss1: 706.12 and loss2: 0.00\n",
      "Epoch [355], train_loss: 711.79 with loss1: 711.79 and loss2: 0.00\n",
      "Epoch [356], train_loss: 902.06 with loss1: 902.06 and loss2: 0.00\n",
      "Epoch [357], train_loss: 741.97 with loss1: 741.97 and loss2: 0.00\n",
      "Epoch [358], train_loss: 792.84 with loss1: 792.84 and loss2: 0.00\n",
      "Epoch [359], train_loss: 874.43 with loss1: 874.43 and loss2: 0.00\n",
      "Epoch [360], train_loss: 817.09 with loss1: 817.09 and loss2: 0.00\n",
      "Epoch [361], train_loss: 706.49 with loss1: 706.49 and loss2: 0.00\n",
      "Epoch [362], train_loss: 800.25 with loss1: 800.25 and loss2: 0.00\n",
      "Epoch [363], train_loss: 691.35 with loss1: 691.35 and loss2: 0.00\n",
      "Epoch [364], train_loss: 835.44 with loss1: 835.44 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [365], train_loss: 935.54 with loss1: 935.54 and loss2: 0.00\n",
      "Epoch [366], train_loss: 833.83 with loss1: 833.83 and loss2: 0.00\n",
      "Epoch [367], train_loss: 839.21 with loss1: 839.21 and loss2: 0.00\n",
      "Epoch [368], train_loss: 695.41 with loss1: 695.41 and loss2: 0.00\n",
      "Epoch [369], train_loss: 920.79 with loss1: 920.79 and loss2: 0.00\n",
      "Epoch [370], train_loss: 807.53 with loss1: 807.53 and loss2: 0.00\n",
      "Epoch [371], train_loss: 910.13 with loss1: 910.13 and loss2: 0.00\n",
      "Epoch [372], train_loss: 820.25 with loss1: 820.25 and loss2: 0.00\n",
      "Epoch [373], train_loss: 680.07 with loss1: 680.07 and loss2: 0.00\n",
      "Epoch [374], train_loss: 921.58 with loss1: 921.58 and loss2: 0.00\n",
      "Epoch [375], train_loss: 815.24 with loss1: 815.24 and loss2: 0.00\n",
      "Epoch [376], train_loss: 907.82 with loss1: 907.82 and loss2: 0.00\n",
      "Epoch [377], train_loss: 922.98 with loss1: 922.98 and loss2: 0.00\n",
      "Epoch [378], train_loss: 824.98 with loss1: 824.98 and loss2: 0.00\n",
      "Epoch [379], train_loss: 738.78 with loss1: 738.78 and loss2: 0.00\n",
      "Epoch [380], train_loss: 796.91 with loss1: 796.91 and loss2: 0.00\n",
      "Epoch [381], train_loss: 853.83 with loss1: 853.83 and loss2: 0.00\n",
      "Epoch [382], train_loss: 819.24 with loss1: 819.24 and loss2: 0.00\n",
      "Epoch [383], train_loss: 1058.05 with loss1: 1058.05 and loss2: 0.00\n",
      "Epoch [384], train_loss: 971.43 with loss1: 971.43 and loss2: 0.00\n",
      "Epoch [385], train_loss: 844.39 with loss1: 844.39 and loss2: 0.00\n",
      "Epoch [386], train_loss: 765.66 with loss1: 765.66 and loss2: 0.00\n",
      "Epoch [387], train_loss: 874.31 with loss1: 874.31 and loss2: 0.00\n",
      "Epoch [388], train_loss: 711.32 with loss1: 711.32 and loss2: 0.00\n",
      "Epoch [389], train_loss: 922.61 with loss1: 922.61 and loss2: 0.00\n",
      "Epoch [390], train_loss: 802.79 with loss1: 802.79 and loss2: 0.00\n",
      "Epoch [391], train_loss: 717.64 with loss1: 717.64 and loss2: 0.00\n",
      "Epoch [392], train_loss: 783.01 with loss1: 783.01 and loss2: 0.00\n",
      "Epoch [393], train_loss: 870.37 with loss1: 870.37 and loss2: 0.00\n",
      "Epoch [394], train_loss: 901.96 with loss1: 901.96 and loss2: 0.00\n",
      "Epoch [395], train_loss: 860.93 with loss1: 860.93 and loss2: 0.00\n",
      "Epoch [396], train_loss: 740.56 with loss1: 740.56 and loss2: 0.00\n",
      "Epoch [397], train_loss: 865.67 with loss1: 865.67 and loss2: 0.00\n",
      "Epoch [398], train_loss: 1037.32 with loss1: 1037.32 and loss2: 0.00\n",
      "Epoch [399], train_loss: 701.14 with loss1: 701.14 and loss2: 0.00\n",
      "Epoch [400], train_loss: 783.56 with loss1: 783.56 and loss2: 0.00\n",
      "Epoch [401], train_loss: 871.19 with loss1: 871.19 and loss2: 0.00\n",
      "Epoch [402], train_loss: 1113.91 with loss1: 1113.91 and loss2: 0.00\n",
      "Epoch [403], train_loss: 825.46 with loss1: 825.46 and loss2: 0.00\n",
      "Epoch [404], train_loss: 676.04 with loss1: 676.04 and loss2: 0.00\n",
      "Epoch [405], train_loss: 687.29 with loss1: 687.29 and loss2: 0.00\n",
      "Epoch [406], train_loss: 766.83 with loss1: 766.83 and loss2: 0.00\n",
      "Epoch [407], train_loss: 779.25 with loss1: 779.25 and loss2: 0.00\n",
      "Epoch [408], train_loss: 1021.23 with loss1: 1021.23 and loss2: 0.00\n",
      "Epoch [409], train_loss: 810.53 with loss1: 810.53 and loss2: 0.00\n",
      "Epoch [410], train_loss: 887.59 with loss1: 887.59 and loss2: 0.00\n",
      "Epoch [411], train_loss: 669.34 with loss1: 669.34 and loss2: 0.00\n",
      "Epoch [412], train_loss: 787.84 with loss1: 787.84 and loss2: 0.00\n",
      "Epoch [413], train_loss: 990.55 with loss1: 990.55 and loss2: 0.00\n",
      "Epoch [414], train_loss: 683.12 with loss1: 683.12 and loss2: 0.00\n",
      "Epoch [415], train_loss: 899.73 with loss1: 899.73 and loss2: 0.00\n",
      "Epoch [416], train_loss: 780.37 with loss1: 780.37 and loss2: 0.00\n",
      "Epoch [417], train_loss: 831.61 with loss1: 831.61 and loss2: 0.00\n",
      "Epoch [418], train_loss: 964.61 with loss1: 964.61 and loss2: 0.00\n",
      "Epoch [419], train_loss: 1086.05 with loss1: 1086.05 and loss2: 0.00\n",
      "Epoch [420], train_loss: 709.38 with loss1: 709.38 and loss2: 0.00\n",
      "Epoch [421], train_loss: 871.04 with loss1: 871.04 and loss2: 0.00\n",
      "Epoch [422], train_loss: 782.87 with loss1: 782.87 and loss2: 0.00\n",
      "Epoch [423], train_loss: 905.89 with loss1: 905.89 and loss2: 0.00\n",
      "Epoch [424], train_loss: 694.88 with loss1: 694.88 and loss2: 0.00\n",
      "Epoch [425], train_loss: 830.16 with loss1: 830.16 and loss2: 0.00\n",
      "Epoch [426], train_loss: 992.17 with loss1: 992.17 and loss2: 0.00\n",
      "Epoch [427], train_loss: 1137.51 with loss1: 1137.51 and loss2: 0.00\n",
      "Epoch [428], train_loss: 703.96 with loss1: 703.96 and loss2: 0.00\n",
      "Epoch [429], train_loss: 837.55 with loss1: 837.55 and loss2: 0.00\n",
      "Epoch [430], train_loss: 848.10 with loss1: 848.10 and loss2: 0.00\n",
      "Epoch [431], train_loss: 786.61 with loss1: 786.61 and loss2: 0.00\n",
      "Epoch [432], train_loss: 773.38 with loss1: 773.38 and loss2: 0.00\n",
      "Epoch [433], train_loss: 830.01 with loss1: 830.01 and loss2: 0.00\n",
      "Epoch [434], train_loss: 1016.78 with loss1: 1016.78 and loss2: 0.00\n",
      "Epoch [435], train_loss: 1006.29 with loss1: 1006.29 and loss2: 0.00\n",
      "Epoch [436], train_loss: 938.08 with loss1: 938.08 and loss2: 0.00\n",
      "Epoch [437], train_loss: 743.45 with loss1: 743.45 and loss2: 0.00\n",
      "Epoch [438], train_loss: 944.44 with loss1: 944.44 and loss2: 0.00\n",
      "Epoch [439], train_loss: 806.94 with loss1: 806.94 and loss2: 0.00\n",
      "Epoch [440], train_loss: 836.95 with loss1: 836.95 and loss2: 0.00\n",
      "Epoch [441], train_loss: 655.06 with loss1: 655.06 and loss2: 0.00\n",
      "Epoch [442], train_loss: 901.66 with loss1: 901.66 and loss2: 0.00\n",
      "Epoch [443], train_loss: 1003.64 with loss1: 1003.64 and loss2: 0.00\n",
      "Epoch [444], train_loss: 850.82 with loss1: 850.82 and loss2: 0.00\n",
      "Epoch [445], train_loss: 1017.37 with loss1: 1017.37 and loss2: 0.00\n",
      "Epoch [446], train_loss: 1066.88 with loss1: 1066.88 and loss2: 0.00\n",
      "Epoch [447], train_loss: 743.24 with loss1: 743.24 and loss2: 0.00\n",
      "Epoch [448], train_loss: 819.88 with loss1: 819.88 and loss2: 0.00\n",
      "Epoch [449], train_loss: 816.00 with loss1: 816.00 and loss2: 0.00\n",
      "Epoch [450], train_loss: 658.90 with loss1: 658.90 and loss2: 0.00\n",
      "Epoch [451], train_loss: 764.63 with loss1: 764.63 and loss2: 0.00\n",
      "Epoch [452], train_loss: 948.28 with loss1: 948.28 and loss2: 0.00\n",
      "Epoch [453], train_loss: 690.95 with loss1: 690.95 and loss2: 0.00\n",
      "Epoch [454], train_loss: 808.13 with loss1: 808.13 and loss2: 0.00\n",
      "Epoch [455], train_loss: 800.79 with loss1: 800.79 and loss2: 0.00\n",
      "Epoch [456], train_loss: 767.19 with loss1: 767.19 and loss2: 0.00\n",
      "Epoch [457], train_loss: 842.29 with loss1: 842.29 and loss2: 0.00\n",
      "Epoch [458], train_loss: 756.90 with loss1: 756.90 and loss2: 0.00\n",
      "Epoch [459], train_loss: 846.34 with loss1: 846.34 and loss2: 0.00\n",
      "Epoch [460], train_loss: 996.57 with loss1: 996.57 and loss2: 0.00\n",
      "Epoch [461], train_loss: 1000.90 with loss1: 1000.90 and loss2: 0.00\n",
      "Epoch [462], train_loss: 830.61 with loss1: 830.61 and loss2: 0.00\n",
      "Epoch [463], train_loss: 686.33 with loss1: 686.33 and loss2: 0.00\n",
      "Epoch [464], train_loss: 930.79 with loss1: 930.79 and loss2: 0.00\n",
      "Epoch [465], train_loss: 775.55 with loss1: 775.55 and loss2: 0.00\n",
      "Epoch [466], train_loss: 1001.78 with loss1: 1001.78 and loss2: 0.00\n",
      "Epoch [467], train_loss: 1126.38 with loss1: 1126.38 and loss2: 0.00\n",
      "Epoch [468], train_loss: 747.89 with loss1: 747.89 and loss2: 0.00\n",
      "Epoch [469], train_loss: 845.85 with loss1: 845.85 and loss2: 0.00\n",
      "Epoch [470], train_loss: 756.68 with loss1: 756.68 and loss2: 0.00\n",
      "Epoch [471], train_loss: 830.01 with loss1: 830.01 and loss2: 0.00\n",
      "Epoch [472], train_loss: 783.53 with loss1: 783.53 and loss2: 0.00\n",
      "Epoch [473], train_loss: 947.96 with loss1: 947.96 and loss2: 0.00\n",
      "Epoch [474], train_loss: 754.94 with loss1: 754.94 and loss2: 0.00\n",
      "Epoch [475], train_loss: 784.68 with loss1: 784.68 and loss2: 0.00\n",
      "Epoch [476], train_loss: 650.22 with loss1: 650.22 and loss2: 0.00\n",
      "Epoch [477], train_loss: 731.20 with loss1: 731.20 and loss2: 0.00\n",
      "Epoch [478], train_loss: 946.49 with loss1: 946.49 and loss2: 0.00\n",
      "Epoch [479], train_loss: 888.42 with loss1: 888.42 and loss2: 0.00\n",
      "Epoch [480], train_loss: 800.45 with loss1: 800.45 and loss2: 0.00\n",
      "Epoch [481], train_loss: 779.28 with loss1: 779.28 and loss2: 0.00\n",
      "Epoch [482], train_loss: 957.66 with loss1: 957.66 and loss2: 0.00\n",
      "Epoch [483], train_loss: 644.80 with loss1: 644.80 and loss2: 0.00\n",
      "Epoch [484], train_loss: 775.55 with loss1: 775.55 and loss2: 0.00\n",
      "Epoch [485], train_loss: 820.22 with loss1: 820.22 and loss2: 0.00\n",
      "Epoch [486], train_loss: 649.02 with loss1: 649.02 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [487], train_loss: 722.29 with loss1: 722.29 and loss2: 0.00\n",
      "Epoch [488], train_loss: 921.08 with loss1: 921.08 and loss2: 0.00\n",
      "Epoch [489], train_loss: 753.39 with loss1: 753.39 and loss2: 0.00\n",
      "Epoch [490], train_loss: 694.48 with loss1: 694.48 and loss2: 0.00\n",
      "Epoch [491], train_loss: 721.07 with loss1: 721.07 and loss2: 0.00\n",
      "Epoch [492], train_loss: 747.56 with loss1: 747.56 and loss2: 0.00\n",
      "Epoch [493], train_loss: 867.09 with loss1: 867.09 and loss2: 0.00\n",
      "Epoch [494], train_loss: 666.37 with loss1: 666.37 and loss2: 0.00\n",
      "Epoch [495], train_loss: 697.53 with loss1: 697.53 and loss2: 0.00\n",
      "Epoch [496], train_loss: 805.02 with loss1: 805.02 and loss2: 0.00\n",
      "Epoch [497], train_loss: 796.52 with loss1: 796.52 and loss2: 0.00\n",
      "Epoch [498], train_loss: 757.75 with loss1: 757.75 and loss2: 0.00\n",
      "Epoch [499], train_loss: 810.82 with loss1: 810.82 and loss2: 0.00\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# model version 3 \n",
    "# delete loss2\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history, mu_history = fit(epochs=500, lr=2e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eca7e03b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 1690.04 with loss1: 1690.04 and loss2: 0.00\n",
      "Epoch [1], train_loss: 25270.99 with loss1: 25270.99 and loss2: 0.00\n",
      "Epoch [2], train_loss: 7796.91 with loss1: 7796.91 and loss2: 0.00\n",
      "Epoch [3], train_loss: 6368.46 with loss1: 6368.46 and loss2: 0.00\n",
      "Epoch [4], train_loss: 5159.51 with loss1: 5159.51 and loss2: 0.00\n",
      "Epoch [5], train_loss: 4377.92 with loss1: 4377.92 and loss2: 0.00\n",
      "Epoch [6], train_loss: 6268.36 with loss1: 6268.36 and loss2: 0.00\n",
      "Epoch [7], train_loss: 5298.87 with loss1: 5298.87 and loss2: 0.00\n",
      "Epoch [8], train_loss: 5589.13 with loss1: 5589.13 and loss2: 0.00\n",
      "Epoch [9], train_loss: 4226.54 with loss1: 4226.54 and loss2: 0.00\n",
      "Epoch [10], train_loss: 2583.08 with loss1: 2583.08 and loss2: 0.00\n",
      "Epoch [11], train_loss: 3197.10 with loss1: 3197.10 and loss2: 0.00\n",
      "Epoch [12], train_loss: 2609.66 with loss1: 2609.66 and loss2: 0.00\n",
      "Epoch [13], train_loss: 2013.29 with loss1: 2013.29 and loss2: 0.00\n",
      "Epoch [14], train_loss: 4701.47 with loss1: 4701.47 and loss2: 0.00\n",
      "Epoch [15], train_loss: 2838.24 with loss1: 2838.24 and loss2: 0.00\n",
      "Epoch [16], train_loss: 2942.82 with loss1: 2942.82 and loss2: 0.00\n",
      "Epoch [17], train_loss: 4649.29 with loss1: 4649.29 and loss2: 0.00\n",
      "Epoch [18], train_loss: 3532.21 with loss1: 3532.21 and loss2: 0.00\n",
      "Epoch [19], train_loss: 1724.25 with loss1: 1724.25 and loss2: 0.00\n",
      "Epoch [20], train_loss: 1624.72 with loss1: 1624.72 and loss2: 0.00\n",
      "Epoch [21], train_loss: 1630.62 with loss1: 1630.62 and loss2: 0.00\n",
      "Epoch [22], train_loss: 1335.16 with loss1: 1335.16 and loss2: 0.00\n",
      "Epoch [23], train_loss: 1231.25 with loss1: 1231.25 and loss2: 0.00\n",
      "Epoch [24], train_loss: 1209.40 with loss1: 1209.40 and loss2: 0.00\n",
      "Epoch [25], train_loss: 962.28 with loss1: 962.28 and loss2: 0.00\n",
      "Epoch [26], train_loss: 1108.85 with loss1: 1108.85 and loss2: 0.00\n",
      "Epoch [27], train_loss: 1077.09 with loss1: 1077.09 and loss2: 0.00\n",
      "Epoch [28], train_loss: 1549.38 with loss1: 1549.38 and loss2: 0.00\n",
      "Epoch [29], train_loss: 2844.79 with loss1: 2844.79 and loss2: 0.00\n",
      "Epoch [30], train_loss: 3782.83 with loss1: 3782.83 and loss2: 0.00\n",
      "Epoch [31], train_loss: 2044.48 with loss1: 2044.48 and loss2: 0.00\n",
      "Epoch [32], train_loss: 1133.56 with loss1: 1133.56 and loss2: 0.00\n",
      "Epoch [33], train_loss: 1001.34 with loss1: 1001.34 and loss2: 0.00\n",
      "Epoch [34], train_loss: 1433.28 with loss1: 1433.28 and loss2: 0.00\n",
      "Epoch [35], train_loss: 1160.31 with loss1: 1160.31 and loss2: 0.00\n",
      "Epoch [36], train_loss: 874.80 with loss1: 874.80 and loss2: 0.00\n",
      "Epoch [37], train_loss: 932.90 with loss1: 932.90 and loss2: 0.00\n",
      "Epoch [38], train_loss: 807.84 with loss1: 807.84 and loss2: 0.00\n",
      "Epoch [39], train_loss: 1337.18 with loss1: 1337.18 and loss2: 0.00\n",
      "Epoch [40], train_loss: 993.21 with loss1: 993.21 and loss2: 0.00\n",
      "Epoch [41], train_loss: 969.54 with loss1: 969.54 and loss2: 0.00\n",
      "Epoch [42], train_loss: 858.43 with loss1: 858.43 and loss2: 0.00\n",
      "Epoch [43], train_loss: 883.13 with loss1: 883.13 and loss2: 0.00\n",
      "Epoch [44], train_loss: 953.71 with loss1: 953.71 and loss2: 0.00\n",
      "Epoch [45], train_loss: 1388.81 with loss1: 1388.81 and loss2: 0.00\n",
      "Epoch [46], train_loss: 1345.59 with loss1: 1345.59 and loss2: 0.00\n",
      "Epoch [47], train_loss: 1144.70 with loss1: 1144.70 and loss2: 0.00\n",
      "Epoch [48], train_loss: 2006.29 with loss1: 2006.29 and loss2: 0.00\n",
      "Epoch [49], train_loss: 881.98 with loss1: 881.98 and loss2: 0.00\n",
      "Epoch [50], train_loss: 1260.49 with loss1: 1260.49 and loss2: 0.00\n",
      "Epoch [51], train_loss: 2064.33 with loss1: 2064.33 and loss2: 0.00\n",
      "Epoch [52], train_loss: 1385.67 with loss1: 1385.67 and loss2: 0.00\n",
      "Epoch [53], train_loss: 970.54 with loss1: 970.54 and loss2: 0.00\n",
      "Epoch [54], train_loss: 1645.69 with loss1: 1645.69 and loss2: 0.00\n",
      "Epoch [55], train_loss: 1868.37 with loss1: 1868.37 and loss2: 0.00\n",
      "Epoch [56], train_loss: 1201.75 with loss1: 1201.75 and loss2: 0.00\n",
      "Epoch [57], train_loss: 1016.83 with loss1: 1016.83 and loss2: 0.00\n",
      "Epoch [58], train_loss: 1530.01 with loss1: 1530.01 and loss2: 0.00\n",
      "Epoch [59], train_loss: 1040.93 with loss1: 1040.93 and loss2: 0.00\n",
      "Epoch [60], train_loss: 1891.17 with loss1: 1891.17 and loss2: 0.00\n",
      "Epoch [61], train_loss: 1085.15 with loss1: 1085.15 and loss2: 0.00\n",
      "Epoch [62], train_loss: 981.30 with loss1: 981.30 and loss2: 0.00\n",
      "Epoch [63], train_loss: 955.49 with loss1: 955.49 and loss2: 0.00\n",
      "Epoch [64], train_loss: 1626.63 with loss1: 1626.63 and loss2: 0.00\n",
      "Epoch [65], train_loss: 2202.74 with loss1: 2202.74 and loss2: 0.00\n",
      "Epoch [66], train_loss: 2620.60 with loss1: 2620.60 and loss2: 0.00\n",
      "Epoch [67], train_loss: 1183.45 with loss1: 1183.45 and loss2: 0.00\n",
      "Epoch [68], train_loss: 1273.06 with loss1: 1273.06 and loss2: 0.00\n",
      "Epoch [69], train_loss: 1061.16 with loss1: 1061.16 and loss2: 0.00\n",
      "Epoch [70], train_loss: 1003.24 with loss1: 1003.24 and loss2: 0.00\n",
      "Epoch [71], train_loss: 1610.33 with loss1: 1610.33 and loss2: 0.00\n",
      "Epoch [72], train_loss: 902.09 with loss1: 902.09 and loss2: 0.00\n",
      "Epoch [73], train_loss: 1425.86 with loss1: 1425.86 and loss2: 0.00\n",
      "Epoch [74], train_loss: 807.24 with loss1: 807.24 and loss2: 0.00\n",
      "Epoch [75], train_loss: 1007.83 with loss1: 1007.83 and loss2: 0.00\n",
      "Epoch [76], train_loss: 878.56 with loss1: 878.56 and loss2: 0.00\n",
      "Epoch [77], train_loss: 889.64 with loss1: 889.64 and loss2: 0.00\n",
      "Epoch [78], train_loss: 906.02 with loss1: 906.02 and loss2: 0.00\n",
      "Epoch [79], train_loss: 1780.42 with loss1: 1780.42 and loss2: 0.00\n",
      "Epoch [80], train_loss: 1574.03 with loss1: 1574.03 and loss2: 0.00\n",
      "Epoch [81], train_loss: 1533.33 with loss1: 1533.33 and loss2: 0.00\n",
      "Epoch [82], train_loss: 1987.16 with loss1: 1987.16 and loss2: 0.00\n",
      "Epoch [83], train_loss: 1078.96 with loss1: 1078.96 and loss2: 0.00\n",
      "Epoch [84], train_loss: 1111.97 with loss1: 1111.97 and loss2: 0.00\n",
      "Epoch [85], train_loss: 1712.97 with loss1: 1712.97 and loss2: 0.00\n",
      "Epoch [86], train_loss: 1086.24 with loss1: 1086.24 and loss2: 0.00\n",
      "Epoch [87], train_loss: 1086.53 with loss1: 1086.53 and loss2: 0.00\n",
      "Epoch [88], train_loss: 880.28 with loss1: 880.28 and loss2: 0.00\n",
      "Epoch [89], train_loss: 935.16 with loss1: 935.16 and loss2: 0.00\n",
      "Epoch [90], train_loss: 897.86 with loss1: 897.86 and loss2: 0.00\n",
      "Epoch [91], train_loss: 894.56 with loss1: 894.56 and loss2: 0.00\n",
      "Epoch [92], train_loss: 874.48 with loss1: 874.48 and loss2: 0.00\n",
      "Epoch [93], train_loss: 1357.91 with loss1: 1357.91 and loss2: 0.00\n",
      "Epoch [94], train_loss: 2068.09 with loss1: 2068.09 and loss2: 0.00\n",
      "Epoch [95], train_loss: 1746.12 with loss1: 1746.12 and loss2: 0.00\n",
      "Epoch [96], train_loss: 997.59 with loss1: 997.59 and loss2: 0.00\n",
      "Epoch [97], train_loss: 1180.64 with loss1: 1180.64 and loss2: 0.00\n",
      "Epoch [98], train_loss: 907.05 with loss1: 907.05 and loss2: 0.00\n",
      "Epoch [99], train_loss: 939.05 with loss1: 939.05 and loss2: 0.00\n",
      "Epoch [100], train_loss: 874.80 with loss1: 874.80 and loss2: 0.00\n",
      "Epoch [101], train_loss: 750.73 with loss1: 750.73 and loss2: 0.00\n",
      "Epoch [102], train_loss: 1024.45 with loss1: 1024.45 and loss2: 0.00\n",
      "Epoch [103], train_loss: 841.77 with loss1: 841.77 and loss2: 0.00\n",
      "Epoch [104], train_loss: 978.22 with loss1: 978.22 and loss2: 0.00\n",
      "Epoch [105], train_loss: 863.53 with loss1: 863.53 and loss2: 0.00\n",
      "Epoch [106], train_loss: 923.45 with loss1: 923.45 and loss2: 0.00\n",
      "Epoch [107], train_loss: 1517.07 with loss1: 1517.07 and loss2: 0.00\n",
      "Epoch [108], train_loss: 934.87 with loss1: 934.87 and loss2: 0.00\n",
      "Epoch [109], train_loss: 901.78 with loss1: 901.78 and loss2: 0.00\n",
      "Epoch [110], train_loss: 980.61 with loss1: 980.61 and loss2: 0.00\n",
      "Epoch [111], train_loss: 831.42 with loss1: 831.42 and loss2: 0.00\n",
      "Epoch [112], train_loss: 983.69 with loss1: 983.69 and loss2: 0.00\n",
      "Epoch [113], train_loss: 855.38 with loss1: 855.38 and loss2: 0.00\n",
      "Epoch [114], train_loss: 905.19 with loss1: 905.19 and loss2: 0.00\n",
      "Epoch [115], train_loss: 858.48 with loss1: 858.48 and loss2: 0.00\n",
      "Epoch [116], train_loss: 855.35 with loss1: 855.35 and loss2: 0.00\n",
      "Epoch [117], train_loss: 1051.08 with loss1: 1051.08 and loss2: 0.00\n",
      "Epoch [118], train_loss: 789.65 with loss1: 789.65 and loss2: 0.00\n",
      "Epoch [119], train_loss: 762.90 with loss1: 762.90 and loss2: 0.00\n",
      "Epoch [120], train_loss: 671.43 with loss1: 671.43 and loss2: 0.00\n",
      "Epoch [121], train_loss: 1142.19 with loss1: 1142.19 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [122], train_loss: 845.61 with loss1: 845.61 and loss2: 0.00\n",
      "Epoch [123], train_loss: 945.28 with loss1: 945.28 and loss2: 0.00\n",
      "Epoch [124], train_loss: 975.74 with loss1: 975.74 and loss2: 0.00\n",
      "Epoch [125], train_loss: 845.20 with loss1: 845.20 and loss2: 0.00\n",
      "Epoch [126], train_loss: 720.64 with loss1: 720.64 and loss2: 0.00\n",
      "Epoch [127], train_loss: 1071.21 with loss1: 1071.21 and loss2: 0.00\n",
      "Epoch [128], train_loss: 822.49 with loss1: 822.49 and loss2: 0.00\n",
      "Epoch [129], train_loss: 1388.49 with loss1: 1388.49 and loss2: 0.00\n",
      "Epoch [130], train_loss: 968.71 with loss1: 968.71 and loss2: 0.00\n",
      "Epoch [131], train_loss: 772.56 with loss1: 772.56 and loss2: 0.00\n",
      "Epoch [132], train_loss: 678.25 with loss1: 678.25 and loss2: 0.00\n",
      "Epoch [133], train_loss: 1301.85 with loss1: 1301.85 and loss2: 0.00\n",
      "Epoch [134], train_loss: 831.52 with loss1: 831.52 and loss2: 0.00\n",
      "Epoch [135], train_loss: 814.57 with loss1: 814.57 and loss2: 0.00\n",
      "Epoch [136], train_loss: 1105.38 with loss1: 1105.38 and loss2: 0.00\n",
      "Epoch [137], train_loss: 814.28 with loss1: 814.28 and loss2: 0.00\n",
      "Epoch [138], train_loss: 725.74 with loss1: 725.74 and loss2: 0.00\n",
      "Epoch [139], train_loss: 885.09 with loss1: 885.09 and loss2: 0.00\n",
      "Epoch [140], train_loss: 816.78 with loss1: 816.78 and loss2: 0.00\n",
      "Epoch [141], train_loss: 739.27 with loss1: 739.27 and loss2: 0.00\n",
      "Epoch [142], train_loss: 1332.71 with loss1: 1332.71 and loss2: 0.00\n",
      "Epoch [143], train_loss: 1000.25 with loss1: 1000.25 and loss2: 0.00\n",
      "Epoch [144], train_loss: 1605.98 with loss1: 1605.98 and loss2: 0.00\n",
      "Epoch [145], train_loss: 1965.44 with loss1: 1965.44 and loss2: 0.00\n",
      "Epoch [146], train_loss: 1837.49 with loss1: 1837.49 and loss2: 0.00\n",
      "Epoch [147], train_loss: 878.27 with loss1: 878.27 and loss2: 0.00\n",
      "Epoch [148], train_loss: 736.08 with loss1: 736.08 and loss2: 0.00\n",
      "Epoch [149], train_loss: 1015.85 with loss1: 1015.85 and loss2: 0.00\n",
      "Epoch [150], train_loss: 917.63 with loss1: 917.63 and loss2: 0.00\n",
      "Epoch [151], train_loss: 710.72 with loss1: 710.72 and loss2: 0.00\n",
      "Epoch [152], train_loss: 1008.40 with loss1: 1008.40 and loss2: 0.00\n",
      "Epoch [153], train_loss: 856.58 with loss1: 856.58 and loss2: 0.00\n",
      "Epoch [154], train_loss: 902.91 with loss1: 902.91 and loss2: 0.00\n",
      "Epoch [155], train_loss: 1444.72 with loss1: 1444.72 and loss2: 0.00\n",
      "Epoch [156], train_loss: 1579.46 with loss1: 1579.46 and loss2: 0.00\n",
      "Epoch [157], train_loss: 1030.85 with loss1: 1030.85 and loss2: 0.00\n",
      "Epoch [158], train_loss: 1201.59 with loss1: 1201.59 and loss2: 0.00\n",
      "Epoch [159], train_loss: 1441.87 with loss1: 1441.87 and loss2: 0.00\n",
      "Epoch [160], train_loss: 902.14 with loss1: 902.14 and loss2: 0.00\n",
      "Epoch [161], train_loss: 968.40 with loss1: 968.40 and loss2: 0.00\n",
      "Epoch [162], train_loss: 735.53 with loss1: 735.53 and loss2: 0.00\n",
      "Epoch [163], train_loss: 650.38 with loss1: 650.38 and loss2: 0.00\n",
      "Epoch [164], train_loss: 962.82 with loss1: 962.82 and loss2: 0.00\n",
      "Epoch [165], train_loss: 1304.37 with loss1: 1304.37 and loss2: 0.00\n",
      "Epoch [166], train_loss: 795.65 with loss1: 795.65 and loss2: 0.00\n",
      "Epoch [167], train_loss: 635.10 with loss1: 635.10 and loss2: 0.00\n",
      "Epoch [168], train_loss: 591.59 with loss1: 591.59 and loss2: 0.00\n",
      "Epoch [169], train_loss: 995.84 with loss1: 995.84 and loss2: 0.00\n",
      "Epoch [170], train_loss: 712.41 with loss1: 712.41 and loss2: 0.00\n",
      "Epoch [171], train_loss: 861.72 with loss1: 861.72 and loss2: 0.00\n",
      "Epoch [172], train_loss: 762.92 with loss1: 762.92 and loss2: 0.00\n",
      "Epoch [173], train_loss: 845.52 with loss1: 845.52 and loss2: 0.00\n",
      "Epoch [174], train_loss: 755.07 with loss1: 755.07 and loss2: 0.00\n",
      "Epoch [175], train_loss: 1141.56 with loss1: 1141.56 and loss2: 0.00\n",
      "Epoch [176], train_loss: 1599.68 with loss1: 1599.68 and loss2: 0.00\n",
      "Epoch [177], train_loss: 1752.82 with loss1: 1752.82 and loss2: 0.00\n",
      "Epoch [178], train_loss: 1569.84 with loss1: 1569.84 and loss2: 0.00\n",
      "Epoch [179], train_loss: 1010.07 with loss1: 1010.07 and loss2: 0.00\n",
      "Epoch [180], train_loss: 785.87 with loss1: 785.87 and loss2: 0.00\n",
      "Epoch [181], train_loss: 1020.05 with loss1: 1020.05 and loss2: 0.00\n",
      "Epoch [182], train_loss: 761.76 with loss1: 761.76 and loss2: 0.00\n",
      "Epoch [183], train_loss: 842.15 with loss1: 842.15 and loss2: 0.00\n",
      "Epoch [184], train_loss: 777.57 with loss1: 777.57 and loss2: 0.00\n",
      "Epoch [185], train_loss: 722.40 with loss1: 722.40 and loss2: 0.00\n",
      "Epoch [186], train_loss: 928.01 with loss1: 928.01 and loss2: 0.00\n",
      "Epoch [187], train_loss: 1345.45 with loss1: 1345.45 and loss2: 0.00\n",
      "Epoch [188], train_loss: 721.60 with loss1: 721.60 and loss2: 0.00\n",
      "Epoch [189], train_loss: 1095.67 with loss1: 1095.67 and loss2: 0.00\n",
      "Epoch [190], train_loss: 1322.36 with loss1: 1322.36 and loss2: 0.00\n",
      "Epoch [191], train_loss: 993.12 with loss1: 993.12 and loss2: 0.00\n",
      "Epoch [192], train_loss: 1144.18 with loss1: 1144.18 and loss2: 0.00\n",
      "Epoch [193], train_loss: 747.01 with loss1: 747.01 and loss2: 0.00\n",
      "Epoch [194], train_loss: 806.79 with loss1: 806.79 and loss2: 0.00\n",
      "Epoch [195], train_loss: 779.45 with loss1: 779.45 and loss2: 0.00\n",
      "Epoch [196], train_loss: 717.72 with loss1: 717.72 and loss2: 0.00\n",
      "Epoch [197], train_loss: 878.98 with loss1: 878.98 and loss2: 0.00\n",
      "Epoch [198], train_loss: 1315.11 with loss1: 1315.11 and loss2: 0.00\n",
      "Epoch [199], train_loss: 789.06 with loss1: 789.06 and loss2: 0.00\n",
      "Epoch [200], train_loss: 1197.65 with loss1: 1197.65 and loss2: 0.00\n",
      "Epoch [201], train_loss: 1382.36 with loss1: 1382.36 and loss2: 0.00\n",
      "Epoch [202], train_loss: 1393.62 with loss1: 1393.62 and loss2: 0.00\n",
      "Epoch [203], train_loss: 1387.32 with loss1: 1387.32 and loss2: 0.00\n",
      "Epoch [204], train_loss: 1281.17 with loss1: 1281.17 and loss2: 0.00\n",
      "Epoch [205], train_loss: 679.72 with loss1: 679.72 and loss2: 0.00\n",
      "Epoch [206], train_loss: 743.98 with loss1: 743.98 and loss2: 0.00\n",
      "Epoch [207], train_loss: 801.71 with loss1: 801.71 and loss2: 0.00\n",
      "Epoch [208], train_loss: 821.98 with loss1: 821.98 and loss2: 0.00\n",
      "Epoch [209], train_loss: 745.27 with loss1: 745.27 and loss2: 0.00\n",
      "Epoch [210], train_loss: 729.58 with loss1: 729.58 and loss2: 0.00\n",
      "Epoch [211], train_loss: 908.94 with loss1: 908.94 and loss2: 0.00\n",
      "Epoch [212], train_loss: 741.43 with loss1: 741.43 and loss2: 0.00\n",
      "Epoch [213], train_loss: 823.90 with loss1: 823.90 and loss2: 0.00\n",
      "Epoch [214], train_loss: 1229.72 with loss1: 1229.72 and loss2: 0.00\n",
      "Epoch [215], train_loss: 1259.89 with loss1: 1259.89 and loss2: 0.00\n",
      "Epoch [216], train_loss: 1292.42 with loss1: 1292.42 and loss2: 0.00\n",
      "Epoch [217], train_loss: 1287.26 with loss1: 1287.26 and loss2: 0.00\n",
      "Epoch [218], train_loss: 897.96 with loss1: 897.96 and loss2: 0.00\n",
      "Epoch [219], train_loss: 1178.72 with loss1: 1178.72 and loss2: 0.00\n",
      "Epoch [220], train_loss: 728.90 with loss1: 728.90 and loss2: 0.00\n",
      "Epoch [221], train_loss: 939.86 with loss1: 939.86 and loss2: 0.00\n",
      "Epoch [222], train_loss: 771.62 with loss1: 771.62 and loss2: 0.00\n",
      "Epoch [223], train_loss: 723.17 with loss1: 723.17 and loss2: 0.00\n",
      "Epoch [224], train_loss: 910.58 with loss1: 910.58 and loss2: 0.00\n",
      "Epoch [225], train_loss: 757.37 with loss1: 757.37 and loss2: 0.00\n",
      "Epoch [226], train_loss: 1059.59 with loss1: 1059.59 and loss2: 0.00\n",
      "Epoch [227], train_loss: 801.17 with loss1: 801.17 and loss2: 0.00\n",
      "Epoch [228], train_loss: 778.52 with loss1: 778.52 and loss2: 0.00\n",
      "Epoch [229], train_loss: 692.37 with loss1: 692.37 and loss2: 0.00\n",
      "Epoch [230], train_loss: 912.95 with loss1: 912.95 and loss2: 0.00\n",
      "Epoch [231], train_loss: 662.65 with loss1: 662.65 and loss2: 0.00\n",
      "Epoch [232], train_loss: 974.90 with loss1: 974.90 and loss2: 0.00\n",
      "Epoch [233], train_loss: 798.13 with loss1: 798.13 and loss2: 0.00\n",
      "Epoch [234], train_loss: 681.86 with loss1: 681.86 and loss2: 0.00\n",
      "Epoch [235], train_loss: 783.20 with loss1: 783.20 and loss2: 0.00\n",
      "Epoch [236], train_loss: 724.41 with loss1: 724.41 and loss2: 0.00\n",
      "Epoch [237], train_loss: 632.85 with loss1: 632.85 and loss2: 0.00\n",
      "Epoch [238], train_loss: 865.30 with loss1: 865.30 and loss2: 0.00\n",
      "Epoch [239], train_loss: 1130.85 with loss1: 1130.85 and loss2: 0.00\n",
      "Epoch [240], train_loss: 813.97 with loss1: 813.97 and loss2: 0.00\n",
      "Epoch [241], train_loss: 856.42 with loss1: 856.42 and loss2: 0.00\n",
      "Epoch [242], train_loss: 737.06 with loss1: 737.06 and loss2: 0.00\n",
      "Epoch [243], train_loss: 769.00 with loss1: 769.00 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [244], train_loss: 694.69 with loss1: 694.69 and loss2: 0.00\n",
      "Epoch [245], train_loss: 782.65 with loss1: 782.65 and loss2: 0.00\n",
      "Epoch [246], train_loss: 779.84 with loss1: 779.84 and loss2: 0.00\n",
      "Epoch [247], train_loss: 689.40 with loss1: 689.40 and loss2: 0.00\n",
      "Epoch [248], train_loss: 1018.06 with loss1: 1018.06 and loss2: 0.00\n",
      "Epoch [249], train_loss: 1291.83 with loss1: 1291.83 and loss2: 0.00\n",
      "Epoch [250], train_loss: 1477.57 with loss1: 1477.57 and loss2: 0.00\n",
      "Epoch [251], train_loss: 843.74 with loss1: 843.74 and loss2: 0.00\n",
      "Epoch [252], train_loss: 1037.75 with loss1: 1037.75 and loss2: 0.00\n",
      "Epoch [253], train_loss: 1217.41 with loss1: 1217.41 and loss2: 0.00\n",
      "Epoch [254], train_loss: 729.13 with loss1: 729.13 and loss2: 0.00\n",
      "Epoch [255], train_loss: 731.91 with loss1: 731.91 and loss2: 0.00\n",
      "Epoch [256], train_loss: 981.13 with loss1: 981.13 and loss2: 0.00\n",
      "Epoch [257], train_loss: 630.70 with loss1: 630.70 and loss2: 0.00\n",
      "Epoch [258], train_loss: 908.04 with loss1: 908.04 and loss2: 0.00\n",
      "Epoch [259], train_loss: 662.37 with loss1: 662.37 and loss2: 0.00\n",
      "Epoch [260], train_loss: 983.77 with loss1: 983.77 and loss2: 0.00\n",
      "Epoch [261], train_loss: 739.00 with loss1: 739.00 and loss2: 0.00\n",
      "Epoch [262], train_loss: 573.14 with loss1: 573.14 and loss2: 0.00\n",
      "Epoch [263], train_loss: 898.86 with loss1: 898.86 and loss2: 0.00\n",
      "Epoch [264], train_loss: 652.69 with loss1: 652.69 and loss2: 0.00\n",
      "Epoch [265], train_loss: 821.01 with loss1: 821.01 and loss2: 0.00\n",
      "Epoch [266], train_loss: 749.69 with loss1: 749.69 and loss2: 0.00\n",
      "Epoch [267], train_loss: 1048.59 with loss1: 1048.59 and loss2: 0.00\n",
      "Epoch [268], train_loss: 709.29 with loss1: 709.29 and loss2: 0.00\n",
      "Epoch [269], train_loss: 621.50 with loss1: 621.50 and loss2: 0.00\n",
      "Epoch [270], train_loss: 725.32 with loss1: 725.32 and loss2: 0.00\n",
      "Epoch [271], train_loss: 724.64 with loss1: 724.64 and loss2: 0.00\n",
      "Epoch [272], train_loss: 1198.62 with loss1: 1198.62 and loss2: 0.00\n",
      "Epoch [273], train_loss: 725.58 with loss1: 725.58 and loss2: 0.00\n",
      "Epoch [274], train_loss: 728.10 with loss1: 728.10 and loss2: 0.00\n",
      "Epoch [275], train_loss: 688.25 with loss1: 688.25 and loss2: 0.00\n",
      "Epoch [276], train_loss: 585.02 with loss1: 585.02 and loss2: 0.00\n",
      "Epoch [277], train_loss: 544.32 with loss1: 544.32 and loss2: 0.00\n",
      "Epoch [278], train_loss: 829.11 with loss1: 829.11 and loss2: 0.00\n",
      "Epoch [279], train_loss: 1122.17 with loss1: 1122.17 and loss2: 0.00\n",
      "Epoch [280], train_loss: 1323.51 with loss1: 1323.51 and loss2: 0.00\n",
      "Epoch [281], train_loss: 1300.84 with loss1: 1300.84 and loss2: 0.00\n",
      "Epoch [282], train_loss: 882.99 with loss1: 882.99 and loss2: 0.00\n",
      "Epoch [283], train_loss: 830.41 with loss1: 830.41 and loss2: 0.00\n",
      "Epoch [284], train_loss: 686.81 with loss1: 686.81 and loss2: 0.00\n",
      "Epoch [285], train_loss: 964.50 with loss1: 964.50 and loss2: 0.00\n",
      "Epoch [286], train_loss: 663.51 with loss1: 663.51 and loss2: 0.00\n",
      "Epoch [287], train_loss: 831.20 with loss1: 831.20 and loss2: 0.00\n",
      "Epoch [288], train_loss: 635.52 with loss1: 635.52 and loss2: 0.00\n",
      "Epoch [289], train_loss: 893.56 with loss1: 893.56 and loss2: 0.00\n",
      "Epoch [290], train_loss: 642.83 with loss1: 642.83 and loss2: 0.00\n",
      "Epoch [291], train_loss: 624.28 with loss1: 624.28 and loss2: 0.00\n",
      "Epoch [292], train_loss: 810.28 with loss1: 810.28 and loss2: 0.00\n",
      "Epoch [293], train_loss: 685.38 with loss1: 685.38 and loss2: 0.00\n",
      "Epoch [294], train_loss: 969.66 with loss1: 969.66 and loss2: 0.00\n",
      "Epoch [295], train_loss: 663.34 with loss1: 663.34 and loss2: 0.00\n",
      "Epoch [296], train_loss: 1042.89 with loss1: 1042.89 and loss2: 0.00\n",
      "Epoch [297], train_loss: 729.83 with loss1: 729.83 and loss2: 0.00\n",
      "Epoch [298], train_loss: 803.65 with loss1: 803.65 and loss2: 0.00\n",
      "Epoch [299], train_loss: 752.28 with loss1: 752.28 and loss2: 0.00\n",
      "Epoch [300], train_loss: 806.99 with loss1: 806.99 and loss2: 0.00\n",
      "Epoch [301], train_loss: 1060.62 with loss1: 1060.62 and loss2: 0.00\n",
      "Epoch [302], train_loss: 1060.67 with loss1: 1060.67 and loss2: 0.00\n",
      "Epoch [303], train_loss: 742.30 with loss1: 742.30 and loss2: 0.00\n",
      "Epoch [304], train_loss: 655.67 with loss1: 655.67 and loss2: 0.00\n",
      "Epoch [305], train_loss: 659.39 with loss1: 659.39 and loss2: 0.00\n",
      "Epoch [306], train_loss: 767.64 with loss1: 767.64 and loss2: 0.00\n",
      "Epoch [307], train_loss: 922.10 with loss1: 922.10 and loss2: 0.00\n",
      "Epoch [308], train_loss: 697.63 with loss1: 697.63 and loss2: 0.00\n",
      "Epoch [309], train_loss: 933.17 with loss1: 933.17 and loss2: 0.00\n",
      "Epoch [310], train_loss: 1078.53 with loss1: 1078.53 and loss2: 0.00\n",
      "Epoch [311], train_loss: 564.19 with loss1: 564.19 and loss2: 0.00\n",
      "Epoch [312], train_loss: 637.28 with loss1: 637.28 and loss2: 0.00\n",
      "Epoch [313], train_loss: 603.49 with loss1: 603.49 and loss2: 0.00\n",
      "Epoch [314], train_loss: 613.40 with loss1: 613.40 and loss2: 0.00\n",
      "Epoch [315], train_loss: 521.50 with loss1: 521.50 and loss2: 0.00\n",
      "Epoch [316], train_loss: 516.63 with loss1: 516.63 and loss2: 0.00\n",
      "Epoch [317], train_loss: 784.48 with loss1: 784.48 and loss2: 0.00\n",
      "Epoch [318], train_loss: 1014.86 with loss1: 1014.86 and loss2: 0.00\n",
      "Epoch [319], train_loss: 963.02 with loss1: 963.02 and loss2: 0.00\n",
      "Epoch [320], train_loss: 1135.60 with loss1: 1135.60 and loss2: 0.00\n",
      "Epoch [321], train_loss: 721.54 with loss1: 721.54 and loss2: 0.00\n",
      "Epoch [322], train_loss: 537.96 with loss1: 537.96 and loss2: 0.00\n",
      "Epoch [323], train_loss: 791.48 with loss1: 791.48 and loss2: 0.00\n",
      "Epoch [324], train_loss: 673.00 with loss1: 673.00 and loss2: 0.00\n",
      "Epoch [325], train_loss: 958.59 with loss1: 958.59 and loss2: 0.00\n",
      "Epoch [326], train_loss: 974.13 with loss1: 974.13 and loss2: 0.00\n",
      "Epoch [327], train_loss: 610.54 with loss1: 610.54 and loss2: 0.00\n",
      "Epoch [328], train_loss: 825.23 with loss1: 825.23 and loss2: 0.00\n",
      "Epoch [329], train_loss: 727.27 with loss1: 727.27 and loss2: 0.00\n",
      "Epoch [330], train_loss: 656.30 with loss1: 656.30 and loss2: 0.00\n",
      "Epoch [331], train_loss: 793.11 with loss1: 793.11 and loss2: 0.00\n",
      "Epoch [332], train_loss: 647.79 with loss1: 647.79 and loss2: 0.00\n",
      "Epoch [333], train_loss: 938.87 with loss1: 938.87 and loss2: 0.00\n",
      "Epoch [334], train_loss: 730.83 with loss1: 730.83 and loss2: 0.00\n",
      "Epoch [335], train_loss: 950.65 with loss1: 950.65 and loss2: 0.00\n",
      "Epoch [336], train_loss: 632.49 with loss1: 632.49 and loss2: 0.00\n",
      "Epoch [337], train_loss: 736.27 with loss1: 736.27 and loss2: 0.00\n",
      "Epoch [338], train_loss: 674.02 with loss1: 674.02 and loss2: 0.00\n",
      "Epoch [339], train_loss: 630.40 with loss1: 630.40 and loss2: 0.00\n",
      "Epoch [340], train_loss: 594.73 with loss1: 594.73 and loss2: 0.00\n",
      "Epoch [341], train_loss: 711.66 with loss1: 711.66 and loss2: 0.00\n",
      "Epoch [342], train_loss: 675.03 with loss1: 675.03 and loss2: 0.00\n",
      "Epoch [343], train_loss: 910.43 with loss1: 910.43 and loss2: 0.00\n",
      "Epoch [344], train_loss: 577.85 with loss1: 577.85 and loss2: 0.00\n",
      "Epoch [345], train_loss: 903.80 with loss1: 903.80 and loss2: 0.00\n",
      "Epoch [346], train_loss: 825.78 with loss1: 825.78 and loss2: 0.00\n",
      "Epoch [347], train_loss: 680.56 with loss1: 680.56 and loss2: 0.00\n",
      "Epoch [348], train_loss: 875.75 with loss1: 875.75 and loss2: 0.00\n",
      "Epoch [349], train_loss: 671.35 with loss1: 671.35 and loss2: 0.00\n",
      "Epoch [350], train_loss: 539.89 with loss1: 539.89 and loss2: 0.00\n",
      "Epoch [351], train_loss: 822.92 with loss1: 822.92 and loss2: 0.00\n",
      "Epoch [352], train_loss: 1132.86 with loss1: 1132.86 and loss2: 0.00\n",
      "Epoch [353], train_loss: 797.45 with loss1: 797.45 and loss2: 0.00\n",
      "Epoch [354], train_loss: 647.08 with loss1: 647.08 and loss2: 0.00\n",
      "Epoch [355], train_loss: 738.21 with loss1: 738.21 and loss2: 0.00\n",
      "Epoch [356], train_loss: 689.81 with loss1: 689.81 and loss2: 0.00\n",
      "Epoch [357], train_loss: 697.37 with loss1: 697.37 and loss2: 0.00\n",
      "Epoch [358], train_loss: 908.97 with loss1: 908.97 and loss2: 0.00\n",
      "Epoch [359], train_loss: 729.85 with loss1: 729.85 and loss2: 0.00\n",
      "Epoch [360], train_loss: 615.60 with loss1: 615.60 and loss2: 0.00\n",
      "Epoch [361], train_loss: 718.83 with loss1: 718.83 and loss2: 0.00\n",
      "Epoch [362], train_loss: 695.09 with loss1: 695.09 and loss2: 0.00\n",
      "Epoch [363], train_loss: 671.59 with loss1: 671.59 and loss2: 0.00\n",
      "Epoch [364], train_loss: 701.43 with loss1: 701.43 and loss2: 0.00\n",
      "Epoch [365], train_loss: 969.78 with loss1: 969.78 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [366], train_loss: 1229.07 with loss1: 1229.07 and loss2: 0.00\n",
      "Epoch [367], train_loss: 1205.76 with loss1: 1205.76 and loss2: 0.00\n",
      "Epoch [368], train_loss: 844.36 with loss1: 844.36 and loss2: 0.00\n",
      "Epoch [369], train_loss: 555.80 with loss1: 555.80 and loss2: 0.00\n",
      "Epoch [370], train_loss: 498.81 with loss1: 498.81 and loss2: 0.00\n",
      "Epoch [371], train_loss: 583.11 with loss1: 583.11 and loss2: 0.00\n",
      "Epoch [372], train_loss: 512.66 with loss1: 512.66 and loss2: 0.00\n",
      "Epoch [373], train_loss: 508.24 with loss1: 508.24 and loss2: 0.00\n",
      "Epoch [374], train_loss: 831.38 with loss1: 831.38 and loss2: 0.00\n",
      "Epoch [375], train_loss: 600.94 with loss1: 600.94 and loss2: 0.00\n",
      "Epoch [376], train_loss: 678.74 with loss1: 678.74 and loss2: 0.00\n",
      "Epoch [377], train_loss: 1028.84 with loss1: 1028.84 and loss2: 0.00\n",
      "Epoch [378], train_loss: 585.75 with loss1: 585.75 and loss2: 0.00\n",
      "Epoch [379], train_loss: 673.62 with loss1: 673.62 and loss2: 0.00\n",
      "Epoch [380], train_loss: 532.96 with loss1: 532.96 and loss2: 0.00\n",
      "Epoch [381], train_loss: 903.39 with loss1: 903.39 and loss2: 0.00\n",
      "Epoch [382], train_loss: 586.69 with loss1: 586.69 and loss2: 0.00\n",
      "Epoch [383], train_loss: 516.11 with loss1: 516.11 and loss2: 0.00\n",
      "Epoch [384], train_loss: 811.83 with loss1: 811.83 and loss2: 0.00\n",
      "Epoch [385], train_loss: 1036.32 with loss1: 1036.32 and loss2: 0.00\n",
      "Epoch [386], train_loss: 1101.46 with loss1: 1101.46 and loss2: 0.00\n",
      "Epoch [387], train_loss: 1024.75 with loss1: 1024.75 and loss2: 0.00\n",
      "Epoch [388], train_loss: 772.00 with loss1: 772.00 and loss2: 0.00\n",
      "Epoch [389], train_loss: 530.85 with loss1: 530.85 and loss2: 0.00\n",
      "Epoch [390], train_loss: 832.25 with loss1: 832.25 and loss2: 0.00\n",
      "Epoch [391], train_loss: 921.36 with loss1: 921.36 and loss2: 0.00\n",
      "Epoch [392], train_loss: 1019.49 with loss1: 1019.49 and loss2: 0.00\n",
      "Epoch [393], train_loss: 508.84 with loss1: 508.84 and loss2: 0.00\n",
      "Epoch [394], train_loss: 723.49 with loss1: 723.49 and loss2: 0.00\n",
      "Epoch [395], train_loss: 679.71 with loss1: 679.71 and loss2: 0.00\n",
      "Epoch [396], train_loss: 625.35 with loss1: 625.35 and loss2: 0.00\n",
      "Epoch [397], train_loss: 829.28 with loss1: 829.28 and loss2: 0.00\n",
      "Epoch [398], train_loss: 679.38 with loss1: 679.38 and loss2: 0.00\n",
      "Epoch [399], train_loss: 960.18 with loss1: 960.18 and loss2: 0.00\n",
      "Epoch [400], train_loss: 1003.52 with loss1: 1003.52 and loss2: 0.00\n",
      "Epoch [401], train_loss: 1040.86 with loss1: 1040.86 and loss2: 0.00\n",
      "Epoch [402], train_loss: 761.11 with loss1: 761.11 and loss2: 0.00\n",
      "Epoch [403], train_loss: 989.18 with loss1: 989.18 and loss2: 0.00\n",
      "Epoch [404], train_loss: 984.79 with loss1: 984.79 and loss2: 0.00\n",
      "Epoch [405], train_loss: 905.97 with loss1: 905.97 and loss2: 0.00\n",
      "Epoch [406], train_loss: 827.32 with loss1: 827.32 and loss2: 0.00\n",
      "Epoch [407], train_loss: 600.96 with loss1: 600.96 and loss2: 0.00\n",
      "Epoch [408], train_loss: 665.12 with loss1: 665.12 and loss2: 0.00\n",
      "Epoch [409], train_loss: 727.86 with loss1: 727.86 and loss2: 0.00\n",
      "Epoch [410], train_loss: 681.09 with loss1: 681.09 and loss2: 0.00\n",
      "Epoch [411], train_loss: 575.40 with loss1: 575.40 and loss2: 0.00\n",
      "Epoch [412], train_loss: 629.10 with loss1: 629.10 and loss2: 0.00\n",
      "Epoch [413], train_loss: 504.52 with loss1: 504.52 and loss2: 0.00\n",
      "Epoch [414], train_loss: 678.96 with loss1: 678.96 and loss2: 0.00\n",
      "Epoch [415], train_loss: 612.00 with loss1: 612.00 and loss2: 0.00\n",
      "Epoch [416], train_loss: 801.16 with loss1: 801.16 and loss2: 0.00\n",
      "Epoch [417], train_loss: 610.67 with loss1: 610.67 and loss2: 0.00\n",
      "Epoch [418], train_loss: 524.35 with loss1: 524.35 and loss2: 0.00\n",
      "Epoch [419], train_loss: 593.96 with loss1: 593.96 and loss2: 0.00\n",
      "Epoch [420], train_loss: 657.03 with loss1: 657.03 and loss2: 0.00\n",
      "Epoch [421], train_loss: 497.78 with loss1: 497.78 and loss2: 0.00\n",
      "Epoch [422], train_loss: 820.61 with loss1: 820.61 and loss2: 0.00\n",
      "Epoch [423], train_loss: 1084.41 with loss1: 1084.41 and loss2: 0.00\n",
      "Epoch [424], train_loss: 1023.66 with loss1: 1023.66 and loss2: 0.00\n",
      "Epoch [425], train_loss: 580.92 with loss1: 580.92 and loss2: 0.00\n",
      "Epoch [426], train_loss: 675.83 with loss1: 675.83 and loss2: 0.00\n",
      "Epoch [427], train_loss: 599.20 with loss1: 599.20 and loss2: 0.00\n",
      "Epoch [428], train_loss: 758.73 with loss1: 758.73 and loss2: 0.00\n",
      "Epoch [429], train_loss: 525.94 with loss1: 525.94 and loss2: 0.00\n",
      "Epoch [430], train_loss: 680.66 with loss1: 680.66 and loss2: 0.00\n",
      "Epoch [431], train_loss: 692.58 with loss1: 692.58 and loss2: 0.00\n",
      "Epoch [432], train_loss: 572.48 with loss1: 572.48 and loss2: 0.00\n",
      "Epoch [433], train_loss: 738.33 with loss1: 738.33 and loss2: 0.00\n",
      "Epoch [434], train_loss: 619.23 with loss1: 619.23 and loss2: 0.00\n",
      "Epoch [435], train_loss: 632.23 with loss1: 632.23 and loss2: 0.00\n",
      "Epoch [436], train_loss: 481.05 with loss1: 481.05 and loss2: 0.00\n",
      "Epoch [437], train_loss: 713.61 with loss1: 713.61 and loss2: 0.00\n",
      "Epoch [438], train_loss: 483.81 with loss1: 483.81 and loss2: 0.00\n",
      "Epoch [439], train_loss: 562.46 with loss1: 562.46 and loss2: 0.00\n",
      "Epoch [440], train_loss: 598.96 with loss1: 598.96 and loss2: 0.00\n",
      "Epoch [441], train_loss: 546.06 with loss1: 546.06 and loss2: 0.00\n",
      "Epoch [442], train_loss: 675.36 with loss1: 675.36 and loss2: 0.00\n",
      "Epoch [443], train_loss: 557.17 with loss1: 557.17 and loss2: 0.00\n",
      "Epoch [444], train_loss: 598.88 with loss1: 598.88 and loss2: 0.00\n",
      "Epoch [445], train_loss: 552.30 with loss1: 552.30 and loss2: 0.00\n",
      "Epoch [446], train_loss: 689.60 with loss1: 689.60 and loss2: 0.00\n",
      "Epoch [447], train_loss: 1036.89 with loss1: 1036.89 and loss2: 0.00\n",
      "Epoch [448], train_loss: 728.51 with loss1: 728.51 and loss2: 0.00\n",
      "Epoch [449], train_loss: 622.98 with loss1: 622.98 and loss2: 0.00\n",
      "Epoch [450], train_loss: 787.56 with loss1: 787.56 and loss2: 0.00\n",
      "Epoch [451], train_loss: 689.32 with loss1: 689.32 and loss2: 0.00\n",
      "Epoch [452], train_loss: 666.57 with loss1: 666.57 and loss2: 0.00\n",
      "Epoch [453], train_loss: 713.67 with loss1: 713.67 and loss2: 0.00\n",
      "Epoch [454], train_loss: 644.50 with loss1: 644.50 and loss2: 0.00\n",
      "Epoch [455], train_loss: 550.92 with loss1: 550.92 and loss2: 0.00\n",
      "Epoch [456], train_loss: 781.28 with loss1: 781.28 and loss2: 0.00\n",
      "Epoch [457], train_loss: 639.64 with loss1: 639.64 and loss2: 0.00\n",
      "Epoch [458], train_loss: 765.36 with loss1: 765.36 and loss2: 0.00\n",
      "Epoch [459], train_loss: 632.87 with loss1: 632.87 and loss2: 0.00\n",
      "Epoch [460], train_loss: 808.31 with loss1: 808.31 and loss2: 0.00\n",
      "Epoch [461], train_loss: 667.65 with loss1: 667.65 and loss2: 0.00\n",
      "Epoch [462], train_loss: 918.89 with loss1: 918.89 and loss2: 0.00\n",
      "Epoch [463], train_loss: 506.67 with loss1: 506.67 and loss2: 0.00\n",
      "Epoch [464], train_loss: 699.46 with loss1: 699.46 and loss2: 0.00\n",
      "Epoch [465], train_loss: 473.36 with loss1: 473.36 and loss2: 0.00\n",
      "Epoch [466], train_loss: 540.92 with loss1: 540.92 and loss2: 0.00\n",
      "Epoch [467], train_loss: 757.00 with loss1: 757.00 and loss2: 0.00\n",
      "Epoch [468], train_loss: 881.96 with loss1: 881.96 and loss2: 0.00\n",
      "Epoch [469], train_loss: 524.45 with loss1: 524.45 and loss2: 0.00\n",
      "Epoch [470], train_loss: 810.77 with loss1: 810.77 and loss2: 0.00\n",
      "Epoch [471], train_loss: 910.36 with loss1: 910.36 and loss2: 0.00\n",
      "Epoch [472], train_loss: 906.22 with loss1: 906.22 and loss2: 0.00\n",
      "Epoch [473], train_loss: 593.18 with loss1: 593.18 and loss2: 0.00\n",
      "Epoch [474], train_loss: 627.89 with loss1: 627.89 and loss2: 0.00\n",
      "Epoch [475], train_loss: 659.74 with loss1: 659.74 and loss2: 0.00\n",
      "Epoch [476], train_loss: 953.71 with loss1: 953.71 and loss2: 0.00\n",
      "Epoch [477], train_loss: 532.30 with loss1: 532.30 and loss2: 0.00\n",
      "Epoch [478], train_loss: 561.63 with loss1: 561.63 and loss2: 0.00\n",
      "Epoch [479], train_loss: 704.15 with loss1: 704.15 and loss2: 0.00\n",
      "Epoch [480], train_loss: 572.18 with loss1: 572.18 and loss2: 0.00\n",
      "Epoch [481], train_loss: 588.71 with loss1: 588.71 and loss2: 0.00\n",
      "Epoch [482], train_loss: 609.32 with loss1: 609.32 and loss2: 0.00\n",
      "Epoch [483], train_loss: 482.38 with loss1: 482.38 and loss2: 0.00\n",
      "Epoch [484], train_loss: 801.68 with loss1: 801.68 and loss2: 0.00\n",
      "Epoch [485], train_loss: 538.76 with loss1: 538.76 and loss2: 0.00\n",
      "Epoch [486], train_loss: 728.37 with loss1: 728.37 and loss2: 0.00\n",
      "Epoch [487], train_loss: 565.05 with loss1: 565.05 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [488], train_loss: 601.08 with loss1: 601.08 and loss2: 0.00\n",
      "Epoch [489], train_loss: 650.72 with loss1: 650.72 and loss2: 0.00\n",
      "Epoch [490], train_loss: 535.66 with loss1: 535.66 and loss2: 0.00\n",
      "Epoch [491], train_loss: 757.13 with loss1: 757.13 and loss2: 0.00\n",
      "Epoch [492], train_loss: 962.26 with loss1: 962.26 and loss2: 0.00\n",
      "Epoch [493], train_loss: 511.32 with loss1: 511.32 and loss2: 0.00\n",
      "Epoch [494], train_loss: 808.73 with loss1: 808.73 and loss2: 0.00\n",
      "Epoch [495], train_loss: 568.52 with loss1: 568.52 and loss2: 0.00\n",
      "Epoch [496], train_loss: 727.84 with loss1: 727.84 and loss2: 0.00\n",
      "Epoch [497], train_loss: 650.36 with loss1: 650.36 and loss2: 0.00\n",
      "Epoch [498], train_loss: 641.34 with loss1: 641.34 and loss2: 0.00\n",
      "Epoch [499], train_loss: 521.39 with loss1: 521.39 and loss2: 0.00\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# model version 3 \n",
    "# delete loss2\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history, mu_history = fit(epochs=500, lr=5e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f14e24c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 702.30 with loss1: 702.30 and loss2: 0.00\n",
      "Epoch [1], train_loss: 930.80 with loss1: 930.80 and loss2: 0.00\n",
      "Epoch [2], train_loss: 929.00 with loss1: 929.00 and loss2: 0.00\n",
      "Epoch [3], train_loss: 617.83 with loss1: 617.83 and loss2: 0.00\n",
      "Epoch [4], train_loss: 582.96 with loss1: 582.96 and loss2: 0.00\n",
      "Epoch [5], train_loss: 753.48 with loss1: 753.48 and loss2: 0.00\n",
      "Epoch [6], train_loss: 670.15 with loss1: 670.15 and loss2: 0.00\n",
      "Epoch [7], train_loss: 507.01 with loss1: 507.01 and loss2: 0.00\n",
      "Epoch [8], train_loss: 818.00 with loss1: 818.00 and loss2: 0.00\n",
      "Epoch [9], train_loss: 530.46 with loss1: 530.46 and loss2: 0.00\n",
      "Epoch [10], train_loss: 692.39 with loss1: 692.39 and loss2: 0.00\n",
      "Epoch [11], train_loss: 457.00 with loss1: 457.00 and loss2: 0.00\n",
      "Epoch [12], train_loss: 540.25 with loss1: 540.25 and loss2: 0.00\n",
      "Epoch [13], train_loss: 750.00 with loss1: 750.00 and loss2: 0.00\n",
      "Epoch [14], train_loss: 510.83 with loss1: 510.83 and loss2: 0.00\n",
      "Epoch [15], train_loss: 482.17 with loss1: 482.17 and loss2: 0.00\n",
      "Epoch [16], train_loss: 591.66 with loss1: 591.66 and loss2: 0.00\n",
      "Epoch [17], train_loss: 950.23 with loss1: 950.23 and loss2: 0.00\n",
      "Epoch [18], train_loss: 979.52 with loss1: 979.52 and loss2: 0.00\n",
      "Epoch [19], train_loss: 535.29 with loss1: 535.29 and loss2: 0.00\n",
      "Epoch [20], train_loss: 720.11 with loss1: 720.11 and loss2: 0.00\n",
      "Epoch [21], train_loss: 899.21 with loss1: 899.21 and loss2: 0.00\n",
      "Epoch [22], train_loss: 473.15 with loss1: 473.15 and loss2: 0.00\n",
      "Epoch [23], train_loss: 558.72 with loss1: 558.72 and loss2: 0.00\n",
      "Epoch [24], train_loss: 754.07 with loss1: 754.07 and loss2: 0.00\n",
      "Epoch [25], train_loss: 873.79 with loss1: 873.79 and loss2: 0.00\n",
      "Epoch [26], train_loss: 576.56 with loss1: 576.56 and loss2: 0.00\n",
      "Epoch [27], train_loss: 734.72 with loss1: 734.72 and loss2: 0.00\n",
      "Epoch [28], train_loss: 975.75 with loss1: 975.75 and loss2: 0.00\n",
      "Epoch [29], train_loss: 593.53 with loss1: 593.53 and loss2: 0.00\n",
      "Epoch [30], train_loss: 518.92 with loss1: 518.92 and loss2: 0.00\n",
      "Epoch [31], train_loss: 542.06 with loss1: 542.06 and loss2: 0.00\n",
      "Epoch [32], train_loss: 767.53 with loss1: 767.53 and loss2: 0.00\n",
      "Epoch [33], train_loss: 884.25 with loss1: 884.25 and loss2: 0.00\n",
      "Epoch [34], train_loss: 891.27 with loss1: 891.27 and loss2: 0.00\n",
      "Epoch [35], train_loss: 465.00 with loss1: 465.00 and loss2: 0.00\n",
      "Epoch [36], train_loss: 573.19 with loss1: 573.19 and loss2: 0.00\n",
      "Epoch [37], train_loss: 544.16 with loss1: 544.16 and loss2: 0.00\n",
      "Epoch [38], train_loss: 663.39 with loss1: 663.39 and loss2: 0.00\n",
      "Epoch [39], train_loss: 440.69 with loss1: 440.69 and loss2: 0.00\n",
      "Epoch [40], train_loss: 567.13 with loss1: 567.13 and loss2: 0.00\n",
      "Epoch [41], train_loss: 529.64 with loss1: 529.64 and loss2: 0.00\n",
      "Epoch [42], train_loss: 789.31 with loss1: 789.31 and loss2: 0.00\n",
      "Epoch [43], train_loss: 798.78 with loss1: 798.78 and loss2: 0.00\n",
      "Epoch [44], train_loss: 511.16 with loss1: 511.16 and loss2: 0.00\n",
      "Epoch [45], train_loss: 648.21 with loss1: 648.21 and loss2: 0.00\n",
      "Epoch [46], train_loss: 782.43 with loss1: 782.43 and loss2: 0.00\n",
      "Epoch [47], train_loss: 530.50 with loss1: 530.50 and loss2: 0.00\n",
      "Epoch [48], train_loss: 594.62 with loss1: 594.62 and loss2: 0.00\n",
      "Epoch [49], train_loss: 538.61 with loss1: 538.61 and loss2: 0.00\n",
      "Epoch [50], train_loss: 610.06 with loss1: 610.06 and loss2: 0.00\n",
      "Epoch [51], train_loss: 516.40 with loss1: 516.40 and loss2: 0.00\n",
      "Epoch [52], train_loss: 518.31 with loss1: 518.31 and loss2: 0.00\n",
      "Epoch [53], train_loss: 538.65 with loss1: 538.65 and loss2: 0.00\n",
      "Epoch [54], train_loss: 814.75 with loss1: 814.75 and loss2: 0.00\n",
      "Epoch [55], train_loss: 565.19 with loss1: 565.19 and loss2: 0.00\n",
      "Epoch [56], train_loss: 608.80 with loss1: 608.80 and loss2: 0.00\n",
      "Epoch [57], train_loss: 530.79 with loss1: 530.79 and loss2: 0.00\n",
      "Epoch [58], train_loss: 443.29 with loss1: 443.29 and loss2: 0.00\n",
      "Epoch [59], train_loss: 494.97 with loss1: 494.97 and loss2: 0.00\n",
      "Epoch [60], train_loss: 644.69 with loss1: 644.69 and loss2: 0.00\n",
      "Epoch [61], train_loss: 605.33 with loss1: 605.33 and loss2: 0.00\n",
      "Epoch [62], train_loss: 562.82 with loss1: 562.82 and loss2: 0.00\n",
      "Epoch [63], train_loss: 437.04 with loss1: 437.04 and loss2: 0.00\n",
      "Epoch [64], train_loss: 639.44 with loss1: 639.44 and loss2: 0.00\n",
      "Epoch [65], train_loss: 530.95 with loss1: 530.95 and loss2: 0.00\n",
      "Epoch [66], train_loss: 758.98 with loss1: 758.98 and loss2: 0.00\n",
      "Epoch [67], train_loss: 584.05 with loss1: 584.05 and loss2: 0.00\n",
      "Epoch [68], train_loss: 1001.38 with loss1: 1001.38 and loss2: 0.00\n",
      "Epoch [69], train_loss: 547.86 with loss1: 547.86 and loss2: 0.00\n",
      "Epoch [70], train_loss: 557.70 with loss1: 557.70 and loss2: 0.00\n",
      "Epoch [71], train_loss: 607.15 with loss1: 607.15 and loss2: 0.00\n",
      "Epoch [72], train_loss: 629.89 with loss1: 629.89 and loss2: 0.00\n",
      "Epoch [73], train_loss: 508.63 with loss1: 508.63 and loss2: 0.00\n",
      "Epoch [74], train_loss: 758.68 with loss1: 758.68 and loss2: 0.00\n",
      "Epoch [75], train_loss: 566.20 with loss1: 566.20 and loss2: 0.00\n",
      "Epoch [76], train_loss: 787.66 with loss1: 787.66 and loss2: 0.00\n",
      "Epoch [77], train_loss: 623.46 with loss1: 623.46 and loss2: 0.00\n",
      "Epoch [78], train_loss: 459.86 with loss1: 459.86 and loss2: 0.00\n",
      "Epoch [79], train_loss: 679.15 with loss1: 679.15 and loss2: 0.00\n",
      "Epoch [80], train_loss: 919.16 with loss1: 919.16 and loss2: 0.00\n",
      "Epoch [81], train_loss: 493.58 with loss1: 493.58 and loss2: 0.00\n",
      "Epoch [82], train_loss: 612.75 with loss1: 612.75 and loss2: 0.00\n",
      "Epoch [83], train_loss: 499.31 with loss1: 499.31 and loss2: 0.00\n",
      "Epoch [84], train_loss: 582.78 with loss1: 582.78 and loss2: 0.00\n",
      "Epoch [85], train_loss: 799.26 with loss1: 799.26 and loss2: 0.00\n",
      "Epoch [86], train_loss: 833.98 with loss1: 833.98 and loss2: 0.00\n",
      "Epoch [87], train_loss: 476.97 with loss1: 476.97 and loss2: 0.00\n",
      "Epoch [88], train_loss: 632.11 with loss1: 632.11 and loss2: 0.00\n",
      "Epoch [89], train_loss: 622.38 with loss1: 622.38 and loss2: 0.00\n",
      "Epoch [90], train_loss: 509.55 with loss1: 509.55 and loss2: 0.00\n",
      "Epoch [91], train_loss: 646.81 with loss1: 646.81 and loss2: 0.00\n",
      "Epoch [92], train_loss: 931.66 with loss1: 931.66 and loss2: 0.00\n",
      "Epoch [93], train_loss: 877.11 with loss1: 877.11 and loss2: 0.00\n",
      "Epoch [94], train_loss: 701.38 with loss1: 701.38 and loss2: 0.00\n",
      "Epoch [95], train_loss: 583.25 with loss1: 583.25 and loss2: 0.00\n",
      "Epoch [96], train_loss: 526.77 with loss1: 526.77 and loss2: 0.00\n",
      "Epoch [97], train_loss: 607.79 with loss1: 607.79 and loss2: 0.00\n",
      "Epoch [98], train_loss: 815.15 with loss1: 815.15 and loss2: 0.00\n",
      "Epoch [99], train_loss: 599.51 with loss1: 599.51 and loss2: 0.00\n",
      "Epoch [100], train_loss: 695.42 with loss1: 695.42 and loss2: 0.00\n",
      "Epoch [101], train_loss: 500.58 with loss1: 500.58 and loss2: 0.00\n",
      "Epoch [102], train_loss: 726.06 with loss1: 726.06 and loss2: 0.00\n",
      "Epoch [103], train_loss: 699.15 with loss1: 699.15 and loss2: 0.00\n",
      "Epoch [104], train_loss: 773.92 with loss1: 773.92 and loss2: 0.00\n",
      "Epoch [105], train_loss: 912.08 with loss1: 912.08 and loss2: 0.00\n",
      "Epoch [106], train_loss: 591.65 with loss1: 591.65 and loss2: 0.00\n",
      "Epoch [107], train_loss: 504.42 with loss1: 504.42 and loss2: 0.00\n",
      "Epoch [108], train_loss: 610.48 with loss1: 610.48 and loss2: 0.00\n",
      "Epoch [109], train_loss: 594.05 with loss1: 594.05 and loss2: 0.00\n",
      "Epoch [110], train_loss: 563.22 with loss1: 563.22 and loss2: 0.00\n",
      "Epoch [111], train_loss: 579.66 with loss1: 579.66 and loss2: 0.00\n",
      "Epoch [112], train_loss: 564.30 with loss1: 564.30 and loss2: 0.00\n",
      "Epoch [113], train_loss: 735.59 with loss1: 735.59 and loss2: 0.00\n",
      "Epoch [114], train_loss: 494.06 with loss1: 494.06 and loss2: 0.00\n",
      "Epoch [115], train_loss: 576.35 with loss1: 576.35 and loss2: 0.00\n",
      "Epoch [116], train_loss: 559.99 with loss1: 559.99 and loss2: 0.00\n",
      "Epoch [117], train_loss: 521.88 with loss1: 521.88 and loss2: 0.00\n",
      "Epoch [118], train_loss: 556.56 with loss1: 556.56 and loss2: 0.00\n",
      "Epoch [119], train_loss: 530.96 with loss1: 530.96 and loss2: 0.00\n",
      "Epoch [120], train_loss: 531.94 with loss1: 531.94 and loss2: 0.00\n",
      "Epoch [121], train_loss: 782.75 with loss1: 782.75 and loss2: 0.00\n",
      "Epoch [122], train_loss: 524.58 with loss1: 524.58 and loss2: 0.00\n",
      "Epoch [123], train_loss: 420.11 with loss1: 420.11 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [124], train_loss: 582.65 with loss1: 582.65 and loss2: 0.00\n",
      "Epoch [125], train_loss: 808.14 with loss1: 808.14 and loss2: 0.00\n",
      "Epoch [126], train_loss: 864.75 with loss1: 864.75 and loss2: 0.00\n",
      "Epoch [127], train_loss: 851.53 with loss1: 851.53 and loss2: 0.00\n",
      "Epoch [128], train_loss: 734.81 with loss1: 734.81 and loss2: 0.00\n",
      "Epoch [129], train_loss: 542.84 with loss1: 542.84 and loss2: 0.00\n",
      "Epoch [130], train_loss: 514.00 with loss1: 514.00 and loss2: 0.00\n",
      "Epoch [131], train_loss: 532.89 with loss1: 532.89 and loss2: 0.00\n",
      "Epoch [132], train_loss: 513.81 with loss1: 513.81 and loss2: 0.00\n",
      "Epoch [133], train_loss: 735.14 with loss1: 735.14 and loss2: 0.00\n",
      "Epoch [134], train_loss: 450.87 with loss1: 450.87 and loss2: 0.00\n",
      "Epoch [135], train_loss: 643.76 with loss1: 643.76 and loss2: 0.00\n",
      "Epoch [136], train_loss: 585.56 with loss1: 585.56 and loss2: 0.00\n",
      "Epoch [137], train_loss: 586.87 with loss1: 586.87 and loss2: 0.00\n",
      "Epoch [138], train_loss: 741.51 with loss1: 741.51 and loss2: 0.00\n",
      "Epoch [139], train_loss: 805.75 with loss1: 805.75 and loss2: 0.00\n",
      "Epoch [140], train_loss: 455.32 with loss1: 455.32 and loss2: 0.00\n",
      "Epoch [141], train_loss: 524.45 with loss1: 524.45 and loss2: 0.00\n",
      "Epoch [142], train_loss: 484.26 with loss1: 484.26 and loss2: 0.00\n",
      "Epoch [143], train_loss: 647.73 with loss1: 647.73 and loss2: 0.00\n",
      "Epoch [144], train_loss: 757.64 with loss1: 757.64 and loss2: 0.00\n",
      "Epoch [145], train_loss: 784.37 with loss1: 784.37 and loss2: 0.00\n",
      "Epoch [146], train_loss: 547.70 with loss1: 547.70 and loss2: 0.00\n",
      "Epoch [147], train_loss: 675.65 with loss1: 675.65 and loss2: 0.00\n",
      "Epoch [148], train_loss: 675.84 with loss1: 675.84 and loss2: 0.00\n",
      "Epoch [149], train_loss: 742.20 with loss1: 742.20 and loss2: 0.00\n",
      "Epoch [150], train_loss: 476.33 with loss1: 476.33 and loss2: 0.00\n",
      "Epoch [151], train_loss: 500.17 with loss1: 500.17 and loss2: 0.00\n",
      "Epoch [152], train_loss: 513.91 with loss1: 513.91 and loss2: 0.00\n",
      "Epoch [153], train_loss: 537.10 with loss1: 537.10 and loss2: 0.00\n",
      "Epoch [154], train_loss: 506.43 with loss1: 506.43 and loss2: 0.00\n",
      "Epoch [155], train_loss: 582.81 with loss1: 582.81 and loss2: 0.00\n",
      "Epoch [156], train_loss: 459.35 with loss1: 459.35 and loss2: 0.00\n",
      "Epoch [157], train_loss: 498.32 with loss1: 498.32 and loss2: 0.00\n",
      "Epoch [158], train_loss: 615.83 with loss1: 615.83 and loss2: 0.00\n",
      "Epoch [159], train_loss: 527.53 with loss1: 527.53 and loss2: 0.00\n",
      "Epoch [160], train_loss: 587.41 with loss1: 587.41 and loss2: 0.00\n",
      "Epoch [161], train_loss: 444.11 with loss1: 444.11 and loss2: 0.00\n",
      "Epoch [162], train_loss: 548.71 with loss1: 548.71 and loss2: 0.00\n",
      "Epoch [163], train_loss: 430.64 with loss1: 430.64 and loss2: 0.00\n",
      "Epoch [164], train_loss: 516.25 with loss1: 516.25 and loss2: 0.00\n",
      "Epoch [165], train_loss: 506.58 with loss1: 506.58 and loss2: 0.00\n",
      "Epoch [166], train_loss: 573.44 with loss1: 573.44 and loss2: 0.00\n",
      "Epoch [167], train_loss: 588.13 with loss1: 588.13 and loss2: 0.00\n",
      "Epoch [168], train_loss: 556.43 with loss1: 556.43 and loss2: 0.00\n",
      "Epoch [169], train_loss: 745.33 with loss1: 745.33 and loss2: 0.00\n",
      "Epoch [170], train_loss: 832.94 with loss1: 832.94 and loss2: 0.00\n",
      "Epoch [171], train_loss: 589.02 with loss1: 589.02 and loss2: 0.00\n",
      "Epoch [172], train_loss: 523.33 with loss1: 523.33 and loss2: 0.00\n",
      "Epoch [173], train_loss: 545.98 with loss1: 545.98 and loss2: 0.00\n",
      "Epoch [174], train_loss: 560.67 with loss1: 560.67 and loss2: 0.00\n",
      "Epoch [175], train_loss: 517.12 with loss1: 517.12 and loss2: 0.00\n",
      "Epoch [176], train_loss: 491.44 with loss1: 491.44 and loss2: 0.00\n",
      "Epoch [177], train_loss: 639.45 with loss1: 639.45 and loss2: 0.00\n",
      "Epoch [178], train_loss: 461.46 with loss1: 461.46 and loss2: 0.00\n",
      "Epoch [179], train_loss: 505.20 with loss1: 505.20 and loss2: 0.00\n",
      "Epoch [180], train_loss: 484.41 with loss1: 484.41 and loss2: 0.00\n",
      "Epoch [181], train_loss: 697.89 with loss1: 697.89 and loss2: 0.00\n",
      "Epoch [182], train_loss: 555.59 with loss1: 555.59 and loss2: 0.00\n",
      "Epoch [183], train_loss: 509.25 with loss1: 509.25 and loss2: 0.00\n",
      "Epoch [184], train_loss: 522.82 with loss1: 522.82 and loss2: 0.00\n",
      "Epoch [185], train_loss: 502.77 with loss1: 502.77 and loss2: 0.00\n",
      "Epoch [186], train_loss: 763.60 with loss1: 763.60 and loss2: 0.00\n",
      "Epoch [187], train_loss: 473.47 with loss1: 473.47 and loss2: 0.00\n",
      "Epoch [188], train_loss: 680.60 with loss1: 680.60 and loss2: 0.00\n",
      "Epoch [189], train_loss: 662.30 with loss1: 662.30 and loss2: 0.00\n",
      "Epoch [190], train_loss: 604.98 with loss1: 604.98 and loss2: 0.00\n",
      "Epoch [191], train_loss: 514.12 with loss1: 514.12 and loss2: 0.00\n",
      "Epoch [192], train_loss: 708.41 with loss1: 708.41 and loss2: 0.00\n",
      "Epoch [193], train_loss: 474.11 with loss1: 474.11 and loss2: 0.00\n",
      "Epoch [194], train_loss: 673.24 with loss1: 673.24 and loss2: 0.00\n",
      "Epoch [195], train_loss: 842.84 with loss1: 842.84 and loss2: 0.00\n",
      "Epoch [196], train_loss: 504.53 with loss1: 504.53 and loss2: 0.00\n",
      "Epoch [197], train_loss: 543.29 with loss1: 543.29 and loss2: 0.00\n",
      "Epoch [198], train_loss: 493.21 with loss1: 493.21 and loss2: 0.00\n",
      "Epoch [199], train_loss: 719.25 with loss1: 719.25 and loss2: 0.00\n",
      "Epoch [200], train_loss: 790.37 with loss1: 790.37 and loss2: 0.00\n",
      "Epoch [201], train_loss: 812.76 with loss1: 812.76 and loss2: 0.00\n",
      "Epoch [202], train_loss: 553.69 with loss1: 553.69 and loss2: 0.00\n",
      "Epoch [203], train_loss: 439.98 with loss1: 439.98 and loss2: 0.00\n",
      "Epoch [204], train_loss: 587.76 with loss1: 587.76 and loss2: 0.00\n",
      "Epoch [205], train_loss: 545.09 with loss1: 545.09 and loss2: 0.00\n",
      "Epoch [206], train_loss: 707.18 with loss1: 707.18 and loss2: 0.00\n",
      "Epoch [207], train_loss: 448.37 with loss1: 448.37 and loss2: 0.00\n",
      "Epoch [208], train_loss: 476.37 with loss1: 476.37 and loss2: 0.00\n",
      "Epoch [209], train_loss: 489.42 with loss1: 489.42 and loss2: 0.00\n",
      "Epoch [210], train_loss: 478.62 with loss1: 478.62 and loss2: 0.00\n",
      "Epoch [211], train_loss: 480.71 with loss1: 480.71 and loss2: 0.00\n",
      "Epoch [212], train_loss: 480.74 with loss1: 480.74 and loss2: 0.00\n",
      "Epoch [213], train_loss: 464.26 with loss1: 464.26 and loss2: 0.00\n",
      "Epoch [214], train_loss: 517.31 with loss1: 517.31 and loss2: 0.00\n",
      "Epoch [215], train_loss: 561.31 with loss1: 561.31 and loss2: 0.00\n",
      "Epoch [216], train_loss: 412.14 with loss1: 412.14 and loss2: 0.00\n",
      "Epoch [217], train_loss: 594.22 with loss1: 594.22 and loss2: 0.00\n",
      "Epoch [218], train_loss: 824.89 with loss1: 824.89 and loss2: 0.00\n",
      "Epoch [219], train_loss: 452.45 with loss1: 452.45 and loss2: 0.00\n",
      "Epoch [220], train_loss: 526.57 with loss1: 526.57 and loss2: 0.00\n",
      "Epoch [221], train_loss: 520.46 with loss1: 520.46 and loss2: 0.00\n",
      "Epoch [222], train_loss: 523.77 with loss1: 523.77 and loss2: 0.00\n",
      "Epoch [223], train_loss: 726.62 with loss1: 726.62 and loss2: 0.00\n",
      "Epoch [224], train_loss: 522.12 with loss1: 522.12 and loss2: 0.00\n",
      "Epoch [225], train_loss: 643.05 with loss1: 643.05 and loss2: 0.00\n",
      "Epoch [226], train_loss: 822.82 with loss1: 822.82 and loss2: 0.00\n",
      "Epoch [227], train_loss: 455.87 with loss1: 455.87 and loss2: 0.00\n",
      "Epoch [228], train_loss: 493.46 with loss1: 493.46 and loss2: 0.00\n",
      "Epoch [229], train_loss: 617.42 with loss1: 617.42 and loss2: 0.00\n",
      "Epoch [230], train_loss: 431.48 with loss1: 431.48 and loss2: 0.00\n",
      "Epoch [231], train_loss: 539.71 with loss1: 539.71 and loss2: 0.00\n",
      "Epoch [232], train_loss: 481.81 with loss1: 481.81 and loss2: 0.00\n",
      "Epoch [233], train_loss: 528.65 with loss1: 528.65 and loss2: 0.00\n",
      "Epoch [234], train_loss: 514.70 with loss1: 514.70 and loss2: 0.00\n",
      "Epoch [235], train_loss: 616.99 with loss1: 616.99 and loss2: 0.00\n",
      "Epoch [236], train_loss: 438.99 with loss1: 438.99 and loss2: 0.00\n",
      "Epoch [237], train_loss: 470.46 with loss1: 470.46 and loss2: 0.00\n",
      "Epoch [238], train_loss: 652.55 with loss1: 652.55 and loss2: 0.00\n",
      "Epoch [239], train_loss: 682.60 with loss1: 682.60 and loss2: 0.00\n",
      "Epoch [240], train_loss: 551.25 with loss1: 551.25 and loss2: 0.00\n",
      "Epoch [241], train_loss: 486.90 with loss1: 486.90 and loss2: 0.00\n",
      "Epoch [242], train_loss: 509.29 with loss1: 509.29 and loss2: 0.00\n",
      "Epoch [243], train_loss: 410.87 with loss1: 410.87 and loss2: 0.00\n",
      "Epoch [244], train_loss: 572.14 with loss1: 572.14 and loss2: 0.00\n",
      "Epoch [245], train_loss: 761.56 with loss1: 761.56 and loss2: 0.00\n",
      "Epoch [246], train_loss: 814.43 with loss1: 814.43 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [247], train_loss: 471.20 with loss1: 471.20 and loss2: 0.00\n",
      "Epoch [248], train_loss: 553.69 with loss1: 553.69 and loss2: 0.00\n",
      "Epoch [249], train_loss: 677.75 with loss1: 677.75 and loss2: 0.00\n",
      "Epoch [250], train_loss: 483.18 with loss1: 483.18 and loss2: 0.00\n",
      "Epoch [251], train_loss: 486.49 with loss1: 486.49 and loss2: 0.00\n",
      "Epoch [252], train_loss: 605.19 with loss1: 605.19 and loss2: 0.00\n",
      "Epoch [253], train_loss: 491.67 with loss1: 491.67 and loss2: 0.00\n",
      "Epoch [254], train_loss: 494.36 with loss1: 494.36 and loss2: 0.00\n",
      "Epoch [255], train_loss: 632.22 with loss1: 632.22 and loss2: 0.00\n",
      "Epoch [256], train_loss: 495.98 with loss1: 495.98 and loss2: 0.00\n",
      "Epoch [257], train_loss: 492.40 with loss1: 492.40 and loss2: 0.00\n",
      "Epoch [258], train_loss: 475.53 with loss1: 475.53 and loss2: 0.00\n",
      "Epoch [259], train_loss: 482.47 with loss1: 482.47 and loss2: 0.00\n",
      "Epoch [260], train_loss: 466.81 with loss1: 466.81 and loss2: 0.00\n",
      "Epoch [261], train_loss: 454.87 with loss1: 454.87 and loss2: 0.00\n",
      "Epoch [262], train_loss: 531.81 with loss1: 531.81 and loss2: 0.00\n",
      "Epoch [263], train_loss: 640.13 with loss1: 640.13 and loss2: 0.00\n",
      "Epoch [264], train_loss: 671.26 with loss1: 671.26 and loss2: 0.00\n",
      "Epoch [265], train_loss: 555.93 with loss1: 555.93 and loss2: 0.00\n",
      "Epoch [266], train_loss: 531.95 with loss1: 531.95 and loss2: 0.00\n",
      "Epoch [267], train_loss: 473.05 with loss1: 473.05 and loss2: 0.00\n",
      "Epoch [268], train_loss: 499.95 with loss1: 499.95 and loss2: 0.00\n",
      "Epoch [269], train_loss: 541.22 with loss1: 541.22 and loss2: 0.00\n",
      "Epoch [270], train_loss: 486.26 with loss1: 486.26 and loss2: 0.00\n",
      "Epoch [271], train_loss: 510.41 with loss1: 510.41 and loss2: 0.00\n",
      "Epoch [272], train_loss: 395.59 with loss1: 395.59 and loss2: 0.00\n",
      "Epoch [273], train_loss: 549.97 with loss1: 549.97 and loss2: 0.00\n",
      "Epoch [274], train_loss: 750.37 with loss1: 750.37 and loss2: 0.00\n",
      "Epoch [275], train_loss: 608.18 with loss1: 608.18 and loss2: 0.00\n",
      "Epoch [276], train_loss: 423.46 with loss1: 423.46 and loss2: 0.00\n",
      "Epoch [277], train_loss: 458.18 with loss1: 458.18 and loss2: 0.00\n",
      "Epoch [278], train_loss: 590.31 with loss1: 590.31 and loss2: 0.00\n",
      "Epoch [279], train_loss: 706.00 with loss1: 706.00 and loss2: 0.00\n",
      "Epoch [280], train_loss: 688.71 with loss1: 688.71 and loss2: 0.00\n",
      "Epoch [281], train_loss: 629.26 with loss1: 629.26 and loss2: 0.00\n",
      "Epoch [282], train_loss: 575.44 with loss1: 575.44 and loss2: 0.00\n",
      "Epoch [283], train_loss: 767.37 with loss1: 767.37 and loss2: 0.00\n",
      "Epoch [284], train_loss: 730.28 with loss1: 730.28 and loss2: 0.00\n",
      "Epoch [285], train_loss: 611.74 with loss1: 611.74 and loss2: 0.00\n",
      "Epoch [286], train_loss: 416.78 with loss1: 416.78 and loss2: 0.00\n",
      "Epoch [287], train_loss: 411.89 with loss1: 411.89 and loss2: 0.00\n",
      "Epoch [288], train_loss: 510.53 with loss1: 510.53 and loss2: 0.00\n",
      "Epoch [289], train_loss: 425.43 with loss1: 425.43 and loss2: 0.00\n",
      "Epoch [290], train_loss: 432.64 with loss1: 432.64 and loss2: 0.00\n",
      "Epoch [291], train_loss: 503.89 with loss1: 503.89 and loss2: 0.00\n",
      "Epoch [292], train_loss: 473.34 with loss1: 473.34 and loss2: 0.00\n",
      "Epoch [293], train_loss: 544.29 with loss1: 544.29 and loss2: 0.00\n",
      "Epoch [294], train_loss: 473.96 with loss1: 473.96 and loss2: 0.00\n",
      "Epoch [295], train_loss: 713.46 with loss1: 713.46 and loss2: 0.00\n",
      "Epoch [296], train_loss: 431.23 with loss1: 431.23 and loss2: 0.00\n",
      "Epoch [297], train_loss: 515.19 with loss1: 515.19 and loss2: 0.00\n",
      "Epoch [298], train_loss: 467.77 with loss1: 467.77 and loss2: 0.00\n",
      "Epoch [299], train_loss: 695.06 with loss1: 695.06 and loss2: 0.00\n",
      "Epoch [300], train_loss: 496.60 with loss1: 496.60 and loss2: 0.00\n",
      "Epoch [301], train_loss: 496.48 with loss1: 496.48 and loss2: 0.00\n",
      "Epoch [302], train_loss: 385.26 with loss1: 385.26 and loss2: 0.00\n",
      "Epoch [303], train_loss: 643.99 with loss1: 643.99 and loss2: 0.00\n",
      "Epoch [304], train_loss: 792.33 with loss1: 792.33 and loss2: 0.00\n",
      "Epoch [305], train_loss: 820.11 with loss1: 820.11 and loss2: 0.00\n",
      "Epoch [306], train_loss: 782.53 with loss1: 782.53 and loss2: 0.00\n",
      "Epoch [307], train_loss: 446.72 with loss1: 446.72 and loss2: 0.00\n",
      "Epoch [308], train_loss: 511.82 with loss1: 511.82 and loss2: 0.00\n",
      "Epoch [309], train_loss: 556.75 with loss1: 556.75 and loss2: 0.00\n",
      "Epoch [310], train_loss: 580.70 with loss1: 580.70 and loss2: 0.00\n",
      "Epoch [311], train_loss: 572.71 with loss1: 572.71 and loss2: 0.00\n",
      "Epoch [312], train_loss: 486.89 with loss1: 486.89 and loss2: 0.00\n",
      "Epoch [313], train_loss: 425.35 with loss1: 425.35 and loss2: 0.00\n",
      "Epoch [314], train_loss: 479.23 with loss1: 479.23 and loss2: 0.00\n",
      "Epoch [315], train_loss: 515.16 with loss1: 515.16 and loss2: 0.00\n",
      "Epoch [316], train_loss: 470.38 with loss1: 470.38 and loss2: 0.00\n",
      "Epoch [317], train_loss: 632.05 with loss1: 632.05 and loss2: 0.00\n",
      "Epoch [318], train_loss: 637.45 with loss1: 637.45 and loss2: 0.00\n",
      "Epoch [319], train_loss: 493.99 with loss1: 493.99 and loss2: 0.00\n",
      "Epoch [320], train_loss: 416.73 with loss1: 416.73 and loss2: 0.00\n",
      "Epoch [321], train_loss: 495.57 with loss1: 495.57 and loss2: 0.00\n",
      "Epoch [322], train_loss: 615.48 with loss1: 615.48 and loss2: 0.00\n",
      "Epoch [323], train_loss: 450.38 with loss1: 450.38 and loss2: 0.00\n",
      "Epoch [324], train_loss: 600.25 with loss1: 600.25 and loss2: 0.00\n",
      "Epoch [325], train_loss: 472.21 with loss1: 472.21 and loss2: 0.00\n",
      "Epoch [326], train_loss: 426.46 with loss1: 426.46 and loss2: 0.00\n",
      "Epoch [327], train_loss: 511.39 with loss1: 511.39 and loss2: 0.00\n",
      "Epoch [328], train_loss: 498.01 with loss1: 498.01 and loss2: 0.00\n",
      "Epoch [329], train_loss: 387.67 with loss1: 387.67 and loss2: 0.00\n",
      "Epoch [330], train_loss: 421.11 with loss1: 421.11 and loss2: 0.00\n",
      "Epoch [331], train_loss: 515.38 with loss1: 515.38 and loss2: 0.00\n",
      "Epoch [332], train_loss: 473.32 with loss1: 473.32 and loss2: 0.00\n",
      "Epoch [333], train_loss: 464.84 with loss1: 464.84 and loss2: 0.00\n",
      "Epoch [334], train_loss: 423.59 with loss1: 423.59 and loss2: 0.00\n",
      "Epoch [335], train_loss: 628.88 with loss1: 628.88 and loss2: 0.00\n",
      "Epoch [336], train_loss: 523.83 with loss1: 523.83 and loss2: 0.00\n",
      "Epoch [337], train_loss: 625.48 with loss1: 625.48 and loss2: 0.00\n",
      "Epoch [338], train_loss: 746.30 with loss1: 746.30 and loss2: 0.00\n",
      "Epoch [339], train_loss: 441.62 with loss1: 441.62 and loss2: 0.00\n",
      "Epoch [340], train_loss: 512.82 with loss1: 512.82 and loss2: 0.00\n",
      "Epoch [341], train_loss: 430.68 with loss1: 430.68 and loss2: 0.00\n",
      "Epoch [342], train_loss: 489.50 with loss1: 489.50 and loss2: 0.00\n",
      "Epoch [343], train_loss: 499.99 with loss1: 499.99 and loss2: 0.00\n",
      "Epoch [344], train_loss: 547.09 with loss1: 547.09 and loss2: 0.00\n",
      "Epoch [345], train_loss: 588.42 with loss1: 588.42 and loss2: 0.00\n",
      "Epoch [346], train_loss: 622.49 with loss1: 622.49 and loss2: 0.00\n",
      "Epoch [347], train_loss: 535.14 with loss1: 535.14 and loss2: 0.00\n",
      "Epoch [348], train_loss: 487.40 with loss1: 487.40 and loss2: 0.00\n",
      "Epoch [349], train_loss: 543.46 with loss1: 543.46 and loss2: 0.00\n",
      "Epoch [350], train_loss: 546.18 with loss1: 546.18 and loss2: 0.00\n",
      "Epoch [351], train_loss: 453.79 with loss1: 453.79 and loss2: 0.00\n",
      "Epoch [352], train_loss: 562.48 with loss1: 562.48 and loss2: 0.00\n",
      "Epoch [353], train_loss: 504.92 with loss1: 504.92 and loss2: 0.00\n",
      "Epoch [354], train_loss: 634.43 with loss1: 634.43 and loss2: 0.00\n",
      "Epoch [355], train_loss: 425.86 with loss1: 425.86 and loss2: 0.00\n",
      "Epoch [356], train_loss: 478.02 with loss1: 478.02 and loss2: 0.00\n",
      "Epoch [357], train_loss: 430.86 with loss1: 430.86 and loss2: 0.00\n",
      "Epoch [358], train_loss: 461.83 with loss1: 461.83 and loss2: 0.00\n",
      "Epoch [359], train_loss: 543.03 with loss1: 543.03 and loss2: 0.00\n",
      "Epoch [360], train_loss: 605.06 with loss1: 605.06 and loss2: 0.00\n",
      "Epoch [361], train_loss: 428.51 with loss1: 428.51 and loss2: 0.00\n",
      "Epoch [362], train_loss: 479.09 with loss1: 479.09 and loss2: 0.00\n",
      "Epoch [363], train_loss: 474.21 with loss1: 474.21 and loss2: 0.00\n",
      "Epoch [364], train_loss: 688.27 with loss1: 688.27 and loss2: 0.00\n",
      "Epoch [365], train_loss: 518.99 with loss1: 518.99 and loss2: 0.00\n",
      "Epoch [366], train_loss: 500.75 with loss1: 500.75 and loss2: 0.00\n",
      "Epoch [367], train_loss: 556.45 with loss1: 556.45 and loss2: 0.00\n",
      "Epoch [368], train_loss: 467.49 with loss1: 467.49 and loss2: 0.00\n",
      "Epoch [369], train_loss: 521.28 with loss1: 521.28 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [370], train_loss: 413.76 with loss1: 413.76 and loss2: 0.00\n",
      "Epoch [371], train_loss: 679.07 with loss1: 679.07 and loss2: 0.00\n",
      "Epoch [372], train_loss: 495.13 with loss1: 495.13 and loss2: 0.00\n",
      "Epoch [373], train_loss: 464.54 with loss1: 464.54 and loss2: 0.00\n",
      "Epoch [374], train_loss: 481.55 with loss1: 481.55 and loss2: 0.00\n",
      "Epoch [375], train_loss: 669.30 with loss1: 669.30 and loss2: 0.00\n",
      "Epoch [376], train_loss: 676.15 with loss1: 676.15 and loss2: 0.00\n",
      "Epoch [377], train_loss: 766.47 with loss1: 766.47 and loss2: 0.00\n",
      "Epoch [378], train_loss: 520.38 with loss1: 520.38 and loss2: 0.00\n",
      "Epoch [379], train_loss: 611.41 with loss1: 611.41 and loss2: 0.00\n",
      "Epoch [380], train_loss: 691.48 with loss1: 691.48 and loss2: 0.00\n",
      "Epoch [381], train_loss: 399.70 with loss1: 399.70 and loss2: 0.00\n",
      "Epoch [382], train_loss: 477.94 with loss1: 477.94 and loss2: 0.00\n",
      "Epoch [383], train_loss: 543.61 with loss1: 543.61 and loss2: 0.00\n",
      "Epoch [384], train_loss: 483.00 with loss1: 483.00 and loss2: 0.00\n",
      "Epoch [385], train_loss: 472.86 with loss1: 472.86 and loss2: 0.00\n",
      "Epoch [386], train_loss: 611.06 with loss1: 611.06 and loss2: 0.00\n",
      "Epoch [387], train_loss: 474.82 with loss1: 474.82 and loss2: 0.00\n",
      "Epoch [388], train_loss: 472.55 with loss1: 472.55 and loss2: 0.00\n",
      "Epoch [389], train_loss: 465.52 with loss1: 465.52 and loss2: 0.00\n",
      "Epoch [390], train_loss: 391.65 with loss1: 391.65 and loss2: 0.00\n",
      "Epoch [391], train_loss: 455.05 with loss1: 455.05 and loss2: 0.00\n",
      "Epoch [392], train_loss: 447.92 with loss1: 447.92 and loss2: 0.00\n",
      "Epoch [393], train_loss: 460.36 with loss1: 460.36 and loss2: 0.00\n",
      "Epoch [394], train_loss: 579.50 with loss1: 579.50 and loss2: 0.00\n",
      "Epoch [395], train_loss: 689.79 with loss1: 689.79 and loss2: 0.00\n",
      "Epoch [396], train_loss: 625.21 with loss1: 625.21 and loss2: 0.00\n",
      "Epoch [397], train_loss: 636.60 with loss1: 636.60 and loss2: 0.00\n",
      "Epoch [398], train_loss: 439.54 with loss1: 439.54 and loss2: 0.00\n",
      "Epoch [399], train_loss: 516.36 with loss1: 516.36 and loss2: 0.00\n",
      "Epoch [400], train_loss: 464.14 with loss1: 464.14 and loss2: 0.00\n",
      "Epoch [401], train_loss: 544.16 with loss1: 544.16 and loss2: 0.00\n",
      "Epoch [402], train_loss: 502.83 with loss1: 502.83 and loss2: 0.00\n",
      "Epoch [403], train_loss: 485.51 with loss1: 485.51 and loss2: 0.00\n",
      "Epoch [404], train_loss: 478.33 with loss1: 478.33 and loss2: 0.00\n",
      "Epoch [405], train_loss: 579.15 with loss1: 579.15 and loss2: 0.00\n",
      "Epoch [406], train_loss: 668.25 with loss1: 668.25 and loss2: 0.00\n",
      "Epoch [407], train_loss: 646.26 with loss1: 646.26 and loss2: 0.00\n",
      "Epoch [408], train_loss: 434.57 with loss1: 434.57 and loss2: 0.00\n",
      "Epoch [409], train_loss: 478.38 with loss1: 478.38 and loss2: 0.00\n",
      "Epoch [410], train_loss: 489.84 with loss1: 489.84 and loss2: 0.00\n",
      "Epoch [411], train_loss: 425.01 with loss1: 425.01 and loss2: 0.00\n",
      "Epoch [412], train_loss: 533.47 with loss1: 533.47 and loss2: 0.00\n",
      "Epoch [413], train_loss: 470.25 with loss1: 470.25 and loss2: 0.00\n",
      "Epoch [414], train_loss: 470.99 with loss1: 470.99 and loss2: 0.00\n",
      "Epoch [415], train_loss: 498.20 with loss1: 498.20 and loss2: 0.00\n",
      "Epoch [416], train_loss: 653.38 with loss1: 653.38 and loss2: 0.00\n",
      "Epoch [417], train_loss: 393.22 with loss1: 393.22 and loss2: 0.00\n",
      "Epoch [418], train_loss: 528.06 with loss1: 528.06 and loss2: 0.00\n",
      "Epoch [419], train_loss: 651.37 with loss1: 651.37 and loss2: 0.00\n",
      "Epoch [420], train_loss: 383.76 with loss1: 383.76 and loss2: 0.00\n",
      "Epoch [421], train_loss: 412.24 with loss1: 412.24 and loss2: 0.00\n",
      "Epoch [422], train_loss: 450.40 with loss1: 450.40 and loss2: 0.00\n",
      "Epoch [423], train_loss: 539.49 with loss1: 539.49 and loss2: 0.00\n",
      "Epoch [424], train_loss: 376.44 with loss1: 376.44 and loss2: 0.00\n",
      "Epoch [425], train_loss: 416.73 with loss1: 416.73 and loss2: 0.00\n",
      "Epoch [426], train_loss: 485.23 with loss1: 485.23 and loss2: 0.00\n",
      "Epoch [427], train_loss: 451.99 with loss1: 451.99 and loss2: 0.00\n",
      "Epoch [428], train_loss: 470.29 with loss1: 470.29 and loss2: 0.00\n",
      "Epoch [429], train_loss: 631.91 with loss1: 631.91 and loss2: 0.00\n",
      "Epoch [430], train_loss: 423.36 with loss1: 423.36 and loss2: 0.00\n",
      "Epoch [431], train_loss: 520.41 with loss1: 520.41 and loss2: 0.00\n",
      "Epoch [432], train_loss: 475.36 with loss1: 475.36 and loss2: 0.00\n",
      "Epoch [433], train_loss: 382.75 with loss1: 382.75 and loss2: 0.00\n",
      "Epoch [434], train_loss: 493.38 with loss1: 493.38 and loss2: 0.00\n",
      "Epoch [435], train_loss: 504.83 with loss1: 504.83 and loss2: 0.00\n",
      "Epoch [436], train_loss: 388.35 with loss1: 388.35 and loss2: 0.00\n",
      "Epoch [437], train_loss: 429.32 with loss1: 429.32 and loss2: 0.00\n",
      "Epoch [438], train_loss: 371.97 with loss1: 371.97 and loss2: 0.00\n",
      "Epoch [439], train_loss: 495.04 with loss1: 495.04 and loss2: 0.00\n",
      "Epoch [440], train_loss: 500.65 with loss1: 500.65 and loss2: 0.00\n",
      "Epoch [441], train_loss: 417.90 with loss1: 417.90 and loss2: 0.00\n",
      "Epoch [442], train_loss: 533.46 with loss1: 533.46 and loss2: 0.00\n",
      "Epoch [443], train_loss: 439.77 with loss1: 439.77 and loss2: 0.00\n",
      "Epoch [444], train_loss: 468.06 with loss1: 468.06 and loss2: 0.00\n",
      "Epoch [445], train_loss: 463.88 with loss1: 463.88 and loss2: 0.00\n",
      "Epoch [446], train_loss: 598.90 with loss1: 598.90 and loss2: 0.00\n",
      "Epoch [447], train_loss: 385.04 with loss1: 385.04 and loss2: 0.00\n",
      "Epoch [448], train_loss: 530.48 with loss1: 530.48 and loss2: 0.00\n",
      "Epoch [449], train_loss: 382.95 with loss1: 382.95 and loss2: 0.00\n",
      "Epoch [450], train_loss: 495.36 with loss1: 495.36 and loss2: 0.00\n",
      "Epoch [451], train_loss: 417.45 with loss1: 417.45 and loss2: 0.00\n",
      "Epoch [452], train_loss: 618.49 with loss1: 618.49 and loss2: 0.00\n",
      "Epoch [453], train_loss: 752.53 with loss1: 752.53 and loss2: 0.00\n",
      "Epoch [454], train_loss: 480.46 with loss1: 480.46 and loss2: 0.00\n",
      "Epoch [455], train_loss: 476.55 with loss1: 476.55 and loss2: 0.00\n",
      "Epoch [456], train_loss: 521.93 with loss1: 521.93 and loss2: 0.00\n",
      "Epoch [457], train_loss: 581.08 with loss1: 581.08 and loss2: 0.00\n",
      "Epoch [458], train_loss: 729.68 with loss1: 729.68 and loss2: 0.00\n",
      "Epoch [459], train_loss: 748.41 with loss1: 748.41 and loss2: 0.00\n",
      "Epoch [460], train_loss: 682.80 with loss1: 682.80 and loss2: 0.00\n",
      "Epoch [461], train_loss: 579.73 with loss1: 579.73 and loss2: 0.00\n",
      "Epoch [462], train_loss: 554.05 with loss1: 554.05 and loss2: 0.00\n",
      "Epoch [463], train_loss: 375.67 with loss1: 375.67 and loss2: 0.00\n",
      "Epoch [464], train_loss: 486.17 with loss1: 486.17 and loss2: 0.00\n",
      "Epoch [465], train_loss: 565.61 with loss1: 565.61 and loss2: 0.00\n",
      "Epoch [466], train_loss: 563.84 with loss1: 563.84 and loss2: 0.00\n",
      "Epoch [467], train_loss: 373.17 with loss1: 373.17 and loss2: 0.00\n",
      "Epoch [468], train_loss: 479.60 with loss1: 479.60 and loss2: 0.00\n",
      "Epoch [469], train_loss: 374.35 with loss1: 374.35 and loss2: 0.00\n",
      "Epoch [470], train_loss: 477.22 with loss1: 477.22 and loss2: 0.00\n",
      "Epoch [471], train_loss: 420.36 with loss1: 420.36 and loss2: 0.00\n",
      "Epoch [472], train_loss: 428.82 with loss1: 428.82 and loss2: 0.00\n",
      "Epoch [473], train_loss: 518.19 with loss1: 518.19 and loss2: 0.00\n",
      "Epoch [474], train_loss: 564.36 with loss1: 564.36 and loss2: 0.00\n",
      "Epoch [475], train_loss: 673.36 with loss1: 673.36 and loss2: 0.00\n",
      "Epoch [476], train_loss: 683.80 with loss1: 683.80 and loss2: 0.00\n",
      "Epoch [477], train_loss: 640.03 with loss1: 640.03 and loss2: 0.00\n",
      "Epoch [478], train_loss: 391.67 with loss1: 391.67 and loss2: 0.00\n",
      "Epoch [479], train_loss: 504.94 with loss1: 504.94 and loss2: 0.00\n",
      "Epoch [480], train_loss: 401.38 with loss1: 401.38 and loss2: 0.00\n",
      "Epoch [481], train_loss: 420.27 with loss1: 420.27 and loss2: 0.00\n",
      "Epoch [482], train_loss: 452.87 with loss1: 452.87 and loss2: 0.00\n",
      "Epoch [483], train_loss: 620.46 with loss1: 620.46 and loss2: 0.00\n",
      "Epoch [484], train_loss: 491.40 with loss1: 491.40 and loss2: 0.00\n",
      "Epoch [485], train_loss: 461.12 with loss1: 461.12 and loss2: 0.00\n",
      "Epoch [486], train_loss: 565.51 with loss1: 565.51 and loss2: 0.00\n",
      "Epoch [487], train_loss: 562.55 with loss1: 562.55 and loss2: 0.00\n",
      "Epoch [488], train_loss: 736.54 with loss1: 736.54 and loss2: 0.00\n",
      "Epoch [489], train_loss: 625.40 with loss1: 625.40 and loss2: 0.00\n",
      "Epoch [490], train_loss: 497.69 with loss1: 497.69 and loss2: 0.00\n",
      "Epoch [491], train_loss: 481.74 with loss1: 481.74 and loss2: 0.00\n",
      "Epoch [492], train_loss: 381.78 with loss1: 381.78 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [493], train_loss: 419.55 with loss1: 419.55 and loss2: 0.00\n",
      "Epoch [494], train_loss: 427.75 with loss1: 427.75 and loss2: 0.00\n",
      "Epoch [495], train_loss: 466.35 with loss1: 466.35 and loss2: 0.00\n",
      "Epoch [496], train_loss: 426.74 with loss1: 426.74 and loss2: 0.00\n",
      "Epoch [497], train_loss: 435.06 with loss1: 435.06 and loss2: 0.00\n",
      "Epoch [498], train_loss: 463.07 with loss1: 463.07 and loss2: 0.00\n",
      "Epoch [499], train_loss: 370.93 with loss1: 370.93 and loss2: 0.00\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# model version 3 \n",
    "# delete loss2\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history, mu_history = fit(epochs=500, lr=5e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "388a5b97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 88813.73 with loss1: 88813.73 and loss2: 0.00\n",
      "Epoch [1], train_loss: 80939.30 with loss1: 80939.30 and loss2: 0.00\n",
      "Epoch [2], train_loss: 45135.54 with loss1: 45135.54 and loss2: 0.00\n",
      "Epoch [3], train_loss: 29181.13 with loss1: 29181.13 and loss2: 0.00\n",
      "Epoch [4], train_loss: 24441.10 with loss1: 24441.10 and loss2: 0.00\n",
      "Epoch [5], train_loss: 19065.02 with loss1: 19065.02 and loss2: 0.00\n",
      "Epoch [6], train_loss: 30772.89 with loss1: 30772.89 and loss2: 0.00\n",
      "Epoch [7], train_loss: 11755.60 with loss1: 11755.60 and loss2: 0.00\n",
      "Epoch [8], train_loss: 11085.99 with loss1: 11085.99 and loss2: 0.00\n",
      "Epoch [9], train_loss: 9607.16 with loss1: 9607.16 and loss2: 0.00\n",
      "Epoch [10], train_loss: 10700.05 with loss1: 10700.05 and loss2: 0.00\n",
      "Epoch [11], train_loss: 14642.04 with loss1: 14642.04 and loss2: 0.00\n",
      "Epoch [12], train_loss: 8521.03 with loss1: 8521.03 and loss2: 0.00\n",
      "Epoch [13], train_loss: 9034.17 with loss1: 9034.17 and loss2: 0.00\n",
      "Epoch [14], train_loss: 9725.47 with loss1: 9725.47 and loss2: 0.00\n",
      "Epoch [15], train_loss: 9256.80 with loss1: 9256.80 and loss2: 0.00\n",
      "Epoch [16], train_loss: 6431.01 with loss1: 6431.01 and loss2: 0.00\n",
      "Epoch [17], train_loss: 8387.80 with loss1: 8387.80 and loss2: 0.00\n",
      "Epoch [18], train_loss: 7273.26 with loss1: 7273.26 and loss2: 0.00\n",
      "Epoch [19], train_loss: 7580.98 with loss1: 7580.98 and loss2: 0.00\n",
      "Epoch [20], train_loss: 6510.96 with loss1: 6510.96 and loss2: 0.00\n",
      "Epoch [21], train_loss: 6492.62 with loss1: 6492.62 and loss2: 0.00\n",
      "Epoch [22], train_loss: 5848.66 with loss1: 5848.66 and loss2: 0.00\n",
      "Epoch [23], train_loss: 5998.23 with loss1: 5998.23 and loss2: 0.00\n",
      "Epoch [24], train_loss: 7968.41 with loss1: 7968.41 and loss2: 0.00\n",
      "Epoch [25], train_loss: 5772.07 with loss1: 5772.07 and loss2: 0.00\n",
      "Epoch [26], train_loss: 5537.34 with loss1: 5537.34 and loss2: 0.00\n",
      "Epoch [27], train_loss: 5983.80 with loss1: 5983.80 and loss2: 0.00\n",
      "Epoch [28], train_loss: 6100.91 with loss1: 6100.91 and loss2: 0.00\n",
      "Epoch [29], train_loss: 5752.68 with loss1: 5752.68 and loss2: 0.00\n",
      "Epoch [30], train_loss: 7611.20 with loss1: 7611.20 and loss2: 0.00\n",
      "Epoch [31], train_loss: 5733.31 with loss1: 5733.31 and loss2: 0.00\n",
      "Epoch [32], train_loss: 6154.23 with loss1: 6154.23 and loss2: 0.00\n",
      "Epoch [33], train_loss: 6305.71 with loss1: 6305.71 and loss2: 0.00\n",
      "Epoch [34], train_loss: 5139.72 with loss1: 5139.72 and loss2: 0.00\n",
      "Epoch [35], train_loss: 6426.95 with loss1: 6426.95 and loss2: 0.00\n",
      "Epoch [36], train_loss: 5439.22 with loss1: 5439.22 and loss2: 0.00\n",
      "Epoch [37], train_loss: 5238.86 with loss1: 5238.86 and loss2: 0.00\n",
      "Epoch [38], train_loss: 5921.11 with loss1: 5921.11 and loss2: 0.00\n",
      "Epoch [39], train_loss: 6488.02 with loss1: 6488.02 and loss2: 0.00\n",
      "Epoch [40], train_loss: 5045.13 with loss1: 5045.13 and loss2: 0.00\n",
      "Epoch [41], train_loss: 6958.01 with loss1: 6958.01 and loss2: 0.00\n",
      "Epoch [42], train_loss: 4942.99 with loss1: 4942.99 and loss2: 0.00\n",
      "Epoch [43], train_loss: 5279.08 with loss1: 5279.08 and loss2: 0.00\n",
      "Epoch [44], train_loss: 5276.37 with loss1: 5276.37 and loss2: 0.00\n",
      "Epoch [45], train_loss: 5392.05 with loss1: 5392.05 and loss2: 0.00\n",
      "Epoch [46], train_loss: 5059.80 with loss1: 5059.80 and loss2: 0.00\n",
      "Epoch [47], train_loss: 6352.13 with loss1: 6352.13 and loss2: 0.00\n",
      "Epoch [48], train_loss: 5246.53 with loss1: 5246.53 and loss2: 0.00\n",
      "Epoch [49], train_loss: 5289.23 with loss1: 5289.23 and loss2: 0.00\n",
      "Epoch [50], train_loss: 6405.06 with loss1: 6405.06 and loss2: 0.00\n",
      "Epoch [51], train_loss: 7043.55 with loss1: 7043.55 and loss2: 0.00\n",
      "Epoch [52], train_loss: 6125.62 with loss1: 6125.62 and loss2: 0.00\n",
      "Epoch [53], train_loss: 4929.05 with loss1: 4929.05 and loss2: 0.00\n",
      "Epoch [54], train_loss: 4494.81 with loss1: 4494.81 and loss2: 0.00\n",
      "Epoch [55], train_loss: 5912.32 with loss1: 5912.32 and loss2: 0.00\n",
      "Epoch [56], train_loss: 5508.69 with loss1: 5508.69 and loss2: 0.00\n",
      "Epoch [57], train_loss: 6081.79 with loss1: 6081.79 and loss2: 0.00\n",
      "Epoch [58], train_loss: 4919.00 with loss1: 4919.00 and loss2: 0.00\n",
      "Epoch [59], train_loss: 5001.00 with loss1: 5001.00 and loss2: 0.00\n",
      "Epoch [60], train_loss: 6223.24 with loss1: 6223.24 and loss2: 0.00\n",
      "Epoch [61], train_loss: 4746.07 with loss1: 4746.07 and loss2: 0.00\n",
      "Epoch [62], train_loss: 4495.15 with loss1: 4495.15 and loss2: 0.00\n",
      "Epoch [63], train_loss: 6257.22 with loss1: 6257.22 and loss2: 0.00\n",
      "Epoch [64], train_loss: 5778.05 with loss1: 5778.05 and loss2: 0.00\n",
      "Epoch [65], train_loss: 6452.51 with loss1: 6452.51 and loss2: 0.00\n",
      "Epoch [66], train_loss: 4872.10 with loss1: 4872.10 and loss2: 0.00\n",
      "Epoch [67], train_loss: 4292.02 with loss1: 4292.02 and loss2: 0.00\n",
      "Epoch [68], train_loss: 6152.82 with loss1: 6152.82 and loss2: 0.00\n",
      "Epoch [69], train_loss: 4577.70 with loss1: 4577.70 and loss2: 0.00\n",
      "Epoch [70], train_loss: 5887.73 with loss1: 5887.73 and loss2: 0.00\n",
      "Epoch [71], train_loss: 4825.02 with loss1: 4825.02 and loss2: 0.00\n",
      "Epoch [72], train_loss: 5637.83 with loss1: 5637.83 and loss2: 0.00\n",
      "Epoch [73], train_loss: 5293.88 with loss1: 5293.88 and loss2: 0.00\n",
      "Epoch [74], train_loss: 5829.59 with loss1: 5829.59 and loss2: 0.00\n",
      "Epoch [75], train_loss: 5597.81 with loss1: 5597.81 and loss2: 0.00\n",
      "Epoch [76], train_loss: 4373.83 with loss1: 4373.83 and loss2: 0.00\n",
      "Epoch [77], train_loss: 4511.42 with loss1: 4511.42 and loss2: 0.00\n",
      "Epoch [78], train_loss: 4018.10 with loss1: 4018.10 and loss2: 0.00\n",
      "Epoch [79], train_loss: 4178.27 with loss1: 4178.27 and loss2: 0.00\n",
      "Epoch [80], train_loss: 4491.41 with loss1: 4491.41 and loss2: 0.00\n",
      "Epoch [81], train_loss: 4560.42 with loss1: 4560.42 and loss2: 0.00\n",
      "Epoch [82], train_loss: 4755.08 with loss1: 4755.08 and loss2: 0.00\n",
      "Epoch [83], train_loss: 5575.94 with loss1: 5575.94 and loss2: 0.00\n",
      "Epoch [84], train_loss: 5678.88 with loss1: 5678.88 and loss2: 0.00\n",
      "Epoch [85], train_loss: 5543.09 with loss1: 5543.09 and loss2: 0.00\n",
      "Epoch [86], train_loss: 5444.17 with loss1: 5444.17 and loss2: 0.00\n",
      "Epoch [87], train_loss: 5398.42 with loss1: 5398.42 and loss2: 0.00\n",
      "Epoch [88], train_loss: 4497.89 with loss1: 4497.89 and loss2: 0.00\n",
      "Epoch [89], train_loss: 4477.74 with loss1: 4477.74 and loss2: 0.00\n",
      "Epoch [90], train_loss: 4216.44 with loss1: 4216.44 and loss2: 0.00\n",
      "Epoch [91], train_loss: 3855.26 with loss1: 3855.26 and loss2: 0.00\n",
      "Epoch [92], train_loss: 4780.67 with loss1: 4780.67 and loss2: 0.00\n",
      "Epoch [93], train_loss: 5808.08 with loss1: 5808.08 and loss2: 0.00\n",
      "Epoch [94], train_loss: 4012.21 with loss1: 4012.21 and loss2: 0.00\n",
      "Epoch [95], train_loss: 5473.72 with loss1: 5473.72 and loss2: 0.00\n",
      "Epoch [96], train_loss: 4319.84 with loss1: 4319.84 and loss2: 0.00\n",
      "Epoch [97], train_loss: 4331.55 with loss1: 4331.55 and loss2: 0.00\n",
      "Epoch [98], train_loss: 5124.42 with loss1: 5124.42 and loss2: 0.00\n",
      "Epoch [99], train_loss: 4922.25 with loss1: 4922.25 and loss2: 0.00\n",
      "Epoch [100], train_loss: 5360.76 with loss1: 5360.76 and loss2: 0.00\n",
      "Epoch [101], train_loss: 4434.35 with loss1: 4434.35 and loss2: 0.00\n",
      "Epoch [102], train_loss: 4194.06 with loss1: 4194.06 and loss2: 0.00\n",
      "Epoch [103], train_loss: 3991.78 with loss1: 3991.78 and loss2: 0.00\n",
      "Epoch [104], train_loss: 4342.09 with loss1: 4342.09 and loss2: 0.00\n",
      "Epoch [105], train_loss: 5329.50 with loss1: 5329.50 and loss2: 0.00\n",
      "Epoch [106], train_loss: 3835.99 with loss1: 3835.99 and loss2: 0.00\n",
      "Epoch [107], train_loss: 3735.85 with loss1: 3735.85 and loss2: 0.00\n",
      "Epoch [108], train_loss: 4018.94 with loss1: 4018.94 and loss2: 0.00\n",
      "Epoch [109], train_loss: 3524.53 with loss1: 3524.53 and loss2: 0.00\n",
      "Epoch [110], train_loss: 4641.27 with loss1: 4641.27 and loss2: 0.00\n",
      "Epoch [111], train_loss: 3487.65 with loss1: 3487.65 and loss2: 0.00\n",
      "Epoch [112], train_loss: 3724.08 with loss1: 3724.08 and loss2: 0.00\n",
      "Epoch [113], train_loss: 3980.29 with loss1: 3980.29 and loss2: 0.00\n",
      "Epoch [114], train_loss: 3799.22 with loss1: 3799.22 and loss2: 0.00\n",
      "Epoch [115], train_loss: 4031.64 with loss1: 4031.64 and loss2: 0.00\n",
      "Epoch [116], train_loss: 3508.64 with loss1: 3508.64 and loss2: 0.00\n",
      "Epoch [117], train_loss: 3914.66 with loss1: 3914.66 and loss2: 0.00\n",
      "Epoch [118], train_loss: 3870.35 with loss1: 3870.35 and loss2: 0.00\n",
      "Epoch [119], train_loss: 4683.43 with loss1: 4683.43 and loss2: 0.00\n",
      "Epoch [120], train_loss: 4054.23 with loss1: 4054.23 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [121], train_loss: 3606.88 with loss1: 3606.88 and loss2: 0.00\n",
      "Epoch [122], train_loss: 3157.82 with loss1: 3157.82 and loss2: 0.00\n",
      "Epoch [123], train_loss: 3746.16 with loss1: 3746.16 and loss2: 0.00\n",
      "Epoch [124], train_loss: 3291.70 with loss1: 3291.70 and loss2: 0.00\n",
      "Epoch [125], train_loss: 3067.20 with loss1: 3067.20 and loss2: 0.00\n",
      "Epoch [126], train_loss: 3778.14 with loss1: 3778.14 and loss2: 0.00\n",
      "Epoch [127], train_loss: 4294.10 with loss1: 4294.10 and loss2: 0.00\n",
      "Epoch [128], train_loss: 3868.67 with loss1: 3868.67 and loss2: 0.00\n",
      "Epoch [129], train_loss: 3083.32 with loss1: 3083.32 and loss2: 0.00\n",
      "Epoch [130], train_loss: 3408.57 with loss1: 3408.57 and loss2: 0.00\n",
      "Epoch [131], train_loss: 3612.58 with loss1: 3612.58 and loss2: 0.00\n",
      "Epoch [132], train_loss: 2974.57 with loss1: 2974.57 and loss2: 0.00\n",
      "Epoch [133], train_loss: 3438.03 with loss1: 3438.03 and loss2: 0.00\n",
      "Epoch [134], train_loss: 3089.08 with loss1: 3089.08 and loss2: 0.00\n",
      "Epoch [135], train_loss: 3719.74 with loss1: 3719.74 and loss2: 0.00\n",
      "Epoch [136], train_loss: 3518.51 with loss1: 3518.51 and loss2: 0.00\n",
      "Epoch [137], train_loss: 3170.99 with loss1: 3170.99 and loss2: 0.00\n",
      "Epoch [138], train_loss: 2708.41 with loss1: 2708.41 and loss2: 0.00\n",
      "Epoch [139], train_loss: 3321.39 with loss1: 3321.39 and loss2: 0.00\n",
      "Epoch [140], train_loss: 3775.44 with loss1: 3775.44 and loss2: 0.00\n",
      "Epoch [141], train_loss: 3595.36 with loss1: 3595.36 and loss2: 0.00\n",
      "Epoch [142], train_loss: 2903.91 with loss1: 2903.91 and loss2: 0.00\n",
      "Epoch [143], train_loss: 2487.31 with loss1: 2487.31 and loss2: 0.00\n",
      "Epoch [144], train_loss: 2530.09 with loss1: 2530.09 and loss2: 0.00\n",
      "Epoch [145], train_loss: 2337.71 with loss1: 2337.71 and loss2: 0.00\n",
      "Epoch [146], train_loss: 2935.63 with loss1: 2935.63 and loss2: 0.00\n",
      "Epoch [147], train_loss: 2462.21 with loss1: 2462.21 and loss2: 0.00\n",
      "Epoch [148], train_loss: 2392.61 with loss1: 2392.61 and loss2: 0.00\n",
      "Epoch [149], train_loss: 2932.33 with loss1: 2932.33 and loss2: 0.00\n",
      "Epoch [150], train_loss: 2690.09 with loss1: 2690.09 and loss2: 0.00\n",
      "Epoch [151], train_loss: 2448.27 with loss1: 2448.27 and loss2: 0.00\n",
      "Epoch [152], train_loss: 2435.21 with loss1: 2435.21 and loss2: 0.00\n",
      "Epoch [153], train_loss: 2930.45 with loss1: 2930.45 and loss2: 0.00\n",
      "Epoch [154], train_loss: 3059.39 with loss1: 3059.39 and loss2: 0.00\n",
      "Epoch [155], train_loss: 2751.49 with loss1: 2751.49 and loss2: 0.00\n",
      "Epoch [156], train_loss: 3110.48 with loss1: 3110.48 and loss2: 0.00\n",
      "Epoch [157], train_loss: 2732.88 with loss1: 2732.88 and loss2: 0.00\n",
      "Epoch [158], train_loss: 2165.48 with loss1: 2165.48 and loss2: 0.00\n",
      "Epoch [159], train_loss: 2713.53 with loss1: 2713.53 and loss2: 0.00\n",
      "Epoch [160], train_loss: 2573.18 with loss1: 2573.18 and loss2: 0.00\n",
      "Epoch [161], train_loss: 3027.37 with loss1: 3027.37 and loss2: 0.00\n",
      "Epoch [162], train_loss: 2226.61 with loss1: 2226.61 and loss2: 0.00\n",
      "Epoch [163], train_loss: 2761.23 with loss1: 2761.23 and loss2: 0.00\n",
      "Epoch [164], train_loss: 2521.50 with loss1: 2521.50 and loss2: 0.00\n",
      "Epoch [165], train_loss: 2597.69 with loss1: 2597.69 and loss2: 0.00\n",
      "Epoch [166], train_loss: 2583.74 with loss1: 2583.74 and loss2: 0.00\n",
      "Epoch [167], train_loss: 3132.37 with loss1: 3132.37 and loss2: 0.00\n",
      "Epoch [168], train_loss: 2745.53 with loss1: 2745.53 and loss2: 0.00\n",
      "Epoch [169], train_loss: 2459.31 with loss1: 2459.31 and loss2: 0.00\n",
      "Epoch [170], train_loss: 2510.21 with loss1: 2510.21 and loss2: 0.00\n",
      "Epoch [171], train_loss: 2738.42 with loss1: 2738.42 and loss2: 0.00\n",
      "Epoch [172], train_loss: 2719.90 with loss1: 2719.90 and loss2: 0.00\n",
      "Epoch [173], train_loss: 2490.55 with loss1: 2490.55 and loss2: 0.00\n",
      "Epoch [174], train_loss: 2364.71 with loss1: 2364.71 and loss2: 0.00\n",
      "Epoch [175], train_loss: 2476.69 with loss1: 2476.69 and loss2: 0.00\n",
      "Epoch [176], train_loss: 2298.66 with loss1: 2298.66 and loss2: 0.00\n",
      "Epoch [177], train_loss: 2384.19 with loss1: 2384.19 and loss2: 0.00\n",
      "Epoch [178], train_loss: 2296.94 with loss1: 2296.94 and loss2: 0.00\n",
      "Epoch [179], train_loss: 2419.33 with loss1: 2419.33 and loss2: 0.00\n",
      "Epoch [180], train_loss: 2956.07 with loss1: 2956.07 and loss2: 0.00\n",
      "Epoch [181], train_loss: 2148.70 with loss1: 2148.70 and loss2: 0.00\n",
      "Epoch [182], train_loss: 2689.96 with loss1: 2689.96 and loss2: 0.00\n",
      "Epoch [183], train_loss: 2869.75 with loss1: 2869.75 and loss2: 0.00\n",
      "Epoch [184], train_loss: 2105.63 with loss1: 2105.63 and loss2: 0.00\n",
      "Epoch [185], train_loss: 2575.68 with loss1: 2575.68 and loss2: 0.00\n",
      "Epoch [186], train_loss: 2045.13 with loss1: 2045.13 and loss2: 0.00\n",
      "Epoch [187], train_loss: 2264.99 with loss1: 2264.99 and loss2: 0.00\n",
      "Epoch [188], train_loss: 2234.31 with loss1: 2234.31 and loss2: 0.00\n",
      "Epoch [189], train_loss: 2677.28 with loss1: 2677.28 and loss2: 0.00\n",
      "Epoch [190], train_loss: 2193.68 with loss1: 2193.68 and loss2: 0.00\n",
      "Epoch [191], train_loss: 1811.71 with loss1: 1811.71 and loss2: 0.00\n",
      "Epoch [192], train_loss: 2257.50 with loss1: 2257.50 and loss2: 0.00\n",
      "Epoch [193], train_loss: 2133.48 with loss1: 2133.48 and loss2: 0.00\n",
      "Epoch [194], train_loss: 2124.97 with loss1: 2124.97 and loss2: 0.00\n",
      "Epoch [195], train_loss: 2281.77 with loss1: 2281.77 and loss2: 0.00\n",
      "Epoch [196], train_loss: 2646.93 with loss1: 2646.93 and loss2: 0.00\n",
      "Epoch [197], train_loss: 2741.65 with loss1: 2741.65 and loss2: 0.00\n",
      "Epoch [198], train_loss: 2104.62 with loss1: 2104.62 and loss2: 0.00\n",
      "Epoch [199], train_loss: 2286.13 with loss1: 2286.13 and loss2: 0.00\n",
      "Epoch [200], train_loss: 2135.58 with loss1: 2135.58 and loss2: 0.00\n",
      "Epoch [201], train_loss: 2275.91 with loss1: 2275.91 and loss2: 0.00\n",
      "Epoch [202], train_loss: 2158.76 with loss1: 2158.76 and loss2: 0.00\n",
      "Epoch [203], train_loss: 2551.25 with loss1: 2551.25 and loss2: 0.00\n",
      "Epoch [204], train_loss: 2520.41 with loss1: 2520.41 and loss2: 0.00\n",
      "Epoch [205], train_loss: 1980.41 with loss1: 1980.41 and loss2: 0.00\n",
      "Epoch [206], train_loss: 2191.51 with loss1: 2191.51 and loss2: 0.00\n",
      "Epoch [207], train_loss: 2083.38 with loss1: 2083.38 and loss2: 0.00\n",
      "Epoch [208], train_loss: 1945.24 with loss1: 1945.24 and loss2: 0.00\n",
      "Epoch [209], train_loss: 2228.60 with loss1: 2228.60 and loss2: 0.00\n",
      "Epoch [210], train_loss: 1811.83 with loss1: 1811.83 and loss2: 0.00\n",
      "Epoch [211], train_loss: 1631.71 with loss1: 1631.71 and loss2: 0.00\n",
      "Epoch [212], train_loss: 2302.44 with loss1: 2302.44 and loss2: 0.00\n",
      "Epoch [213], train_loss: 2005.47 with loss1: 2005.47 and loss2: 0.00\n",
      "Epoch [214], train_loss: 1686.50 with loss1: 1686.50 and loss2: 0.00\n",
      "Epoch [215], train_loss: 2055.07 with loss1: 2055.07 and loss2: 0.00\n",
      "Epoch [216], train_loss: 2366.98 with loss1: 2366.98 and loss2: 0.00\n",
      "Epoch [217], train_loss: 1840.52 with loss1: 1840.52 and loss2: 0.00\n",
      "Epoch [218], train_loss: 2186.40 with loss1: 2186.40 and loss2: 0.00\n",
      "Epoch [219], train_loss: 1883.55 with loss1: 1883.55 and loss2: 0.00\n",
      "Epoch [220], train_loss: 2129.94 with loss1: 2129.94 and loss2: 0.00\n",
      "Epoch [221], train_loss: 1666.85 with loss1: 1666.85 and loss2: 0.00\n",
      "Epoch [222], train_loss: 2053.02 with loss1: 2053.02 and loss2: 0.00\n",
      "Epoch [223], train_loss: 2316.34 with loss1: 2316.34 and loss2: 0.00\n",
      "Epoch [224], train_loss: 2381.04 with loss1: 2381.04 and loss2: 0.00\n",
      "Epoch [225], train_loss: 2305.73 with loss1: 2305.73 and loss2: 0.00\n",
      "Epoch [226], train_loss: 2327.55 with loss1: 2327.55 and loss2: 0.00\n",
      "Epoch [227], train_loss: 2135.35 with loss1: 2135.35 and loss2: 0.00\n",
      "Epoch [228], train_loss: 1929.32 with loss1: 1929.32 and loss2: 0.00\n",
      "Epoch [229], train_loss: 2078.41 with loss1: 2078.41 and loss2: 0.00\n",
      "Epoch [230], train_loss: 2076.86 with loss1: 2076.86 and loss2: 0.00\n",
      "Epoch [231], train_loss: 1568.50 with loss1: 1568.50 and loss2: 0.00\n",
      "Epoch [232], train_loss: 1999.09 with loss1: 1999.09 and loss2: 0.00\n",
      "Epoch [233], train_loss: 1681.73 with loss1: 1681.73 and loss2: 0.00\n",
      "Epoch [234], train_loss: 2128.85 with loss1: 2128.85 and loss2: 0.00\n",
      "Epoch [235], train_loss: 1656.92 with loss1: 1656.92 and loss2: 0.00\n",
      "Epoch [236], train_loss: 2004.41 with loss1: 2004.41 and loss2: 0.00\n",
      "Epoch [237], train_loss: 2032.85 with loss1: 2032.85 and loss2: 0.00\n",
      "Epoch [238], train_loss: 1947.63 with loss1: 1947.63 and loss2: 0.00\n",
      "Epoch [239], train_loss: 2015.61 with loss1: 2015.61 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [240], train_loss: 1505.02 with loss1: 1505.02 and loss2: 0.00\n",
      "Epoch [241], train_loss: 1998.04 with loss1: 1998.04 and loss2: 0.00\n",
      "Epoch [242], train_loss: 2226.14 with loss1: 2226.14 and loss2: 0.00\n",
      "Epoch [243], train_loss: 1884.11 with loss1: 1884.11 and loss2: 0.00\n",
      "Epoch [244], train_loss: 1652.80 with loss1: 1652.80 and loss2: 0.00\n",
      "Epoch [245], train_loss: 2050.34 with loss1: 2050.34 and loss2: 0.00\n",
      "Epoch [246], train_loss: 1799.64 with loss1: 1799.64 and loss2: 0.00\n",
      "Epoch [247], train_loss: 2034.33 with loss1: 2034.33 and loss2: 0.00\n",
      "Epoch [248], train_loss: 1641.69 with loss1: 1641.69 and loss2: 0.00\n",
      "Epoch [249], train_loss: 2024.09 with loss1: 2024.09 and loss2: 0.00\n",
      "Epoch [250], train_loss: 2244.94 with loss1: 2244.94 and loss2: 0.00\n",
      "Epoch [251], train_loss: 2160.01 with loss1: 2160.01 and loss2: 0.00\n",
      "Epoch [252], train_loss: 2060.47 with loss1: 2060.47 and loss2: 0.00\n",
      "Epoch [253], train_loss: 1738.19 with loss1: 1738.19 and loss2: 0.00\n",
      "Epoch [254], train_loss: 1635.73 with loss1: 1635.73 and loss2: 0.00\n",
      "Epoch [255], train_loss: 1787.99 with loss1: 1787.99 and loss2: 0.00\n",
      "Epoch [256], train_loss: 1614.51 with loss1: 1614.51 and loss2: 0.00\n",
      "Epoch [257], train_loss: 1816.66 with loss1: 1816.66 and loss2: 0.00\n",
      "Epoch [258], train_loss: 1672.97 with loss1: 1672.97 and loss2: 0.00\n",
      "Epoch [259], train_loss: 1852.16 with loss1: 1852.16 and loss2: 0.00\n",
      "Epoch [260], train_loss: 1959.44 with loss1: 1959.44 and loss2: 0.00\n",
      "Epoch [261], train_loss: 1636.31 with loss1: 1636.31 and loss2: 0.00\n",
      "Epoch [262], train_loss: 1418.99 with loss1: 1418.99 and loss2: 0.00\n",
      "Epoch [263], train_loss: 1978.62 with loss1: 1978.62 and loss2: 0.00\n",
      "Epoch [264], train_loss: 1525.37 with loss1: 1525.37 and loss2: 0.00\n",
      "Epoch [265], train_loss: 1369.58 with loss1: 1369.58 and loss2: 0.00\n",
      "Epoch [266], train_loss: 1938.11 with loss1: 1938.11 and loss2: 0.00\n",
      "Epoch [267], train_loss: 1531.77 with loss1: 1531.77 and loss2: 0.00\n",
      "Epoch [268], train_loss: 1526.31 with loss1: 1526.31 and loss2: 0.00\n",
      "Epoch [269], train_loss: 1892.46 with loss1: 1892.46 and loss2: 0.00\n",
      "Epoch [270], train_loss: 1458.96 with loss1: 1458.96 and loss2: 0.00\n",
      "Epoch [271], train_loss: 1910.12 with loss1: 1910.12 and loss2: 0.00\n",
      "Epoch [272], train_loss: 1707.07 with loss1: 1707.07 and loss2: 0.00\n",
      "Epoch [273], train_loss: 1370.36 with loss1: 1370.36 and loss2: 0.00\n",
      "Epoch [274], train_loss: 1351.67 with loss1: 1351.67 and loss2: 0.00\n",
      "Epoch [275], train_loss: 1490.26 with loss1: 1490.26 and loss2: 0.00\n",
      "Epoch [276], train_loss: 1692.68 with loss1: 1692.68 and loss2: 0.00\n",
      "Epoch [277], train_loss: 1366.61 with loss1: 1366.61 and loss2: 0.00\n",
      "Epoch [278], train_loss: 1872.41 with loss1: 1872.41 and loss2: 0.00\n",
      "Epoch [279], train_loss: 2083.88 with loss1: 2083.88 and loss2: 0.00\n",
      "Epoch [280], train_loss: 1560.82 with loss1: 1560.82 and loss2: 0.00\n",
      "Epoch [281], train_loss: 1838.14 with loss1: 1838.14 and loss2: 0.00\n",
      "Epoch [282], train_loss: 2069.99 with loss1: 2069.99 and loss2: 0.00\n",
      "Epoch [283], train_loss: 2019.90 with loss1: 2019.90 and loss2: 0.00\n",
      "Epoch [284], train_loss: 1408.52 with loss1: 1408.52 and loss2: 0.00\n",
      "Epoch [285], train_loss: 1933.63 with loss1: 1933.63 and loss2: 0.00\n",
      "Epoch [286], train_loss: 1971.07 with loss1: 1971.07 and loss2: 0.00\n",
      "Epoch [287], train_loss: 1538.73 with loss1: 1538.73 and loss2: 0.00\n",
      "Epoch [288], train_loss: 1798.35 with loss1: 1798.35 and loss2: 0.00\n",
      "Epoch [289], train_loss: 1959.25 with loss1: 1959.25 and loss2: 0.00\n",
      "Epoch [290], train_loss: 1359.02 with loss1: 1359.02 and loss2: 0.00\n",
      "Epoch [291], train_loss: 1365.15 with loss1: 1365.15 and loss2: 0.00\n",
      "Epoch [292], train_loss: 1798.12 with loss1: 1798.12 and loss2: 0.00\n",
      "Epoch [293], train_loss: 1653.66 with loss1: 1653.66 and loss2: 0.00\n",
      "Epoch [294], train_loss: 1409.76 with loss1: 1409.76 and loss2: 0.00\n",
      "Epoch [295], train_loss: 1550.65 with loss1: 1550.65 and loss2: 0.00\n",
      "Epoch [296], train_loss: 1655.91 with loss1: 1655.91 and loss2: 0.00\n",
      "Epoch [297], train_loss: 1652.00 with loss1: 1652.00 and loss2: 0.00\n",
      "Epoch [298], train_loss: 1397.84 with loss1: 1397.84 and loss2: 0.00\n",
      "Epoch [299], train_loss: 1743.34 with loss1: 1743.34 and loss2: 0.00\n",
      "Epoch [300], train_loss: 2065.12 with loss1: 2065.12 and loss2: 0.00\n",
      "Epoch [301], train_loss: 1704.36 with loss1: 1704.36 and loss2: 0.00\n",
      "Epoch [302], train_loss: 1734.63 with loss1: 1734.63 and loss2: 0.00\n",
      "Epoch [303], train_loss: 2075.72 with loss1: 2075.72 and loss2: 0.00\n",
      "Epoch [304], train_loss: 2068.07 with loss1: 2068.07 and loss2: 0.00\n",
      "Epoch [305], train_loss: 1725.25 with loss1: 1725.25 and loss2: 0.00\n",
      "Epoch [306], train_loss: 1530.03 with loss1: 1530.03 and loss2: 0.00\n",
      "Epoch [307], train_loss: 1624.04 with loss1: 1624.04 and loss2: 0.00\n",
      "Epoch [308], train_loss: 1579.70 with loss1: 1579.70 and loss2: 0.00\n",
      "Epoch [309], train_loss: 1556.25 with loss1: 1556.25 and loss2: 0.00\n",
      "Epoch [310], train_loss: 1238.99 with loss1: 1238.99 and loss2: 0.00\n",
      "Epoch [311], train_loss: 1246.06 with loss1: 1246.06 and loss2: 0.00\n",
      "Epoch [312], train_loss: 1256.33 with loss1: 1256.33 and loss2: 0.00\n",
      "Epoch [313], train_loss: 1858.75 with loss1: 1858.75 and loss2: 0.00\n",
      "Epoch [314], train_loss: 1866.49 with loss1: 1866.49 and loss2: 0.00\n",
      "Epoch [315], train_loss: 1282.04 with loss1: 1282.04 and loss2: 0.00\n",
      "Epoch [316], train_loss: 1487.94 with loss1: 1487.94 and loss2: 0.00\n",
      "Epoch [317], train_loss: 1654.83 with loss1: 1654.83 and loss2: 0.00\n",
      "Epoch [318], train_loss: 1548.78 with loss1: 1548.78 and loss2: 0.00\n",
      "Epoch [319], train_loss: 1817.17 with loss1: 1817.17 and loss2: 0.00\n",
      "Epoch [320], train_loss: 1887.79 with loss1: 1887.79 and loss2: 0.00\n",
      "Epoch [321], train_loss: 1741.83 with loss1: 1741.83 and loss2: 0.00\n",
      "Epoch [322], train_loss: 1294.30 with loss1: 1294.30 and loss2: 0.00\n",
      "Epoch [323], train_loss: 1634.75 with loss1: 1634.75 and loss2: 0.00\n",
      "Epoch [324], train_loss: 1536.91 with loss1: 1536.91 and loss2: 0.00\n",
      "Epoch [325], train_loss: 1370.69 with loss1: 1370.69 and loss2: 0.00\n",
      "Epoch [326], train_loss: 1607.21 with loss1: 1607.21 and loss2: 0.00\n",
      "Epoch [327], train_loss: 1383.90 with loss1: 1383.90 and loss2: 0.00\n",
      "Epoch [328], train_loss: 1613.92 with loss1: 1613.92 and loss2: 0.00\n",
      "Epoch [329], train_loss: 1430.49 with loss1: 1430.49 and loss2: 0.00\n",
      "Epoch [330], train_loss: 1219.56 with loss1: 1219.56 and loss2: 0.00\n",
      "Epoch [331], train_loss: 1588.66 with loss1: 1588.66 and loss2: 0.00\n",
      "Epoch [332], train_loss: 1471.37 with loss1: 1471.37 and loss2: 0.00\n",
      "Epoch [333], train_loss: 1714.07 with loss1: 1714.07 and loss2: 0.00\n",
      "Epoch [334], train_loss: 1384.28 with loss1: 1384.28 and loss2: 0.00\n",
      "Epoch [335], train_loss: 1588.97 with loss1: 1588.97 and loss2: 0.00\n",
      "Epoch [336], train_loss: 1332.24 with loss1: 1332.24 and loss2: 0.00\n",
      "Epoch [337], train_loss: 1549.72 with loss1: 1549.72 and loss2: 0.00\n",
      "Epoch [338], train_loss: 1622.45 with loss1: 1622.45 and loss2: 0.00\n",
      "Epoch [339], train_loss: 1529.43 with loss1: 1529.43 and loss2: 0.00\n",
      "Epoch [340], train_loss: 1338.22 with loss1: 1338.22 and loss2: 0.00\n",
      "Epoch [341], train_loss: 1835.18 with loss1: 1835.18 and loss2: 0.00\n",
      "Epoch [342], train_loss: 1555.36 with loss1: 1555.36 and loss2: 0.00\n",
      "Epoch [343], train_loss: 1706.68 with loss1: 1706.68 and loss2: 0.00\n",
      "Epoch [344], train_loss: 1538.89 with loss1: 1538.89 and loss2: 0.00\n",
      "Epoch [345], train_loss: 1368.67 with loss1: 1368.67 and loss2: 0.00\n",
      "Epoch [346], train_loss: 1583.21 with loss1: 1583.21 and loss2: 0.00\n",
      "Epoch [347], train_loss: 1198.08 with loss1: 1198.08 and loss2: 0.00\n",
      "Epoch [348], train_loss: 1568.64 with loss1: 1568.64 and loss2: 0.00\n",
      "Epoch [349], train_loss: 1333.04 with loss1: 1333.04 and loss2: 0.00\n",
      "Epoch [350], train_loss: 1556.11 with loss1: 1556.11 and loss2: 0.00\n",
      "Epoch [351], train_loss: 1160.79 with loss1: 1160.79 and loss2: 0.00\n",
      "Epoch [352], train_loss: 1578.10 with loss1: 1578.10 and loss2: 0.00\n",
      "Epoch [353], train_loss: 1158.74 with loss1: 1158.74 and loss2: 0.00\n",
      "Epoch [354], train_loss: 1365.93 with loss1: 1365.93 and loss2: 0.00\n",
      "Epoch [355], train_loss: 1294.04 with loss1: 1294.04 and loss2: 0.00\n",
      "Epoch [356], train_loss: 1152.74 with loss1: 1152.74 and loss2: 0.00\n",
      "Epoch [357], train_loss: 1406.61 with loss1: 1406.61 and loss2: 0.00\n",
      "Epoch [358], train_loss: 1506.95 with loss1: 1506.95 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [359], train_loss: 1319.05 with loss1: 1319.05 and loss2: 0.00\n",
      "Epoch [360], train_loss: 1616.91 with loss1: 1616.91 and loss2: 0.00\n",
      "Epoch [361], train_loss: 1691.56 with loss1: 1691.56 and loss2: 0.00\n",
      "Epoch [362], train_loss: 1280.21 with loss1: 1280.21 and loss2: 0.00\n",
      "Epoch [363], train_loss: 1356.22 with loss1: 1356.22 and loss2: 0.00\n",
      "Epoch [364], train_loss: 1282.51 with loss1: 1282.51 and loss2: 0.00\n",
      "Epoch [365], train_loss: 1272.40 with loss1: 1272.40 and loss2: 0.00\n",
      "Epoch [366], train_loss: 1566.32 with loss1: 1566.32 and loss2: 0.00\n",
      "Epoch [367], train_loss: 1391.98 with loss1: 1391.98 and loss2: 0.00\n",
      "Epoch [368], train_loss: 1329.57 with loss1: 1329.57 and loss2: 0.00\n",
      "Epoch [369], train_loss: 1716.43 with loss1: 1716.43 and loss2: 0.00\n",
      "Epoch [370], train_loss: 1288.27 with loss1: 1288.27 and loss2: 0.00\n",
      "Epoch [371], train_loss: 1436.59 with loss1: 1436.59 and loss2: 0.00\n",
      "Epoch [372], train_loss: 1547.57 with loss1: 1547.57 and loss2: 0.00\n",
      "Epoch [373], train_loss: 1469.05 with loss1: 1469.05 and loss2: 0.00\n",
      "Epoch [374], train_loss: 1372.47 with loss1: 1372.47 and loss2: 0.00\n",
      "Epoch [375], train_loss: 1244.59 with loss1: 1244.59 and loss2: 0.00\n",
      "Epoch [376], train_loss: 1455.65 with loss1: 1455.65 and loss2: 0.00\n",
      "Epoch [377], train_loss: 1127.70 with loss1: 1127.70 and loss2: 0.00\n",
      "Epoch [378], train_loss: 1088.80 with loss1: 1088.80 and loss2: 0.00\n",
      "Epoch [379], train_loss: 1520.44 with loss1: 1520.44 and loss2: 0.00\n",
      "Epoch [380], train_loss: 1368.06 with loss1: 1368.06 and loss2: 0.00\n",
      "Epoch [381], train_loss: 1122.80 with loss1: 1122.80 and loss2: 0.00\n",
      "Epoch [382], train_loss: 1566.62 with loss1: 1566.62 and loss2: 0.00\n",
      "Epoch [383], train_loss: 1633.38 with loss1: 1633.38 and loss2: 0.00\n",
      "Epoch [384], train_loss: 1200.65 with loss1: 1200.65 and loss2: 0.00\n",
      "Epoch [385], train_loss: 1089.03 with loss1: 1089.03 and loss2: 0.00\n",
      "Epoch [386], train_loss: 1530.96 with loss1: 1530.96 and loss2: 0.00\n",
      "Epoch [387], train_loss: 1094.24 with loss1: 1094.24 and loss2: 0.00\n",
      "Epoch [388], train_loss: 1190.96 with loss1: 1190.96 and loss2: 0.00\n",
      "Epoch [389], train_loss: 1192.07 with loss1: 1192.07 and loss2: 0.00\n",
      "Epoch [390], train_loss: 1171.61 with loss1: 1171.61 and loss2: 0.00\n",
      "Epoch [391], train_loss: 1387.75 with loss1: 1387.75 and loss2: 0.00\n",
      "Epoch [392], train_loss: 1694.46 with loss1: 1694.46 and loss2: 0.00\n",
      "Epoch [393], train_loss: 1211.51 with loss1: 1211.51 and loss2: 0.00\n",
      "Epoch [394], train_loss: 1564.22 with loss1: 1564.22 and loss2: 0.00\n",
      "Epoch [395], train_loss: 1092.16 with loss1: 1092.16 and loss2: 0.00\n",
      "Epoch [396], train_loss: 1537.20 with loss1: 1537.20 and loss2: 0.00\n",
      "Epoch [397], train_loss: 1070.49 with loss1: 1070.49 and loss2: 0.00\n",
      "Epoch [398], train_loss: 1064.56 with loss1: 1064.56 and loss2: 0.00\n",
      "Epoch [399], train_loss: 1279.26 with loss1: 1279.26 and loss2: 0.00\n",
      "Epoch [400], train_loss: 1056.58 with loss1: 1056.58 and loss2: 0.00\n",
      "Epoch [401], train_loss: 1514.32 with loss1: 1514.32 and loss2: 0.00\n",
      "Epoch [402], train_loss: 1136.95 with loss1: 1136.95 and loss2: 0.00\n",
      "Epoch [403], train_loss: 1011.13 with loss1: 1011.13 and loss2: 0.00\n",
      "Epoch [404], train_loss: 1027.54 with loss1: 1027.54 and loss2: 0.00\n",
      "Epoch [405], train_loss: 1456.37 with loss1: 1456.37 and loss2: 0.00\n",
      "Epoch [406], train_loss: 1610.81 with loss1: 1610.81 and loss2: 0.00\n",
      "Epoch [407], train_loss: 1152.53 with loss1: 1152.53 and loss2: 0.00\n",
      "Epoch [408], train_loss: 1514.37 with loss1: 1514.37 and loss2: 0.00\n",
      "Epoch [409], train_loss: 1611.00 with loss1: 1611.00 and loss2: 0.00\n",
      "Epoch [410], train_loss: 1110.10 with loss1: 1110.10 and loss2: 0.00\n",
      "Epoch [411], train_loss: 1434.87 with loss1: 1434.87 and loss2: 0.00\n",
      "Epoch [412], train_loss: 1512.67 with loss1: 1512.67 and loss2: 0.00\n",
      "Epoch [413], train_loss: 1608.39 with loss1: 1608.39 and loss2: 0.00\n",
      "Epoch [414], train_loss: 1137.17 with loss1: 1137.17 and loss2: 0.00\n",
      "Epoch [415], train_loss: 1298.67 with loss1: 1298.67 and loss2: 0.00\n",
      "Epoch [416], train_loss: 1177.84 with loss1: 1177.84 and loss2: 0.00\n",
      "Epoch [417], train_loss: 1450.21 with loss1: 1450.21 and loss2: 0.00\n",
      "Epoch [418], train_loss: 1138.24 with loss1: 1138.24 and loss2: 0.00\n",
      "Epoch [419], train_loss: 1029.03 with loss1: 1029.03 and loss2: 0.00\n",
      "Epoch [420], train_loss: 1452.52 with loss1: 1452.52 and loss2: 0.00\n",
      "Epoch [421], train_loss: 1519.29 with loss1: 1519.29 and loss2: 0.00\n",
      "Epoch [422], train_loss: 1128.20 with loss1: 1128.20 and loss2: 0.00\n",
      "Epoch [423], train_loss: 1104.96 with loss1: 1104.96 and loss2: 0.00\n",
      "Epoch [424], train_loss: 1016.57 with loss1: 1016.57 and loss2: 0.00\n",
      "Epoch [425], train_loss: 1390.65 with loss1: 1390.65 and loss2: 0.00\n",
      "Epoch [426], train_loss: 1077.08 with loss1: 1077.08 and loss2: 0.00\n",
      "Epoch [427], train_loss: 1449.06 with loss1: 1449.06 and loss2: 0.00\n",
      "Epoch [428], train_loss: 1052.59 with loss1: 1052.59 and loss2: 0.00\n",
      "Epoch [429], train_loss: 1451.88 with loss1: 1451.88 and loss2: 0.00\n",
      "Epoch [430], train_loss: 1073.38 with loss1: 1073.38 and loss2: 0.00\n",
      "Epoch [431], train_loss: 1207.36 with loss1: 1207.36 and loss2: 0.00\n",
      "Epoch [432], train_loss: 1237.76 with loss1: 1237.76 and loss2: 0.00\n",
      "Epoch [433], train_loss: 1017.23 with loss1: 1017.23 and loss2: 0.00\n",
      "Epoch [434], train_loss: 1428.17 with loss1: 1428.17 and loss2: 0.00\n",
      "Epoch [435], train_loss: 1110.34 with loss1: 1110.34 and loss2: 0.00\n",
      "Epoch [436], train_loss: 1334.61 with loss1: 1334.61 and loss2: 0.00\n",
      "Epoch [437], train_loss: 1229.99 with loss1: 1229.99 and loss2: 0.00\n",
      "Epoch [438], train_loss: 1355.60 with loss1: 1355.60 and loss2: 0.00\n",
      "Epoch [439], train_loss: 1112.68 with loss1: 1112.68 and loss2: 0.00\n",
      "Epoch [440], train_loss: 1545.56 with loss1: 1545.56 and loss2: 0.00\n",
      "Epoch [441], train_loss: 1171.77 with loss1: 1171.77 and loss2: 0.00\n",
      "Epoch [442], train_loss: 1487.01 with loss1: 1487.01 and loss2: 0.00\n",
      "Epoch [443], train_loss: 1626.61 with loss1: 1626.61 and loss2: 0.00\n",
      "Epoch [444], train_loss: 1468.62 with loss1: 1468.62 and loss2: 0.00\n",
      "Epoch [445], train_loss: 1244.26 with loss1: 1244.26 and loss2: 0.00\n",
      "Epoch [446], train_loss: 1116.33 with loss1: 1116.33 and loss2: 0.00\n",
      "Epoch [447], train_loss: 1212.89 with loss1: 1212.89 and loss2: 0.00\n",
      "Epoch [448], train_loss: 1323.82 with loss1: 1323.82 and loss2: 0.00\n",
      "Epoch [449], train_loss: 1765.65 with loss1: 1765.65 and loss2: 0.00\n",
      "Epoch [450], train_loss: 1324.46 with loss1: 1324.46 and loss2: 0.00\n",
      "Epoch [451], train_loss: 1439.61 with loss1: 1439.61 and loss2: 0.00\n",
      "Epoch [452], train_loss: 1174.32 with loss1: 1174.32 and loss2: 0.00\n",
      "Epoch [453], train_loss: 1090.49 with loss1: 1090.49 and loss2: 0.00\n",
      "Epoch [454], train_loss: 1065.10 with loss1: 1065.10 and loss2: 0.00\n",
      "Epoch [455], train_loss: 1352.69 with loss1: 1352.69 and loss2: 0.00\n",
      "Epoch [456], train_loss: 1049.82 with loss1: 1049.82 and loss2: 0.00\n",
      "Epoch [457], train_loss: 1344.92 with loss1: 1344.92 and loss2: 0.00\n",
      "Epoch [458], train_loss: 1249.75 with loss1: 1249.75 and loss2: 0.00\n",
      "Epoch [459], train_loss: 1181.29 with loss1: 1181.29 and loss2: 0.00\n",
      "Epoch [460], train_loss: 1272.54 with loss1: 1272.54 and loss2: 0.00\n",
      "Epoch [461], train_loss: 1020.16 with loss1: 1020.16 and loss2: 0.00\n",
      "Epoch [462], train_loss: 1091.92 with loss1: 1091.92 and loss2: 0.00\n",
      "Epoch [463], train_loss: 1422.62 with loss1: 1422.62 and loss2: 0.00\n",
      "Epoch [464], train_loss: 1003.20 with loss1: 1003.20 and loss2: 0.00\n",
      "Epoch [465], train_loss: 1027.02 with loss1: 1027.02 and loss2: 0.00\n",
      "Epoch [466], train_loss: 1394.46 with loss1: 1394.46 and loss2: 0.00\n",
      "Epoch [467], train_loss: 1414.87 with loss1: 1414.87 and loss2: 0.00\n",
      "Epoch [468], train_loss: 1093.15 with loss1: 1093.15 and loss2: 0.00\n",
      "Epoch [469], train_loss: 1314.19 with loss1: 1314.19 and loss2: 0.00\n",
      "Epoch [470], train_loss: 948.85 with loss1: 948.85 and loss2: 0.00\n",
      "Epoch [471], train_loss: 1329.39 with loss1: 1329.39 and loss2: 0.00\n",
      "Epoch [472], train_loss: 1445.29 with loss1: 1445.29 and loss2: 0.00\n",
      "Epoch [473], train_loss: 1214.66 with loss1: 1214.66 and loss2: 0.00\n",
      "Epoch [474], train_loss: 1158.34 with loss1: 1158.34 and loss2: 0.00\n",
      "Epoch [475], train_loss: 1355.22 with loss1: 1355.22 and loss2: 0.00\n",
      "Epoch [476], train_loss: 1196.65 with loss1: 1196.65 and loss2: 0.00\n",
      "Epoch [477], train_loss: 1386.06 with loss1: 1386.06 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [478], train_loss: 991.87 with loss1: 991.87 and loss2: 0.00\n",
      "Epoch [479], train_loss: 1243.84 with loss1: 1243.84 and loss2: 0.00\n",
      "Epoch [480], train_loss: 1062.44 with loss1: 1062.44 and loss2: 0.00\n",
      "Epoch [481], train_loss: 1006.40 with loss1: 1006.40 and loss2: 0.00\n",
      "Epoch [482], train_loss: 1364.85 with loss1: 1364.85 and loss2: 0.00\n",
      "Epoch [483], train_loss: 1352.99 with loss1: 1352.99 and loss2: 0.00\n",
      "Epoch [484], train_loss: 1194.38 with loss1: 1194.38 and loss2: 0.00\n",
      "Epoch [485], train_loss: 1304.39 with loss1: 1304.39 and loss2: 0.00\n",
      "Epoch [486], train_loss: 1122.46 with loss1: 1122.46 and loss2: 0.00\n",
      "Epoch [487], train_loss: 1234.48 with loss1: 1234.48 and loss2: 0.00\n",
      "Epoch [488], train_loss: 1330.67 with loss1: 1330.67 and loss2: 0.00\n",
      "Epoch [489], train_loss: 1124.51 with loss1: 1124.51 and loss2: 0.00\n",
      "Epoch [490], train_loss: 1039.09 with loss1: 1039.09 and loss2: 0.00\n",
      "Epoch [491], train_loss: 1232.07 with loss1: 1232.07 and loss2: 0.00\n",
      "Epoch [492], train_loss: 1049.46 with loss1: 1049.46 and loss2: 0.00\n",
      "Epoch [493], train_loss: 1151.73 with loss1: 1151.73 and loss2: 0.00\n",
      "Epoch [494], train_loss: 1014.32 with loss1: 1014.32 and loss2: 0.00\n",
      "Epoch [495], train_loss: 1246.34 with loss1: 1246.34 and loss2: 0.00\n",
      "Epoch [496], train_loss: 1306.60 with loss1: 1306.60 and loss2: 0.00\n",
      "Epoch [497], train_loss: 913.36 with loss1: 913.36 and loss2: 0.00\n",
      "Epoch [498], train_loss: 1194.53 with loss1: 1194.53 and loss2: 0.00\n",
      "Epoch [499], train_loss: 916.84 with loss1: 916.84 and loss2: 0.00\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# model version 3 \n",
    "# delete loss2\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history = fit(epochs=500, lr=5e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "413c1238",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 951.24 with loss1: 951.24 and loss2: 0.00\n",
      "Epoch [1], train_loss: 932.78 with loss1: 932.78 and loss2: 0.00\n",
      "Epoch [2], train_loss: 976.49 with loss1: 976.49 and loss2: 0.00\n",
      "Epoch [3], train_loss: 923.78 with loss1: 923.78 and loss2: 0.00\n",
      "Epoch [4], train_loss: 1004.21 with loss1: 1004.21 and loss2: 0.00\n",
      "Epoch [5], train_loss: 963.82 with loss1: 963.82 and loss2: 0.00\n",
      "Epoch [6], train_loss: 1145.92 with loss1: 1145.92 and loss2: 0.00\n",
      "Epoch [7], train_loss: 1554.02 with loss1: 1554.02 and loss2: 0.00\n",
      "Epoch [8], train_loss: 1541.57 with loss1: 1541.57 and loss2: 0.00\n",
      "Epoch [9], train_loss: 1373.69 with loss1: 1373.69 and loss2: 0.00\n",
      "Epoch [10], train_loss: 994.47 with loss1: 994.47 and loss2: 0.00\n",
      "Epoch [11], train_loss: 1315.73 with loss1: 1315.73 and loss2: 0.00\n",
      "Epoch [12], train_loss: 1401.27 with loss1: 1401.27 and loss2: 0.00\n",
      "Epoch [13], train_loss: 1391.48 with loss1: 1391.48 and loss2: 0.00\n",
      "Epoch [14], train_loss: 1168.31 with loss1: 1168.31 and loss2: 0.00\n",
      "Epoch [15], train_loss: 1038.74 with loss1: 1038.74 and loss2: 0.00\n",
      "Epoch [16], train_loss: 1010.35 with loss1: 1010.35 and loss2: 0.00\n",
      "Epoch [17], train_loss: 1047.55 with loss1: 1047.55 and loss2: 0.00\n",
      "Epoch [18], train_loss: 939.81 with loss1: 939.81 and loss2: 0.00\n",
      "Epoch [19], train_loss: 1193.28 with loss1: 1193.28 and loss2: 0.00\n",
      "Epoch [20], train_loss: 974.65 with loss1: 974.65 and loss2: 0.00\n",
      "Epoch [21], train_loss: 1216.78 with loss1: 1216.78 and loss2: 0.00\n",
      "Epoch [22], train_loss: 1311.59 with loss1: 1311.59 and loss2: 0.00\n",
      "Epoch [23], train_loss: 1054.98 with loss1: 1054.98 and loss2: 0.00\n",
      "Epoch [24], train_loss: 1005.12 with loss1: 1005.12 and loss2: 0.00\n",
      "Epoch [25], train_loss: 1148.56 with loss1: 1148.56 and loss2: 0.00\n",
      "Epoch [26], train_loss: 1046.35 with loss1: 1046.35 and loss2: 0.00\n",
      "Epoch [27], train_loss: 987.61 with loss1: 987.61 and loss2: 0.00\n",
      "Epoch [28], train_loss: 924.63 with loss1: 924.63 and loss2: 0.00\n",
      "Epoch [29], train_loss: 876.51 with loss1: 876.51 and loss2: 0.00\n",
      "Epoch [30], train_loss: 1198.79 with loss1: 1198.79 and loss2: 0.00\n",
      "Epoch [31], train_loss: 1264.94 with loss1: 1264.94 and loss2: 0.00\n",
      "Epoch [32], train_loss: 983.71 with loss1: 983.71 and loss2: 0.00\n",
      "Epoch [33], train_loss: 864.59 with loss1: 864.59 and loss2: 0.00\n",
      "Epoch [34], train_loss: 854.96 with loss1: 854.96 and loss2: 0.00\n",
      "Epoch [35], train_loss: 1129.97 with loss1: 1129.97 and loss2: 0.00\n",
      "Epoch [36], train_loss: 1287.26 with loss1: 1287.26 and loss2: 0.00\n",
      "Epoch [37], train_loss: 1015.86 with loss1: 1015.86 and loss2: 0.00\n",
      "Epoch [38], train_loss: 1083.06 with loss1: 1083.06 and loss2: 0.00\n",
      "Epoch [39], train_loss: 1026.38 with loss1: 1026.38 and loss2: 0.00\n",
      "Epoch [40], train_loss: 1261.33 with loss1: 1261.33 and loss2: 0.00\n",
      "Epoch [41], train_loss: 1261.45 with loss1: 1261.45 and loss2: 0.00\n",
      "Epoch [42], train_loss: 1284.00 with loss1: 1284.00 and loss2: 0.00\n",
      "Epoch [43], train_loss: 1336.87 with loss1: 1336.87 and loss2: 0.00\n",
      "Epoch [44], train_loss: 1355.50 with loss1: 1355.50 and loss2: 0.00\n",
      "Epoch [45], train_loss: 1151.61 with loss1: 1151.61 and loss2: 0.00\n",
      "Epoch [46], train_loss: 1040.74 with loss1: 1040.74 and loss2: 0.00\n",
      "Epoch [47], train_loss: 1202.04 with loss1: 1202.04 and loss2: 0.00\n",
      "Epoch [48], train_loss: 1240.14 with loss1: 1240.14 and loss2: 0.00\n",
      "Epoch [49], train_loss: 900.31 with loss1: 900.31 and loss2: 0.00\n",
      "Epoch [50], train_loss: 953.96 with loss1: 953.96 and loss2: 0.00\n",
      "Epoch [51], train_loss: 930.24 with loss1: 930.24 and loss2: 0.00\n",
      "Epoch [52], train_loss: 1040.70 with loss1: 1040.70 and loss2: 0.00\n",
      "Epoch [53], train_loss: 1152.66 with loss1: 1152.66 and loss2: 0.00\n",
      "Epoch [54], train_loss: 915.35 with loss1: 915.35 and loss2: 0.00\n",
      "Epoch [55], train_loss: 1073.89 with loss1: 1073.89 and loss2: 0.00\n",
      "Epoch [56], train_loss: 1088.08 with loss1: 1088.08 and loss2: 0.00\n",
      "Epoch [57], train_loss: 1169.26 with loss1: 1169.26 and loss2: 0.00\n",
      "Epoch [58], train_loss: 843.30 with loss1: 843.30 and loss2: 0.00\n",
      "Epoch [59], train_loss: 871.71 with loss1: 871.71 and loss2: 0.00\n",
      "Epoch [60], train_loss: 1110.01 with loss1: 1110.01 and loss2: 0.00\n",
      "Epoch [61], train_loss: 1047.71 with loss1: 1047.71 and loss2: 0.00\n",
      "Epoch [62], train_loss: 1043.99 with loss1: 1043.99 and loss2: 0.00\n",
      "Epoch [63], train_loss: 974.44 with loss1: 974.44 and loss2: 0.00\n",
      "Epoch [64], train_loss: 1018.07 with loss1: 1018.07 and loss2: 0.00\n",
      "Epoch [65], train_loss: 966.92 with loss1: 966.92 and loss2: 0.00\n",
      "Epoch [66], train_loss: 908.09 with loss1: 908.09 and loss2: 0.00\n",
      "Epoch [67], train_loss: 902.16 with loss1: 902.16 and loss2: 0.00\n",
      "Epoch [68], train_loss: 924.55 with loss1: 924.55 and loss2: 0.00\n",
      "Epoch [69], train_loss: 1182.25 with loss1: 1182.25 and loss2: 0.00\n",
      "Epoch [70], train_loss: 854.30 with loss1: 854.30 and loss2: 0.00\n",
      "Epoch [71], train_loss: 812.83 with loss1: 812.83 and loss2: 0.00\n",
      "Epoch [72], train_loss: 1102.11 with loss1: 1102.11 and loss2: 0.00\n",
      "Epoch [73], train_loss: 949.18 with loss1: 949.18 and loss2: 0.00\n",
      "Epoch [74], train_loss: 831.55 with loss1: 831.55 and loss2: 0.00\n",
      "Epoch [75], train_loss: 939.66 with loss1: 939.66 and loss2: 0.00\n",
      "Epoch [76], train_loss: 1057.39 with loss1: 1057.39 and loss2: 0.00\n",
      "Epoch [77], train_loss: 917.58 with loss1: 917.58 and loss2: 0.00\n",
      "Epoch [78], train_loss: 912.15 with loss1: 912.15 and loss2: 0.00\n",
      "Epoch [79], train_loss: 1167.88 with loss1: 1167.88 and loss2: 0.00\n",
      "Epoch [80], train_loss: 1235.28 with loss1: 1235.28 and loss2: 0.00\n",
      "Epoch [81], train_loss: 933.60 with loss1: 933.60 and loss2: 0.00\n",
      "Epoch [82], train_loss: 946.37 with loss1: 946.37 and loss2: 0.00\n",
      "Epoch [83], train_loss: 1057.37 with loss1: 1057.37 and loss2: 0.00\n",
      "Epoch [84], train_loss: 1141.38 with loss1: 1141.38 and loss2: 0.00\n",
      "Epoch [85], train_loss: 1176.68 with loss1: 1176.68 and loss2: 0.00\n",
      "Epoch [86], train_loss: 797.65 with loss1: 797.65 and loss2: 0.00\n",
      "Epoch [87], train_loss: 902.75 with loss1: 902.75 and loss2: 0.00\n",
      "Epoch [88], train_loss: 895.85 with loss1: 895.85 and loss2: 0.00\n",
      "Epoch [89], train_loss: 947.26 with loss1: 947.26 and loss2: 0.00\n",
      "Epoch [90], train_loss: 1156.80 with loss1: 1156.80 and loss2: 0.00\n",
      "Epoch [91], train_loss: 1221.32 with loss1: 1221.32 and loss2: 0.00\n",
      "Epoch [92], train_loss: 1158.36 with loss1: 1158.36 and loss2: 0.00\n",
      "Epoch [93], train_loss: 1063.89 with loss1: 1063.89 and loss2: 0.00\n",
      "Epoch [94], train_loss: 995.57 with loss1: 995.57 and loss2: 0.00\n",
      "Epoch [95], train_loss: 939.83 with loss1: 939.83 and loss2: 0.00\n",
      "Epoch [96], train_loss: 1032.98 with loss1: 1032.98 and loss2: 0.00\n",
      "Epoch [97], train_loss: 1055.17 with loss1: 1055.17 and loss2: 0.00\n",
      "Epoch [98], train_loss: 948.92 with loss1: 948.92 and loss2: 0.00\n",
      "Epoch [99], train_loss: 1007.86 with loss1: 1007.86 and loss2: 0.00\n",
      "Epoch [100], train_loss: 1100.30 with loss1: 1100.30 and loss2: 0.00\n",
      "Epoch [101], train_loss: 828.73 with loss1: 828.73 and loss2: 0.00\n",
      "Epoch [102], train_loss: 808.35 with loss1: 808.35 and loss2: 0.00\n",
      "Epoch [103], train_loss: 791.72 with loss1: 791.72 and loss2: 0.00\n",
      "Epoch [104], train_loss: 1030.31 with loss1: 1030.31 and loss2: 0.00\n",
      "Epoch [105], train_loss: 911.14 with loss1: 911.14 and loss2: 0.00\n",
      "Epoch [106], train_loss: 865.91 with loss1: 865.91 and loss2: 0.00\n",
      "Epoch [107], train_loss: 912.16 with loss1: 912.16 and loss2: 0.00\n",
      "Epoch [108], train_loss: 1066.51 with loss1: 1066.51 and loss2: 0.00\n",
      "Epoch [109], train_loss: 900.63 with loss1: 900.63 and loss2: 0.00\n",
      "Epoch [110], train_loss: 839.08 with loss1: 839.08 and loss2: 0.00\n",
      "Epoch [111], train_loss: 970.88 with loss1: 970.88 and loss2: 0.00\n",
      "Epoch [112], train_loss: 919.15 with loss1: 919.15 and loss2: 0.00\n",
      "Epoch [113], train_loss: 1014.98 with loss1: 1014.98 and loss2: 0.00\n",
      "Epoch [114], train_loss: 819.43 with loss1: 819.43 and loss2: 0.00\n",
      "Epoch [115], train_loss: 877.99 with loss1: 877.99 and loss2: 0.00\n",
      "Epoch [116], train_loss: 1125.97 with loss1: 1125.97 and loss2: 0.00\n",
      "Epoch [117], train_loss: 922.96 with loss1: 922.96 and loss2: 0.00\n",
      "Epoch [118], train_loss: 917.82 with loss1: 917.82 and loss2: 0.00\n",
      "Epoch [119], train_loss: 865.20 with loss1: 865.20 and loss2: 0.00\n",
      "Epoch [120], train_loss: 799.88 with loss1: 799.88 and loss2: 0.00\n",
      "Epoch [121], train_loss: 890.54 with loss1: 890.54 and loss2: 0.00\n",
      "Epoch [122], train_loss: 865.58 with loss1: 865.58 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [123], train_loss: 840.62 with loss1: 840.62 and loss2: 0.00\n",
      "Epoch [124], train_loss: 868.28 with loss1: 868.28 and loss2: 0.00\n",
      "Epoch [125], train_loss: 1004.07 with loss1: 1004.07 and loss2: 0.00\n",
      "Epoch [126], train_loss: 1137.56 with loss1: 1137.56 and loss2: 0.00\n",
      "Epoch [127], train_loss: 872.58 with loss1: 872.58 and loss2: 0.00\n",
      "Epoch [128], train_loss: 789.90 with loss1: 789.90 and loss2: 0.00\n",
      "Epoch [129], train_loss: 794.13 with loss1: 794.13 and loss2: 0.00\n",
      "Epoch [130], train_loss: 973.52 with loss1: 973.52 and loss2: 0.00\n",
      "Epoch [131], train_loss: 723.87 with loss1: 723.87 and loss2: 0.00\n",
      "Epoch [132], train_loss: 1011.95 with loss1: 1011.95 and loss2: 0.00\n",
      "Epoch [133], train_loss: 752.70 with loss1: 752.70 and loss2: 0.00\n",
      "Epoch [134], train_loss: 774.71 with loss1: 774.71 and loss2: 0.00\n",
      "Epoch [135], train_loss: 928.61 with loss1: 928.61 and loss2: 0.00\n",
      "Epoch [136], train_loss: 897.14 with loss1: 897.14 and loss2: 0.00\n",
      "Epoch [137], train_loss: 950.48 with loss1: 950.48 and loss2: 0.00\n",
      "Epoch [138], train_loss: 875.71 with loss1: 875.71 and loss2: 0.00\n",
      "Epoch [139], train_loss: 776.50 with loss1: 776.50 and loss2: 0.00\n",
      "Epoch [140], train_loss: 777.83 with loss1: 777.83 and loss2: 0.00\n",
      "Epoch [141], train_loss: 785.52 with loss1: 785.52 and loss2: 0.00\n",
      "Epoch [142], train_loss: 823.04 with loss1: 823.04 and loss2: 0.00\n",
      "Epoch [143], train_loss: 778.00 with loss1: 778.00 and loss2: 0.00\n",
      "Epoch [144], train_loss: 784.05 with loss1: 784.05 and loss2: 0.00\n",
      "Epoch [145], train_loss: 1010.47 with loss1: 1010.47 and loss2: 0.00\n",
      "Epoch [146], train_loss: 731.78 with loss1: 731.78 and loss2: 0.00\n",
      "Epoch [147], train_loss: 781.18 with loss1: 781.18 and loss2: 0.00\n",
      "Epoch [148], train_loss: 1178.78 with loss1: 1178.78 and loss2: 0.00\n",
      "Epoch [149], train_loss: 967.52 with loss1: 967.52 and loss2: 0.00\n",
      "Epoch [150], train_loss: 1115.54 with loss1: 1115.54 and loss2: 0.00\n",
      "Epoch [151], train_loss: 1129.29 with loss1: 1129.29 and loss2: 0.00\n",
      "Epoch [152], train_loss: 905.81 with loss1: 905.81 and loss2: 0.00\n",
      "Epoch [153], train_loss: 834.09 with loss1: 834.09 and loss2: 0.00\n",
      "Epoch [154], train_loss: 754.90 with loss1: 754.90 and loss2: 0.00\n",
      "Epoch [155], train_loss: 760.28 with loss1: 760.28 and loss2: 0.00\n",
      "Epoch [156], train_loss: 1055.67 with loss1: 1055.67 and loss2: 0.00\n",
      "Epoch [157], train_loss: 1231.77 with loss1: 1231.77 and loss2: 0.00\n",
      "Epoch [158], train_loss: 1115.43 with loss1: 1115.43 and loss2: 0.00\n",
      "Epoch [159], train_loss: 1048.04 with loss1: 1048.04 and loss2: 0.00\n",
      "Epoch [160], train_loss: 1087.97 with loss1: 1087.97 and loss2: 0.00\n",
      "Epoch [161], train_loss: 920.75 with loss1: 920.75 and loss2: 0.00\n",
      "Epoch [162], train_loss: 727.15 with loss1: 727.15 and loss2: 0.00\n",
      "Epoch [163], train_loss: 798.99 with loss1: 798.99 and loss2: 0.00\n",
      "Epoch [164], train_loss: 866.11 with loss1: 866.11 and loss2: 0.00\n",
      "Epoch [165], train_loss: 802.46 with loss1: 802.46 and loss2: 0.00\n",
      "Epoch [166], train_loss: 701.60 with loss1: 701.60 and loss2: 0.00\n",
      "Epoch [167], train_loss: 1066.93 with loss1: 1066.93 and loss2: 0.00\n",
      "Epoch [168], train_loss: 853.78 with loss1: 853.78 and loss2: 0.00\n",
      "Epoch [169], train_loss: 1015.61 with loss1: 1015.61 and loss2: 0.00\n",
      "Epoch [170], train_loss: 915.77 with loss1: 915.77 and loss2: 0.00\n",
      "Epoch [171], train_loss: 777.78 with loss1: 777.78 and loss2: 0.00\n",
      "Epoch [172], train_loss: 787.62 with loss1: 787.62 and loss2: 0.00\n",
      "Epoch [173], train_loss: 799.13 with loss1: 799.13 and loss2: 0.00\n",
      "Epoch [174], train_loss: 827.78 with loss1: 827.78 and loss2: 0.00\n",
      "Epoch [175], train_loss: 798.94 with loss1: 798.94 and loss2: 0.00\n",
      "Epoch [176], train_loss: 725.77 with loss1: 725.77 and loss2: 0.00\n",
      "Epoch [177], train_loss: 923.98 with loss1: 923.98 and loss2: 0.00\n",
      "Epoch [178], train_loss: 833.75 with loss1: 833.75 and loss2: 0.00\n",
      "Epoch [179], train_loss: 1026.11 with loss1: 1026.11 and loss2: 0.00\n",
      "Epoch [180], train_loss: 697.56 with loss1: 697.56 and loss2: 0.00\n",
      "Epoch [181], train_loss: 1072.65 with loss1: 1072.65 and loss2: 0.00\n",
      "Epoch [182], train_loss: 1245.78 with loss1: 1245.78 and loss2: 0.00\n",
      "Epoch [183], train_loss: 902.77 with loss1: 902.77 and loss2: 0.00\n",
      "Epoch [184], train_loss: 732.33 with loss1: 732.33 and loss2: 0.00\n",
      "Epoch [185], train_loss: 822.05 with loss1: 822.05 and loss2: 0.00\n",
      "Epoch [186], train_loss: 900.40 with loss1: 900.40 and loss2: 0.00\n",
      "Epoch [187], train_loss: 865.59 with loss1: 865.59 and loss2: 0.00\n",
      "Epoch [188], train_loss: 999.72 with loss1: 999.72 and loss2: 0.00\n",
      "Epoch [189], train_loss: 701.40 with loss1: 701.40 and loss2: 0.00\n",
      "Epoch [190], train_loss: 793.63 with loss1: 793.63 and loss2: 0.00\n",
      "Epoch [191], train_loss: 1039.93 with loss1: 1039.93 and loss2: 0.00\n",
      "Epoch [192], train_loss: 1179.24 with loss1: 1179.24 and loss2: 0.00\n",
      "Epoch [193], train_loss: 1142.73 with loss1: 1142.73 and loss2: 0.00\n",
      "Epoch [194], train_loss: 811.12 with loss1: 811.12 and loss2: 0.00\n",
      "Epoch [195], train_loss: 780.89 with loss1: 780.89 and loss2: 0.00\n",
      "Epoch [196], train_loss: 697.69 with loss1: 697.69 and loss2: 0.00\n",
      "Epoch [197], train_loss: 857.39 with loss1: 857.39 and loss2: 0.00\n",
      "Epoch [198], train_loss: 989.61 with loss1: 989.61 and loss2: 0.00\n",
      "Epoch [199], train_loss: 892.23 with loss1: 892.23 and loss2: 0.00\n",
      "Epoch [200], train_loss: 680.32 with loss1: 680.32 and loss2: 0.00\n",
      "Epoch [201], train_loss: 898.79 with loss1: 898.79 and loss2: 0.00\n",
      "Epoch [202], train_loss: 680.02 with loss1: 680.02 and loss2: 0.00\n",
      "Epoch [203], train_loss: 722.34 with loss1: 722.34 and loss2: 0.00\n",
      "Epoch [204], train_loss: 884.36 with loss1: 884.36 and loss2: 0.00\n",
      "Epoch [205], train_loss: 890.59 with loss1: 890.59 and loss2: 0.00\n",
      "Epoch [206], train_loss: 869.43 with loss1: 869.43 and loss2: 0.00\n",
      "Epoch [207], train_loss: 704.66 with loss1: 704.66 and loss2: 0.00\n",
      "Epoch [208], train_loss: 941.66 with loss1: 941.66 and loss2: 0.00\n",
      "Epoch [209], train_loss: 929.71 with loss1: 929.71 and loss2: 0.00\n",
      "Epoch [210], train_loss: 877.00 with loss1: 877.00 and loss2: 0.00\n",
      "Epoch [211], train_loss: 857.09 with loss1: 857.09 and loss2: 0.00\n",
      "Epoch [212], train_loss: 691.68 with loss1: 691.68 and loss2: 0.00\n",
      "Epoch [213], train_loss: 659.66 with loss1: 659.66 and loss2: 0.00\n",
      "Epoch [214], train_loss: 767.52 with loss1: 767.52 and loss2: 0.00\n",
      "Epoch [215], train_loss: 858.75 with loss1: 858.75 and loss2: 0.00\n",
      "Epoch [216], train_loss: 820.22 with loss1: 820.22 and loss2: 0.00\n",
      "Epoch [217], train_loss: 792.98 with loss1: 792.98 and loss2: 0.00\n",
      "Epoch [218], train_loss: 658.85 with loss1: 658.85 and loss2: 0.00\n",
      "Epoch [219], train_loss: 768.12 with loss1: 768.12 and loss2: 0.00\n",
      "Epoch [220], train_loss: 833.21 with loss1: 833.21 and loss2: 0.00\n",
      "Epoch [221], train_loss: 781.13 with loss1: 781.13 and loss2: 0.00\n",
      "Epoch [222], train_loss: 673.34 with loss1: 673.34 and loss2: 0.00\n",
      "Epoch [223], train_loss: 889.46 with loss1: 889.46 and loss2: 0.00\n",
      "Epoch [224], train_loss: 1044.83 with loss1: 1044.83 and loss2: 0.00\n",
      "Epoch [225], train_loss: 1005.11 with loss1: 1005.11 and loss2: 0.00\n",
      "Epoch [226], train_loss: 802.69 with loss1: 802.69 and loss2: 0.00\n",
      "Epoch [227], train_loss: 905.55 with loss1: 905.55 and loss2: 0.00\n",
      "Epoch [228], train_loss: 1126.39 with loss1: 1126.39 and loss2: 0.00\n",
      "Epoch [229], train_loss: 734.20 with loss1: 734.20 and loss2: 0.00\n",
      "Epoch [230], train_loss: 955.81 with loss1: 955.81 and loss2: 0.00\n",
      "Epoch [231], train_loss: 760.59 with loss1: 760.59 and loss2: 0.00\n",
      "Epoch [232], train_loss: 709.57 with loss1: 709.57 and loss2: 0.00\n",
      "Epoch [233], train_loss: 673.97 with loss1: 673.97 and loss2: 0.00\n",
      "Epoch [234], train_loss: 785.92 with loss1: 785.92 and loss2: 0.00\n",
      "Epoch [235], train_loss: 1007.32 with loss1: 1007.32 and loss2: 0.00\n",
      "Epoch [236], train_loss: 1099.49 with loss1: 1099.49 and loss2: 0.00\n",
      "Epoch [237], train_loss: 804.87 with loss1: 804.87 and loss2: 0.00\n",
      "Epoch [238], train_loss: 783.86 with loss1: 783.86 and loss2: 0.00\n",
      "Epoch [239], train_loss: 651.59 with loss1: 651.59 and loss2: 0.00\n",
      "Epoch [240], train_loss: 731.86 with loss1: 731.86 and loss2: 0.00\n",
      "Epoch [241], train_loss: 817.07 with loss1: 817.07 and loss2: 0.00\n",
      "Epoch [242], train_loss: 946.94 with loss1: 946.94 and loss2: 0.00\n",
      "Epoch [243], train_loss: 683.40 with loss1: 683.40 and loss2: 0.00\n",
      "Epoch [244], train_loss: 878.37 with loss1: 878.37 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [245], train_loss: 1003.94 with loss1: 1003.94 and loss2: 0.00\n",
      "Epoch [246], train_loss: 617.00 with loss1: 617.00 and loss2: 0.00\n",
      "Epoch [247], train_loss: 741.01 with loss1: 741.01 and loss2: 0.00\n",
      "Epoch [248], train_loss: 882.32 with loss1: 882.32 and loss2: 0.00\n",
      "Epoch [249], train_loss: 658.17 with loss1: 658.17 and loss2: 0.00\n",
      "Epoch [250], train_loss: 699.88 with loss1: 699.88 and loss2: 0.00\n",
      "Epoch [251], train_loss: 906.76 with loss1: 906.76 and loss2: 0.00\n",
      "Epoch [252], train_loss: 893.60 with loss1: 893.60 and loss2: 0.00\n",
      "Epoch [253], train_loss: 810.20 with loss1: 810.20 and loss2: 0.00\n",
      "Epoch [254], train_loss: 788.48 with loss1: 788.48 and loss2: 0.00\n",
      "Epoch [255], train_loss: 1033.37 with loss1: 1033.37 and loss2: 0.00\n",
      "Epoch [256], train_loss: 668.03 with loss1: 668.03 and loss2: 0.00\n",
      "Epoch [257], train_loss: 787.52 with loss1: 787.52 and loss2: 0.00\n",
      "Epoch [258], train_loss: 839.61 with loss1: 839.61 and loss2: 0.00\n",
      "Epoch [259], train_loss: 790.19 with loss1: 790.19 and loss2: 0.00\n",
      "Epoch [260], train_loss: 911.40 with loss1: 911.40 and loss2: 0.00\n",
      "Epoch [261], train_loss: 658.78 with loss1: 658.78 and loss2: 0.00\n",
      "Epoch [262], train_loss: 751.81 with loss1: 751.81 and loss2: 0.00\n",
      "Epoch [263], train_loss: 803.70 with loss1: 803.70 and loss2: 0.00\n",
      "Epoch [264], train_loss: 787.70 with loss1: 787.70 and loss2: 0.00\n",
      "Epoch [265], train_loss: 658.51 with loss1: 658.51 and loss2: 0.00\n",
      "Epoch [266], train_loss: 866.62 with loss1: 866.62 and loss2: 0.00\n",
      "Epoch [267], train_loss: 890.15 with loss1: 890.15 and loss2: 0.00\n",
      "Epoch [268], train_loss: 815.03 with loss1: 815.03 and loss2: 0.00\n",
      "Epoch [269], train_loss: 834.57 with loss1: 834.57 and loss2: 0.00\n",
      "Epoch [270], train_loss: 804.52 with loss1: 804.52 and loss2: 0.00\n",
      "Epoch [271], train_loss: 757.47 with loss1: 757.47 and loss2: 0.00\n",
      "Epoch [272], train_loss: 691.16 with loss1: 691.16 and loss2: 0.00\n",
      "Epoch [273], train_loss: 771.96 with loss1: 771.96 and loss2: 0.00\n",
      "Epoch [274], train_loss: 835.41 with loss1: 835.41 and loss2: 0.00\n",
      "Epoch [275], train_loss: 830.12 with loss1: 830.12 and loss2: 0.00\n",
      "Epoch [276], train_loss: 1098.91 with loss1: 1098.91 and loss2: 0.00\n",
      "Epoch [277], train_loss: 743.37 with loss1: 743.37 and loss2: 0.00\n",
      "Epoch [278], train_loss: 838.58 with loss1: 838.58 and loss2: 0.00\n",
      "Epoch [279], train_loss: 887.05 with loss1: 887.05 and loss2: 0.00\n",
      "Epoch [280], train_loss: 727.71 with loss1: 727.71 and loss2: 0.00\n",
      "Epoch [281], train_loss: 820.53 with loss1: 820.53 and loss2: 0.00\n",
      "Epoch [282], train_loss: 995.03 with loss1: 995.03 and loss2: 0.00\n",
      "Epoch [283], train_loss: 932.80 with loss1: 932.80 and loss2: 0.00\n",
      "Epoch [284], train_loss: 770.23 with loss1: 770.23 and loss2: 0.00\n",
      "Epoch [285], train_loss: 901.86 with loss1: 901.86 and loss2: 0.00\n",
      "Epoch [286], train_loss: 749.37 with loss1: 749.37 and loss2: 0.00\n",
      "Epoch [287], train_loss: 847.94 with loss1: 847.94 and loss2: 0.00\n",
      "Epoch [288], train_loss: 795.07 with loss1: 795.07 and loss2: 0.00\n",
      "Epoch [289], train_loss: 1013.69 with loss1: 1013.69 and loss2: 0.00\n",
      "Epoch [290], train_loss: 879.04 with loss1: 879.04 and loss2: 0.00\n",
      "Epoch [291], train_loss: 999.19 with loss1: 999.19 and loss2: 0.00\n",
      "Epoch [292], train_loss: 790.66 with loss1: 790.66 and loss2: 0.00\n",
      "Epoch [293], train_loss: 787.96 with loss1: 787.96 and loss2: 0.00\n",
      "Epoch [294], train_loss: 644.19 with loss1: 644.19 and loss2: 0.00\n",
      "Epoch [295], train_loss: 810.01 with loss1: 810.01 and loss2: 0.00\n",
      "Epoch [296], train_loss: 615.10 with loss1: 615.10 and loss2: 0.00\n",
      "Epoch [297], train_loss: 793.82 with loss1: 793.82 and loss2: 0.00\n",
      "Epoch [298], train_loss: 877.11 with loss1: 877.11 and loss2: 0.00\n",
      "Epoch [299], train_loss: 815.31 with loss1: 815.31 and loss2: 0.00\n",
      "Epoch [300], train_loss: 733.76 with loss1: 733.76 and loss2: 0.00\n",
      "Epoch [301], train_loss: 689.84 with loss1: 689.84 and loss2: 0.00\n",
      "Epoch [302], train_loss: 796.20 with loss1: 796.20 and loss2: 0.00\n",
      "Epoch [303], train_loss: 634.82 with loss1: 634.82 and loss2: 0.00\n",
      "Epoch [304], train_loss: 771.12 with loss1: 771.12 and loss2: 0.00\n",
      "Epoch [305], train_loss: 734.29 with loss1: 734.29 and loss2: 0.00\n",
      "Epoch [306], train_loss: 738.54 with loss1: 738.54 and loss2: 0.00\n",
      "Epoch [307], train_loss: 720.43 with loss1: 720.43 and loss2: 0.00\n",
      "Epoch [308], train_loss: 733.55 with loss1: 733.55 and loss2: 0.00\n",
      "Epoch [309], train_loss: 712.71 with loss1: 712.71 and loss2: 0.00\n",
      "Epoch [310], train_loss: 761.02 with loss1: 761.02 and loss2: 0.00\n",
      "Epoch [311], train_loss: 705.85 with loss1: 705.85 and loss2: 0.00\n",
      "Epoch [312], train_loss: 756.40 with loss1: 756.40 and loss2: 0.00\n",
      "Epoch [313], train_loss: 656.53 with loss1: 656.53 and loss2: 0.00\n",
      "Epoch [314], train_loss: 702.75 with loss1: 702.75 and loss2: 0.00\n",
      "Epoch [315], train_loss: 613.12 with loss1: 613.12 and loss2: 0.00\n",
      "Epoch [316], train_loss: 675.24 with loss1: 675.24 and loss2: 0.00\n",
      "Epoch [317], train_loss: 914.92 with loss1: 914.92 and loss2: 0.00\n",
      "Epoch [318], train_loss: 774.92 with loss1: 774.92 and loss2: 0.00\n",
      "Epoch [319], train_loss: 790.07 with loss1: 790.07 and loss2: 0.00\n",
      "Epoch [320], train_loss: 715.88 with loss1: 715.88 and loss2: 0.00\n",
      "Epoch [321], train_loss: 741.60 with loss1: 741.60 and loss2: 0.00\n",
      "Epoch [322], train_loss: 732.97 with loss1: 732.97 and loss2: 0.00\n",
      "Epoch [323], train_loss: 716.22 with loss1: 716.22 and loss2: 0.00\n",
      "Epoch [324], train_loss: 760.82 with loss1: 760.82 and loss2: 0.00\n",
      "Epoch [325], train_loss: 698.72 with loss1: 698.72 and loss2: 0.00\n",
      "Epoch [326], train_loss: 621.87 with loss1: 621.87 and loss2: 0.00\n",
      "Epoch [327], train_loss: 631.65 with loss1: 631.65 and loss2: 0.00\n",
      "Epoch [328], train_loss: 906.01 with loss1: 906.01 and loss2: 0.00\n",
      "Epoch [329], train_loss: 762.80 with loss1: 762.80 and loss2: 0.00\n",
      "Epoch [330], train_loss: 890.94 with loss1: 890.94 and loss2: 0.00\n",
      "Epoch [331], train_loss: 880.65 with loss1: 880.65 and loss2: 0.00\n",
      "Epoch [332], train_loss: 794.61 with loss1: 794.61 and loss2: 0.00\n",
      "Epoch [333], train_loss: 1039.55 with loss1: 1039.55 and loss2: 0.00\n",
      "Epoch [334], train_loss: 781.16 with loss1: 781.16 and loss2: 0.00\n",
      "Epoch [335], train_loss: 806.65 with loss1: 806.65 and loss2: 0.00\n",
      "Epoch [336], train_loss: 745.91 with loss1: 745.91 and loss2: 0.00\n",
      "Epoch [337], train_loss: 1022.55 with loss1: 1022.55 and loss2: 0.00\n",
      "Epoch [338], train_loss: 752.70 with loss1: 752.70 and loss2: 0.00\n",
      "Epoch [339], train_loss: 796.80 with loss1: 796.80 and loss2: 0.00\n",
      "Epoch [340], train_loss: 762.31 with loss1: 762.31 and loss2: 0.00\n",
      "Epoch [341], train_loss: 715.27 with loss1: 715.27 and loss2: 0.00\n",
      "Epoch [342], train_loss: 886.29 with loss1: 886.29 and loss2: 0.00\n",
      "Epoch [343], train_loss: 719.51 with loss1: 719.51 and loss2: 0.00\n",
      "Epoch [344], train_loss: 773.00 with loss1: 773.00 and loss2: 0.00\n",
      "Epoch [345], train_loss: 682.66 with loss1: 682.66 and loss2: 0.00\n",
      "Epoch [346], train_loss: 718.46 with loss1: 718.46 and loss2: 0.00\n",
      "Epoch [347], train_loss: 792.71 with loss1: 792.71 and loss2: 0.00\n",
      "Epoch [348], train_loss: 726.74 with loss1: 726.74 and loss2: 0.00\n",
      "Epoch [349], train_loss: 740.89 with loss1: 740.89 and loss2: 0.00\n",
      "Epoch [350], train_loss: 707.69 with loss1: 707.69 and loss2: 0.00\n",
      "Epoch [351], train_loss: 705.26 with loss1: 705.26 and loss2: 0.00\n",
      "Epoch [352], train_loss: 720.48 with loss1: 720.48 and loss2: 0.00\n",
      "Epoch [353], train_loss: 636.27 with loss1: 636.27 and loss2: 0.00\n",
      "Epoch [354], train_loss: 646.60 with loss1: 646.60 and loss2: 0.00\n",
      "Epoch [355], train_loss: 599.42 with loss1: 599.42 and loss2: 0.00\n",
      "Epoch [356], train_loss: 630.57 with loss1: 630.57 and loss2: 0.00\n",
      "Epoch [357], train_loss: 679.64 with loss1: 679.64 and loss2: 0.00\n",
      "Epoch [358], train_loss: 715.71 with loss1: 715.71 and loss2: 0.00\n",
      "Epoch [359], train_loss: 723.36 with loss1: 723.36 and loss2: 0.00\n",
      "Epoch [360], train_loss: 879.44 with loss1: 879.44 and loss2: 0.00\n",
      "Epoch [361], train_loss: 562.18 with loss1: 562.18 and loss2: 0.00\n",
      "Epoch [362], train_loss: 794.77 with loss1: 794.77 and loss2: 0.00\n",
      "Epoch [363], train_loss: 701.49 with loss1: 701.49 and loss2: 0.00\n",
      "Epoch [364], train_loss: 807.59 with loss1: 807.59 and loss2: 0.00\n",
      "Epoch [365], train_loss: 720.82 with loss1: 720.82 and loss2: 0.00\n",
      "Epoch [366], train_loss: 862.10 with loss1: 862.10 and loss2: 0.00\n",
      "Epoch [367], train_loss: 798.74 with loss1: 798.74 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [368], train_loss: 644.08 with loss1: 644.08 and loss2: 0.00\n",
      "Epoch [369], train_loss: 677.93 with loss1: 677.93 and loss2: 0.00\n",
      "Epoch [370], train_loss: 675.79 with loss1: 675.79 and loss2: 0.00\n",
      "Epoch [371], train_loss: 594.02 with loss1: 594.02 and loss2: 0.00\n",
      "Epoch [372], train_loss: 634.39 with loss1: 634.39 and loss2: 0.00\n",
      "Epoch [373], train_loss: 697.60 with loss1: 697.60 and loss2: 0.00\n",
      "Epoch [374], train_loss: 748.84 with loss1: 748.84 and loss2: 0.00\n",
      "Epoch [375], train_loss: 763.15 with loss1: 763.15 and loss2: 0.00\n",
      "Epoch [376], train_loss: 587.15 with loss1: 587.15 and loss2: 0.00\n",
      "Epoch [377], train_loss: 653.78 with loss1: 653.78 and loss2: 0.00\n",
      "Epoch [378], train_loss: 559.51 with loss1: 559.51 and loss2: 0.00\n",
      "Epoch [379], train_loss: 608.23 with loss1: 608.23 and loss2: 0.00\n",
      "Epoch [380], train_loss: 687.72 with loss1: 687.72 and loss2: 0.00\n",
      "Epoch [381], train_loss: 571.49 with loss1: 571.49 and loss2: 0.00\n",
      "Epoch [382], train_loss: 864.33 with loss1: 864.33 and loss2: 0.00\n",
      "Epoch [383], train_loss: 805.34 with loss1: 805.34 and loss2: 0.00\n",
      "Epoch [384], train_loss: 703.95 with loss1: 703.95 and loss2: 0.00\n",
      "Epoch [385], train_loss: 811.34 with loss1: 811.34 and loss2: 0.00\n",
      "Epoch [386], train_loss: 683.55 with loss1: 683.55 and loss2: 0.00\n",
      "Epoch [387], train_loss: 634.08 with loss1: 634.08 and loss2: 0.00\n",
      "Epoch [388], train_loss: 748.70 with loss1: 748.70 and loss2: 0.00\n",
      "Epoch [389], train_loss: 580.81 with loss1: 580.81 and loss2: 0.00\n",
      "Epoch [390], train_loss: 763.94 with loss1: 763.94 and loss2: 0.00\n",
      "Epoch [391], train_loss: 951.41 with loss1: 951.41 and loss2: 0.00\n",
      "Epoch [392], train_loss: 978.09 with loss1: 978.09 and loss2: 0.00\n",
      "Epoch [393], train_loss: 752.59 with loss1: 752.59 and loss2: 0.00\n",
      "Epoch [394], train_loss: 610.39 with loss1: 610.39 and loss2: 0.00\n",
      "Epoch [395], train_loss: 640.74 with loss1: 640.74 and loss2: 0.00\n",
      "Epoch [396], train_loss: 641.12 with loss1: 641.12 and loss2: 0.00\n",
      "Epoch [397], train_loss: 594.29 with loss1: 594.29 and loss2: 0.00\n",
      "Epoch [398], train_loss: 589.16 with loss1: 589.16 and loss2: 0.00\n",
      "Epoch [399], train_loss: 603.63 with loss1: 603.63 and loss2: 0.00\n",
      "Epoch [400], train_loss: 637.33 with loss1: 637.33 and loss2: 0.00\n",
      "Epoch [401], train_loss: 745.74 with loss1: 745.74 and loss2: 0.00\n",
      "Epoch [402], train_loss: 919.83 with loss1: 919.83 and loss2: 0.00\n",
      "Epoch [403], train_loss: 989.87 with loss1: 989.87 and loss2: 0.00\n",
      "Epoch [404], train_loss: 689.95 with loss1: 689.95 and loss2: 0.00\n",
      "Epoch [405], train_loss: 694.77 with loss1: 694.77 and loss2: 0.00\n",
      "Epoch [406], train_loss: 707.84 with loss1: 707.84 and loss2: 0.00\n",
      "Epoch [407], train_loss: 701.45 with loss1: 701.45 and loss2: 0.00\n",
      "Epoch [408], train_loss: 692.42 with loss1: 692.42 and loss2: 0.00\n",
      "Epoch [409], train_loss: 885.16 with loss1: 885.16 and loss2: 0.00\n",
      "Epoch [410], train_loss: 880.64 with loss1: 880.64 and loss2: 0.00\n",
      "Epoch [411], train_loss: 945.26 with loss1: 945.26 and loss2: 0.00\n",
      "Epoch [412], train_loss: 836.42 with loss1: 836.42 and loss2: 0.00\n",
      "Epoch [413], train_loss: 924.66 with loss1: 924.66 and loss2: 0.00\n",
      "Epoch [414], train_loss: 625.55 with loss1: 625.55 and loss2: 0.00\n",
      "Epoch [415], train_loss: 697.10 with loss1: 697.10 and loss2: 0.00\n",
      "Epoch [416], train_loss: 750.82 with loss1: 750.82 and loss2: 0.00\n",
      "Epoch [417], train_loss: 686.11 with loss1: 686.11 and loss2: 0.00\n",
      "Epoch [418], train_loss: 664.09 with loss1: 664.09 and loss2: 0.00\n",
      "Epoch [419], train_loss: 551.62 with loss1: 551.62 and loss2: 0.00\n",
      "Epoch [420], train_loss: 681.95 with loss1: 681.95 and loss2: 0.00\n",
      "Epoch [421], train_loss: 683.32 with loss1: 683.32 and loss2: 0.00\n",
      "Epoch [422], train_loss: 668.04 with loss1: 668.04 and loss2: 0.00\n",
      "Epoch [423], train_loss: 566.14 with loss1: 566.14 and loss2: 0.00\n",
      "Epoch [424], train_loss: 648.75 with loss1: 648.75 and loss2: 0.00\n",
      "Epoch [425], train_loss: 767.33 with loss1: 767.33 and loss2: 0.00\n",
      "Epoch [426], train_loss: 590.67 with loss1: 590.67 and loss2: 0.00\n",
      "Epoch [427], train_loss: 591.45 with loss1: 591.45 and loss2: 0.00\n",
      "Epoch [428], train_loss: 693.80 with loss1: 693.80 and loss2: 0.00\n",
      "Epoch [429], train_loss: 665.58 with loss1: 665.58 and loss2: 0.00\n",
      "Epoch [430], train_loss: 771.57 with loss1: 771.57 and loss2: 0.00\n",
      "Epoch [431], train_loss: 576.90 with loss1: 576.90 and loss2: 0.00\n",
      "Epoch [432], train_loss: 634.85 with loss1: 634.85 and loss2: 0.00\n",
      "Epoch [433], train_loss: 733.63 with loss1: 733.63 and loss2: 0.00\n",
      "Epoch [434], train_loss: 593.58 with loss1: 593.58 and loss2: 0.00\n",
      "Epoch [435], train_loss: 713.24 with loss1: 713.24 and loss2: 0.00\n",
      "Epoch [436], train_loss: 781.64 with loss1: 781.64 and loss2: 0.00\n",
      "Epoch [437], train_loss: 625.69 with loss1: 625.69 and loss2: 0.00\n",
      "Epoch [438], train_loss: 661.20 with loss1: 661.20 and loss2: 0.00\n",
      "Epoch [439], train_loss: 660.09 with loss1: 660.09 and loss2: 0.00\n",
      "Epoch [440], train_loss: 549.24 with loss1: 549.24 and loss2: 0.00\n",
      "Epoch [441], train_loss: 628.62 with loss1: 628.62 and loss2: 0.00\n",
      "Epoch [442], train_loss: 572.76 with loss1: 572.76 and loss2: 0.00\n",
      "Epoch [443], train_loss: 649.08 with loss1: 649.08 and loss2: 0.00\n",
      "Epoch [444], train_loss: 696.36 with loss1: 696.36 and loss2: 0.00\n",
      "Epoch [445], train_loss: 561.21 with loss1: 561.21 and loss2: 0.00\n",
      "Epoch [446], train_loss: 676.32 with loss1: 676.32 and loss2: 0.00\n",
      "Epoch [447], train_loss: 687.75 with loss1: 687.75 and loss2: 0.00\n",
      "Epoch [448], train_loss: 719.70 with loss1: 719.70 and loss2: 0.00\n",
      "Epoch [449], train_loss: 711.77 with loss1: 711.77 and loss2: 0.00\n",
      "Epoch [450], train_loss: 946.37 with loss1: 946.37 and loss2: 0.00\n",
      "Epoch [451], train_loss: 706.36 with loss1: 706.36 and loss2: 0.00\n",
      "Epoch [452], train_loss: 659.51 with loss1: 659.51 and loss2: 0.00\n",
      "Epoch [453], train_loss: 670.19 with loss1: 670.19 and loss2: 0.00\n",
      "Epoch [454], train_loss: 652.54 with loss1: 652.54 and loss2: 0.00\n",
      "Epoch [455], train_loss: 637.91 with loss1: 637.91 and loss2: 0.00\n",
      "Epoch [456], train_loss: 555.66 with loss1: 555.66 and loss2: 0.00\n",
      "Epoch [457], train_loss: 700.48 with loss1: 700.48 and loss2: 0.00\n",
      "Epoch [458], train_loss: 604.77 with loss1: 604.77 and loss2: 0.00\n",
      "Epoch [459], train_loss: 578.33 with loss1: 578.33 and loss2: 0.00\n",
      "Epoch [460], train_loss: 832.70 with loss1: 832.70 and loss2: 0.00\n",
      "Epoch [461], train_loss: 709.20 with loss1: 709.20 and loss2: 0.00\n",
      "Epoch [462], train_loss: 629.17 with loss1: 629.17 and loss2: 0.00\n",
      "Epoch [463], train_loss: 608.61 with loss1: 608.61 and loss2: 0.00\n",
      "Epoch [464], train_loss: 728.61 with loss1: 728.61 and loss2: 0.00\n",
      "Epoch [465], train_loss: 526.26 with loss1: 526.26 and loss2: 0.00\n",
      "Epoch [466], train_loss: 581.80 with loss1: 581.80 and loss2: 0.00\n",
      "Epoch [467], train_loss: 694.36 with loss1: 694.36 and loss2: 0.00\n",
      "Epoch [468], train_loss: 697.29 with loss1: 697.29 and loss2: 0.00\n",
      "Epoch [469], train_loss: 710.18 with loss1: 710.18 and loss2: 0.00\n",
      "Epoch [470], train_loss: 547.64 with loss1: 547.64 and loss2: 0.00\n",
      "Epoch [471], train_loss: 724.54 with loss1: 724.54 and loss2: 0.00\n",
      "Epoch [472], train_loss: 680.45 with loss1: 680.45 and loss2: 0.00\n",
      "Epoch [473], train_loss: 528.53 with loss1: 528.53 and loss2: 0.00\n",
      "Epoch [474], train_loss: 560.80 with loss1: 560.80 and loss2: 0.00\n",
      "Epoch [475], train_loss: 638.09 with loss1: 638.09 and loss2: 0.00\n",
      "Epoch [476], train_loss: 863.71 with loss1: 863.71 and loss2: 0.00\n",
      "Epoch [477], train_loss: 772.67 with loss1: 772.67 and loss2: 0.00\n",
      "Epoch [478], train_loss: 687.12 with loss1: 687.12 and loss2: 0.00\n",
      "Epoch [479], train_loss: 583.63 with loss1: 583.63 and loss2: 0.00\n",
      "Epoch [480], train_loss: 747.93 with loss1: 747.93 and loss2: 0.00\n",
      "Epoch [481], train_loss: 674.72 with loss1: 674.72 and loss2: 0.00\n",
      "Epoch [482], train_loss: 726.60 with loss1: 726.60 and loss2: 0.00\n",
      "Epoch [483], train_loss: 619.94 with loss1: 619.94 and loss2: 0.00\n",
      "Epoch [484], train_loss: 631.60 with loss1: 631.60 and loss2: 0.00\n",
      "Epoch [485], train_loss: 799.52 with loss1: 799.52 and loss2: 0.00\n",
      "Epoch [486], train_loss: 532.75 with loss1: 532.75 and loss2: 0.00\n",
      "Epoch [487], train_loss: 661.84 with loss1: 661.84 and loss2: 0.00\n",
      "Epoch [488], train_loss: 639.70 with loss1: 639.70 and loss2: 0.00\n",
      "Epoch [489], train_loss: 616.39 with loss1: 616.39 and loss2: 0.00\n",
      "Epoch [490], train_loss: 627.09 with loss1: 627.09 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [491], train_loss: 732.18 with loss1: 732.18 and loss2: 0.00\n",
      "Epoch [492], train_loss: 706.84 with loss1: 706.84 and loss2: 0.00\n",
      "Epoch [493], train_loss: 661.49 with loss1: 661.49 and loss2: 0.00\n",
      "Epoch [494], train_loss: 624.87 with loss1: 624.87 and loss2: 0.00\n",
      "Epoch [495], train_loss: 598.49 with loss1: 598.49 and loss2: 0.00\n",
      "Epoch [496], train_loss: 758.74 with loss1: 758.74 and loss2: 0.00\n",
      "Epoch [497], train_loss: 613.27 with loss1: 613.27 and loss2: 0.00\n",
      "Epoch [498], train_loss: 618.54 with loss1: 618.54 and loss2: 0.00\n",
      "Epoch [499], train_loss: 606.90 with loss1: 606.90 and loss2: 0.00\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# model version 3 \n",
    "# delete loss2\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history = fit(epochs=500, lr=5e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fb32ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 630.49 with loss1: 630.49 and loss2: 0.00\n",
      "Epoch [1], train_loss: 646.98 with loss1: 646.98 and loss2: 0.00\n",
      "Epoch [2], train_loss: 707.62 with loss1: 707.62 and loss2: 0.00\n",
      "Epoch [3], train_loss: 679.71 with loss1: 679.71 and loss2: 0.00\n",
      "Epoch [4], train_loss: 643.82 with loss1: 643.82 and loss2: 0.00\n",
      "Epoch [5], train_loss: 546.10 with loss1: 546.10 and loss2: 0.00\n",
      "Epoch [6], train_loss: 714.74 with loss1: 714.74 and loss2: 0.00\n",
      "Epoch [7], train_loss: 607.78 with loss1: 607.78 and loss2: 0.00\n",
      "Epoch [8], train_loss: 629.06 with loss1: 629.06 and loss2: 0.00\n",
      "Epoch [9], train_loss: 564.82 with loss1: 564.82 and loss2: 0.00\n",
      "Epoch [10], train_loss: 634.43 with loss1: 634.43 and loss2: 0.00\n",
      "Epoch [11], train_loss: 657.92 with loss1: 657.92 and loss2: 0.00\n",
      "Epoch [12], train_loss: 733.79 with loss1: 733.79 and loss2: 0.00\n",
      "Epoch [13], train_loss: 655.79 with loss1: 655.79 and loss2: 0.00\n",
      "Epoch [14], train_loss: 636.94 with loss1: 636.94 and loss2: 0.00\n",
      "Epoch [15], train_loss: 749.70 with loss1: 749.70 and loss2: 0.00\n",
      "Epoch [16], train_loss: 597.79 with loss1: 597.79 and loss2: 0.00\n",
      "Epoch [17], train_loss: 755.23 with loss1: 755.23 and loss2: 0.00\n",
      "Epoch [18], train_loss: 930.05 with loss1: 930.05 and loss2: 0.00\n",
      "Epoch [19], train_loss: 943.92 with loss1: 943.92 and loss2: 0.00\n",
      "Epoch [20], train_loss: 852.90 with loss1: 852.90 and loss2: 0.00\n",
      "Epoch [21], train_loss: 653.78 with loss1: 653.78 and loss2: 0.00\n",
      "Epoch [22], train_loss: 541.17 with loss1: 541.17 and loss2: 0.00\n",
      "Epoch [23], train_loss: 562.16 with loss1: 562.16 and loss2: 0.00\n",
      "Epoch [24], train_loss: 623.49 with loss1: 623.49 and loss2: 0.00\n",
      "Epoch [25], train_loss: 538.22 with loss1: 538.22 and loss2: 0.00\n",
      "Epoch [26], train_loss: 547.59 with loss1: 547.59 and loss2: 0.00\n",
      "Epoch [27], train_loss: 653.35 with loss1: 653.35 and loss2: 0.00\n",
      "Epoch [28], train_loss: 660.72 with loss1: 660.72 and loss2: 0.00\n",
      "Epoch [29], train_loss: 537.60 with loss1: 537.60 and loss2: 0.00\n",
      "Epoch [30], train_loss: 559.26 with loss1: 559.26 and loss2: 0.00\n",
      "Epoch [31], train_loss: 647.95 with loss1: 647.95 and loss2: 0.00\n",
      "Epoch [32], train_loss: 499.96 with loss1: 499.96 and loss2: 0.00\n",
      "Epoch [33], train_loss: 589.70 with loss1: 589.70 and loss2: 0.00\n",
      "Epoch [34], train_loss: 776.47 with loss1: 776.47 and loss2: 0.00\n",
      "Epoch [35], train_loss: 664.39 with loss1: 664.39 and loss2: 0.00\n",
      "Epoch [36], train_loss: 725.98 with loss1: 725.98 and loss2: 0.00\n",
      "Epoch [37], train_loss: 517.27 with loss1: 517.27 and loss2: 0.00\n",
      "Epoch [38], train_loss: 591.90 with loss1: 591.90 and loss2: 0.00\n",
      "Epoch [39], train_loss: 587.31 with loss1: 587.31 and loss2: 0.00\n",
      "Epoch [40], train_loss: 601.60 with loss1: 601.60 and loss2: 0.00\n",
      "Epoch [41], train_loss: 600.81 with loss1: 600.81 and loss2: 0.00\n",
      "Epoch [42], train_loss: 676.11 with loss1: 676.11 and loss2: 0.00\n",
      "Epoch [43], train_loss: 638.74 with loss1: 638.74 and loss2: 0.00\n",
      "Epoch [44], train_loss: 612.85 with loss1: 612.85 and loss2: 0.00\n",
      "Epoch [45], train_loss: 784.67 with loss1: 784.67 and loss2: 0.00\n",
      "Epoch [46], train_loss: 598.75 with loss1: 598.75 and loss2: 0.00\n",
      "Epoch [47], train_loss: 551.60 with loss1: 551.60 and loss2: 0.00\n",
      "Epoch [48], train_loss: 600.61 with loss1: 600.61 and loss2: 0.00\n",
      "Epoch [49], train_loss: 628.14 with loss1: 628.14 and loss2: 0.00\n",
      "Epoch [50], train_loss: 519.38 with loss1: 519.38 and loss2: 0.00\n",
      "Epoch [51], train_loss: 561.26 with loss1: 561.26 and loss2: 0.00\n",
      "Epoch [52], train_loss: 592.19 with loss1: 592.19 and loss2: 0.00\n",
      "Epoch [53], train_loss: 626.90 with loss1: 626.90 and loss2: 0.00\n",
      "Epoch [54], train_loss: 642.19 with loss1: 642.19 and loss2: 0.00\n",
      "Epoch [55], train_loss: 556.15 with loss1: 556.15 and loss2: 0.00\n",
      "Epoch [56], train_loss: 596.72 with loss1: 596.72 and loss2: 0.00\n",
      "Epoch [57], train_loss: 511.05 with loss1: 511.05 and loss2: 0.00\n",
      "Epoch [58], train_loss: 766.26 with loss1: 766.26 and loss2: 0.00\n",
      "Epoch [59], train_loss: 840.77 with loss1: 840.77 and loss2: 0.00\n",
      "Epoch [60], train_loss: 558.63 with loss1: 558.63 and loss2: 0.00\n",
      "Epoch [61], train_loss: 575.91 with loss1: 575.91 and loss2: 0.00\n",
      "Epoch [62], train_loss: 645.07 with loss1: 645.07 and loss2: 0.00\n",
      "Epoch [63], train_loss: 546.59 with loss1: 546.59 and loss2: 0.00\n",
      "Epoch [64], train_loss: 607.36 with loss1: 607.36 and loss2: 0.00\n",
      "Epoch [65], train_loss: 600.01 with loss1: 600.01 and loss2: 0.00\n",
      "Epoch [66], train_loss: 493.24 with loss1: 493.24 and loss2: 0.00\n",
      "Epoch [67], train_loss: 599.70 with loss1: 599.70 and loss2: 0.00\n",
      "Epoch [68], train_loss: 623.08 with loss1: 623.08 and loss2: 0.00\n",
      "Epoch [69], train_loss: 512.20 with loss1: 512.20 and loss2: 0.00\n",
      "Epoch [70], train_loss: 582.69 with loss1: 582.69 and loss2: 0.00\n",
      "Epoch [71], train_loss: 553.78 with loss1: 553.78 and loss2: 0.00\n",
      "Epoch [72], train_loss: 648.38 with loss1: 648.38 and loss2: 0.00\n",
      "Epoch [73], train_loss: 518.02 with loss1: 518.02 and loss2: 0.00\n",
      "Epoch [74], train_loss: 601.07 with loss1: 601.07 and loss2: 0.00\n",
      "Epoch [75], train_loss: 485.30 with loss1: 485.30 and loss2: 0.00\n",
      "Epoch [76], train_loss: 679.81 with loss1: 679.81 and loss2: 0.00\n",
      "Epoch [77], train_loss: 891.89 with loss1: 891.89 and loss2: 0.00\n",
      "Epoch [78], train_loss: 691.42 with loss1: 691.42 and loss2: 0.00\n",
      "Epoch [79], train_loss: 661.93 with loss1: 661.93 and loss2: 0.00\n",
      "Epoch [80], train_loss: 674.58 with loss1: 674.58 and loss2: 0.00\n",
      "Epoch [81], train_loss: 732.08 with loss1: 732.08 and loss2: 0.00\n",
      "Epoch [82], train_loss: 587.58 with loss1: 587.58 and loss2: 0.00\n",
      "Epoch [83], train_loss: 526.99 with loss1: 526.99 and loss2: 0.00\n",
      "Epoch [84], train_loss: 653.18 with loss1: 653.18 and loss2: 0.00\n",
      "Epoch [85], train_loss: 769.23 with loss1: 769.23 and loss2: 0.00\n",
      "Epoch [86], train_loss: 796.81 with loss1: 796.81 and loss2: 0.00\n",
      "Epoch [87], train_loss: 620.91 with loss1: 620.91 and loss2: 0.00\n",
      "Epoch [88], train_loss: 579.49 with loss1: 579.49 and loss2: 0.00\n",
      "Epoch [89], train_loss: 590.56 with loss1: 590.56 and loss2: 0.00\n",
      "Epoch [90], train_loss: 518.22 with loss1: 518.22 and loss2: 0.00\n",
      "Epoch [91], train_loss: 590.30 with loss1: 590.30 and loss2: 0.00\n",
      "Epoch [92], train_loss: 586.95 with loss1: 586.95 and loss2: 0.00\n",
      "Epoch [93], train_loss: 630.62 with loss1: 630.62 and loss2: 0.00\n",
      "Epoch [94], train_loss: 676.67 with loss1: 676.67 and loss2: 0.00\n",
      "Epoch [95], train_loss: 725.07 with loss1: 725.07 and loss2: 0.00\n",
      "Epoch [96], train_loss: 569.16 with loss1: 569.16 and loss2: 0.00\n",
      "Epoch [97], train_loss: 537.22 with loss1: 537.22 and loss2: 0.00\n",
      "Epoch [98], train_loss: 495.68 with loss1: 495.68 and loss2: 0.00\n",
      "Epoch [99], train_loss: 543.86 with loss1: 543.86 and loss2: 0.00\n",
      "Epoch [100], train_loss: 540.71 with loss1: 540.71 and loss2: 0.00\n",
      "Epoch [101], train_loss: 598.76 with loss1: 598.76 and loss2: 0.00\n",
      "Epoch [102], train_loss: 498.20 with loss1: 498.20 and loss2: 0.00\n",
      "Epoch [103], train_loss: 577.82 with loss1: 577.82 and loss2: 0.00\n",
      "Epoch [104], train_loss: 580.01 with loss1: 580.01 and loss2: 0.00\n",
      "Epoch [105], train_loss: 573.03 with loss1: 573.03 and loss2: 0.00\n",
      "Epoch [106], train_loss: 511.93 with loss1: 511.93 and loss2: 0.00\n",
      "Epoch [107], train_loss: 578.44 with loss1: 578.44 and loss2: 0.00\n",
      "Epoch [108], train_loss: 627.81 with loss1: 627.81 and loss2: 0.00\n",
      "Epoch [109], train_loss: 625.06 with loss1: 625.06 and loss2: 0.00\n",
      "Epoch [110], train_loss: 523.73 with loss1: 523.73 and loss2: 0.00\n",
      "Epoch [111], train_loss: 634.05 with loss1: 634.05 and loss2: 0.00\n",
      "Epoch [112], train_loss: 523.89 with loss1: 523.89 and loss2: 0.00\n",
      "Epoch [113], train_loss: 598.28 with loss1: 598.28 and loss2: 0.00\n",
      "Epoch [114], train_loss: 645.89 with loss1: 645.89 and loss2: 0.00\n",
      "Epoch [115], train_loss: 664.36 with loss1: 664.36 and loss2: 0.00\n",
      "Epoch [116], train_loss: 913.16 with loss1: 913.16 and loss2: 0.00\n",
      "Epoch [117], train_loss: 614.46 with loss1: 614.46 and loss2: 0.00\n",
      "Epoch [118], train_loss: 759.70 with loss1: 759.70 and loss2: 0.00\n",
      "Epoch [119], train_loss: 560.31 with loss1: 560.31 and loss2: 0.00\n",
      "Epoch [120], train_loss: 577.63 with loss1: 577.63 and loss2: 0.00\n",
      "Epoch [121], train_loss: 576.51 with loss1: 576.51 and loss2: 0.00\n",
      "Epoch [122], train_loss: 626.99 with loss1: 626.99 and loss2: 0.00\n",
      "Epoch [123], train_loss: 716.16 with loss1: 716.16 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [124], train_loss: 568.06 with loss1: 568.06 and loss2: 0.00\n",
      "Epoch [125], train_loss: 687.28 with loss1: 687.28 and loss2: 0.00\n",
      "Epoch [126], train_loss: 510.76 with loss1: 510.76 and loss2: 0.00\n",
      "Epoch [127], train_loss: 471.06 with loss1: 471.06 and loss2: 0.00\n",
      "Epoch [128], train_loss: 577.83 with loss1: 577.83 and loss2: 0.00\n",
      "Epoch [129], train_loss: 535.94 with loss1: 535.94 and loss2: 0.00\n",
      "Epoch [130], train_loss: 572.91 with loss1: 572.91 and loss2: 0.00\n",
      "Epoch [131], train_loss: 686.60 with loss1: 686.60 and loss2: 0.00\n",
      "Epoch [132], train_loss: 513.33 with loss1: 513.33 and loss2: 0.00\n",
      "Epoch [133], train_loss: 507.03 with loss1: 507.03 and loss2: 0.00\n",
      "Epoch [134], train_loss: 567.74 with loss1: 567.74 and loss2: 0.00\n",
      "Epoch [135], train_loss: 491.46 with loss1: 491.46 and loss2: 0.00\n",
      "Epoch [136], train_loss: 584.89 with loss1: 584.89 and loss2: 0.00\n",
      "Epoch [137], train_loss: 663.09 with loss1: 663.09 and loss2: 0.00\n",
      "Epoch [138], train_loss: 687.21 with loss1: 687.21 and loss2: 0.00\n",
      "Epoch [139], train_loss: 571.62 with loss1: 571.62 and loss2: 0.00\n",
      "Epoch [140], train_loss: 647.18 with loss1: 647.18 and loss2: 0.00\n",
      "Epoch [141], train_loss: 591.06 with loss1: 591.06 and loss2: 0.00\n",
      "Epoch [142], train_loss: 552.23 with loss1: 552.23 and loss2: 0.00\n",
      "Epoch [143], train_loss: 642.99 with loss1: 642.99 and loss2: 0.00\n",
      "Epoch [144], train_loss: 524.26 with loss1: 524.26 and loss2: 0.00\n",
      "Epoch [145], train_loss: 591.92 with loss1: 591.92 and loss2: 0.00\n",
      "Epoch [146], train_loss: 571.10 with loss1: 571.10 and loss2: 0.00\n",
      "Epoch [147], train_loss: 591.91 with loss1: 591.91 and loss2: 0.00\n",
      "Epoch [148], train_loss: 578.34 with loss1: 578.34 and loss2: 0.00\n",
      "Epoch [149], train_loss: 563.77 with loss1: 563.77 and loss2: 0.00\n",
      "Epoch [150], train_loss: 496.22 with loss1: 496.22 and loss2: 0.00\n",
      "Epoch [151], train_loss: 620.36 with loss1: 620.36 and loss2: 0.00\n",
      "Epoch [152], train_loss: 574.04 with loss1: 574.04 and loss2: 0.00\n",
      "Epoch [153], train_loss: 496.39 with loss1: 496.39 and loss2: 0.00\n",
      "Epoch [154], train_loss: 531.92 with loss1: 531.92 and loss2: 0.00\n",
      "Epoch [155], train_loss: 636.97 with loss1: 636.97 and loss2: 0.00\n",
      "Epoch [156], train_loss: 601.65 with loss1: 601.65 and loss2: 0.00\n",
      "Epoch [157], train_loss: 571.72 with loss1: 571.72 and loss2: 0.00\n",
      "Epoch [158], train_loss: 581.50 with loss1: 581.50 and loss2: 0.00\n",
      "Epoch [159], train_loss: 627.56 with loss1: 627.56 and loss2: 0.00\n",
      "Epoch [160], train_loss: 626.29 with loss1: 626.29 and loss2: 0.00\n",
      "Epoch [161], train_loss: 533.21 with loss1: 533.21 and loss2: 0.00\n",
      "Epoch [162], train_loss: 718.32 with loss1: 718.32 and loss2: 0.00\n",
      "Epoch [163], train_loss: 789.23 with loss1: 789.23 and loss2: 0.00\n",
      "Epoch [164], train_loss: 712.70 with loss1: 712.70 and loss2: 0.00\n",
      "Epoch [165], train_loss: 729.54 with loss1: 729.54 and loss2: 0.00\n",
      "Epoch [166], train_loss: 594.90 with loss1: 594.90 and loss2: 0.00\n",
      "Epoch [167], train_loss: 544.30 with loss1: 544.30 and loss2: 0.00\n",
      "Epoch [168], train_loss: 556.40 with loss1: 556.40 and loss2: 0.00\n",
      "Epoch [169], train_loss: 668.95 with loss1: 668.95 and loss2: 0.00\n",
      "Epoch [170], train_loss: 652.11 with loss1: 652.11 and loss2: 0.00\n",
      "Epoch [171], train_loss: 532.51 with loss1: 532.51 and loss2: 0.00\n",
      "Epoch [172], train_loss: 573.57 with loss1: 573.57 and loss2: 0.00\n",
      "Epoch [173], train_loss: 631.04 with loss1: 631.04 and loss2: 0.00\n",
      "Epoch [174], train_loss: 632.01 with loss1: 632.01 and loss2: 0.00\n",
      "Epoch [175], train_loss: 504.10 with loss1: 504.10 and loss2: 0.00\n",
      "Epoch [176], train_loss: 521.27 with loss1: 521.27 and loss2: 0.00\n",
      "Epoch [177], train_loss: 539.24 with loss1: 539.24 and loss2: 0.00\n",
      "Epoch [178], train_loss: 586.53 with loss1: 586.53 and loss2: 0.00\n",
      "Epoch [179], train_loss: 598.84 with loss1: 598.84 and loss2: 0.00\n",
      "Epoch [180], train_loss: 486.87 with loss1: 486.87 and loss2: 0.00\n",
      "Epoch [181], train_loss: 602.02 with loss1: 602.02 and loss2: 0.00\n",
      "Epoch [182], train_loss: 586.51 with loss1: 586.51 and loss2: 0.00\n",
      "Epoch [183], train_loss: 560.17 with loss1: 560.17 and loss2: 0.00\n",
      "Epoch [184], train_loss: 569.25 with loss1: 569.25 and loss2: 0.00\n",
      "Epoch [185], train_loss: 592.09 with loss1: 592.09 and loss2: 0.00\n",
      "Epoch [186], train_loss: 617.28 with loss1: 617.28 and loss2: 0.00\n",
      "Epoch [187], train_loss: 686.54 with loss1: 686.54 and loss2: 0.00\n",
      "Epoch [188], train_loss: 840.46 with loss1: 840.46 and loss2: 0.00\n",
      "Epoch [189], train_loss: 792.26 with loss1: 792.26 and loss2: 0.00\n",
      "Epoch [190], train_loss: 760.55 with loss1: 760.55 and loss2: 0.00\n",
      "Epoch [191], train_loss: 622.37 with loss1: 622.37 and loss2: 0.00\n",
      "Epoch [192], train_loss: 599.73 with loss1: 599.73 and loss2: 0.00\n",
      "Epoch [193], train_loss: 607.52 with loss1: 607.52 and loss2: 0.00\n",
      "Epoch [194], train_loss: 560.48 with loss1: 560.48 and loss2: 0.00\n",
      "Epoch [195], train_loss: 581.85 with loss1: 581.85 and loss2: 0.00\n",
      "Epoch [196], train_loss: 573.29 with loss1: 573.29 and loss2: 0.00\n",
      "Epoch [197], train_loss: 528.09 with loss1: 528.09 and loss2: 0.00\n",
      "Epoch [198], train_loss: 543.47 with loss1: 543.47 and loss2: 0.00\n",
      "Epoch [199], train_loss: 477.81 with loss1: 477.81 and loss2: 0.00\n",
      "Epoch [200], train_loss: 474.50 with loss1: 474.50 and loss2: 0.00\n",
      "Epoch [201], train_loss: 532.79 with loss1: 532.79 and loss2: 0.00\n",
      "Epoch [202], train_loss: 525.36 with loss1: 525.36 and loss2: 0.00\n",
      "Epoch [203], train_loss: 652.24 with loss1: 652.24 and loss2: 0.00\n",
      "Epoch [204], train_loss: 597.63 with loss1: 597.63 and loss2: 0.00\n",
      "Epoch [205], train_loss: 549.42 with loss1: 549.42 and loss2: 0.00\n",
      "Epoch [206], train_loss: 588.67 with loss1: 588.67 and loss2: 0.00\n",
      "Epoch [207], train_loss: 610.15 with loss1: 610.15 and loss2: 0.00\n",
      "Epoch [208], train_loss: 577.30 with loss1: 577.30 and loss2: 0.00\n",
      "Epoch [209], train_loss: 571.90 with loss1: 571.90 and loss2: 0.00\n",
      "Epoch [210], train_loss: 484.76 with loss1: 484.76 and loss2: 0.00\n",
      "Epoch [211], train_loss: 548.93 with loss1: 548.93 and loss2: 0.00\n",
      "Epoch [212], train_loss: 555.48 with loss1: 555.48 and loss2: 0.00\n",
      "Epoch [213], train_loss: 443.05 with loss1: 443.05 and loss2: 0.00\n",
      "Epoch [214], train_loss: 601.99 with loss1: 601.99 and loss2: 0.00\n",
      "Epoch [215], train_loss: 639.98 with loss1: 639.98 and loss2: 0.00\n",
      "Epoch [216], train_loss: 739.09 with loss1: 739.09 and loss2: 0.00\n",
      "Epoch [217], train_loss: 776.71 with loss1: 776.71 and loss2: 0.00\n",
      "Epoch [218], train_loss: 649.62 with loss1: 649.62 and loss2: 0.00\n",
      "Epoch [219], train_loss: 643.79 with loss1: 643.79 and loss2: 0.00\n",
      "Epoch [220], train_loss: 565.48 with loss1: 565.48 and loss2: 0.00\n",
      "Epoch [221], train_loss: 567.94 with loss1: 567.94 and loss2: 0.00\n",
      "Epoch [222], train_loss: 494.35 with loss1: 494.35 and loss2: 0.00\n",
      "Epoch [223], train_loss: 585.33 with loss1: 585.33 and loss2: 0.00\n",
      "Epoch [224], train_loss: 520.38 with loss1: 520.38 and loss2: 0.00\n",
      "Epoch [225], train_loss: 509.93 with loss1: 509.93 and loss2: 0.00\n",
      "Epoch [226], train_loss: 547.93 with loss1: 547.93 and loss2: 0.00\n",
      "Epoch [227], train_loss: 512.22 with loss1: 512.22 and loss2: 0.00\n",
      "Epoch [228], train_loss: 541.21 with loss1: 541.21 and loss2: 0.00\n",
      "Epoch [229], train_loss: 587.88 with loss1: 587.88 and loss2: 0.00\n",
      "Epoch [230], train_loss: 455.09 with loss1: 455.09 and loss2: 0.00\n",
      "Epoch [231], train_loss: 450.57 with loss1: 450.57 and loss2: 0.00\n",
      "Epoch [232], train_loss: 537.10 with loss1: 537.10 and loss2: 0.00\n",
      "Epoch [233], train_loss: 505.17 with loss1: 505.17 and loss2: 0.00\n",
      "Epoch [234], train_loss: 584.03 with loss1: 584.03 and loss2: 0.00\n",
      "Epoch [235], train_loss: 651.06 with loss1: 651.06 and loss2: 0.00\n",
      "Epoch [236], train_loss: 594.57 with loss1: 594.57 and loss2: 0.00\n",
      "Epoch [237], train_loss: 576.48 with loss1: 576.48 and loss2: 0.00\n",
      "Epoch [238], train_loss: 603.82 with loss1: 603.82 and loss2: 0.00\n",
      "Epoch [239], train_loss: 608.02 with loss1: 608.02 and loss2: 0.00\n",
      "Epoch [240], train_loss: 711.74 with loss1: 711.74 and loss2: 0.00\n",
      "Epoch [241], train_loss: 548.38 with loss1: 548.38 and loss2: 0.00\n",
      "Epoch [242], train_loss: 569.17 with loss1: 569.17 and loss2: 0.00\n",
      "Epoch [243], train_loss: 580.55 with loss1: 580.55 and loss2: 0.00\n",
      "Epoch [244], train_loss: 658.62 with loss1: 658.62 and loss2: 0.00\n",
      "Epoch [245], train_loss: 694.85 with loss1: 694.85 and loss2: 0.00\n",
      "Epoch [246], train_loss: 525.79 with loss1: 525.79 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [247], train_loss: 522.71 with loss1: 522.71 and loss2: 0.00\n",
      "Epoch [248], train_loss: 598.74 with loss1: 598.74 and loss2: 0.00\n",
      "Epoch [249], train_loss: 554.74 with loss1: 554.74 and loss2: 0.00\n",
      "Epoch [250], train_loss: 609.55 with loss1: 609.55 and loss2: 0.00\n",
      "Epoch [251], train_loss: 514.72 with loss1: 514.72 and loss2: 0.00\n",
      "Epoch [252], train_loss: 571.22 with loss1: 571.22 and loss2: 0.00\n",
      "Epoch [253], train_loss: 603.82 with loss1: 603.82 and loss2: 0.00\n",
      "Epoch [254], train_loss: 802.50 with loss1: 802.50 and loss2: 0.00\n",
      "Epoch [255], train_loss: 736.32 with loss1: 736.32 and loss2: 0.00\n",
      "Epoch [256], train_loss: 604.41 with loss1: 604.41 and loss2: 0.00\n",
      "Epoch [257], train_loss: 511.71 with loss1: 511.71 and loss2: 0.00\n",
      "Epoch [258], train_loss: 485.96 with loss1: 485.96 and loss2: 0.00\n",
      "Epoch [259], train_loss: 570.17 with loss1: 570.17 and loss2: 0.00\n",
      "Epoch [260], train_loss: 492.02 with loss1: 492.02 and loss2: 0.00\n",
      "Epoch [261], train_loss: 566.53 with loss1: 566.53 and loss2: 0.00\n",
      "Epoch [262], train_loss: 559.51 with loss1: 559.51 and loss2: 0.00\n",
      "Epoch [263], train_loss: 696.95 with loss1: 696.95 and loss2: 0.00\n",
      "Epoch [264], train_loss: 568.98 with loss1: 568.98 and loss2: 0.00\n",
      "Epoch [265], train_loss: 561.85 with loss1: 561.85 and loss2: 0.00\n",
      "Epoch [266], train_loss: 481.62 with loss1: 481.62 and loss2: 0.00\n",
      "Epoch [267], train_loss: 611.08 with loss1: 611.08 and loss2: 0.00\n",
      "Epoch [268], train_loss: 511.77 with loss1: 511.77 and loss2: 0.00\n",
      "Epoch [269], train_loss: 555.31 with loss1: 555.31 and loss2: 0.00\n",
      "Epoch [270], train_loss: 505.83 with loss1: 505.83 and loss2: 0.00\n",
      "Epoch [271], train_loss: 517.87 with loss1: 517.87 and loss2: 0.00\n",
      "Epoch [272], train_loss: 581.60 with loss1: 581.60 and loss2: 0.00\n",
      "Epoch [273], train_loss: 613.53 with loss1: 613.53 and loss2: 0.00\n",
      "Epoch [274], train_loss: 496.83 with loss1: 496.83 and loss2: 0.00\n",
      "Epoch [275], train_loss: 559.32 with loss1: 559.32 and loss2: 0.00\n",
      "Epoch [276], train_loss: 473.81 with loss1: 473.81 and loss2: 0.00\n",
      "Epoch [277], train_loss: 541.06 with loss1: 541.06 and loss2: 0.00\n",
      "Epoch [278], train_loss: 554.56 with loss1: 554.56 and loss2: 0.00\n",
      "Epoch [279], train_loss: 468.04 with loss1: 468.04 and loss2: 0.00\n",
      "Epoch [280], train_loss: 423.93 with loss1: 423.93 and loss2: 0.00\n",
      "Epoch [281], train_loss: 458.08 with loss1: 458.08 and loss2: 0.00\n",
      "Epoch [282], train_loss: 597.54 with loss1: 597.54 and loss2: 0.00\n",
      "Epoch [283], train_loss: 702.40 with loss1: 702.40 and loss2: 0.00\n",
      "Epoch [284], train_loss: 850.68 with loss1: 850.68 and loss2: 0.00\n",
      "Epoch [285], train_loss: 567.91 with loss1: 567.91 and loss2: 0.00\n",
      "Epoch [286], train_loss: 556.93 with loss1: 556.93 and loss2: 0.00\n",
      "Epoch [287], train_loss: 470.94 with loss1: 470.94 and loss2: 0.00\n",
      "Epoch [288], train_loss: 538.33 with loss1: 538.33 and loss2: 0.00\n",
      "Epoch [289], train_loss: 574.76 with loss1: 574.76 and loss2: 0.00\n",
      "Epoch [290], train_loss: 522.06 with loss1: 522.06 and loss2: 0.00\n",
      "Epoch [291], train_loss: 528.59 with loss1: 528.59 and loss2: 0.00\n",
      "Epoch [292], train_loss: 541.97 with loss1: 541.97 and loss2: 0.00\n",
      "Epoch [293], train_loss: 587.50 with loss1: 587.50 and loss2: 0.00\n",
      "Epoch [294], train_loss: 589.83 with loss1: 589.83 and loss2: 0.00\n",
      "Epoch [295], train_loss: 541.49 with loss1: 541.49 and loss2: 0.00\n",
      "Epoch [296], train_loss: 468.69 with loss1: 468.69 and loss2: 0.00\n",
      "Epoch [297], train_loss: 547.39 with loss1: 547.39 and loss2: 0.00\n",
      "Epoch [298], train_loss: 619.14 with loss1: 619.14 and loss2: 0.00\n",
      "Epoch [299], train_loss: 602.04 with loss1: 602.04 and loss2: 0.00\n",
      "Epoch [300], train_loss: 489.29 with loss1: 489.29 and loss2: 0.00\n",
      "Epoch [301], train_loss: 553.28 with loss1: 553.28 and loss2: 0.00\n",
      "Epoch [302], train_loss: 523.96 with loss1: 523.96 and loss2: 0.00\n",
      "Epoch [303], train_loss: 431.53 with loss1: 431.53 and loss2: 0.00\n",
      "Epoch [304], train_loss: 511.45 with loss1: 511.45 and loss2: 0.00\n",
      "Epoch [305], train_loss: 625.89 with loss1: 625.89 and loss2: 0.00\n",
      "Epoch [306], train_loss: 644.95 with loss1: 644.95 and loss2: 0.00\n",
      "Epoch [307], train_loss: 586.22 with loss1: 586.22 and loss2: 0.00\n",
      "Epoch [308], train_loss: 553.61 with loss1: 553.61 and loss2: 0.00\n",
      "Epoch [309], train_loss: 469.38 with loss1: 469.38 and loss2: 0.00\n",
      "Epoch [310], train_loss: 438.62 with loss1: 438.62 and loss2: 0.00\n",
      "Epoch [311], train_loss: 526.91 with loss1: 526.91 and loss2: 0.00\n",
      "Epoch [312], train_loss: 538.27 with loss1: 538.27 and loss2: 0.00\n",
      "Epoch [313], train_loss: 426.89 with loss1: 426.89 and loss2: 0.00\n",
      "Epoch [314], train_loss: 466.17 with loss1: 466.17 and loss2: 0.00\n",
      "Epoch [315], train_loss: 446.94 with loss1: 446.94 and loss2: 0.00\n",
      "Epoch [316], train_loss: 428.80 with loss1: 428.80 and loss2: 0.00\n",
      "Epoch [317], train_loss: 514.60 with loss1: 514.60 and loss2: 0.00\n",
      "Epoch [318], train_loss: 602.94 with loss1: 602.94 and loss2: 0.00\n",
      "Epoch [319], train_loss: 473.08 with loss1: 473.08 and loss2: 0.00\n",
      "Epoch [320], train_loss: 619.68 with loss1: 619.68 and loss2: 0.00\n",
      "Epoch [321], train_loss: 625.22 with loss1: 625.22 and loss2: 0.00\n",
      "Epoch [322], train_loss: 489.64 with loss1: 489.64 and loss2: 0.00\n",
      "Epoch [323], train_loss: 550.94 with loss1: 550.94 and loss2: 0.00\n",
      "Epoch [324], train_loss: 465.84 with loss1: 465.84 and loss2: 0.00\n",
      "Epoch [325], train_loss: 531.28 with loss1: 531.28 and loss2: 0.00\n",
      "Epoch [326], train_loss: 548.90 with loss1: 548.90 and loss2: 0.00\n",
      "Epoch [327], train_loss: 473.36 with loss1: 473.36 and loss2: 0.00\n",
      "Epoch [328], train_loss: 520.27 with loss1: 520.27 and loss2: 0.00\n",
      "Epoch [329], train_loss: 607.55 with loss1: 607.55 and loss2: 0.00\n",
      "Epoch [330], train_loss: 754.68 with loss1: 754.68 and loss2: 0.00\n",
      "Epoch [331], train_loss: 685.92 with loss1: 685.92 and loss2: 0.00\n",
      "Epoch [332], train_loss: 498.62 with loss1: 498.62 and loss2: 0.00\n",
      "Epoch [333], train_loss: 528.82 with loss1: 528.82 and loss2: 0.00\n",
      "Epoch [334], train_loss: 540.18 with loss1: 540.18 and loss2: 0.00\n",
      "Epoch [335], train_loss: 445.91 with loss1: 445.91 and loss2: 0.00\n",
      "Epoch [336], train_loss: 540.89 with loss1: 540.89 and loss2: 0.00\n",
      "Epoch [337], train_loss: 577.84 with loss1: 577.84 and loss2: 0.00\n",
      "Epoch [338], train_loss: 634.73 with loss1: 634.73 and loss2: 0.00\n",
      "Epoch [339], train_loss: 741.08 with loss1: 741.08 and loss2: 0.00\n",
      "Epoch [340], train_loss: 539.22 with loss1: 539.22 and loss2: 0.00\n",
      "Epoch [341], train_loss: 466.13 with loss1: 466.13 and loss2: 0.00\n",
      "Epoch [342], train_loss: 419.79 with loss1: 419.79 and loss2: 0.00\n",
      "Epoch [343], train_loss: 514.80 with loss1: 514.80 and loss2: 0.00\n",
      "Epoch [344], train_loss: 553.13 with loss1: 553.13 and loss2: 0.00\n",
      "Epoch [345], train_loss: 421.78 with loss1: 421.78 and loss2: 0.00\n",
      "Epoch [346], train_loss: 543.84 with loss1: 543.84 and loss2: 0.00\n",
      "Epoch [347], train_loss: 442.04 with loss1: 442.04 and loss2: 0.00\n",
      "Epoch [348], train_loss: 420.50 with loss1: 420.50 and loss2: 0.00\n",
      "Epoch [349], train_loss: 510.44 with loss1: 510.44 and loss2: 0.00\n",
      "Epoch [350], train_loss: 561.98 with loss1: 561.98 and loss2: 0.00\n",
      "Epoch [351], train_loss: 514.60 with loss1: 514.60 and loss2: 0.00\n",
      "Epoch [352], train_loss: 589.23 with loss1: 589.23 and loss2: 0.00\n",
      "Epoch [353], train_loss: 582.93 with loss1: 582.93 and loss2: 0.00\n",
      "Epoch [354], train_loss: 551.20 with loss1: 551.20 and loss2: 0.00\n",
      "Epoch [355], train_loss: 479.22 with loss1: 479.22 and loss2: 0.00\n",
      "Epoch [356], train_loss: 435.03 with loss1: 435.03 and loss2: 0.00\n",
      "Epoch [357], train_loss: 468.73 with loss1: 468.73 and loss2: 0.00\n",
      "Epoch [358], train_loss: 498.63 with loss1: 498.63 and loss2: 0.00\n",
      "Epoch [359], train_loss: 473.41 with loss1: 473.41 and loss2: 0.00\n",
      "Epoch [360], train_loss: 490.88 with loss1: 490.88 and loss2: 0.00\n",
      "Epoch [361], train_loss: 416.29 with loss1: 416.29 and loss2: 0.00\n",
      "Epoch [362], train_loss: 512.14 with loss1: 512.14 and loss2: 0.00\n",
      "Epoch [363], train_loss: 563.22 with loss1: 563.22 and loss2: 0.00\n",
      "Epoch [364], train_loss: 636.98 with loss1: 636.98 and loss2: 0.00\n",
      "Epoch [365], train_loss: 697.81 with loss1: 697.81 and loss2: 0.00\n",
      "Epoch [366], train_loss: 485.41 with loss1: 485.41 and loss2: 0.00\n",
      "Epoch [367], train_loss: 584.57 with loss1: 584.57 and loss2: 0.00\n",
      "Epoch [368], train_loss: 466.64 with loss1: 466.64 and loss2: 0.00\n",
      "Epoch [369], train_loss: 586.11 with loss1: 586.11 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [370], train_loss: 590.51 with loss1: 590.51 and loss2: 0.00\n",
      "Epoch [371], train_loss: 495.17 with loss1: 495.17 and loss2: 0.00\n",
      "Epoch [372], train_loss: 488.10 with loss1: 488.10 and loss2: 0.00\n",
      "Epoch [373], train_loss: 491.45 with loss1: 491.45 and loss2: 0.00\n",
      "Epoch [374], train_loss: 552.90 with loss1: 552.90 and loss2: 0.00\n",
      "Epoch [375], train_loss: 460.35 with loss1: 460.35 and loss2: 0.00\n",
      "Epoch [376], train_loss: 498.70 with loss1: 498.70 and loss2: 0.00\n",
      "Epoch [377], train_loss: 538.60 with loss1: 538.60 and loss2: 0.00\n",
      "Epoch [378], train_loss: 513.87 with loss1: 513.87 and loss2: 0.00\n",
      "Epoch [379], train_loss: 584.57 with loss1: 584.57 and loss2: 0.00\n",
      "Epoch [380], train_loss: 468.29 with loss1: 468.29 and loss2: 0.00\n",
      "Epoch [381], train_loss: 434.79 with loss1: 434.79 and loss2: 0.00\n",
      "Epoch [382], train_loss: 542.52 with loss1: 542.52 and loss2: 0.00\n",
      "Epoch [383], train_loss: 436.21 with loss1: 436.21 and loss2: 0.00\n",
      "Epoch [384], train_loss: 547.23 with loss1: 547.23 and loss2: 0.00\n",
      "Epoch [385], train_loss: 569.96 with loss1: 569.96 and loss2: 0.00\n",
      "Epoch [386], train_loss: 620.19 with loss1: 620.19 and loss2: 0.00\n",
      "Epoch [387], train_loss: 573.76 with loss1: 573.76 and loss2: 0.00\n",
      "Epoch [388], train_loss: 537.48 with loss1: 537.48 and loss2: 0.00\n",
      "Epoch [389], train_loss: 579.22 with loss1: 579.22 and loss2: 0.00\n",
      "Epoch [390], train_loss: 500.72 with loss1: 500.72 and loss2: 0.00\n",
      "Epoch [391], train_loss: 592.64 with loss1: 592.64 and loss2: 0.00\n",
      "Epoch [392], train_loss: 507.66 with loss1: 507.66 and loss2: 0.00\n",
      "Epoch [393], train_loss: 570.24 with loss1: 570.24 and loss2: 0.00\n",
      "Epoch [394], train_loss: 453.71 with loss1: 453.71 and loss2: 0.00\n",
      "Epoch [395], train_loss: 515.39 with loss1: 515.39 and loss2: 0.00\n",
      "Epoch [396], train_loss: 414.39 with loss1: 414.39 and loss2: 0.00\n",
      "Epoch [397], train_loss: 418.20 with loss1: 418.20 and loss2: 0.00\n",
      "Epoch [398], train_loss: 533.03 with loss1: 533.03 and loss2: 0.00\n",
      "Epoch [399], train_loss: 686.53 with loss1: 686.53 and loss2: 0.00\n",
      "Epoch [400], train_loss: 910.88 with loss1: 910.88 and loss2: 0.00\n",
      "Epoch [401], train_loss: 852.19 with loss1: 852.19 and loss2: 0.00\n",
      "Epoch [402], train_loss: 581.48 with loss1: 581.48 and loss2: 0.00\n",
      "Epoch [403], train_loss: 555.75 with loss1: 555.75 and loss2: 0.00\n",
      "Epoch [404], train_loss: 536.25 with loss1: 536.25 and loss2: 0.00\n",
      "Epoch [405], train_loss: 571.29 with loss1: 571.29 and loss2: 0.00\n",
      "Epoch [406], train_loss: 446.93 with loss1: 446.93 and loss2: 0.00\n",
      "Epoch [407], train_loss: 525.53 with loss1: 525.53 and loss2: 0.00\n",
      "Epoch [408], train_loss: 589.85 with loss1: 589.85 and loss2: 0.00\n",
      "Epoch [409], train_loss: 597.13 with loss1: 597.13 and loss2: 0.00\n",
      "Epoch [410], train_loss: 456.91 with loss1: 456.91 and loss2: 0.00\n",
      "Epoch [411], train_loss: 529.14 with loss1: 529.14 and loss2: 0.00\n",
      "Epoch [412], train_loss: 564.55 with loss1: 564.55 and loss2: 0.00\n",
      "Epoch [413], train_loss: 566.83 with loss1: 566.83 and loss2: 0.00\n",
      "Epoch [414], train_loss: 658.59 with loss1: 658.59 and loss2: 0.00\n",
      "Epoch [415], train_loss: 493.20 with loss1: 493.20 and loss2: 0.00\n",
      "Epoch [416], train_loss: 543.34 with loss1: 543.34 and loss2: 0.00\n",
      "Epoch [417], train_loss: 544.73 with loss1: 544.73 and loss2: 0.00\n",
      "Epoch [418], train_loss: 515.01 with loss1: 515.01 and loss2: 0.00\n",
      "Epoch [419], train_loss: 506.38 with loss1: 506.38 and loss2: 0.00\n",
      "Epoch [420], train_loss: 513.01 with loss1: 513.01 and loss2: 0.00\n",
      "Epoch [421], train_loss: 438.98 with loss1: 438.98 and loss2: 0.00\n",
      "Epoch [422], train_loss: 494.83 with loss1: 494.83 and loss2: 0.00\n",
      "Epoch [423], train_loss: 440.69 with loss1: 440.69 and loss2: 0.00\n",
      "Epoch [424], train_loss: 489.97 with loss1: 489.97 and loss2: 0.00\n",
      "Epoch [425], train_loss: 577.46 with loss1: 577.46 and loss2: 0.00\n",
      "Epoch [426], train_loss: 491.06 with loss1: 491.06 and loss2: 0.00\n",
      "Epoch [427], train_loss: 504.11 with loss1: 504.11 and loss2: 0.00\n",
      "Epoch [428], train_loss: 536.86 with loss1: 536.86 and loss2: 0.00\n",
      "Epoch [429], train_loss: 587.57 with loss1: 587.57 and loss2: 0.00\n",
      "Epoch [430], train_loss: 454.31 with loss1: 454.31 and loss2: 0.00\n",
      "Epoch [431], train_loss: 415.68 with loss1: 415.68 and loss2: 0.00\n",
      "Epoch [432], train_loss: 488.80 with loss1: 488.80 and loss2: 0.00\n",
      "Epoch [433], train_loss: 448.76 with loss1: 448.76 and loss2: 0.00\n",
      "Epoch [434], train_loss: 488.88 with loss1: 488.88 and loss2: 0.00\n",
      "Epoch [435], train_loss: 483.12 with loss1: 483.12 and loss2: 0.00\n",
      "Epoch [436], train_loss: 453.84 with loss1: 453.84 and loss2: 0.00\n",
      "Epoch [437], train_loss: 529.54 with loss1: 529.54 and loss2: 0.00\n",
      "Epoch [438], train_loss: 480.80 with loss1: 480.80 and loss2: 0.00\n",
      "Epoch [439], train_loss: 422.88 with loss1: 422.88 and loss2: 0.00\n",
      "Epoch [440], train_loss: 512.56 with loss1: 512.56 and loss2: 0.00\n",
      "Epoch [441], train_loss: 611.00 with loss1: 611.00 and loss2: 0.00\n",
      "Epoch [442], train_loss: 449.76 with loss1: 449.76 and loss2: 0.00\n",
      "Epoch [443], train_loss: 489.50 with loss1: 489.50 and loss2: 0.00\n",
      "Epoch [444], train_loss: 485.92 with loss1: 485.92 and loss2: 0.00\n",
      "Epoch [445], train_loss: 425.89 with loss1: 425.89 and loss2: 0.00\n",
      "Epoch [446], train_loss: 484.00 with loss1: 484.00 and loss2: 0.00\n",
      "Epoch [447], train_loss: 405.98 with loss1: 405.98 and loss2: 0.00\n",
      "Epoch [448], train_loss: 525.51 with loss1: 525.51 and loss2: 0.00\n",
      "Epoch [449], train_loss: 431.82 with loss1: 431.82 and loss2: 0.00\n",
      "Epoch [450], train_loss: 537.89 with loss1: 537.89 and loss2: 0.00\n",
      "Epoch [451], train_loss: 581.75 with loss1: 581.75 and loss2: 0.00\n",
      "Epoch [452], train_loss: 727.88 with loss1: 727.88 and loss2: 0.00\n",
      "Epoch [453], train_loss: 528.89 with loss1: 528.89 and loss2: 0.00\n",
      "Epoch [454], train_loss: 611.33 with loss1: 611.33 and loss2: 0.00\n",
      "Epoch [455], train_loss: 479.58 with loss1: 479.58 and loss2: 0.00\n",
      "Epoch [456], train_loss: 561.16 with loss1: 561.16 and loss2: 0.00\n",
      "Epoch [457], train_loss: 592.05 with loss1: 592.05 and loss2: 0.00\n",
      "Epoch [458], train_loss: 460.78 with loss1: 460.78 and loss2: 0.00\n",
      "Epoch [459], train_loss: 533.68 with loss1: 533.68 and loss2: 0.00\n",
      "Epoch [460], train_loss: 408.46 with loss1: 408.46 and loss2: 0.00\n",
      "Epoch [461], train_loss: 417.38 with loss1: 417.38 and loss2: 0.00\n",
      "Epoch [462], train_loss: 491.70 with loss1: 491.70 and loss2: 0.00\n",
      "Epoch [463], train_loss: 519.92 with loss1: 519.92 and loss2: 0.00\n",
      "Epoch [464], train_loss: 570.15 with loss1: 570.15 and loss2: 0.00\n",
      "Epoch [465], train_loss: 466.38 with loss1: 466.38 and loss2: 0.00\n",
      "Epoch [466], train_loss: 422.87 with loss1: 422.87 and loss2: 0.00\n",
      "Epoch [467], train_loss: 450.63 with loss1: 450.63 and loss2: 0.00\n",
      "Epoch [468], train_loss: 478.68 with loss1: 478.68 and loss2: 0.00\n",
      "Epoch [469], train_loss: 402.89 with loss1: 402.89 and loss2: 0.00\n",
      "Epoch [470], train_loss: 479.01 with loss1: 479.01 and loss2: 0.00\n",
      "Epoch [471], train_loss: 489.31 with loss1: 489.31 and loss2: 0.00\n",
      "Epoch [472], train_loss: 449.96 with loss1: 449.96 and loss2: 0.00\n",
      "Epoch [473], train_loss: 521.02 with loss1: 521.02 and loss2: 0.00\n",
      "Epoch [474], train_loss: 528.78 with loss1: 528.78 and loss2: 0.00\n",
      "Epoch [475], train_loss: 515.09 with loss1: 515.09 and loss2: 0.00\n",
      "Epoch [476], train_loss: 509.39 with loss1: 509.39 and loss2: 0.00\n",
      "Epoch [477], train_loss: 411.65 with loss1: 411.65 and loss2: 0.00\n",
      "Epoch [478], train_loss: 407.58 with loss1: 407.58 and loss2: 0.00\n",
      "Epoch [479], train_loss: 468.14 with loss1: 468.14 and loss2: 0.00\n",
      "Epoch [480], train_loss: 620.47 with loss1: 620.47 and loss2: 0.00\n",
      "Epoch [481], train_loss: 597.31 with loss1: 597.31 and loss2: 0.00\n",
      "Epoch [482], train_loss: 564.04 with loss1: 564.04 and loss2: 0.00\n",
      "Epoch [483], train_loss: 517.73 with loss1: 517.73 and loss2: 0.00\n",
      "Epoch [484], train_loss: 512.69 with loss1: 512.69 and loss2: 0.00\n",
      "Epoch [485], train_loss: 696.89 with loss1: 696.89 and loss2: 0.00\n",
      "Epoch [486], train_loss: 748.23 with loss1: 748.23 and loss2: 0.00\n",
      "Epoch [487], train_loss: 613.88 with loss1: 613.88 and loss2: 0.00\n",
      "Epoch [488], train_loss: 601.11 with loss1: 601.11 and loss2: 0.00\n",
      "Epoch [489], train_loss: 588.64 with loss1: 588.64 and loss2: 0.00\n",
      "Epoch [490], train_loss: 650.82 with loss1: 650.82 and loss2: 0.00\n",
      "Epoch [491], train_loss: 576.26 with loss1: 576.26 and loss2: 0.00\n",
      "Epoch [492], train_loss: 561.38 with loss1: 561.38 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [493], train_loss: 594.98 with loss1: 594.98 and loss2: 0.00\n",
      "Epoch [494], train_loss: 682.01 with loss1: 682.01 and loss2: 0.00\n",
      "Epoch [495], train_loss: 541.48 with loss1: 541.48 and loss2: 0.00\n",
      "Epoch [496], train_loss: 452.16 with loss1: 452.16 and loss2: 0.00\n",
      "Epoch [497], train_loss: 476.62 with loss1: 476.62 and loss2: 0.00\n",
      "Epoch [498], train_loss: 408.29 with loss1: 408.29 and loss2: 0.00\n",
      "Epoch [499], train_loss: 432.81 with loss1: 432.81 and loss2: 0.00\n",
      "Epoch [500], train_loss: 504.87 with loss1: 504.87 and loss2: 0.00\n",
      "Epoch [501], train_loss: 420.26 with loss1: 420.26 and loss2: 0.00\n",
      "Epoch [502], train_loss: 500.70 with loss1: 500.70 and loss2: 0.00\n",
      "Epoch [503], train_loss: 478.61 with loss1: 478.61 and loss2: 0.00\n",
      "Epoch [504], train_loss: 528.20 with loss1: 528.20 and loss2: 0.00\n",
      "Epoch [505], train_loss: 448.40 with loss1: 448.40 and loss2: 0.00\n",
      "Epoch [506], train_loss: 505.24 with loss1: 505.24 and loss2: 0.00\n",
      "Epoch [507], train_loss: 534.86 with loss1: 534.86 and loss2: 0.00\n",
      "Epoch [508], train_loss: 438.89 with loss1: 438.89 and loss2: 0.00\n",
      "Epoch [509], train_loss: 462.15 with loss1: 462.15 and loss2: 0.00\n",
      "Epoch [510], train_loss: 471.78 with loss1: 471.78 and loss2: 0.00\n",
      "Epoch [511], train_loss: 516.54 with loss1: 516.54 and loss2: 0.00\n",
      "Epoch [512], train_loss: 552.35 with loss1: 552.35 and loss2: 0.00\n",
      "Epoch [513], train_loss: 520.47 with loss1: 520.47 and loss2: 0.00\n",
      "Epoch [514], train_loss: 487.95 with loss1: 487.95 and loss2: 0.00\n",
      "Epoch [515], train_loss: 512.28 with loss1: 512.28 and loss2: 0.00\n",
      "Epoch [516], train_loss: 568.70 with loss1: 568.70 and loss2: 0.00\n",
      "Epoch [517], train_loss: 475.23 with loss1: 475.23 and loss2: 0.00\n",
      "Epoch [518], train_loss: 614.29 with loss1: 614.29 and loss2: 0.00\n",
      "Epoch [519], train_loss: 590.74 with loss1: 590.74 and loss2: 0.00\n",
      "Epoch [520], train_loss: 511.24 with loss1: 511.24 and loss2: 0.00\n",
      "Epoch [521], train_loss: 470.63 with loss1: 470.63 and loss2: 0.00\n",
      "Epoch [522], train_loss: 483.26 with loss1: 483.26 and loss2: 0.00\n",
      "Epoch [523], train_loss: 450.42 with loss1: 450.42 and loss2: 0.00\n",
      "Epoch [524], train_loss: 412.18 with loss1: 412.18 and loss2: 0.00\n",
      "Epoch [525], train_loss: 485.13 with loss1: 485.13 and loss2: 0.00\n",
      "Epoch [526], train_loss: 422.09 with loss1: 422.09 and loss2: 0.00\n",
      "Epoch [527], train_loss: 481.58 with loss1: 481.58 and loss2: 0.00\n",
      "Epoch [528], train_loss: 509.70 with loss1: 509.70 and loss2: 0.00\n",
      "Epoch [529], train_loss: 555.22 with loss1: 555.22 and loss2: 0.00\n",
      "Epoch [530], train_loss: 463.79 with loss1: 463.79 and loss2: 0.00\n",
      "Epoch [531], train_loss: 461.99 with loss1: 461.99 and loss2: 0.00\n",
      "Epoch [532], train_loss: 516.63 with loss1: 516.63 and loss2: 0.00\n",
      "Epoch [533], train_loss: 450.60 with loss1: 450.60 and loss2: 0.00\n",
      "Epoch [534], train_loss: 526.25 with loss1: 526.25 and loss2: 0.00\n",
      "Epoch [535], train_loss: 489.14 with loss1: 489.14 and loss2: 0.00\n",
      "Epoch [536], train_loss: 409.02 with loss1: 409.02 and loss2: 0.00\n",
      "Epoch [537], train_loss: 494.90 with loss1: 494.90 and loss2: 0.00\n",
      "Epoch [538], train_loss: 519.91 with loss1: 519.91 and loss2: 0.00\n",
      "Epoch [539], train_loss: 564.67 with loss1: 564.67 and loss2: 0.00\n",
      "Epoch [540], train_loss: 491.16 with loss1: 491.16 and loss2: 0.00\n",
      "Epoch [541], train_loss: 548.71 with loss1: 548.71 and loss2: 0.00\n",
      "Epoch [542], train_loss: 522.37 with loss1: 522.37 and loss2: 0.00\n",
      "Epoch [543], train_loss: 526.49 with loss1: 526.49 and loss2: 0.00\n",
      "Epoch [544], train_loss: 589.49 with loss1: 589.49 and loss2: 0.00\n",
      "Epoch [545], train_loss: 670.05 with loss1: 670.05 and loss2: 0.00\n",
      "Epoch [546], train_loss: 462.35 with loss1: 462.35 and loss2: 0.00\n",
      "Epoch [547], train_loss: 535.61 with loss1: 535.61 and loss2: 0.00\n",
      "Epoch [548], train_loss: 583.11 with loss1: 583.11 and loss2: 0.00\n",
      "Epoch [549], train_loss: 509.51 with loss1: 509.51 and loss2: 0.00\n",
      "Epoch [550], train_loss: 535.55 with loss1: 535.55 and loss2: 0.00\n",
      "Epoch [551], train_loss: 489.58 with loss1: 489.58 and loss2: 0.00\n",
      "Epoch [552], train_loss: 530.96 with loss1: 530.96 and loss2: 0.00\n",
      "Epoch [553], train_loss: 461.31 with loss1: 461.31 and loss2: 0.00\n",
      "Epoch [554], train_loss: 454.30 with loss1: 454.30 and loss2: 0.00\n",
      "Epoch [555], train_loss: 473.44 with loss1: 473.44 and loss2: 0.00\n",
      "Epoch [556], train_loss: 492.30 with loss1: 492.30 and loss2: 0.00\n",
      "Epoch [557], train_loss: 590.10 with loss1: 590.10 and loss2: 0.00\n",
      "Epoch [558], train_loss: 519.38 with loss1: 519.38 and loss2: 0.00\n",
      "Epoch [559], train_loss: 439.05 with loss1: 439.05 and loss2: 0.00\n",
      "Epoch [560], train_loss: 467.19 with loss1: 467.19 and loss2: 0.00\n",
      "Epoch [561], train_loss: 472.43 with loss1: 472.43 and loss2: 0.00\n",
      "Epoch [562], train_loss: 458.50 with loss1: 458.50 and loss2: 0.00\n",
      "Epoch [563], train_loss: 400.29 with loss1: 400.29 and loss2: 0.00\n",
      "Epoch [564], train_loss: 476.76 with loss1: 476.76 and loss2: 0.00\n",
      "Epoch [565], train_loss: 475.23 with loss1: 475.23 and loss2: 0.00\n",
      "Epoch [566], train_loss: 592.85 with loss1: 592.85 and loss2: 0.00\n",
      "Epoch [567], train_loss: 630.91 with loss1: 630.91 and loss2: 0.00\n",
      "Epoch [568], train_loss: 498.22 with loss1: 498.22 and loss2: 0.00\n",
      "Epoch [569], train_loss: 579.99 with loss1: 579.99 and loss2: 0.00\n",
      "Epoch [570], train_loss: 523.65 with loss1: 523.65 and loss2: 0.00\n",
      "Epoch [571], train_loss: 546.89 with loss1: 546.89 and loss2: 0.00\n",
      "Epoch [572], train_loss: 449.84 with loss1: 449.84 and loss2: 0.00\n",
      "Epoch [573], train_loss: 500.57 with loss1: 500.57 and loss2: 0.00\n",
      "Epoch [574], train_loss: 448.11 with loss1: 448.11 and loss2: 0.00\n",
      "Epoch [575], train_loss: 455.73 with loss1: 455.73 and loss2: 0.00\n",
      "Epoch [576], train_loss: 497.85 with loss1: 497.85 and loss2: 0.00\n",
      "Epoch [577], train_loss: 411.23 with loss1: 411.23 and loss2: 0.00\n",
      "Epoch [578], train_loss: 490.63 with loss1: 490.63 and loss2: 0.00\n",
      "Epoch [579], train_loss: 435.36 with loss1: 435.36 and loss2: 0.00\n",
      "Epoch [580], train_loss: 485.14 with loss1: 485.14 and loss2: 0.00\n",
      "Epoch [581], train_loss: 573.61 with loss1: 573.61 and loss2: 0.00\n",
      "Epoch [582], train_loss: 652.67 with loss1: 652.67 and loss2: 0.00\n",
      "Epoch [583], train_loss: 554.81 with loss1: 554.81 and loss2: 0.00\n",
      "Epoch [584], train_loss: 546.33 with loss1: 546.33 and loss2: 0.00\n",
      "Epoch [585], train_loss: 582.87 with loss1: 582.87 and loss2: 0.00\n",
      "Epoch [586], train_loss: 502.66 with loss1: 502.66 and loss2: 0.00\n",
      "Epoch [587], train_loss: 446.10 with loss1: 446.10 and loss2: 0.00\n",
      "Epoch [588], train_loss: 463.60 with loss1: 463.60 and loss2: 0.00\n",
      "Epoch [589], train_loss: 459.08 with loss1: 459.08 and loss2: 0.00\n",
      "Epoch [590], train_loss: 399.76 with loss1: 399.76 and loss2: 0.00\n",
      "Epoch [591], train_loss: 474.89 with loss1: 474.89 and loss2: 0.00\n",
      "Epoch [592], train_loss: 478.78 with loss1: 478.78 and loss2: 0.00\n",
      "Epoch [593], train_loss: 494.45 with loss1: 494.45 and loss2: 0.00\n",
      "Epoch [594], train_loss: 531.71 with loss1: 531.71 and loss2: 0.00\n",
      "Epoch [595], train_loss: 582.12 with loss1: 582.12 and loss2: 0.00\n",
      "Epoch [596], train_loss: 418.47 with loss1: 418.47 and loss2: 0.00\n",
      "Epoch [597], train_loss: 447.79 with loss1: 447.79 and loss2: 0.00\n",
      "Epoch [598], train_loss: 479.48 with loss1: 479.48 and loss2: 0.00\n",
      "Epoch [599], train_loss: 571.99 with loss1: 571.99 and loss2: 0.00\n",
      "Epoch [600], train_loss: 627.21 with loss1: 627.21 and loss2: 0.00\n",
      "Epoch [601], train_loss: 652.87 with loss1: 652.87 and loss2: 0.00\n",
      "Epoch [602], train_loss: 438.11 with loss1: 438.11 and loss2: 0.00\n",
      "Epoch [603], train_loss: 471.22 with loss1: 471.22 and loss2: 0.00\n",
      "Epoch [604], train_loss: 435.97 with loss1: 435.97 and loss2: 0.00\n",
      "Epoch [605], train_loss: 474.59 with loss1: 474.59 and loss2: 0.00\n",
      "Epoch [606], train_loss: 430.00 with loss1: 430.00 and loss2: 0.00\n",
      "Epoch [607], train_loss: 392.99 with loss1: 392.99 and loss2: 0.00\n",
      "Epoch [608], train_loss: 381.13 with loss1: 381.13 and loss2: 0.00\n",
      "Epoch [609], train_loss: 478.93 with loss1: 478.93 and loss2: 0.00\n",
      "Epoch [610], train_loss: 425.08 with loss1: 425.08 and loss2: 0.00\n",
      "Epoch [611], train_loss: 473.21 with loss1: 473.21 and loss2: 0.00\n",
      "Epoch [612], train_loss: 536.15 with loss1: 536.15 and loss2: 0.00\n",
      "Epoch [613], train_loss: 500.87 with loss1: 500.87 and loss2: 0.00\n",
      "Epoch [614], train_loss: 475.44 with loss1: 475.44 and loss2: 0.00\n",
      "Epoch [615], train_loss: 449.22 with loss1: 449.22 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [616], train_loss: 496.79 with loss1: 496.79 and loss2: 0.00\n",
      "Epoch [617], train_loss: 452.21 with loss1: 452.21 and loss2: 0.00\n",
      "Epoch [618], train_loss: 392.96 with loss1: 392.96 and loss2: 0.00\n",
      "Epoch [619], train_loss: 444.68 with loss1: 444.68 and loss2: 0.00\n",
      "Epoch [620], train_loss: 400.81 with loss1: 400.81 and loss2: 0.00\n",
      "Epoch [621], train_loss: 492.93 with loss1: 492.93 and loss2: 0.00\n",
      "Epoch [622], train_loss: 576.31 with loss1: 576.31 and loss2: 0.00\n",
      "Epoch [623], train_loss: 647.62 with loss1: 647.62 and loss2: 0.00\n",
      "Epoch [624], train_loss: 506.73 with loss1: 506.73 and loss2: 0.00\n",
      "Epoch [625], train_loss: 474.80 with loss1: 474.80 and loss2: 0.00\n",
      "Epoch [626], train_loss: 544.30 with loss1: 544.30 and loss2: 0.00\n",
      "Epoch [627], train_loss: 524.06 with loss1: 524.06 and loss2: 0.00\n",
      "Epoch [628], train_loss: 447.63 with loss1: 447.63 and loss2: 0.00\n",
      "Epoch [629], train_loss: 472.87 with loss1: 472.87 and loss2: 0.00\n",
      "Epoch [630], train_loss: 487.04 with loss1: 487.04 and loss2: 0.00\n",
      "Epoch [631], train_loss: 425.99 with loss1: 425.99 and loss2: 0.00\n",
      "Epoch [632], train_loss: 422.09 with loss1: 422.09 and loss2: 0.00\n",
      "Epoch [633], train_loss: 474.50 with loss1: 474.50 and loss2: 0.00\n",
      "Epoch [634], train_loss: 479.05 with loss1: 479.05 and loss2: 0.00\n",
      "Epoch [635], train_loss: 466.00 with loss1: 466.00 and loss2: 0.00\n",
      "Epoch [636], train_loss: 463.79 with loss1: 463.79 and loss2: 0.00\n",
      "Epoch [637], train_loss: 480.32 with loss1: 480.32 and loss2: 0.00\n",
      "Epoch [638], train_loss: 487.51 with loss1: 487.51 and loss2: 0.00\n",
      "Epoch [639], train_loss: 520.51 with loss1: 520.51 and loss2: 0.00\n",
      "Epoch [640], train_loss: 428.52 with loss1: 428.52 and loss2: 0.00\n",
      "Epoch [641], train_loss: 574.12 with loss1: 574.12 and loss2: 0.00\n",
      "Epoch [642], train_loss: 596.09 with loss1: 596.09 and loss2: 0.00\n",
      "Epoch [643], train_loss: 417.28 with loss1: 417.28 and loss2: 0.00\n",
      "Epoch [644], train_loss: 418.49 with loss1: 418.49 and loss2: 0.00\n",
      "Epoch [645], train_loss: 438.18 with loss1: 438.18 and loss2: 0.00\n",
      "Epoch [646], train_loss: 541.98 with loss1: 541.98 and loss2: 0.00\n",
      "Epoch [647], train_loss: 571.16 with loss1: 571.16 and loss2: 0.00\n",
      "Epoch [648], train_loss: 543.40 with loss1: 543.40 and loss2: 0.00\n",
      "Epoch [649], train_loss: 504.00 with loss1: 504.00 and loss2: 0.00\n",
      "Epoch [650], train_loss: 510.47 with loss1: 510.47 and loss2: 0.00\n",
      "Epoch [651], train_loss: 414.36 with loss1: 414.36 and loss2: 0.00\n",
      "Epoch [652], train_loss: 391.17 with loss1: 391.17 and loss2: 0.00\n",
      "Epoch [653], train_loss: 456.67 with loss1: 456.67 and loss2: 0.00\n",
      "Epoch [654], train_loss: 464.85 with loss1: 464.85 and loss2: 0.00\n",
      "Epoch [655], train_loss: 451.72 with loss1: 451.72 and loss2: 0.00\n",
      "Epoch [656], train_loss: 538.04 with loss1: 538.04 and loss2: 0.00\n",
      "Epoch [657], train_loss: 492.20 with loss1: 492.20 and loss2: 0.00\n",
      "Epoch [658], train_loss: 460.51 with loss1: 460.51 and loss2: 0.00\n",
      "Epoch [659], train_loss: 469.54 with loss1: 469.54 and loss2: 0.00\n",
      "Epoch [660], train_loss: 380.01 with loss1: 380.01 and loss2: 0.00\n",
      "Epoch [661], train_loss: 475.31 with loss1: 475.31 and loss2: 0.00\n",
      "Epoch [662], train_loss: 461.77 with loss1: 461.77 and loss2: 0.00\n",
      "Epoch [663], train_loss: 410.59 with loss1: 410.59 and loss2: 0.00\n",
      "Epoch [664], train_loss: 438.25 with loss1: 438.25 and loss2: 0.00\n",
      "Epoch [665], train_loss: 459.84 with loss1: 459.84 and loss2: 0.00\n",
      "Epoch [666], train_loss: 471.87 with loss1: 471.87 and loss2: 0.00\n",
      "Epoch [667], train_loss: 458.95 with loss1: 458.95 and loss2: 0.00\n",
      "Epoch [668], train_loss: 464.13 with loss1: 464.13 and loss2: 0.00\n",
      "Epoch [669], train_loss: 380.35 with loss1: 380.35 and loss2: 0.00\n",
      "Epoch [670], train_loss: 474.24 with loss1: 474.24 and loss2: 0.00\n",
      "Epoch [671], train_loss: 393.45 with loss1: 393.45 and loss2: 0.00\n",
      "Epoch [672], train_loss: 496.41 with loss1: 496.41 and loss2: 0.00\n",
      "Epoch [673], train_loss: 456.75 with loss1: 456.75 and loss2: 0.00\n",
      "Epoch [674], train_loss: 566.69 with loss1: 566.69 and loss2: 0.00\n",
      "Epoch [675], train_loss: 609.95 with loss1: 609.95 and loss2: 0.00\n",
      "Epoch [676], train_loss: 592.49 with loss1: 592.49 and loss2: 0.00\n",
      "Epoch [677], train_loss: 458.96 with loss1: 458.96 and loss2: 0.00\n",
      "Epoch [678], train_loss: 512.94 with loss1: 512.94 and loss2: 0.00\n",
      "Epoch [679], train_loss: 630.07 with loss1: 630.07 and loss2: 0.00\n",
      "Epoch [680], train_loss: 438.15 with loss1: 438.15 and loss2: 0.00\n",
      "Epoch [681], train_loss: 491.72 with loss1: 491.72 and loss2: 0.00\n",
      "Epoch [682], train_loss: 401.44 with loss1: 401.44 and loss2: 0.00\n",
      "Epoch [683], train_loss: 466.88 with loss1: 466.88 and loss2: 0.00\n",
      "Epoch [684], train_loss: 495.93 with loss1: 495.93 and loss2: 0.00\n",
      "Epoch [685], train_loss: 429.83 with loss1: 429.83 and loss2: 0.00\n",
      "Epoch [686], train_loss: 457.83 with loss1: 457.83 and loss2: 0.00\n",
      "Epoch [687], train_loss: 512.16 with loss1: 512.16 and loss2: 0.00\n",
      "Epoch [688], train_loss: 508.90 with loss1: 508.90 and loss2: 0.00\n",
      "Epoch [689], train_loss: 455.70 with loss1: 455.70 and loss2: 0.00\n",
      "Epoch [690], train_loss: 488.90 with loss1: 488.90 and loss2: 0.00\n",
      "Epoch [691], train_loss: 384.23 with loss1: 384.23 and loss2: 0.00\n",
      "Epoch [692], train_loss: 478.18 with loss1: 478.18 and loss2: 0.00\n",
      "Epoch [693], train_loss: 501.53 with loss1: 501.53 and loss2: 0.00\n",
      "Epoch [694], train_loss: 513.26 with loss1: 513.26 and loss2: 0.00\n",
      "Epoch [695], train_loss: 506.02 with loss1: 506.02 and loss2: 0.00\n",
      "Epoch [696], train_loss: 394.41 with loss1: 394.41 and loss2: 0.00\n",
      "Epoch [697], train_loss: 399.44 with loss1: 399.44 and loss2: 0.00\n",
      "Epoch [698], train_loss: 459.57 with loss1: 459.57 and loss2: 0.00\n",
      "Epoch [699], train_loss: 381.70 with loss1: 381.70 and loss2: 0.00\n",
      "Epoch [700], train_loss: 503.03 with loss1: 503.03 and loss2: 0.00\n",
      "Epoch [701], train_loss: 590.37 with loss1: 590.37 and loss2: 0.00\n",
      "Epoch [702], train_loss: 553.06 with loss1: 553.06 and loss2: 0.00\n",
      "Epoch [703], train_loss: 472.79 with loss1: 472.79 and loss2: 0.00\n",
      "Epoch [704], train_loss: 477.35 with loss1: 477.35 and loss2: 0.00\n",
      "Epoch [705], train_loss: 368.87 with loss1: 368.87 and loss2: 0.00\n",
      "Epoch [706], train_loss: 461.59 with loss1: 461.59 and loss2: 0.00\n",
      "Epoch [707], train_loss: 393.45 with loss1: 393.45 and loss2: 0.00\n",
      "Epoch [708], train_loss: 431.61 with loss1: 431.61 and loss2: 0.00\n",
      "Epoch [709], train_loss: 433.34 with loss1: 433.34 and loss2: 0.00\n",
      "Epoch [710], train_loss: 372.77 with loss1: 372.77 and loss2: 0.00\n",
      "Epoch [711], train_loss: 455.60 with loss1: 455.60 and loss2: 0.00\n",
      "Epoch [712], train_loss: 465.72 with loss1: 465.72 and loss2: 0.00\n",
      "Epoch [713], train_loss: 509.36 with loss1: 509.36 and loss2: 0.00\n",
      "Epoch [714], train_loss: 585.54 with loss1: 585.54 and loss2: 0.00\n",
      "Epoch [715], train_loss: 487.37 with loss1: 487.37 and loss2: 0.00\n",
      "Epoch [716], train_loss: 540.98 with loss1: 540.98 and loss2: 0.00\n",
      "Epoch [717], train_loss: 588.41 with loss1: 588.41 and loss2: 0.00\n",
      "Epoch [718], train_loss: 565.59 with loss1: 565.59 and loss2: 0.00\n",
      "Epoch [719], train_loss: 432.90 with loss1: 432.90 and loss2: 0.00\n",
      "Epoch [720], train_loss: 500.20 with loss1: 500.20 and loss2: 0.00\n",
      "Epoch [721], train_loss: 489.16 with loss1: 489.16 and loss2: 0.00\n",
      "Epoch [722], train_loss: 459.14 with loss1: 459.14 and loss2: 0.00\n",
      "Epoch [723], train_loss: 456.96 with loss1: 456.96 and loss2: 0.00\n",
      "Epoch [724], train_loss: 437.56 with loss1: 437.56 and loss2: 0.00\n",
      "Epoch [725], train_loss: 498.92 with loss1: 498.92 and loss2: 0.00\n",
      "Epoch [726], train_loss: 510.65 with loss1: 510.65 and loss2: 0.00\n",
      "Epoch [727], train_loss: 452.69 with loss1: 452.69 and loss2: 0.00\n",
      "Epoch [728], train_loss: 407.39 with loss1: 407.39 and loss2: 0.00\n",
      "Epoch [729], train_loss: 447.41 with loss1: 447.41 and loss2: 0.00\n",
      "Epoch [730], train_loss: 412.92 with loss1: 412.92 and loss2: 0.00\n",
      "Epoch [731], train_loss: 468.42 with loss1: 468.42 and loss2: 0.00\n",
      "Epoch [732], train_loss: 408.72 with loss1: 408.72 and loss2: 0.00\n",
      "Epoch [733], train_loss: 449.26 with loss1: 449.26 and loss2: 0.00\n",
      "Epoch [734], train_loss: 427.20 with loss1: 427.20 and loss2: 0.00\n",
      "Epoch [735], train_loss: 494.81 with loss1: 494.81 and loss2: 0.00\n",
      "Epoch [736], train_loss: 456.33 with loss1: 456.33 and loss2: 0.00\n",
      "Epoch [737], train_loss: 425.04 with loss1: 425.04 and loss2: 0.00\n",
      "Epoch [738], train_loss: 472.73 with loss1: 472.73 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [739], train_loss: 537.38 with loss1: 537.38 and loss2: 0.00\n",
      "Epoch [740], train_loss: 478.71 with loss1: 478.71 and loss2: 0.00\n",
      "Epoch [741], train_loss: 609.24 with loss1: 609.24 and loss2: 0.00\n",
      "Epoch [742], train_loss: 601.06 with loss1: 601.06 and loss2: 0.00\n",
      "Epoch [743], train_loss: 503.50 with loss1: 503.50 and loss2: 0.00\n",
      "Epoch [744], train_loss: 536.25 with loss1: 536.25 and loss2: 0.00\n",
      "Epoch [745], train_loss: 505.93 with loss1: 505.93 and loss2: 0.00\n",
      "Epoch [746], train_loss: 443.83 with loss1: 443.83 and loss2: 0.00\n",
      "Epoch [747], train_loss: 383.07 with loss1: 383.07 and loss2: 0.00\n",
      "Epoch [748], train_loss: 465.45 with loss1: 465.45 and loss2: 0.00\n",
      "Epoch [749], train_loss: 403.95 with loss1: 403.95 and loss2: 0.00\n",
      "Epoch [750], train_loss: 461.13 with loss1: 461.13 and loss2: 0.00\n",
      "Epoch [751], train_loss: 451.62 with loss1: 451.62 and loss2: 0.00\n",
      "Epoch [752], train_loss: 411.46 with loss1: 411.46 and loss2: 0.00\n",
      "Epoch [753], train_loss: 423.74 with loss1: 423.74 and loss2: 0.00\n",
      "Epoch [754], train_loss: 462.82 with loss1: 462.82 and loss2: 0.00\n",
      "Epoch [755], train_loss: 452.25 with loss1: 452.25 and loss2: 0.00\n",
      "Epoch [756], train_loss: 365.44 with loss1: 365.44 and loss2: 0.00\n",
      "Epoch [757], train_loss: 467.05 with loss1: 467.05 and loss2: 0.00\n",
      "Epoch [758], train_loss: 480.84 with loss1: 480.84 and loss2: 0.00\n",
      "Epoch [759], train_loss: 449.78 with loss1: 449.78 and loss2: 0.00\n",
      "Epoch [760], train_loss: 466.59 with loss1: 466.59 and loss2: 0.00\n",
      "Epoch [761], train_loss: 512.77 with loss1: 512.77 and loss2: 0.00\n",
      "Epoch [762], train_loss: 410.40 with loss1: 410.40 and loss2: 0.00\n",
      "Epoch [763], train_loss: 463.78 with loss1: 463.78 and loss2: 0.00\n",
      "Epoch [764], train_loss: 422.77 with loss1: 422.77 and loss2: 0.00\n",
      "Epoch [765], train_loss: 489.02 with loss1: 489.02 and loss2: 0.00\n",
      "Epoch [766], train_loss: 586.86 with loss1: 586.86 and loss2: 0.00\n",
      "Epoch [767], train_loss: 636.97 with loss1: 636.97 and loss2: 0.00\n",
      "Epoch [768], train_loss: 696.75 with loss1: 696.75 and loss2: 0.00\n",
      "Epoch [769], train_loss: 528.01 with loss1: 528.01 and loss2: 0.00\n",
      "Epoch [770], train_loss: 429.26 with loss1: 429.26 and loss2: 0.00\n",
      "Epoch [771], train_loss: 478.78 with loss1: 478.78 and loss2: 0.00\n",
      "Epoch [772], train_loss: 414.60 with loss1: 414.60 and loss2: 0.00\n",
      "Epoch [773], train_loss: 470.98 with loss1: 470.98 and loss2: 0.00\n",
      "Epoch [774], train_loss: 417.83 with loss1: 417.83 and loss2: 0.00\n",
      "Epoch [775], train_loss: 365.61 with loss1: 365.61 and loss2: 0.00\n",
      "Epoch [776], train_loss: 455.11 with loss1: 455.11 and loss2: 0.00\n",
      "Epoch [777], train_loss: 513.14 with loss1: 513.14 and loss2: 0.00\n",
      "Epoch [778], train_loss: 590.38 with loss1: 590.38 and loss2: 0.00\n",
      "Epoch [779], train_loss: 590.09 with loss1: 590.09 and loss2: 0.00\n",
      "Epoch [780], train_loss: 523.59 with loss1: 523.59 and loss2: 0.00\n",
      "Epoch [781], train_loss: 415.71 with loss1: 415.71 and loss2: 0.00\n",
      "Epoch [782], train_loss: 373.07 with loss1: 373.07 and loss2: 0.00\n",
      "Epoch [783], train_loss: 454.88 with loss1: 454.88 and loss2: 0.00\n",
      "Epoch [784], train_loss: 389.26 with loss1: 389.26 and loss2: 0.00\n",
      "Epoch [785], train_loss: 404.14 with loss1: 404.14 and loss2: 0.00\n",
      "Epoch [786], train_loss: 376.16 with loss1: 376.16 and loss2: 0.00\n",
      "Epoch [787], train_loss: 358.76 with loss1: 358.76 and loss2: 0.00\n",
      "Epoch [788], train_loss: 455.86 with loss1: 455.86 and loss2: 0.00\n",
      "Epoch [789], train_loss: 446.48 with loss1: 446.48 and loss2: 0.00\n",
      "Epoch [790], train_loss: 401.86 with loss1: 401.86 and loss2: 0.00\n",
      "Epoch [791], train_loss: 417.41 with loss1: 417.41 and loss2: 0.00\n",
      "Epoch [792], train_loss: 377.26 with loss1: 377.26 and loss2: 0.00\n",
      "Epoch [793], train_loss: 429.19 with loss1: 429.19 and loss2: 0.00\n",
      "Epoch [794], train_loss: 478.15 with loss1: 478.15 and loss2: 0.00\n",
      "Epoch [795], train_loss: 468.54 with loss1: 468.54 and loss2: 0.00\n",
      "Epoch [796], train_loss: 401.93 with loss1: 401.93 and loss2: 0.00\n",
      "Epoch [797], train_loss: 451.50 with loss1: 451.50 and loss2: 0.00\n",
      "Epoch [798], train_loss: 381.89 with loss1: 381.89 and loss2: 0.00\n",
      "Epoch [799], train_loss: 358.97 with loss1: 358.97 and loss2: 0.00\n",
      "Epoch [800], train_loss: 467.64 with loss1: 467.64 and loss2: 0.00\n",
      "Epoch [801], train_loss: 396.79 with loss1: 396.79 and loss2: 0.00\n",
      "Epoch [802], train_loss: 384.89 with loss1: 384.89 and loss2: 0.00\n",
      "Epoch [803], train_loss: 421.66 with loss1: 421.66 and loss2: 0.00\n",
      "Epoch [804], train_loss: 372.20 with loss1: 372.20 and loss2: 0.00\n",
      "Epoch [805], train_loss: 376.41 with loss1: 376.41 and loss2: 0.00\n",
      "Epoch [806], train_loss: 453.54 with loss1: 453.54 and loss2: 0.00\n",
      "Epoch [807], train_loss: 447.24 with loss1: 447.24 and loss2: 0.00\n",
      "Epoch [808], train_loss: 498.79 with loss1: 498.79 and loss2: 0.00\n",
      "Epoch [809], train_loss: 524.50 with loss1: 524.50 and loss2: 0.00\n",
      "Epoch [810], train_loss: 479.38 with loss1: 479.38 and loss2: 0.00\n",
      "Epoch [811], train_loss: 440.67 with loss1: 440.67 and loss2: 0.00\n",
      "Epoch [812], train_loss: 508.60 with loss1: 508.60 and loss2: 0.00\n",
      "Epoch [813], train_loss: 518.51 with loss1: 518.51 and loss2: 0.00\n",
      "Epoch [814], train_loss: 642.58 with loss1: 642.58 and loss2: 0.00\n",
      "Epoch [815], train_loss: 560.39 with loss1: 560.39 and loss2: 0.00\n",
      "Epoch [816], train_loss: 509.84 with loss1: 509.84 and loss2: 0.00\n",
      "Epoch [817], train_loss: 453.84 with loss1: 453.84 and loss2: 0.00\n",
      "Epoch [818], train_loss: 453.00 with loss1: 453.00 and loss2: 0.00\n",
      "Epoch [819], train_loss: 411.26 with loss1: 411.26 and loss2: 0.00\n",
      "Epoch [820], train_loss: 435.64 with loss1: 435.64 and loss2: 0.00\n",
      "Epoch [821], train_loss: 367.75 with loss1: 367.75 and loss2: 0.00\n",
      "Epoch [822], train_loss: 437.14 with loss1: 437.14 and loss2: 0.00\n",
      "Epoch [823], train_loss: 451.51 with loss1: 451.51 and loss2: 0.00\n",
      "Epoch [824], train_loss: 459.15 with loss1: 459.15 and loss2: 0.00\n",
      "Epoch [825], train_loss: 382.07 with loss1: 382.07 and loss2: 0.00\n",
      "Epoch [826], train_loss: 366.56 with loss1: 366.56 and loss2: 0.00\n",
      "Epoch [827], train_loss: 362.52 with loss1: 362.52 and loss2: 0.00\n",
      "Epoch [828], train_loss: 389.38 with loss1: 389.38 and loss2: 0.00\n",
      "Epoch [829], train_loss: 432.42 with loss1: 432.42 and loss2: 0.00\n",
      "Epoch [830], train_loss: 457.81 with loss1: 457.81 and loss2: 0.00\n",
      "Epoch [831], train_loss: 399.26 with loss1: 399.26 and loss2: 0.00\n",
      "Epoch [832], train_loss: 481.04 with loss1: 481.04 and loss2: 0.00\n",
      "Epoch [833], train_loss: 504.10 with loss1: 504.10 and loss2: 0.00\n",
      "Epoch [834], train_loss: 434.98 with loss1: 434.98 and loss2: 0.00\n",
      "Epoch [835], train_loss: 497.82 with loss1: 497.82 and loss2: 0.00\n",
      "Epoch [836], train_loss: 491.40 with loss1: 491.40 and loss2: 0.00\n",
      "Epoch [837], train_loss: 609.02 with loss1: 609.02 and loss2: 0.00\n",
      "Epoch [838], train_loss: 710.55 with loss1: 710.55 and loss2: 0.00\n",
      "Epoch [839], train_loss: 644.39 with loss1: 644.39 and loss2: 0.00\n",
      "Epoch [840], train_loss: 686.04 with loss1: 686.04 and loss2: 0.00\n",
      "Epoch [841], train_loss: 593.05 with loss1: 593.05 and loss2: 0.00\n",
      "Epoch [842], train_loss: 503.70 with loss1: 503.70 and loss2: 0.00\n",
      "Epoch [843], train_loss: 466.50 with loss1: 466.50 and loss2: 0.00\n",
      "Epoch [844], train_loss: 477.73 with loss1: 477.73 and loss2: 0.00\n",
      "Epoch [845], train_loss: 386.69 with loss1: 386.69 and loss2: 0.00\n",
      "Epoch [846], train_loss: 448.22 with loss1: 448.22 and loss2: 0.00\n",
      "Epoch [847], train_loss: 443.04 with loss1: 443.04 and loss2: 0.00\n",
      "Epoch [848], train_loss: 411.04 with loss1: 411.04 and loss2: 0.00\n",
      "Epoch [849], train_loss: 428.69 with loss1: 428.69 and loss2: 0.00\n",
      "Epoch [850], train_loss: 472.08 with loss1: 472.08 and loss2: 0.00\n",
      "Epoch [851], train_loss: 472.16 with loss1: 472.16 and loss2: 0.00\n",
      "Epoch [852], train_loss: 389.90 with loss1: 389.90 and loss2: 0.00\n",
      "Epoch [853], train_loss: 426.79 with loss1: 426.79 and loss2: 0.00\n",
      "Epoch [854], train_loss: 417.45 with loss1: 417.45 and loss2: 0.00\n",
      "Epoch [855], train_loss: 355.27 with loss1: 355.27 and loss2: 0.00\n",
      "Epoch [856], train_loss: 442.70 with loss1: 442.70 and loss2: 0.00\n",
      "Epoch [857], train_loss: 380.30 with loss1: 380.30 and loss2: 0.00\n",
      "Epoch [858], train_loss: 424.29 with loss1: 424.29 and loss2: 0.00\n",
      "Epoch [859], train_loss: 459.82 with loss1: 459.82 and loss2: 0.00\n",
      "Epoch [860], train_loss: 512.60 with loss1: 512.60 and loss2: 0.00\n",
      "Epoch [861], train_loss: 496.80 with loss1: 496.80 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [862], train_loss: 429.74 with loss1: 429.74 and loss2: 0.00\n",
      "Epoch [863], train_loss: 396.05 with loss1: 396.05 and loss2: 0.00\n",
      "Epoch [864], train_loss: 479.52 with loss1: 479.52 and loss2: 0.00\n",
      "Epoch [865], train_loss: 500.09 with loss1: 500.09 and loss2: 0.00\n",
      "Epoch [866], train_loss: 559.42 with loss1: 559.42 and loss2: 0.00\n",
      "Epoch [867], train_loss: 456.65 with loss1: 456.65 and loss2: 0.00\n",
      "Epoch [868], train_loss: 477.52 with loss1: 477.52 and loss2: 0.00\n",
      "Epoch [869], train_loss: 402.07 with loss1: 402.07 and loss2: 0.00\n",
      "Epoch [870], train_loss: 372.99 with loss1: 372.99 and loss2: 0.00\n",
      "Epoch [871], train_loss: 429.21 with loss1: 429.21 and loss2: 0.00\n",
      "Epoch [872], train_loss: 406.89 with loss1: 406.89 and loss2: 0.00\n",
      "Epoch [873], train_loss: 404.99 with loss1: 404.99 and loss2: 0.00\n",
      "Epoch [874], train_loss: 440.95 with loss1: 440.95 and loss2: 0.00\n",
      "Epoch [875], train_loss: 445.61 with loss1: 445.61 and loss2: 0.00\n",
      "Epoch [876], train_loss: 391.92 with loss1: 391.92 and loss2: 0.00\n",
      "Epoch [877], train_loss: 450.46 with loss1: 450.46 and loss2: 0.00\n",
      "Epoch [878], train_loss: 362.93 with loss1: 362.93 and loss2: 0.00\n",
      "Epoch [879], train_loss: 430.32 with loss1: 430.32 and loss2: 0.00\n",
      "Epoch [880], train_loss: 466.45 with loss1: 466.45 and loss2: 0.00\n",
      "Epoch [881], train_loss: 383.05 with loss1: 383.05 and loss2: 0.00\n",
      "Epoch [882], train_loss: 436.00 with loss1: 436.00 and loss2: 0.00\n",
      "Epoch [883], train_loss: 401.10 with loss1: 401.10 and loss2: 0.00\n",
      "Epoch [884], train_loss: 455.96 with loss1: 455.96 and loss2: 0.00\n",
      "Epoch [885], train_loss: 427.38 with loss1: 427.38 and loss2: 0.00\n",
      "Epoch [886], train_loss: 438.94 with loss1: 438.94 and loss2: 0.00\n",
      "Epoch [887], train_loss: 383.24 with loss1: 383.24 and loss2: 0.00\n",
      "Epoch [888], train_loss: 451.51 with loss1: 451.51 and loss2: 0.00\n",
      "Epoch [889], train_loss: 407.68 with loss1: 407.68 and loss2: 0.00\n",
      "Epoch [890], train_loss: 454.37 with loss1: 454.37 and loss2: 0.00\n",
      "Epoch [891], train_loss: 404.10 with loss1: 404.10 and loss2: 0.00\n",
      "Epoch [892], train_loss: 450.07 with loss1: 450.07 and loss2: 0.00\n",
      "Epoch [893], train_loss: 441.98 with loss1: 441.98 and loss2: 0.00\n",
      "Epoch [894], train_loss: 369.90 with loss1: 369.90 and loss2: 0.00\n",
      "Epoch [895], train_loss: 450.17 with loss1: 450.17 and loss2: 0.00\n",
      "Epoch [896], train_loss: 384.34 with loss1: 384.34 and loss2: 0.00\n",
      "Epoch [897], train_loss: 410.29 with loss1: 410.29 and loss2: 0.00\n",
      "Epoch [898], train_loss: 442.75 with loss1: 442.75 and loss2: 0.00\n",
      "Epoch [899], train_loss: 430.28 with loss1: 430.28 and loss2: 0.00\n",
      "Epoch [900], train_loss: 427.24 with loss1: 427.24 and loss2: 0.00\n",
      "Epoch [901], train_loss: 372.03 with loss1: 372.03 and loss2: 0.00\n",
      "Epoch [902], train_loss: 419.94 with loss1: 419.94 and loss2: 0.00\n",
      "Epoch [903], train_loss: 440.12 with loss1: 440.12 and loss2: 0.00\n",
      "Epoch [904], train_loss: 466.30 with loss1: 466.30 and loss2: 0.00\n",
      "Epoch [905], train_loss: 549.80 with loss1: 549.80 and loss2: 0.00\n",
      "Epoch [906], train_loss: 495.38 with loss1: 495.38 and loss2: 0.00\n",
      "Epoch [907], train_loss: 436.19 with loss1: 436.19 and loss2: 0.00\n",
      "Epoch [908], train_loss: 448.37 with loss1: 448.37 and loss2: 0.00\n",
      "Epoch [909], train_loss: 419.64 with loss1: 419.64 and loss2: 0.00\n",
      "Epoch [910], train_loss: 370.67 with loss1: 370.67 and loss2: 0.00\n",
      "Epoch [911], train_loss: 414.54 with loss1: 414.54 and loss2: 0.00\n",
      "Epoch [912], train_loss: 385.94 with loss1: 385.94 and loss2: 0.00\n",
      "Epoch [913], train_loss: 412.06 with loss1: 412.06 and loss2: 0.00\n",
      "Epoch [914], train_loss: 357.46 with loss1: 357.46 and loss2: 0.00\n",
      "Epoch [915], train_loss: 434.46 with loss1: 434.46 and loss2: 0.00\n",
      "Epoch [916], train_loss: 367.52 with loss1: 367.52 and loss2: 0.00\n",
      "Epoch [917], train_loss: 396.82 with loss1: 396.82 and loss2: 0.00\n",
      "Epoch [918], train_loss: 445.78 with loss1: 445.78 and loss2: 0.00\n",
      "Epoch [919], train_loss: 493.18 with loss1: 493.18 and loss2: 0.00\n",
      "Epoch [920], train_loss: 450.82 with loss1: 450.82 and loss2: 0.00\n",
      "Epoch [921], train_loss: 447.18 with loss1: 447.18 and loss2: 0.00\n",
      "Epoch [922], train_loss: 394.66 with loss1: 394.66 and loss2: 0.00\n",
      "Epoch [923], train_loss: 448.03 with loss1: 448.03 and loss2: 0.00\n",
      "Epoch [924], train_loss: 439.38 with loss1: 439.38 and loss2: 0.00\n",
      "Epoch [925], train_loss: 456.99 with loss1: 456.99 and loss2: 0.00\n",
      "Epoch [926], train_loss: 375.35 with loss1: 375.35 and loss2: 0.00\n",
      "Epoch [927], train_loss: 453.16 with loss1: 453.16 and loss2: 0.00\n",
      "Epoch [928], train_loss: 529.17 with loss1: 529.17 and loss2: 0.00\n",
      "Epoch [929], train_loss: 406.59 with loss1: 406.59 and loss2: 0.00\n",
      "Epoch [930], train_loss: 483.61 with loss1: 483.61 and loss2: 0.00\n",
      "Epoch [931], train_loss: 485.09 with loss1: 485.09 and loss2: 0.00\n",
      "Epoch [932], train_loss: 434.96 with loss1: 434.96 and loss2: 0.00\n",
      "Epoch [933], train_loss: 442.06 with loss1: 442.06 and loss2: 0.00\n",
      "Epoch [934], train_loss: 431.64 with loss1: 431.64 and loss2: 0.00\n",
      "Epoch [935], train_loss: 359.24 with loss1: 359.24 and loss2: 0.00\n",
      "Epoch [936], train_loss: 502.15 with loss1: 502.15 and loss2: 0.00\n",
      "Epoch [937], train_loss: 429.11 with loss1: 429.11 and loss2: 0.00\n",
      "Epoch [938], train_loss: 438.91 with loss1: 438.91 and loss2: 0.00\n",
      "Epoch [939], train_loss: 461.42 with loss1: 461.42 and loss2: 0.00\n",
      "Epoch [940], train_loss: 408.37 with loss1: 408.37 and loss2: 0.00\n",
      "Epoch [941], train_loss: 361.71 with loss1: 361.71 and loss2: 0.00\n",
      "Epoch [942], train_loss: 453.26 with loss1: 453.26 and loss2: 0.00\n",
      "Epoch [943], train_loss: 466.22 with loss1: 466.22 and loss2: 0.00\n",
      "Epoch [944], train_loss: 417.16 with loss1: 417.16 and loss2: 0.00\n",
      "Epoch [945], train_loss: 422.37 with loss1: 422.37 and loss2: 0.00\n",
      "Epoch [946], train_loss: 425.96 with loss1: 425.96 and loss2: 0.00\n",
      "Epoch [947], train_loss: 391.71 with loss1: 391.71 and loss2: 0.00\n",
      "Epoch [948], train_loss: 466.65 with loss1: 466.65 and loss2: 0.00\n",
      "Epoch [949], train_loss: 425.54 with loss1: 425.54 and loss2: 0.00\n",
      "Epoch [950], train_loss: 470.37 with loss1: 470.37 and loss2: 0.00\n",
      "Epoch [951], train_loss: 388.74 with loss1: 388.74 and loss2: 0.00\n",
      "Epoch [952], train_loss: 474.68 with loss1: 474.68 and loss2: 0.00\n",
      "Epoch [953], train_loss: 458.32 with loss1: 458.32 and loss2: 0.00\n",
      "Epoch [954], train_loss: 426.84 with loss1: 426.84 and loss2: 0.00\n",
      "Epoch [955], train_loss: 408.74 with loss1: 408.74 and loss2: 0.00\n",
      "Epoch [956], train_loss: 341.18 with loss1: 341.18 and loss2: 0.00\n",
      "Epoch [957], train_loss: 432.62 with loss1: 432.62 and loss2: 0.00\n",
      "Epoch [958], train_loss: 430.16 with loss1: 430.16 and loss2: 0.00\n",
      "Epoch [959], train_loss: 428.65 with loss1: 428.65 and loss2: 0.00\n",
      "Epoch [960], train_loss: 582.79 with loss1: 582.79 and loss2: 0.00\n",
      "Epoch [961], train_loss: 743.87 with loss1: 743.87 and loss2: 0.00\n",
      "Epoch [962], train_loss: 736.82 with loss1: 736.82 and loss2: 0.00\n",
      "Epoch [963], train_loss: 694.77 with loss1: 694.77 and loss2: 0.00\n",
      "Epoch [964], train_loss: 577.52 with loss1: 577.52 and loss2: 0.00\n",
      "Epoch [965], train_loss: 482.26 with loss1: 482.26 and loss2: 0.00\n",
      "Epoch [966], train_loss: 457.80 with loss1: 457.80 and loss2: 0.00\n",
      "Epoch [967], train_loss: 482.11 with loss1: 482.11 and loss2: 0.00\n",
      "Epoch [968], train_loss: 394.96 with loss1: 394.96 and loss2: 0.00\n",
      "Epoch [969], train_loss: 423.48 with loss1: 423.48 and loss2: 0.00\n",
      "Epoch [970], train_loss: 416.70 with loss1: 416.70 and loss2: 0.00\n",
      "Epoch [971], train_loss: 438.96 with loss1: 438.96 and loss2: 0.00\n",
      "Epoch [972], train_loss: 355.34 with loss1: 355.34 and loss2: 0.00\n",
      "Epoch [973], train_loss: 401.73 with loss1: 401.73 and loss2: 0.00\n",
      "Epoch [974], train_loss: 429.64 with loss1: 429.64 and loss2: 0.00\n",
      "Epoch [975], train_loss: 405.87 with loss1: 405.87 and loss2: 0.00\n",
      "Epoch [976], train_loss: 425.35 with loss1: 425.35 and loss2: 0.00\n",
      "Epoch [977], train_loss: 374.38 with loss1: 374.38 and loss2: 0.00\n",
      "Epoch [978], train_loss: 442.75 with loss1: 442.75 and loss2: 0.00\n",
      "Epoch [979], train_loss: 433.17 with loss1: 433.17 and loss2: 0.00\n",
      "Epoch [980], train_loss: 405.66 with loss1: 405.66 and loss2: 0.00\n",
      "Epoch [981], train_loss: 438.40 with loss1: 438.40 and loss2: 0.00\n",
      "Epoch [982], train_loss: 468.74 with loss1: 468.74 and loss2: 0.00\n",
      "Epoch [983], train_loss: 499.96 with loss1: 499.96 and loss2: 0.00\n",
      "Epoch [984], train_loss: 515.70 with loss1: 515.70 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [985], train_loss: 457.22 with loss1: 457.22 and loss2: 0.00\n",
      "Epoch [986], train_loss: 386.61 with loss1: 386.61 and loss2: 0.00\n",
      "Epoch [987], train_loss: 388.25 with loss1: 388.25 and loss2: 0.00\n",
      "Epoch [988], train_loss: 410.53 with loss1: 410.53 and loss2: 0.00\n",
      "Epoch [989], train_loss: 383.80 with loss1: 383.80 and loss2: 0.00\n",
      "Epoch [990], train_loss: 396.29 with loss1: 396.29 and loss2: 0.00\n",
      "Epoch [991], train_loss: 419.22 with loss1: 419.22 and loss2: 0.00\n",
      "Epoch [992], train_loss: 358.36 with loss1: 358.36 and loss2: 0.00\n",
      "Epoch [993], train_loss: 340.97 with loss1: 340.97 and loss2: 0.00\n",
      "Epoch [994], train_loss: 399.17 with loss1: 399.17 and loss2: 0.00\n",
      "Epoch [995], train_loss: 458.35 with loss1: 458.35 and loss2: 0.00\n",
      "Epoch [996], train_loss: 491.53 with loss1: 491.53 and loss2: 0.00\n",
      "Epoch [997], train_loss: 486.82 with loss1: 486.82 and loss2: 0.00\n",
      "Epoch [998], train_loss: 432.78 with loss1: 432.78 and loss2: 0.00\n",
      "Epoch [999], train_loss: 421.32 with loss1: 421.32 and loss2: 0.00\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# model version 3 \n",
    "# delete loss2\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history = fit(epochs=1000, lr=5e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc43b563",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 324116.62 with loss1: 324116.62 and loss2: 0.00\n",
      "Epoch [1], train_loss: 247498.59 with loss1: 247498.59 and loss2: 0.00\n",
      "Epoch [2], train_loss: 186540.31 with loss1: 186540.31 and loss2: 0.00\n",
      "Epoch [3], train_loss: 130488.12 with loss1: 130488.12 and loss2: 0.00\n",
      "Epoch [4], train_loss: 94067.79 with loss1: 94067.79 and loss2: 0.00\n",
      "Epoch [5], train_loss: 74234.16 with loss1: 74234.16 and loss2: 0.00\n",
      "Epoch [6], train_loss: 62876.54 with loss1: 62876.54 and loss2: 0.00\n",
      "Epoch [7], train_loss: 55911.56 with loss1: 55911.56 and loss2: 0.00\n",
      "Epoch [8], train_loss: 50518.67 with loss1: 50518.67 and loss2: 0.00\n",
      "Epoch [9], train_loss: 46328.07 with loss1: 46328.07 and loss2: 0.00\n",
      "Epoch [10], train_loss: 42661.52 with loss1: 42661.52 and loss2: 0.00\n",
      "Epoch [11], train_loss: 39075.46 with loss1: 39075.46 and loss2: 0.00\n",
      "Epoch [12], train_loss: 36512.82 with loss1: 36512.82 and loss2: 0.00\n",
      "Epoch [13], train_loss: 33550.15 with loss1: 33550.15 and loss2: 0.00\n",
      "Epoch [14], train_loss: 31410.16 with loss1: 31410.16 and loss2: 0.00\n",
      "Epoch [15], train_loss: 29048.70 with loss1: 29048.70 and loss2: 0.00\n",
      "Epoch [16], train_loss: 26968.48 with loss1: 26968.48 and loss2: 0.00\n",
      "Epoch [17], train_loss: 25558.14 with loss1: 25558.14 and loss2: 0.00\n",
      "Epoch [18], train_loss: 23600.46 with loss1: 23600.46 and loss2: 0.00\n",
      "Epoch [19], train_loss: 22406.96 with loss1: 22406.96 and loss2: 0.00\n",
      "Epoch [20], train_loss: 21107.74 with loss1: 21107.74 and loss2: 0.00\n",
      "Epoch [21], train_loss: 20160.67 with loss1: 20160.67 and loss2: 0.00\n",
      "Epoch [22], train_loss: 18753.36 with loss1: 18753.36 and loss2: 0.00\n",
      "Epoch [23], train_loss: 17896.97 with loss1: 17896.97 and loss2: 0.00\n",
      "Epoch [24], train_loss: 17203.48 with loss1: 17203.48 and loss2: 0.00\n",
      "Epoch [25], train_loss: 16489.91 with loss1: 16489.91 and loss2: 0.00\n",
      "Epoch [26], train_loss: 15851.93 with loss1: 15851.93 and loss2: 0.00\n",
      "Epoch [27], train_loss: 15250.01 with loss1: 15250.01 and loss2: 0.00\n",
      "Epoch [28], train_loss: 14507.37 with loss1: 14507.37 and loss2: 0.00\n",
      "Epoch [29], train_loss: 14167.24 with loss1: 14167.24 and loss2: 0.00\n",
      "Epoch [30], train_loss: 13640.56 with loss1: 13640.56 and loss2: 0.00\n",
      "Epoch [31], train_loss: 13273.33 with loss1: 13273.33 and loss2: 0.00\n",
      "Epoch [32], train_loss: 12759.98 with loss1: 12759.98 and loss2: 0.00\n",
      "Epoch [33], train_loss: 12294.93 with loss1: 12294.93 and loss2: 0.00\n",
      "Epoch [34], train_loss: 11948.40 with loss1: 11948.40 and loss2: 0.00\n",
      "Epoch [35], train_loss: 11658.86 with loss1: 11658.86 and loss2: 0.00\n",
      "Epoch [36], train_loss: 11372.41 with loss1: 11372.41 and loss2: 0.00\n",
      "Epoch [37], train_loss: 11094.42 with loss1: 11094.42 and loss2: 0.00\n",
      "Epoch [38], train_loss: 10876.03 with loss1: 10876.03 and loss2: 0.00\n",
      "Epoch [39], train_loss: 10559.40 with loss1: 10559.40 and loss2: 0.00\n",
      "Epoch [40], train_loss: 10338.42 with loss1: 10338.42 and loss2: 0.00\n",
      "Epoch [41], train_loss: 10108.52 with loss1: 10108.52 and loss2: 0.00\n",
      "Epoch [42], train_loss: 9957.97 with loss1: 9957.97 and loss2: 0.00\n",
      "Epoch [43], train_loss: 9675.01 with loss1: 9675.01 and loss2: 0.00\n",
      "Epoch [44], train_loss: 9506.74 with loss1: 9506.74 and loss2: 0.00\n",
      "Epoch [45], train_loss: 9275.41 with loss1: 9275.41 and loss2: 0.00\n",
      "Epoch [46], train_loss: 9111.26 with loss1: 9111.26 and loss2: 0.00\n",
      "Epoch [47], train_loss: 9005.50 with loss1: 9005.50 and loss2: 0.00\n",
      "Epoch [48], train_loss: 8878.80 with loss1: 8878.80 and loss2: 0.00\n",
      "Epoch [49], train_loss: 8696.18 with loss1: 8696.18 and loss2: 0.00\n",
      "Epoch [50], train_loss: 8612.62 with loss1: 8612.62 and loss2: 0.00\n",
      "Epoch [51], train_loss: 8375.60 with loss1: 8375.60 and loss2: 0.00\n",
      "Epoch [52], train_loss: 8262.46 with loss1: 8262.46 and loss2: 0.00\n",
      "Epoch [53], train_loss: 8178.13 with loss1: 8178.13 and loss2: 0.00\n",
      "Epoch [54], train_loss: 8048.99 with loss1: 8048.99 and loss2: 0.00\n",
      "Epoch [55], train_loss: 7927.46 with loss1: 7927.46 and loss2: 0.00\n",
      "Epoch [56], train_loss: 7840.50 with loss1: 7840.50 and loss2: 0.00\n",
      "Epoch [57], train_loss: 7697.62 with loss1: 7697.62 and loss2: 0.00\n",
      "Epoch [58], train_loss: 7667.01 with loss1: 7667.01 and loss2: 0.00\n",
      "Epoch [59], train_loss: 7573.47 with loss1: 7573.47 and loss2: 0.00\n",
      "Epoch [60], train_loss: 7439.61 with loss1: 7439.61 and loss2: 0.00\n",
      "Epoch [61], train_loss: 7372.83 with loss1: 7372.83 and loss2: 0.00\n",
      "Epoch [62], train_loss: 7262.30 with loss1: 7262.30 and loss2: 0.00\n",
      "Epoch [63], train_loss: 7230.92 with loss1: 7230.92 and loss2: 0.00\n",
      "Epoch [64], train_loss: 7130.35 with loss1: 7130.35 and loss2: 0.00\n",
      "Epoch [65], train_loss: 7071.16 with loss1: 7071.16 and loss2: 0.00\n",
      "Epoch [66], train_loss: 6995.29 with loss1: 6995.29 and loss2: 0.00\n",
      "Epoch [67], train_loss: 6933.64 with loss1: 6933.64 and loss2: 0.00\n",
      "Epoch [68], train_loss: 6851.45 with loss1: 6851.45 and loss2: 0.00\n",
      "Epoch [69], train_loss: 6797.34 with loss1: 6797.34 and loss2: 0.00\n",
      "Epoch [70], train_loss: 6753.30 with loss1: 6753.30 and loss2: 0.00\n",
      "Epoch [71], train_loss: 6717.32 with loss1: 6717.32 and loss2: 0.00\n",
      "Epoch [72], train_loss: 6601.15 with loss1: 6601.15 and loss2: 0.00\n",
      "Epoch [73], train_loss: 6553.61 with loss1: 6553.61 and loss2: 0.00\n",
      "Epoch [74], train_loss: 6484.82 with loss1: 6484.82 and loss2: 0.00\n",
      "Epoch [75], train_loss: 6479.59 with loss1: 6479.59 and loss2: 0.00\n",
      "Epoch [76], train_loss: 6436.96 with loss1: 6436.96 and loss2: 0.00\n",
      "Epoch [77], train_loss: 6369.26 with loss1: 6369.26 and loss2: 0.00\n",
      "Epoch [78], train_loss: 6307.51 with loss1: 6307.51 and loss2: 0.00\n",
      "Epoch [79], train_loss: 6241.74 with loss1: 6241.74 and loss2: 0.00\n",
      "Epoch [80], train_loss: 6197.50 with loss1: 6197.50 and loss2: 0.00\n",
      "Epoch [81], train_loss: 6154.56 with loss1: 6154.56 and loss2: 0.00\n",
      "Epoch [82], train_loss: 6135.05 with loss1: 6135.05 and loss2: 0.00\n",
      "Epoch [83], train_loss: 6106.74 with loss1: 6106.74 and loss2: 0.00\n",
      "Epoch [84], train_loss: 6042.26 with loss1: 6042.26 and loss2: 0.00\n",
      "Epoch [85], train_loss: 6023.85 with loss1: 6023.85 and loss2: 0.00\n",
      "Epoch [86], train_loss: 6017.68 with loss1: 6017.68 and loss2: 0.00\n",
      "Epoch [87], train_loss: 5959.40 with loss1: 5959.40 and loss2: 0.00\n",
      "Epoch [88], train_loss: 5939.57 with loss1: 5939.57 and loss2: 0.00\n",
      "Epoch [89], train_loss: 5879.38 with loss1: 5879.38 and loss2: 0.00\n",
      "Epoch [90], train_loss: 5860.50 with loss1: 5860.50 and loss2: 0.00\n",
      "Epoch [91], train_loss: 5782.13 with loss1: 5782.13 and loss2: 0.00\n",
      "Epoch [92], train_loss: 5798.46 with loss1: 5798.46 and loss2: 0.00\n",
      "Epoch [93], train_loss: 5780.00 with loss1: 5780.00 and loss2: 0.00\n",
      "Epoch [94], train_loss: 5750.02 with loss1: 5750.02 and loss2: 0.00\n",
      "Epoch [95], train_loss: 5728.35 with loss1: 5728.35 and loss2: 0.00\n",
      "Epoch [96], train_loss: 5668.67 with loss1: 5668.67 and loss2: 0.00\n",
      "Epoch [97], train_loss: 5663.34 with loss1: 5663.34 and loss2: 0.00\n",
      "Epoch [98], train_loss: 5622.31 with loss1: 5622.31 and loss2: 0.00\n",
      "Epoch [99], train_loss: 5621.85 with loss1: 5621.85 and loss2: 0.00\n",
      "Epoch [100], train_loss: 5556.81 with loss1: 5556.81 and loss2: 0.00\n",
      "Epoch [101], train_loss: 5508.46 with loss1: 5508.46 and loss2: 0.00\n",
      "Epoch [102], train_loss: 5504.90 with loss1: 5504.90 and loss2: 0.00\n",
      "Epoch [103], train_loss: 5481.07 with loss1: 5481.07 and loss2: 0.00\n",
      "Epoch [104], train_loss: 5470.51 with loss1: 5470.51 and loss2: 0.00\n",
      "Epoch [105], train_loss: 5416.42 with loss1: 5416.42 and loss2: 0.00\n",
      "Epoch [106], train_loss: 5416.37 with loss1: 5416.37 and loss2: 0.00\n",
      "Epoch [107], train_loss: 5396.62 with loss1: 5396.62 and loss2: 0.00\n",
      "Epoch [108], train_loss: 5364.38 with loss1: 5364.38 and loss2: 0.00\n",
      "Epoch [109], train_loss: 5364.47 with loss1: 5364.47 and loss2: 0.00\n",
      "Epoch [110], train_loss: 5343.70 with loss1: 5343.70 and loss2: 0.00\n",
      "Epoch [111], train_loss: 5325.85 with loss1: 5325.85 and loss2: 0.00\n",
      "Epoch [112], train_loss: 5315.58 with loss1: 5315.58 and loss2: 0.00\n",
      "Epoch [113], train_loss: 5270.57 with loss1: 5270.57 and loss2: 0.00\n",
      "Epoch [114], train_loss: 5251.38 with loss1: 5251.38 and loss2: 0.00\n",
      "Epoch [115], train_loss: 5277.10 with loss1: 5277.10 and loss2: 0.00\n",
      "Epoch [116], train_loss: 5245.22 with loss1: 5245.22 and loss2: 0.00\n",
      "Epoch [117], train_loss: 5177.82 with loss1: 5177.82 and loss2: 0.00\n",
      "Epoch [118], train_loss: 5192.73 with loss1: 5192.73 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [119], train_loss: 5179.68 with loss1: 5179.68 and loss2: 0.00\n",
      "Epoch [120], train_loss: 5127.67 with loss1: 5127.67 and loss2: 0.00\n",
      "Epoch [121], train_loss: 5125.60 with loss1: 5125.60 and loss2: 0.00\n",
      "Epoch [122], train_loss: 5115.43 with loss1: 5115.43 and loss2: 0.00\n",
      "Epoch [123], train_loss: 5123.41 with loss1: 5123.41 and loss2: 0.00\n",
      "Epoch [124], train_loss: 5118.99 with loss1: 5118.99 and loss2: 0.00\n",
      "Epoch [125], train_loss: 5082.29 with loss1: 5082.29 and loss2: 0.00\n",
      "Epoch [126], train_loss: 5065.16 with loss1: 5065.16 and loss2: 0.00\n",
      "Epoch [127], train_loss: 5052.27 with loss1: 5052.27 and loss2: 0.00\n",
      "Epoch [128], train_loss: 5025.98 with loss1: 5025.98 and loss2: 0.00\n",
      "Epoch [129], train_loss: 5021.22 with loss1: 5021.22 and loss2: 0.00\n",
      "Epoch [130], train_loss: 5021.88 with loss1: 5021.88 and loss2: 0.00\n",
      "Epoch [131], train_loss: 4965.95 with loss1: 4965.95 and loss2: 0.00\n",
      "Epoch [132], train_loss: 5006.45 with loss1: 5006.45 and loss2: 0.00\n",
      "Epoch [133], train_loss: 4981.29 with loss1: 4981.29 and loss2: 0.00\n",
      "Epoch [134], train_loss: 4940.10 with loss1: 4940.10 and loss2: 0.00\n",
      "Epoch [135], train_loss: 4939.39 with loss1: 4939.39 and loss2: 0.00\n",
      "Epoch [136], train_loss: 4911.81 with loss1: 4911.81 and loss2: 0.00\n",
      "Epoch [137], train_loss: 4907.69 with loss1: 4907.69 and loss2: 0.00\n",
      "Epoch [138], train_loss: 4893.70 with loss1: 4893.70 and loss2: 0.00\n",
      "Epoch [139], train_loss: 4896.08 with loss1: 4896.08 and loss2: 0.00\n",
      "Epoch [140], train_loss: 4877.12 with loss1: 4877.12 and loss2: 0.00\n",
      "Epoch [141], train_loss: 4869.27 with loss1: 4869.27 and loss2: 0.00\n",
      "Epoch [142], train_loss: 4842.46 with loss1: 4842.46 and loss2: 0.00\n",
      "Epoch [143], train_loss: 4838.69 with loss1: 4838.69 and loss2: 0.00\n",
      "Epoch [144], train_loss: 4826.49 with loss1: 4826.49 and loss2: 0.00\n",
      "Epoch [145], train_loss: 4811.58 with loss1: 4811.58 and loss2: 0.00\n",
      "Epoch [146], train_loss: 4806.60 with loss1: 4806.60 and loss2: 0.00\n",
      "Epoch [147], train_loss: 4788.25 with loss1: 4788.25 and loss2: 0.00\n",
      "Epoch [148], train_loss: 4782.95 with loss1: 4782.95 and loss2: 0.00\n",
      "Epoch [149], train_loss: 4772.46 with loss1: 4772.46 and loss2: 0.00\n",
      "Epoch [150], train_loss: 4739.16 with loss1: 4739.16 and loss2: 0.00\n",
      "Epoch [151], train_loss: 4761.09 with loss1: 4761.09 and loss2: 0.00\n",
      "Epoch [152], train_loss: 4748.86 with loss1: 4748.86 and loss2: 0.00\n",
      "Epoch [153], train_loss: 4726.50 with loss1: 4726.50 and loss2: 0.00\n",
      "Epoch [154], train_loss: 4741.11 with loss1: 4741.11 and loss2: 0.00\n",
      "Epoch [155], train_loss: 4706.69 with loss1: 4706.69 and loss2: 0.00\n",
      "Epoch [156], train_loss: 4697.78 with loss1: 4697.78 and loss2: 0.00\n",
      "Epoch [157], train_loss: 4670.18 with loss1: 4670.18 and loss2: 0.00\n",
      "Epoch [158], train_loss: 4663.26 with loss1: 4663.26 and loss2: 0.00\n",
      "Epoch [159], train_loss: 4648.39 with loss1: 4648.39 and loss2: 0.00\n",
      "Epoch [160], train_loss: 4646.02 with loss1: 4646.02 and loss2: 0.00\n",
      "Epoch [161], train_loss: 4658.15 with loss1: 4658.15 and loss2: 0.00\n",
      "Epoch [162], train_loss: 4632.34 with loss1: 4632.34 and loss2: 0.00\n",
      "Epoch [163], train_loss: 4626.77 with loss1: 4626.77 and loss2: 0.00\n",
      "Epoch [164], train_loss: 4618.22 with loss1: 4618.22 and loss2: 0.00\n",
      "Epoch [165], train_loss: 4616.32 with loss1: 4616.32 and loss2: 0.00\n",
      "Epoch [166], train_loss: 4618.23 with loss1: 4618.23 and loss2: 0.00\n",
      "Epoch [167], train_loss: 4595.14 with loss1: 4595.14 and loss2: 0.00\n",
      "Epoch [168], train_loss: 4589.58 with loss1: 4589.58 and loss2: 0.00\n",
      "Epoch [169], train_loss: 4574.37 with loss1: 4574.37 and loss2: 0.00\n",
      "Epoch [170], train_loss: 4574.15 with loss1: 4574.15 and loss2: 0.00\n",
      "Epoch [171], train_loss: 4565.17 with loss1: 4565.17 and loss2: 0.00\n",
      "Epoch [172], train_loss: 4557.58 with loss1: 4557.58 and loss2: 0.00\n",
      "Epoch [173], train_loss: 4552.30 with loss1: 4552.30 and loss2: 0.00\n",
      "Epoch [174], train_loss: 4546.73 with loss1: 4546.73 and loss2: 0.00\n",
      "Epoch [175], train_loss: 4536.42 with loss1: 4536.42 and loss2: 0.00\n",
      "Epoch [176], train_loss: 4529.91 with loss1: 4529.91 and loss2: 0.00\n",
      "Epoch [177], train_loss: 4535.23 with loss1: 4535.23 and loss2: 0.00\n",
      "Epoch [178], train_loss: 4510.17 with loss1: 4510.17 and loss2: 0.00\n",
      "Epoch [179], train_loss: 4502.33 with loss1: 4502.33 and loss2: 0.00\n",
      "Epoch [180], train_loss: 4516.97 with loss1: 4516.97 and loss2: 0.00\n",
      "Epoch [181], train_loss: 4480.02 with loss1: 4480.02 and loss2: 0.00\n",
      "Epoch [182], train_loss: 4475.89 with loss1: 4475.89 and loss2: 0.00\n",
      "Epoch [183], train_loss: 4469.78 with loss1: 4469.78 and loss2: 0.00\n",
      "Epoch [184], train_loss: 4461.76 with loss1: 4461.76 and loss2: 0.00\n",
      "Epoch [185], train_loss: 4494.47 with loss1: 4494.47 and loss2: 0.00\n",
      "Epoch [186], train_loss: 4462.22 with loss1: 4462.22 and loss2: 0.00\n",
      "Epoch [187], train_loss: 4436.20 with loss1: 4436.20 and loss2: 0.00\n",
      "Epoch [188], train_loss: 4445.34 with loss1: 4445.34 and loss2: 0.00\n",
      "Epoch [189], train_loss: 4428.68 with loss1: 4428.68 and loss2: 0.00\n",
      "Epoch [190], train_loss: 4425.26 with loss1: 4425.26 and loss2: 0.00\n",
      "Epoch [191], train_loss: 4410.28 with loss1: 4410.28 and loss2: 0.00\n",
      "Epoch [192], train_loss: 4434.95 with loss1: 4434.95 and loss2: 0.00\n",
      "Epoch [193], train_loss: 4393.05 with loss1: 4393.05 and loss2: 0.00\n",
      "Epoch [194], train_loss: 4412.18 with loss1: 4412.18 and loss2: 0.00\n",
      "Epoch [195], train_loss: 4381.17 with loss1: 4381.17 and loss2: 0.00\n",
      "Epoch [196], train_loss: 4390.45 with loss1: 4390.45 and loss2: 0.00\n",
      "Epoch [197], train_loss: 4377.23 with loss1: 4377.23 and loss2: 0.00\n",
      "Epoch [198], train_loss: 4364.14 with loss1: 4364.14 and loss2: 0.00\n",
      "Epoch [199], train_loss: 4365.52 with loss1: 4365.52 and loss2: 0.00\n",
      "Epoch [200], train_loss: 4355.60 with loss1: 4355.60 and loss2: 0.00\n",
      "Epoch [201], train_loss: 4353.83 with loss1: 4353.83 and loss2: 0.00\n",
      "Epoch [202], train_loss: 4360.90 with loss1: 4360.90 and loss2: 0.00\n",
      "Epoch [203], train_loss: 4339.41 with loss1: 4339.41 and loss2: 0.00\n",
      "Epoch [204], train_loss: 4322.89 with loss1: 4322.89 and loss2: 0.00\n",
      "Epoch [205], train_loss: 4328.62 with loss1: 4328.62 and loss2: 0.00\n",
      "Epoch [206], train_loss: 4310.39 with loss1: 4310.39 and loss2: 0.00\n",
      "Epoch [207], train_loss: 4313.47 with loss1: 4313.47 and loss2: 0.00\n",
      "Epoch [208], train_loss: 4325.15 with loss1: 4325.15 and loss2: 0.00\n",
      "Epoch [209], train_loss: 4305.30 with loss1: 4305.30 and loss2: 0.00\n",
      "Epoch [210], train_loss: 4290.03 with loss1: 4290.03 and loss2: 0.00\n",
      "Epoch [211], train_loss: 4278.31 with loss1: 4278.31 and loss2: 0.00\n",
      "Epoch [212], train_loss: 4271.54 with loss1: 4271.54 and loss2: 0.00\n",
      "Epoch [213], train_loss: 4274.54 with loss1: 4274.54 and loss2: 0.00\n",
      "Epoch [214], train_loss: 4263.03 with loss1: 4263.03 and loss2: 0.00\n",
      "Epoch [215], train_loss: 4276.69 with loss1: 4276.69 and loss2: 0.00\n",
      "Epoch [216], train_loss: 4253.82 with loss1: 4253.82 and loss2: 0.00\n",
      "Epoch [217], train_loss: 4253.92 with loss1: 4253.92 and loss2: 0.00\n",
      "Epoch [218], train_loss: 4244.14 with loss1: 4244.14 and loss2: 0.00\n",
      "Epoch [219], train_loss: 4247.36 with loss1: 4247.36 and loss2: 0.00\n",
      "Epoch [220], train_loss: 4237.56 with loss1: 4237.56 and loss2: 0.00\n",
      "Epoch [221], train_loss: 4216.14 with loss1: 4216.14 and loss2: 0.00\n",
      "Epoch [222], train_loss: 4223.42 with loss1: 4223.42 and loss2: 0.00\n",
      "Epoch [223], train_loss: 4216.30 with loss1: 4216.30 and loss2: 0.00\n",
      "Epoch [224], train_loss: 4217.15 with loss1: 4217.15 and loss2: 0.00\n",
      "Epoch [225], train_loss: 4199.94 with loss1: 4199.94 and loss2: 0.00\n",
      "Epoch [226], train_loss: 4239.62 with loss1: 4239.62 and loss2: 0.00\n",
      "Epoch [227], train_loss: 4187.76 with loss1: 4187.76 and loss2: 0.00\n",
      "Epoch [228], train_loss: 4194.69 with loss1: 4194.69 and loss2: 0.00\n",
      "Epoch [229], train_loss: 4184.06 with loss1: 4184.06 and loss2: 0.00\n",
      "Epoch [230], train_loss: 4185.66 with loss1: 4185.66 and loss2: 0.00\n",
      "Epoch [231], train_loss: 4177.53 with loss1: 4177.53 and loss2: 0.00\n",
      "Epoch [232], train_loss: 4167.08 with loss1: 4167.08 and loss2: 0.00\n",
      "Epoch [233], train_loss: 4152.20 with loss1: 4152.20 and loss2: 0.00\n",
      "Epoch [234], train_loss: 4162.23 with loss1: 4162.23 and loss2: 0.00\n",
      "Epoch [235], train_loss: 4172.04 with loss1: 4172.04 and loss2: 0.00\n",
      "Epoch [236], train_loss: 4177.14 with loss1: 4177.14 and loss2: 0.00\n",
      "Epoch [237], train_loss: 4138.13 with loss1: 4138.13 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [238], train_loss: 4142.06 with loss1: 4142.06 and loss2: 0.00\n",
      "Epoch [239], train_loss: 4123.97 with loss1: 4123.97 and loss2: 0.00\n",
      "Epoch [240], train_loss: 4116.65 with loss1: 4116.65 and loss2: 0.00\n",
      "Epoch [241], train_loss: 4154.38 with loss1: 4154.38 and loss2: 0.00\n",
      "Epoch [242], train_loss: 4109.85 with loss1: 4109.85 and loss2: 0.00\n",
      "Epoch [243], train_loss: 4117.61 with loss1: 4117.61 and loss2: 0.00\n",
      "Epoch [244], train_loss: 4107.98 with loss1: 4107.98 and loss2: 0.00\n",
      "Epoch [245], train_loss: 4111.10 with loss1: 4111.10 and loss2: 0.00\n",
      "Epoch [246], train_loss: 4093.32 with loss1: 4093.32 and loss2: 0.00\n",
      "Epoch [247], train_loss: 4089.21 with loss1: 4089.21 and loss2: 0.00\n",
      "Epoch [248], train_loss: 4086.49 with loss1: 4086.49 and loss2: 0.00\n",
      "Epoch [249], train_loss: 4094.06 with loss1: 4094.06 and loss2: 0.00\n",
      "Epoch [250], train_loss: 4111.18 with loss1: 4111.18 and loss2: 0.00\n",
      "Epoch [251], train_loss: 4073.70 with loss1: 4073.70 and loss2: 0.00\n",
      "Epoch [252], train_loss: 4092.43 with loss1: 4092.43 and loss2: 0.00\n",
      "Epoch [253], train_loss: 4054.43 with loss1: 4054.43 and loss2: 0.00\n",
      "Epoch [254], train_loss: 4066.26 with loss1: 4066.26 and loss2: 0.00\n",
      "Epoch [255], train_loss: 4079.74 with loss1: 4079.74 and loss2: 0.00\n",
      "Epoch [256], train_loss: 4065.91 with loss1: 4065.91 and loss2: 0.00\n",
      "Epoch [257], train_loss: 4050.75 with loss1: 4050.75 and loss2: 0.00\n",
      "Epoch [258], train_loss: 4041.86 with loss1: 4041.86 and loss2: 0.00\n",
      "Epoch [259], train_loss: 4069.66 with loss1: 4069.66 and loss2: 0.00\n",
      "Epoch [260], train_loss: 4032.92 with loss1: 4032.92 and loss2: 0.00\n",
      "Epoch [261], train_loss: 4024.76 with loss1: 4024.76 and loss2: 0.00\n",
      "Epoch [262], train_loss: 4041.49 with loss1: 4041.49 and loss2: 0.00\n",
      "Epoch [263], train_loss: 4017.15 with loss1: 4017.15 and loss2: 0.00\n",
      "Epoch [264], train_loss: 4017.50 with loss1: 4017.50 and loss2: 0.00\n",
      "Epoch [265], train_loss: 4008.06 with loss1: 4008.06 and loss2: 0.00\n",
      "Epoch [266], train_loss: 4044.74 with loss1: 4044.74 and loss2: 0.00\n",
      "Epoch [267], train_loss: 4003.99 with loss1: 4003.99 and loss2: 0.00\n",
      "Epoch [268], train_loss: 4000.85 with loss1: 4000.85 and loss2: 0.00\n",
      "Epoch [269], train_loss: 4000.79 with loss1: 4000.79 and loss2: 0.00\n",
      "Epoch [270], train_loss: 4006.10 with loss1: 4006.10 and loss2: 0.00\n",
      "Epoch [271], train_loss: 3981.07 with loss1: 3981.07 and loss2: 0.00\n",
      "Epoch [272], train_loss: 3998.04 with loss1: 3998.04 and loss2: 0.00\n",
      "Epoch [273], train_loss: 3963.65 with loss1: 3963.65 and loss2: 0.00\n",
      "Epoch [274], train_loss: 3964.24 with loss1: 3964.24 and loss2: 0.00\n",
      "Epoch [275], train_loss: 3962.36 with loss1: 3962.36 and loss2: 0.00\n",
      "Epoch [276], train_loss: 3978.44 with loss1: 3978.44 and loss2: 0.00\n",
      "Epoch [277], train_loss: 3970.71 with loss1: 3970.71 and loss2: 0.00\n",
      "Epoch [278], train_loss: 3964.65 with loss1: 3964.65 and loss2: 0.00\n",
      "Epoch [279], train_loss: 3961.70 with loss1: 3961.70 and loss2: 0.00\n",
      "Epoch [280], train_loss: 3945.56 with loss1: 3945.56 and loss2: 0.00\n",
      "Epoch [281], train_loss: 3950.78 with loss1: 3950.78 and loss2: 0.00\n",
      "Epoch [282], train_loss: 3967.65 with loss1: 3967.65 and loss2: 0.00\n",
      "Epoch [283], train_loss: 3925.66 with loss1: 3925.66 and loss2: 0.00\n",
      "Epoch [284], train_loss: 3939.05 with loss1: 3939.05 and loss2: 0.00\n",
      "Epoch [285], train_loss: 3934.22 with loss1: 3934.22 and loss2: 0.00\n",
      "Epoch [286], train_loss: 3914.28 with loss1: 3914.28 and loss2: 0.00\n",
      "Epoch [287], train_loss: 3927.12 with loss1: 3927.12 and loss2: 0.00\n",
      "Epoch [288], train_loss: 3929.97 with loss1: 3929.97 and loss2: 0.00\n",
      "Epoch [289], train_loss: 3911.50 with loss1: 3911.50 and loss2: 0.00\n",
      "Epoch [290], train_loss: 3935.21 with loss1: 3935.21 and loss2: 0.00\n",
      "Epoch [291], train_loss: 3932.43 with loss1: 3932.43 and loss2: 0.00\n",
      "Epoch [292], train_loss: 3928.61 with loss1: 3928.61 and loss2: 0.00\n",
      "Epoch [293], train_loss: 3899.90 with loss1: 3899.90 and loss2: 0.00\n",
      "Epoch [294], train_loss: 3893.57 with loss1: 3893.57 and loss2: 0.00\n",
      "Epoch [295], train_loss: 3873.19 with loss1: 3873.19 and loss2: 0.00\n",
      "Epoch [296], train_loss: 3892.25 with loss1: 3892.25 and loss2: 0.00\n",
      "Epoch [297], train_loss: 3868.16 with loss1: 3868.16 and loss2: 0.00\n",
      "Epoch [298], train_loss: 3864.97 with loss1: 3864.97 and loss2: 0.00\n",
      "Epoch [299], train_loss: 3867.23 with loss1: 3867.23 and loss2: 0.00\n",
      "Epoch [300], train_loss: 3854.63 with loss1: 3854.63 and loss2: 0.00\n",
      "Epoch [301], train_loss: 3901.22 with loss1: 3901.22 and loss2: 0.00\n",
      "Epoch [302], train_loss: 3858.30 with loss1: 3858.30 and loss2: 0.00\n",
      "Epoch [303], train_loss: 3863.56 with loss1: 3863.56 and loss2: 0.00\n",
      "Epoch [304], train_loss: 3828.39 with loss1: 3828.39 and loss2: 0.00\n",
      "Epoch [305], train_loss: 3842.94 with loss1: 3842.94 and loss2: 0.00\n",
      "Epoch [306], train_loss: 3831.15 with loss1: 3831.15 and loss2: 0.00\n",
      "Epoch [307], train_loss: 3845.13 with loss1: 3845.13 and loss2: 0.00\n",
      "Epoch [308], train_loss: 3831.56 with loss1: 3831.56 and loss2: 0.00\n",
      "Epoch [309], train_loss: 3803.82 with loss1: 3803.82 and loss2: 0.00\n",
      "Epoch [310], train_loss: 3845.51 with loss1: 3845.51 and loss2: 0.00\n",
      "Epoch [311], train_loss: 3823.15 with loss1: 3823.15 and loss2: 0.00\n",
      "Epoch [312], train_loss: 3804.62 with loss1: 3804.62 and loss2: 0.00\n",
      "Epoch [313], train_loss: 3796.94 with loss1: 3796.94 and loss2: 0.00\n",
      "Epoch [314], train_loss: 3793.50 with loss1: 3793.50 and loss2: 0.00\n",
      "Epoch [315], train_loss: 3792.01 with loss1: 3792.01 and loss2: 0.00\n",
      "Epoch [316], train_loss: 3792.62 with loss1: 3792.62 and loss2: 0.00\n",
      "Epoch [317], train_loss: 3789.95 with loss1: 3789.95 and loss2: 0.00\n",
      "Epoch [318], train_loss: 3811.69 with loss1: 3811.69 and loss2: 0.00\n",
      "Epoch [319], train_loss: 3766.27 with loss1: 3766.27 and loss2: 0.00\n",
      "Epoch [320], train_loss: 3773.07 with loss1: 3773.07 and loss2: 0.00\n",
      "Epoch [321], train_loss: 3780.78 with loss1: 3780.78 and loss2: 0.00\n",
      "Epoch [322], train_loss: 3767.26 with loss1: 3767.26 and loss2: 0.00\n",
      "Epoch [323], train_loss: 3768.92 with loss1: 3768.92 and loss2: 0.00\n",
      "Epoch [324], train_loss: 3758.62 with loss1: 3758.62 and loss2: 0.00\n",
      "Epoch [325], train_loss: 3747.14 with loss1: 3747.14 and loss2: 0.00\n",
      "Epoch [326], train_loss: 3751.19 with loss1: 3751.19 and loss2: 0.00\n",
      "Epoch [327], train_loss: 3724.74 with loss1: 3724.74 and loss2: 0.00\n",
      "Epoch [328], train_loss: 3720.57 with loss1: 3720.57 and loss2: 0.00\n",
      "Epoch [329], train_loss: 3727.86 with loss1: 3727.86 and loss2: 0.00\n",
      "Epoch [330], train_loss: 3719.74 with loss1: 3719.74 and loss2: 0.00\n",
      "Epoch [331], train_loss: 3721.32 with loss1: 3721.32 and loss2: 0.00\n",
      "Epoch [332], train_loss: 3720.16 with loss1: 3720.16 and loss2: 0.00\n",
      "Epoch [333], train_loss: 3708.24 with loss1: 3708.24 and loss2: 0.00\n",
      "Epoch [334], train_loss: 3700.88 with loss1: 3700.88 and loss2: 0.00\n",
      "Epoch [335], train_loss: 3761.23 with loss1: 3761.23 and loss2: 0.00\n",
      "Epoch [336], train_loss: 3706.11 with loss1: 3706.11 and loss2: 0.00\n",
      "Epoch [337], train_loss: 3695.41 with loss1: 3695.41 and loss2: 0.00\n",
      "Epoch [338], train_loss: 3698.31 with loss1: 3698.31 and loss2: 0.00\n",
      "Epoch [339], train_loss: 3693.27 with loss1: 3693.27 and loss2: 0.00\n",
      "Epoch [340], train_loss: 3690.47 with loss1: 3690.47 and loss2: 0.00\n",
      "Epoch [341], train_loss: 3660.93 with loss1: 3660.93 and loss2: 0.00\n",
      "Epoch [342], train_loss: 3674.71 with loss1: 3674.71 and loss2: 0.00\n",
      "Epoch [343], train_loss: 3676.54 with loss1: 3676.54 and loss2: 0.00\n",
      "Epoch [344], train_loss: 3652.82 with loss1: 3652.82 and loss2: 0.00\n",
      "Epoch [345], train_loss: 3633.74 with loss1: 3633.74 and loss2: 0.00\n",
      "Epoch [346], train_loss: 3640.52 with loss1: 3640.52 and loss2: 0.00\n",
      "Epoch [347], train_loss: 3647.20 with loss1: 3647.20 and loss2: 0.00\n",
      "Epoch [348], train_loss: 3624.23 with loss1: 3624.23 and loss2: 0.00\n",
      "Epoch [349], train_loss: 3671.45 with loss1: 3671.45 and loss2: 0.00\n",
      "Epoch [350], train_loss: 3626.14 with loss1: 3626.14 and loss2: 0.00\n",
      "Epoch [351], train_loss: 3610.59 with loss1: 3610.59 and loss2: 0.00\n",
      "Epoch [352], train_loss: 3622.16 with loss1: 3622.16 and loss2: 0.00\n",
      "Epoch [353], train_loss: 3609.62 with loss1: 3609.62 and loss2: 0.00\n",
      "Epoch [354], train_loss: 3621.04 with loss1: 3621.04 and loss2: 0.00\n",
      "Epoch [355], train_loss: 3605.25 with loss1: 3605.25 and loss2: 0.00\n",
      "Epoch [356], train_loss: 3591.27 with loss1: 3591.27 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [357], train_loss: 3573.82 with loss1: 3573.82 and loss2: 0.00\n",
      "Epoch [358], train_loss: 3602.04 with loss1: 3602.04 and loss2: 0.00\n",
      "Epoch [359], train_loss: 3551.52 with loss1: 3551.52 and loss2: 0.00\n",
      "Epoch [360], train_loss: 3580.00 with loss1: 3580.00 and loss2: 0.00\n",
      "Epoch [361], train_loss: 3590.10 with loss1: 3590.10 and loss2: 0.00\n",
      "Epoch [362], train_loss: 3568.27 with loss1: 3568.27 and loss2: 0.00\n",
      "Epoch [363], train_loss: 3552.77 with loss1: 3552.77 and loss2: 0.00\n",
      "Epoch [364], train_loss: 3557.18 with loss1: 3557.18 and loss2: 0.00\n",
      "Epoch [365], train_loss: 3543.32 with loss1: 3543.32 and loss2: 0.00\n",
      "Epoch [366], train_loss: 3545.71 with loss1: 3545.71 and loss2: 0.00\n",
      "Epoch [367], train_loss: 3539.03 with loss1: 3539.03 and loss2: 0.00\n",
      "Epoch [368], train_loss: 3548.34 with loss1: 3548.34 and loss2: 0.00\n",
      "Epoch [369], train_loss: 3552.83 with loss1: 3552.83 and loss2: 0.00\n",
      "Epoch [370], train_loss: 3503.45 with loss1: 3503.45 and loss2: 0.00\n",
      "Epoch [371], train_loss: 3536.08 with loss1: 3536.08 and loss2: 0.00\n",
      "Epoch [372], train_loss: 3493.84 with loss1: 3493.84 and loss2: 0.00\n",
      "Epoch [373], train_loss: 3515.23 with loss1: 3515.23 and loss2: 0.00\n",
      "Epoch [374], train_loss: 3506.54 with loss1: 3506.54 and loss2: 0.00\n",
      "Epoch [375], train_loss: 3481.35 with loss1: 3481.35 and loss2: 0.00\n",
      "Epoch [376], train_loss: 3485.18 with loss1: 3485.18 and loss2: 0.00\n",
      "Epoch [377], train_loss: 3462.33 with loss1: 3462.33 and loss2: 0.00\n",
      "Epoch [378], train_loss: 3450.54 with loss1: 3450.54 and loss2: 0.00\n",
      "Epoch [379], train_loss: 3458.74 with loss1: 3458.74 and loss2: 0.00\n",
      "Epoch [380], train_loss: 3448.92 with loss1: 3448.92 and loss2: 0.00\n",
      "Epoch [381], train_loss: 3455.50 with loss1: 3455.50 and loss2: 0.00\n",
      "Epoch [382], train_loss: 3446.07 with loss1: 3446.07 and loss2: 0.00\n",
      "Epoch [383], train_loss: 3427.49 with loss1: 3427.49 and loss2: 0.00\n",
      "Epoch [384], train_loss: 3442.71 with loss1: 3442.71 and loss2: 0.00\n",
      "Epoch [385], train_loss: 3436.41 with loss1: 3436.41 and loss2: 0.00\n",
      "Epoch [386], train_loss: 3445.91 with loss1: 3445.91 and loss2: 0.00\n",
      "Epoch [387], train_loss: 3420.58 with loss1: 3420.58 and loss2: 0.00\n",
      "Epoch [388], train_loss: 3421.31 with loss1: 3421.31 and loss2: 0.00\n",
      "Epoch [389], train_loss: 3429.21 with loss1: 3429.21 and loss2: 0.00\n",
      "Epoch [390], train_loss: 3460.34 with loss1: 3460.34 and loss2: 0.00\n",
      "Epoch [391], train_loss: 3389.19 with loss1: 3389.19 and loss2: 0.00\n",
      "Epoch [392], train_loss: 3387.48 with loss1: 3387.48 and loss2: 0.00\n",
      "Epoch [393], train_loss: 3376.16 with loss1: 3376.16 and loss2: 0.00\n",
      "Epoch [394], train_loss: 3371.33 with loss1: 3371.33 and loss2: 0.00\n",
      "Epoch [395], train_loss: 3403.30 with loss1: 3403.30 and loss2: 0.00\n",
      "Epoch [396], train_loss: 3369.33 with loss1: 3369.33 and loss2: 0.00\n",
      "Epoch [397], train_loss: 3357.05 with loss1: 3357.05 and loss2: 0.00\n",
      "Epoch [398], train_loss: 3371.03 with loss1: 3371.03 and loss2: 0.00\n",
      "Epoch [399], train_loss: 3338.10 with loss1: 3338.10 and loss2: 0.00\n",
      "Epoch [400], train_loss: 3337.73 with loss1: 3337.73 and loss2: 0.00\n",
      "Epoch [401], train_loss: 3342.80 with loss1: 3342.80 and loss2: 0.00\n",
      "Epoch [402], train_loss: 3329.53 with loss1: 3329.53 and loss2: 0.00\n",
      "Epoch [403], train_loss: 3334.48 with loss1: 3334.48 and loss2: 0.00\n",
      "Epoch [404], train_loss: 3315.99 with loss1: 3315.99 and loss2: 0.00\n",
      "Epoch [405], train_loss: 3309.56 with loss1: 3309.56 and loss2: 0.00\n",
      "Epoch [406], train_loss: 3290.25 with loss1: 3290.25 and loss2: 0.00\n",
      "Epoch [407], train_loss: 3315.26 with loss1: 3315.26 and loss2: 0.00\n",
      "Epoch [408], train_loss: 3275.89 with loss1: 3275.89 and loss2: 0.00\n",
      "Epoch [409], train_loss: 3288.36 with loss1: 3288.36 and loss2: 0.00\n",
      "Epoch [410], train_loss: 3280.42 with loss1: 3280.42 and loss2: 0.00\n",
      "Epoch [411], train_loss: 3259.11 with loss1: 3259.11 and loss2: 0.00\n",
      "Epoch [412], train_loss: 3276.25 with loss1: 3276.25 and loss2: 0.00\n",
      "Epoch [413], train_loss: 3253.08 with loss1: 3253.08 and loss2: 0.00\n",
      "Epoch [414], train_loss: 3273.61 with loss1: 3273.61 and loss2: 0.00\n",
      "Epoch [415], train_loss: 3267.06 with loss1: 3267.06 and loss2: 0.00\n",
      "Epoch [416], train_loss: 3246.26 with loss1: 3246.26 and loss2: 0.00\n",
      "Epoch [417], train_loss: 3215.36 with loss1: 3215.36 and loss2: 0.00\n",
      "Epoch [418], train_loss: 3221.73 with loss1: 3221.73 and loss2: 0.00\n",
      "Epoch [419], train_loss: 3226.98 with loss1: 3226.98 and loss2: 0.00\n",
      "Epoch [420], train_loss: 3223.58 with loss1: 3223.58 and loss2: 0.00\n",
      "Epoch [421], train_loss: 3205.61 with loss1: 3205.61 and loss2: 0.00\n",
      "Epoch [422], train_loss: 3211.46 with loss1: 3211.46 and loss2: 0.00\n",
      "Epoch [423], train_loss: 3234.79 with loss1: 3234.79 and loss2: 0.00\n",
      "Epoch [424], train_loss: 3221.64 with loss1: 3221.64 and loss2: 0.00\n",
      "Epoch [425], train_loss: 3189.84 with loss1: 3189.84 and loss2: 0.00\n",
      "Epoch [426], train_loss: 3191.42 with loss1: 3191.42 and loss2: 0.00\n",
      "Epoch [427], train_loss: 3193.76 with loss1: 3193.76 and loss2: 0.00\n",
      "Epoch [428], train_loss: 3190.96 with loss1: 3190.96 and loss2: 0.00\n",
      "Epoch [429], train_loss: 3199.60 with loss1: 3199.60 and loss2: 0.00\n",
      "Epoch [430], train_loss: 3202.02 with loss1: 3202.02 and loss2: 0.00\n",
      "Epoch [431], train_loss: 3162.99 with loss1: 3162.99 and loss2: 0.00\n",
      "Epoch [432], train_loss: 3180.74 with loss1: 3180.74 and loss2: 0.00\n",
      "Epoch [433], train_loss: 3178.25 with loss1: 3178.25 and loss2: 0.00\n",
      "Epoch [434], train_loss: 3172.22 with loss1: 3172.22 and loss2: 0.00\n",
      "Epoch [435], train_loss: 3176.39 with loss1: 3176.39 and loss2: 0.00\n",
      "Epoch [436], train_loss: 3184.77 with loss1: 3184.77 and loss2: 0.00\n",
      "Epoch [437], train_loss: 3192.28 with loss1: 3192.28 and loss2: 0.00\n",
      "Epoch [438], train_loss: 3195.08 with loss1: 3195.08 and loss2: 0.00\n",
      "Epoch [439], train_loss: 3189.01 with loss1: 3189.01 and loss2: 0.00\n",
      "Epoch [440], train_loss: 3189.02 with loss1: 3189.02 and loss2: 0.00\n",
      "Epoch [441], train_loss: 3224.24 with loss1: 3224.24 and loss2: 0.00\n",
      "Epoch [442], train_loss: 3227.79 with loss1: 3227.79 and loss2: 0.00\n",
      "Epoch [443], train_loss: 3227.59 with loss1: 3227.59 and loss2: 0.00\n",
      "Epoch [444], train_loss: 3264.73 with loss1: 3264.73 and loss2: 0.00\n",
      "Epoch [445], train_loss: 3282.92 with loss1: 3282.92 and loss2: 0.00\n",
      "Epoch [446], train_loss: 3297.38 with loss1: 3297.38 and loss2: 0.00\n",
      "Epoch [447], train_loss: 3319.31 with loss1: 3319.31 and loss2: 0.00\n",
      "Epoch [448], train_loss: 3306.09 with loss1: 3306.09 and loss2: 0.00\n",
      "Epoch [449], train_loss: 3339.13 with loss1: 3339.13 and loss2: 0.00\n",
      "Epoch [450], train_loss: 3336.04 with loss1: 3336.04 and loss2: 0.00\n",
      "Epoch [451], train_loss: 3345.37 with loss1: 3345.37 and loss2: 0.00\n",
      "Epoch [452], train_loss: 3379.72 with loss1: 3379.72 and loss2: 0.00\n",
      "Epoch [453], train_loss: 3325.15 with loss1: 3325.15 and loss2: 0.00\n",
      "Epoch [454], train_loss: 3352.41 with loss1: 3352.41 and loss2: 0.00\n",
      "Epoch [455], train_loss: 3297.88 with loss1: 3297.88 and loss2: 0.00\n",
      "Epoch [456], train_loss: 3319.56 with loss1: 3319.56 and loss2: 0.00\n",
      "Epoch [457], train_loss: 3287.93 with loss1: 3287.93 and loss2: 0.00\n",
      "Epoch [458], train_loss: 3266.47 with loss1: 3266.47 and loss2: 0.00\n",
      "Epoch [459], train_loss: 3242.60 with loss1: 3242.60 and loss2: 0.00\n",
      "Epoch [460], train_loss: 3246.71 with loss1: 3246.71 and loss2: 0.00\n",
      "Epoch [461], train_loss: 3185.97 with loss1: 3185.97 and loss2: 0.00\n",
      "Epoch [462], train_loss: 3198.18 with loss1: 3198.18 and loss2: 0.00\n",
      "Epoch [463], train_loss: 3177.81 with loss1: 3177.81 and loss2: 0.00\n",
      "Epoch [464], train_loss: 3152.40 with loss1: 3152.40 and loss2: 0.00\n",
      "Epoch [465], train_loss: 3122.38 with loss1: 3122.38 and loss2: 0.00\n",
      "Epoch [466], train_loss: 3117.51 with loss1: 3117.51 and loss2: 0.00\n",
      "Epoch [467], train_loss: 3099.35 with loss1: 3099.35 and loss2: 0.00\n",
      "Epoch [468], train_loss: 3080.02 with loss1: 3080.02 and loss2: 0.00\n",
      "Epoch [469], train_loss: 3079.47 with loss1: 3079.47 and loss2: 0.00\n",
      "Epoch [470], train_loss: 3098.48 with loss1: 3098.48 and loss2: 0.00\n",
      "Epoch [471], train_loss: 3058.39 with loss1: 3058.39 and loss2: 0.00\n",
      "Epoch [472], train_loss: 3048.13 with loss1: 3048.13 and loss2: 0.00\n",
      "Epoch [473], train_loss: 3011.16 with loss1: 3011.16 and loss2: 0.00\n",
      "Epoch [474], train_loss: 3014.41 with loss1: 3014.41 and loss2: 0.00\n",
      "Epoch [475], train_loss: 3038.40 with loss1: 3038.40 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [476], train_loss: 3032.65 with loss1: 3032.65 and loss2: 0.00\n",
      "Epoch [477], train_loss: 3001.40 with loss1: 3001.40 and loss2: 0.00\n",
      "Epoch [478], train_loss: 3009.60 with loss1: 3009.60 and loss2: 0.00\n",
      "Epoch [479], train_loss: 3012.46 with loss1: 3012.46 and loss2: 0.00\n",
      "Epoch [480], train_loss: 2982.30 with loss1: 2982.30 and loss2: 0.00\n",
      "Epoch [481], train_loss: 2996.72 with loss1: 2996.72 and loss2: 0.00\n",
      "Epoch [482], train_loss: 2986.46 with loss1: 2986.46 and loss2: 0.00\n",
      "Epoch [483], train_loss: 2982.80 with loss1: 2982.80 and loss2: 0.00\n",
      "Epoch [484], train_loss: 2982.13 with loss1: 2982.13 and loss2: 0.00\n",
      "Epoch [485], train_loss: 2980.78 with loss1: 2980.78 and loss2: 0.00\n",
      "Epoch [486], train_loss: 2994.60 with loss1: 2994.60 and loss2: 0.00\n",
      "Epoch [487], train_loss: 2954.38 with loss1: 2954.38 and loss2: 0.00\n",
      "Epoch [488], train_loss: 2948.99 with loss1: 2948.99 and loss2: 0.00\n",
      "Epoch [489], train_loss: 2965.72 with loss1: 2965.72 and loss2: 0.00\n",
      "Epoch [490], train_loss: 2946.99 with loss1: 2946.99 and loss2: 0.00\n",
      "Epoch [491], train_loss: 2920.07 with loss1: 2920.07 and loss2: 0.00\n",
      "Epoch [492], train_loss: 2939.54 with loss1: 2939.54 and loss2: 0.00\n",
      "Epoch [493], train_loss: 2928.88 with loss1: 2928.88 and loss2: 0.00\n",
      "Epoch [494], train_loss: 2925.26 with loss1: 2925.26 and loss2: 0.00\n",
      "Epoch [495], train_loss: 2932.72 with loss1: 2932.72 and loss2: 0.00\n",
      "Epoch [496], train_loss: 2924.64 with loss1: 2924.64 and loss2: 0.00\n",
      "Epoch [497], train_loss: 2901.22 with loss1: 2901.22 and loss2: 0.00\n",
      "Epoch [498], train_loss: 2938.98 with loss1: 2938.98 and loss2: 0.00\n",
      "Epoch [499], train_loss: 2926.49 with loss1: 2926.49 and loss2: 0.00\n",
      "Epoch [500], train_loss: 2929.95 with loss1: 2929.95 and loss2: 0.00\n",
      "Epoch [501], train_loss: 2939.36 with loss1: 2939.36 and loss2: 0.00\n",
      "Epoch [502], train_loss: 2922.39 with loss1: 2922.39 and loss2: 0.00\n",
      "Epoch [503], train_loss: 2946.01 with loss1: 2946.01 and loss2: 0.00\n",
      "Epoch [504], train_loss: 2914.38 with loss1: 2914.38 and loss2: 0.00\n",
      "Epoch [505], train_loss: 2916.66 with loss1: 2916.66 and loss2: 0.00\n",
      "Epoch [506], train_loss: 2977.07 with loss1: 2977.07 and loss2: 0.00\n",
      "Epoch [507], train_loss: 2908.51 with loss1: 2908.51 and loss2: 0.00\n",
      "Epoch [508], train_loss: 2920.15 with loss1: 2920.15 and loss2: 0.00\n",
      "Epoch [509], train_loss: 2901.42 with loss1: 2901.42 and loss2: 0.00\n",
      "Epoch [510], train_loss: 2898.81 with loss1: 2898.81 and loss2: 0.00\n",
      "Epoch [511], train_loss: 2894.22 with loss1: 2894.22 and loss2: 0.00\n",
      "Epoch [512], train_loss: 2888.80 with loss1: 2888.80 and loss2: 0.00\n",
      "Epoch [513], train_loss: 2871.42 with loss1: 2871.42 and loss2: 0.00\n",
      "Epoch [514], train_loss: 2869.55 with loss1: 2869.55 and loss2: 0.00\n",
      "Epoch [515], train_loss: 2866.43 with loss1: 2866.43 and loss2: 0.00\n",
      "Epoch [516], train_loss: 2887.28 with loss1: 2887.28 and loss2: 0.00\n",
      "Epoch [517], train_loss: 2888.51 with loss1: 2888.51 and loss2: 0.00\n",
      "Epoch [518], train_loss: 2861.88 with loss1: 2861.88 and loss2: 0.00\n",
      "Epoch [519], train_loss: 2842.88 with loss1: 2842.88 and loss2: 0.00\n",
      "Epoch [520], train_loss: 2860.97 with loss1: 2860.97 and loss2: 0.00\n",
      "Epoch [521], train_loss: 2850.65 with loss1: 2850.65 and loss2: 0.00\n",
      "Epoch [522], train_loss: 2833.18 with loss1: 2833.18 and loss2: 0.00\n",
      "Epoch [523], train_loss: 2856.57 with loss1: 2856.57 and loss2: 0.00\n",
      "Epoch [524], train_loss: 2849.61 with loss1: 2849.61 and loss2: 0.00\n",
      "Epoch [525], train_loss: 2861.32 with loss1: 2861.32 and loss2: 0.00\n",
      "Epoch [526], train_loss: 2823.97 with loss1: 2823.97 and loss2: 0.00\n",
      "Epoch [527], train_loss: 2840.54 with loss1: 2840.54 and loss2: 0.00\n",
      "Epoch [528], train_loss: 2800.23 with loss1: 2800.23 and loss2: 0.00\n",
      "Epoch [529], train_loss: 2796.69 with loss1: 2796.69 and loss2: 0.00\n",
      "Epoch [530], train_loss: 2829.54 with loss1: 2829.54 and loss2: 0.00\n",
      "Epoch [531], train_loss: 2790.16 with loss1: 2790.16 and loss2: 0.00\n",
      "Epoch [532], train_loss: 2795.92 with loss1: 2795.92 and loss2: 0.00\n",
      "Epoch [533], train_loss: 2802.37 with loss1: 2802.37 and loss2: 0.00\n",
      "Epoch [534], train_loss: 2795.69 with loss1: 2795.69 and loss2: 0.00\n",
      "Epoch [535], train_loss: 2772.40 with loss1: 2772.40 and loss2: 0.00\n",
      "Epoch [536], train_loss: 2811.04 with loss1: 2811.04 and loss2: 0.00\n",
      "Epoch [537], train_loss: 2772.45 with loss1: 2772.45 and loss2: 0.00\n",
      "Epoch [538], train_loss: 2764.58 with loss1: 2764.58 and loss2: 0.00\n",
      "Epoch [539], train_loss: 2760.79 with loss1: 2760.79 and loss2: 0.00\n",
      "Epoch [540], train_loss: 2800.85 with loss1: 2800.85 and loss2: 0.00\n",
      "Epoch [541], train_loss: 2750.16 with loss1: 2750.16 and loss2: 0.00\n",
      "Epoch [542], train_loss: 2741.13 with loss1: 2741.13 and loss2: 0.00\n",
      "Epoch [543], train_loss: 2749.47 with loss1: 2749.47 and loss2: 0.00\n",
      "Epoch [544], train_loss: 2731.89 with loss1: 2731.89 and loss2: 0.00\n",
      "Epoch [545], train_loss: 2743.53 with loss1: 2743.53 and loss2: 0.00\n",
      "Epoch [546], train_loss: 2733.06 with loss1: 2733.06 and loss2: 0.00\n",
      "Epoch [547], train_loss: 2729.29 with loss1: 2729.29 and loss2: 0.00\n",
      "Epoch [548], train_loss: 2706.18 with loss1: 2706.18 and loss2: 0.00\n",
      "Epoch [549], train_loss: 2727.26 with loss1: 2727.26 and loss2: 0.00\n",
      "Epoch [550], train_loss: 2695.11 with loss1: 2695.11 and loss2: 0.00\n",
      "Epoch [551], train_loss: 2756.50 with loss1: 2756.50 and loss2: 0.00\n",
      "Epoch [552], train_loss: 2693.47 with loss1: 2693.47 and loss2: 0.00\n",
      "Epoch [553], train_loss: 2701.44 with loss1: 2701.44 and loss2: 0.00\n",
      "Epoch [554], train_loss: 2682.56 with loss1: 2682.56 and loss2: 0.00\n",
      "Epoch [555], train_loss: 2711.53 with loss1: 2711.53 and loss2: 0.00\n",
      "Epoch [556], train_loss: 2696.82 with loss1: 2696.82 and loss2: 0.00\n",
      "Epoch [557], train_loss: 2680.14 with loss1: 2680.14 and loss2: 0.00\n",
      "Epoch [558], train_loss: 2662.10 with loss1: 2662.10 and loss2: 0.00\n",
      "Epoch [559], train_loss: 2675.57 with loss1: 2675.57 and loss2: 0.00\n",
      "Epoch [560], train_loss: 2667.65 with loss1: 2667.65 and loss2: 0.00\n",
      "Epoch [561], train_loss: 2671.98 with loss1: 2671.98 and loss2: 0.00\n",
      "Epoch [562], train_loss: 2652.46 with loss1: 2652.46 and loss2: 0.00\n",
      "Epoch [563], train_loss: 2654.88 with loss1: 2654.88 and loss2: 0.00\n",
      "Epoch [564], train_loss: 2649.51 with loss1: 2649.51 and loss2: 0.00\n",
      "Epoch [565], train_loss: 2634.26 with loss1: 2634.26 and loss2: 0.00\n",
      "Epoch [566], train_loss: 2625.20 with loss1: 2625.20 and loss2: 0.00\n",
      "Epoch [567], train_loss: 2639.61 with loss1: 2639.61 and loss2: 0.00\n",
      "Epoch [568], train_loss: 2655.56 with loss1: 2655.56 and loss2: 0.00\n",
      "Epoch [569], train_loss: 2642.61 with loss1: 2642.61 and loss2: 0.00\n",
      "Epoch [570], train_loss: 2689.51 with loss1: 2689.51 and loss2: 0.00\n",
      "Epoch [571], train_loss: 2648.93 with loss1: 2648.93 and loss2: 0.00\n",
      "Epoch [572], train_loss: 2615.05 with loss1: 2615.05 and loss2: 0.00\n",
      "Epoch [573], train_loss: 2625.84 with loss1: 2625.84 and loss2: 0.00\n",
      "Epoch [574], train_loss: 2623.98 with loss1: 2623.98 and loss2: 0.00\n",
      "Epoch [575], train_loss: 2636.49 with loss1: 2636.49 and loss2: 0.00\n",
      "Epoch [576], train_loss: 2625.25 with loss1: 2625.25 and loss2: 0.00\n",
      "Epoch [577], train_loss: 2622.37 with loss1: 2622.37 and loss2: 0.00\n",
      "Epoch [578], train_loss: 2623.07 with loss1: 2623.07 and loss2: 0.00\n",
      "Epoch [579], train_loss: 2601.41 with loss1: 2601.41 and loss2: 0.00\n",
      "Epoch [580], train_loss: 2621.14 with loss1: 2621.14 and loss2: 0.00\n",
      "Epoch [581], train_loss: 2636.68 with loss1: 2636.68 and loss2: 0.00\n",
      "Epoch [582], train_loss: 2610.17 with loss1: 2610.17 and loss2: 0.00\n",
      "Epoch [583], train_loss: 2612.12 with loss1: 2612.12 and loss2: 0.00\n",
      "Epoch [584], train_loss: 2613.29 with loss1: 2613.29 and loss2: 0.00\n",
      "Epoch [585], train_loss: 2610.80 with loss1: 2610.80 and loss2: 0.00\n",
      "Epoch [586], train_loss: 2613.18 with loss1: 2613.18 and loss2: 0.00\n",
      "Epoch [587], train_loss: 2620.43 with loss1: 2620.43 and loss2: 0.00\n",
      "Epoch [588], train_loss: 2615.86 with loss1: 2615.86 and loss2: 0.00\n",
      "Epoch [589], train_loss: 2605.10 with loss1: 2605.10 and loss2: 0.00\n",
      "Epoch [590], train_loss: 2616.84 with loss1: 2616.84 and loss2: 0.00\n",
      "Epoch [591], train_loss: 2598.08 with loss1: 2598.08 and loss2: 0.00\n",
      "Epoch [592], train_loss: 2626.43 with loss1: 2626.43 and loss2: 0.00\n",
      "Epoch [593], train_loss: 2614.46 with loss1: 2614.46 and loss2: 0.00\n",
      "Epoch [594], train_loss: 2622.40 with loss1: 2622.40 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [595], train_loss: 2615.26 with loss1: 2615.26 and loss2: 0.00\n",
      "Epoch [596], train_loss: 2622.03 with loss1: 2622.03 and loss2: 0.00\n",
      "Epoch [597], train_loss: 2588.92 with loss1: 2588.92 and loss2: 0.00\n",
      "Epoch [598], train_loss: 2598.06 with loss1: 2598.06 and loss2: 0.00\n",
      "Epoch [599], train_loss: 2598.27 with loss1: 2598.27 and loss2: 0.00\n",
      "Epoch [600], train_loss: 2583.65 with loss1: 2583.65 and loss2: 0.00\n",
      "Epoch [601], train_loss: 2583.72 with loss1: 2583.72 and loss2: 0.00\n",
      "Epoch [602], train_loss: 2570.46 with loss1: 2570.46 and loss2: 0.00\n",
      "Epoch [603], train_loss: 2541.94 with loss1: 2541.94 and loss2: 0.00\n",
      "Epoch [604], train_loss: 2565.88 with loss1: 2565.88 and loss2: 0.00\n",
      "Epoch [605], train_loss: 2549.17 with loss1: 2549.17 and loss2: 0.00\n",
      "Epoch [606], train_loss: 2522.81 with loss1: 2522.81 and loss2: 0.00\n",
      "Epoch [607], train_loss: 2540.06 with loss1: 2540.06 and loss2: 0.00\n",
      "Epoch [608], train_loss: 2526.33 with loss1: 2526.33 and loss2: 0.00\n",
      "Epoch [609], train_loss: 2519.78 with loss1: 2519.78 and loss2: 0.00\n",
      "Epoch [610], train_loss: 2513.34 with loss1: 2513.34 and loss2: 0.00\n",
      "Epoch [611], train_loss: 2516.32 with loss1: 2516.32 and loss2: 0.00\n",
      "Epoch [612], train_loss: 2522.65 with loss1: 2522.65 and loss2: 0.00\n",
      "Epoch [613], train_loss: 2516.88 with loss1: 2516.88 and loss2: 0.00\n",
      "Epoch [614], train_loss: 2496.00 with loss1: 2496.00 and loss2: 0.00\n",
      "Epoch [615], train_loss: 2467.78 with loss1: 2467.78 and loss2: 0.00\n",
      "Epoch [616], train_loss: 2485.27 with loss1: 2485.27 and loss2: 0.00\n",
      "Epoch [617], train_loss: 2504.06 with loss1: 2504.06 and loss2: 0.00\n",
      "Epoch [618], train_loss: 2497.85 with loss1: 2497.85 and loss2: 0.00\n",
      "Epoch [619], train_loss: 2455.20 with loss1: 2455.20 and loss2: 0.00\n",
      "Epoch [620], train_loss: 2492.73 with loss1: 2492.73 and loss2: 0.00\n",
      "Epoch [621], train_loss: 2464.35 with loss1: 2464.35 and loss2: 0.00\n",
      "Epoch [622], train_loss: 2486.65 with loss1: 2486.65 and loss2: 0.00\n",
      "Epoch [623], train_loss: 2494.10 with loss1: 2494.10 and loss2: 0.00\n",
      "Epoch [624], train_loss: 2468.75 with loss1: 2468.75 and loss2: 0.00\n",
      "Epoch [625], train_loss: 2459.87 with loss1: 2459.87 and loss2: 0.00\n",
      "Epoch [626], train_loss: 2453.09 with loss1: 2453.09 and loss2: 0.00\n",
      "Epoch [627], train_loss: 2467.10 with loss1: 2467.10 and loss2: 0.00\n",
      "Epoch [628], train_loss: 2443.32 with loss1: 2443.32 and loss2: 0.00\n",
      "Epoch [629], train_loss: 2468.80 with loss1: 2468.80 and loss2: 0.00\n",
      "Epoch [630], train_loss: 2458.46 with loss1: 2458.46 and loss2: 0.00\n",
      "Epoch [631], train_loss: 2459.36 with loss1: 2459.36 and loss2: 0.00\n",
      "Epoch [632], train_loss: 2476.66 with loss1: 2476.66 and loss2: 0.00\n",
      "Epoch [633], train_loss: 2485.94 with loss1: 2485.94 and loss2: 0.00\n",
      "Epoch [634], train_loss: 2468.07 with loss1: 2468.07 and loss2: 0.00\n",
      "Epoch [635], train_loss: 2470.49 with loss1: 2470.49 and loss2: 0.00\n",
      "Epoch [636], train_loss: 2456.99 with loss1: 2456.99 and loss2: 0.00\n",
      "Epoch [637], train_loss: 2452.16 with loss1: 2452.16 and loss2: 0.00\n",
      "Epoch [638], train_loss: 2421.50 with loss1: 2421.50 and loss2: 0.00\n",
      "Epoch [639], train_loss: 2445.89 with loss1: 2445.89 and loss2: 0.00\n",
      "Epoch [640], train_loss: 2421.01 with loss1: 2421.01 and loss2: 0.00\n",
      "Epoch [641], train_loss: 2443.73 with loss1: 2443.73 and loss2: 0.00\n",
      "Epoch [642], train_loss: 2423.31 with loss1: 2423.31 and loss2: 0.00\n",
      "Epoch [643], train_loss: 2413.17 with loss1: 2413.17 and loss2: 0.00\n",
      "Epoch [644], train_loss: 2411.39 with loss1: 2411.39 and loss2: 0.00\n",
      "Epoch [645], train_loss: 2415.99 with loss1: 2415.99 and loss2: 0.00\n",
      "Epoch [646], train_loss: 2418.58 with loss1: 2418.58 and loss2: 0.00\n",
      "Epoch [647], train_loss: 2425.59 with loss1: 2425.59 and loss2: 0.00\n",
      "Epoch [648], train_loss: 2451.22 with loss1: 2451.22 and loss2: 0.00\n",
      "Epoch [649], train_loss: 2443.12 with loss1: 2443.12 and loss2: 0.00\n",
      "Epoch [650], train_loss: 2428.56 with loss1: 2428.56 and loss2: 0.00\n",
      "Epoch [651], train_loss: 2427.98 with loss1: 2427.98 and loss2: 0.00\n",
      "Epoch [652], train_loss: 2419.08 with loss1: 2419.08 and loss2: 0.00\n",
      "Epoch [653], train_loss: 2429.15 with loss1: 2429.15 and loss2: 0.00\n",
      "Epoch [654], train_loss: 2412.01 with loss1: 2412.01 and loss2: 0.00\n",
      "Epoch [655], train_loss: 2416.93 with loss1: 2416.93 and loss2: 0.00\n",
      "Epoch [656], train_loss: 2418.86 with loss1: 2418.86 and loss2: 0.00\n",
      "Epoch [657], train_loss: 2408.16 with loss1: 2408.16 and loss2: 0.00\n",
      "Epoch [658], train_loss: 2412.16 with loss1: 2412.16 and loss2: 0.00\n",
      "Epoch [659], train_loss: 2434.32 with loss1: 2434.32 and loss2: 0.00\n",
      "Epoch [660], train_loss: 2410.73 with loss1: 2410.73 and loss2: 0.00\n",
      "Epoch [661], train_loss: 2404.17 with loss1: 2404.17 and loss2: 0.00\n",
      "Epoch [662], train_loss: 2404.80 with loss1: 2404.80 and loss2: 0.00\n",
      "Epoch [663], train_loss: 2409.64 with loss1: 2409.64 and loss2: 0.00\n",
      "Epoch [664], train_loss: 2384.05 with loss1: 2384.05 and loss2: 0.00\n",
      "Epoch [665], train_loss: 2377.72 with loss1: 2377.72 and loss2: 0.00\n",
      "Epoch [666], train_loss: 2379.69 with loss1: 2379.69 and loss2: 0.00\n",
      "Epoch [667], train_loss: 2385.15 with loss1: 2385.15 and loss2: 0.00\n",
      "Epoch [668], train_loss: 2399.93 with loss1: 2399.93 and loss2: 0.00\n",
      "Epoch [669], train_loss: 2375.34 with loss1: 2375.34 and loss2: 0.00\n",
      "Epoch [670], train_loss: 2370.20 with loss1: 2370.20 and loss2: 0.00\n",
      "Epoch [671], train_loss: 2384.24 with loss1: 2384.24 and loss2: 0.00\n",
      "Epoch [672], train_loss: 2366.69 with loss1: 2366.69 and loss2: 0.00\n",
      "Epoch [673], train_loss: 2360.25 with loss1: 2360.25 and loss2: 0.00\n",
      "Epoch [674], train_loss: 2369.20 with loss1: 2369.20 and loss2: 0.00\n",
      "Epoch [675], train_loss: 2363.64 with loss1: 2363.64 and loss2: 0.00\n",
      "Epoch [676], train_loss: 2370.52 with loss1: 2370.52 and loss2: 0.00\n",
      "Epoch [677], train_loss: 2353.19 with loss1: 2353.19 and loss2: 0.00\n",
      "Epoch [678], train_loss: 2341.50 with loss1: 2341.50 and loss2: 0.00\n",
      "Epoch [679], train_loss: 2368.38 with loss1: 2368.38 and loss2: 0.00\n",
      "Epoch [680], train_loss: 2343.88 with loss1: 2343.88 and loss2: 0.00\n",
      "Epoch [681], train_loss: 2324.10 with loss1: 2324.10 and loss2: 0.00\n",
      "Epoch [682], train_loss: 2339.09 with loss1: 2339.09 and loss2: 0.00\n",
      "Epoch [683], train_loss: 2321.94 with loss1: 2321.94 and loss2: 0.00\n",
      "Epoch [684], train_loss: 2325.65 with loss1: 2325.65 and loss2: 0.00\n",
      "Epoch [685], train_loss: 2290.29 with loss1: 2290.29 and loss2: 0.00\n",
      "Epoch [686], train_loss: 2315.35 with loss1: 2315.35 and loss2: 0.00\n",
      "Epoch [687], train_loss: 2305.36 with loss1: 2305.36 and loss2: 0.00\n",
      "Epoch [688], train_loss: 2345.53 with loss1: 2345.53 and loss2: 0.00\n",
      "Epoch [689], train_loss: 2303.91 with loss1: 2303.91 and loss2: 0.00\n",
      "Epoch [690], train_loss: 2304.46 with loss1: 2304.46 and loss2: 0.00\n",
      "Epoch [691], train_loss: 2326.46 with loss1: 2326.46 and loss2: 0.00\n",
      "Epoch [692], train_loss: 2316.67 with loss1: 2316.67 and loss2: 0.00\n",
      "Epoch [693], train_loss: 2301.37 with loss1: 2301.37 and loss2: 0.00\n",
      "Epoch [694], train_loss: 2311.81 with loss1: 2311.81 and loss2: 0.00\n",
      "Epoch [695], train_loss: 2312.78 with loss1: 2312.78 and loss2: 0.00\n",
      "Epoch [696], train_loss: 2300.31 with loss1: 2300.31 and loss2: 0.00\n",
      "Epoch [697], train_loss: 2290.45 with loss1: 2290.45 and loss2: 0.00\n",
      "Epoch [698], train_loss: 2330.88 with loss1: 2330.88 and loss2: 0.00\n",
      "Epoch [699], train_loss: 2310.88 with loss1: 2310.88 and loss2: 0.00\n",
      "Epoch [700], train_loss: 2316.42 with loss1: 2316.42 and loss2: 0.00\n",
      "Epoch [701], train_loss: 2298.17 with loss1: 2298.17 and loss2: 0.00\n",
      "Epoch [702], train_loss: 2301.44 with loss1: 2301.44 and loss2: 0.00\n",
      "Epoch [703], train_loss: 2288.59 with loss1: 2288.59 and loss2: 0.00\n",
      "Epoch [704], train_loss: 2287.40 with loss1: 2287.40 and loss2: 0.00\n",
      "Epoch [705], train_loss: 2308.03 with loss1: 2308.03 and loss2: 0.00\n",
      "Epoch [706], train_loss: 2296.09 with loss1: 2296.09 and loss2: 0.00\n",
      "Epoch [707], train_loss: 2288.22 with loss1: 2288.22 and loss2: 0.00\n",
      "Epoch [708], train_loss: 2300.88 with loss1: 2300.88 and loss2: 0.00\n",
      "Epoch [709], train_loss: 2273.65 with loss1: 2273.65 and loss2: 0.00\n",
      "Epoch [710], train_loss: 2276.19 with loss1: 2276.19 and loss2: 0.00\n",
      "Epoch [711], train_loss: 2280.80 with loss1: 2280.80 and loss2: 0.00\n",
      "Epoch [712], train_loss: 2264.63 with loss1: 2264.63 and loss2: 0.00\n",
      "Epoch [713], train_loss: 2278.98 with loss1: 2278.98 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [714], train_loss: 2270.14 with loss1: 2270.14 and loss2: 0.00\n",
      "Epoch [715], train_loss: 2279.73 with loss1: 2279.73 and loss2: 0.00\n",
      "Epoch [716], train_loss: 2278.88 with loss1: 2278.88 and loss2: 0.00\n",
      "Epoch [717], train_loss: 2269.22 with loss1: 2269.22 and loss2: 0.00\n",
      "Epoch [718], train_loss: 2266.90 with loss1: 2266.90 and loss2: 0.00\n",
      "Epoch [719], train_loss: 2245.54 with loss1: 2245.54 and loss2: 0.00\n",
      "Epoch [720], train_loss: 2260.12 with loss1: 2260.12 and loss2: 0.00\n",
      "Epoch [721], train_loss: 2281.33 with loss1: 2281.33 and loss2: 0.00\n",
      "Epoch [722], train_loss: 2253.98 with loss1: 2253.98 and loss2: 0.00\n",
      "Epoch [723], train_loss: 2255.88 with loss1: 2255.88 and loss2: 0.00\n",
      "Epoch [724], train_loss: 2244.32 with loss1: 2244.32 and loss2: 0.00\n",
      "Epoch [725], train_loss: 2255.43 with loss1: 2255.43 and loss2: 0.00\n",
      "Epoch [726], train_loss: 2244.94 with loss1: 2244.94 and loss2: 0.00\n",
      "Epoch [727], train_loss: 2237.56 with loss1: 2237.56 and loss2: 0.00\n",
      "Epoch [728], train_loss: 2238.47 with loss1: 2238.47 and loss2: 0.00\n",
      "Epoch [729], train_loss: 2253.04 with loss1: 2253.04 and loss2: 0.00\n",
      "Epoch [730], train_loss: 2233.32 with loss1: 2233.32 and loss2: 0.00\n",
      "Epoch [731], train_loss: 2216.64 with loss1: 2216.64 and loss2: 0.00\n",
      "Epoch [732], train_loss: 2248.69 with loss1: 2248.69 and loss2: 0.00\n",
      "Epoch [733], train_loss: 2225.55 with loss1: 2225.55 and loss2: 0.00\n",
      "Epoch [734], train_loss: 2234.04 with loss1: 2234.04 and loss2: 0.00\n",
      "Epoch [735], train_loss: 2237.49 with loss1: 2237.49 and loss2: 0.00\n",
      "Epoch [736], train_loss: 2236.89 with loss1: 2236.89 and loss2: 0.00\n",
      "Epoch [737], train_loss: 2225.39 with loss1: 2225.39 and loss2: 0.00\n",
      "Epoch [738], train_loss: 2230.23 with loss1: 2230.23 and loss2: 0.00\n",
      "Epoch [739], train_loss: 2195.61 with loss1: 2195.61 and loss2: 0.00\n",
      "Epoch [740], train_loss: 2207.94 with loss1: 2207.94 and loss2: 0.00\n",
      "Epoch [741], train_loss: 2205.26 with loss1: 2205.26 and loss2: 0.00\n",
      "Epoch [742], train_loss: 2225.86 with loss1: 2225.86 and loss2: 0.00\n",
      "Epoch [743], train_loss: 2225.59 with loss1: 2225.59 and loss2: 0.00\n",
      "Epoch [744], train_loss: 2213.24 with loss1: 2213.24 and loss2: 0.00\n",
      "Epoch [745], train_loss: 2192.94 with loss1: 2192.94 and loss2: 0.00\n",
      "Epoch [746], train_loss: 2199.10 with loss1: 2199.10 and loss2: 0.00\n",
      "Epoch [747], train_loss: 2208.34 with loss1: 2208.34 and loss2: 0.00\n",
      "Epoch [748], train_loss: 2210.12 with loss1: 2210.12 and loss2: 0.00\n",
      "Epoch [749], train_loss: 2192.59 with loss1: 2192.59 and loss2: 0.00\n",
      "Epoch [750], train_loss: 2194.10 with loss1: 2194.10 and loss2: 0.00\n",
      "Epoch [751], train_loss: 2198.33 with loss1: 2198.33 and loss2: 0.00\n",
      "Epoch [752], train_loss: 2189.75 with loss1: 2189.75 and loss2: 0.00\n",
      "Epoch [753], train_loss: 2186.62 with loss1: 2186.62 and loss2: 0.00\n",
      "Epoch [754], train_loss: 2194.55 with loss1: 2194.55 and loss2: 0.00\n",
      "Epoch [755], train_loss: 2191.12 with loss1: 2191.12 and loss2: 0.00\n",
      "Epoch [756], train_loss: 2174.47 with loss1: 2174.47 and loss2: 0.00\n",
      "Epoch [757], train_loss: 2176.37 with loss1: 2176.37 and loss2: 0.00\n",
      "Epoch [758], train_loss: 2170.04 with loss1: 2170.04 and loss2: 0.00\n",
      "Epoch [759], train_loss: 2184.33 with loss1: 2184.33 and loss2: 0.00\n",
      "Epoch [760], train_loss: 2183.65 with loss1: 2183.65 and loss2: 0.00\n",
      "Epoch [761], train_loss: 2166.53 with loss1: 2166.53 and loss2: 0.00\n",
      "Epoch [762], train_loss: 2169.99 with loss1: 2169.99 and loss2: 0.00\n",
      "Epoch [763], train_loss: 2166.29 with loss1: 2166.29 and loss2: 0.00\n",
      "Epoch [764], train_loss: 2181.74 with loss1: 2181.74 and loss2: 0.00\n",
      "Epoch [765], train_loss: 2161.08 with loss1: 2161.08 and loss2: 0.00\n",
      "Epoch [766], train_loss: 2174.93 with loss1: 2174.93 and loss2: 0.00\n",
      "Epoch [767], train_loss: 2177.92 with loss1: 2177.92 and loss2: 0.00\n",
      "Epoch [768], train_loss: 2175.97 with loss1: 2175.97 and loss2: 0.00\n",
      "Epoch [769], train_loss: 2177.36 with loss1: 2177.36 and loss2: 0.00\n",
      "Epoch [770], train_loss: 2186.68 with loss1: 2186.68 and loss2: 0.00\n",
      "Epoch [771], train_loss: 2173.96 with loss1: 2173.96 and loss2: 0.00\n",
      "Epoch [772], train_loss: 2193.36 with loss1: 2193.36 and loss2: 0.00\n",
      "Epoch [773], train_loss: 2173.43 with loss1: 2173.43 and loss2: 0.00\n",
      "Epoch [774], train_loss: 2209.36 with loss1: 2209.36 and loss2: 0.00\n",
      "Epoch [775], train_loss: 2167.10 with loss1: 2167.10 and loss2: 0.00\n",
      "Epoch [776], train_loss: 2166.52 with loss1: 2166.52 and loss2: 0.00\n",
      "Epoch [777], train_loss: 2174.95 with loss1: 2174.95 and loss2: 0.00\n",
      "Epoch [778], train_loss: 2159.15 with loss1: 2159.15 and loss2: 0.00\n",
      "Epoch [779], train_loss: 2176.37 with loss1: 2176.37 and loss2: 0.00\n",
      "Epoch [780], train_loss: 2184.06 with loss1: 2184.06 and loss2: 0.00\n",
      "Epoch [781], train_loss: 2152.63 with loss1: 2152.63 and loss2: 0.00\n",
      "Epoch [782], train_loss: 2136.41 with loss1: 2136.41 and loss2: 0.00\n",
      "Epoch [783], train_loss: 2151.77 with loss1: 2151.77 and loss2: 0.00\n",
      "Epoch [784], train_loss: 2143.77 with loss1: 2143.77 and loss2: 0.00\n",
      "Epoch [785], train_loss: 2138.29 with loss1: 2138.29 and loss2: 0.00\n",
      "Epoch [786], train_loss: 2149.74 with loss1: 2149.74 and loss2: 0.00\n",
      "Epoch [787], train_loss: 2125.74 with loss1: 2125.74 and loss2: 0.00\n",
      "Epoch [788], train_loss: 2156.03 with loss1: 2156.03 and loss2: 0.00\n",
      "Epoch [789], train_loss: 2133.55 with loss1: 2133.55 and loss2: 0.00\n",
      "Epoch [790], train_loss: 2132.89 with loss1: 2132.89 and loss2: 0.00\n",
      "Epoch [791], train_loss: 2142.44 with loss1: 2142.44 and loss2: 0.00\n",
      "Epoch [792], train_loss: 2135.18 with loss1: 2135.18 and loss2: 0.00\n",
      "Epoch [793], train_loss: 2148.39 with loss1: 2148.39 and loss2: 0.00\n",
      "Epoch [794], train_loss: 2115.02 with loss1: 2115.02 and loss2: 0.00\n",
      "Epoch [795], train_loss: 2126.94 with loss1: 2126.94 and loss2: 0.00\n",
      "Epoch [796], train_loss: 2097.38 with loss1: 2097.38 and loss2: 0.00\n",
      "Epoch [797], train_loss: 2115.70 with loss1: 2115.70 and loss2: 0.00\n",
      "Epoch [798], train_loss: 2104.12 with loss1: 2104.12 and loss2: 0.00\n",
      "Epoch [799], train_loss: 2087.56 with loss1: 2087.56 and loss2: 0.00\n",
      "Epoch [800], train_loss: 2086.06 with loss1: 2086.06 and loss2: 0.00\n",
      "Epoch [801], train_loss: 2109.23 with loss1: 2109.23 and loss2: 0.00\n",
      "Epoch [802], train_loss: 2083.98 with loss1: 2083.98 and loss2: 0.00\n",
      "Epoch [803], train_loss: 2093.60 with loss1: 2093.60 and loss2: 0.00\n",
      "Epoch [804], train_loss: 2089.50 with loss1: 2089.50 and loss2: 0.00\n",
      "Epoch [805], train_loss: 2099.63 with loss1: 2099.63 and loss2: 0.00\n",
      "Epoch [806], train_loss: 2096.34 with loss1: 2096.34 and loss2: 0.00\n",
      "Epoch [807], train_loss: 2089.94 with loss1: 2089.94 and loss2: 0.00\n",
      "Epoch [808], train_loss: 2094.13 with loss1: 2094.13 and loss2: 0.00\n",
      "Epoch [809], train_loss: 2071.99 with loss1: 2071.99 and loss2: 0.00\n",
      "Epoch [810], train_loss: 2085.73 with loss1: 2085.73 and loss2: 0.00\n",
      "Epoch [811], train_loss: 2067.19 with loss1: 2067.19 and loss2: 0.00\n",
      "Epoch [812], train_loss: 2078.46 with loss1: 2078.46 and loss2: 0.00\n",
      "Epoch [813], train_loss: 2078.66 with loss1: 2078.66 and loss2: 0.00\n",
      "Epoch [814], train_loss: 2094.86 with loss1: 2094.86 and loss2: 0.00\n",
      "Epoch [815], train_loss: 2068.46 with loss1: 2068.46 and loss2: 0.00\n",
      "Epoch [816], train_loss: 2083.69 with loss1: 2083.69 and loss2: 0.00\n",
      "Epoch [817], train_loss: 2085.03 with loss1: 2085.03 and loss2: 0.00\n",
      "Epoch [818], train_loss: 2068.84 with loss1: 2068.84 and loss2: 0.00\n",
      "Epoch [819], train_loss: 2070.06 with loss1: 2070.06 and loss2: 0.00\n",
      "Epoch [820], train_loss: 2076.67 with loss1: 2076.67 and loss2: 0.00\n",
      "Epoch [821], train_loss: 2080.27 with loss1: 2080.27 and loss2: 0.00\n",
      "Epoch [822], train_loss: 2068.25 with loss1: 2068.25 and loss2: 0.00\n",
      "Epoch [823], train_loss: 2093.23 with loss1: 2093.23 and loss2: 0.00\n",
      "Epoch [824], train_loss: 2075.70 with loss1: 2075.70 and loss2: 0.00\n",
      "Epoch [825], train_loss: 2061.28 with loss1: 2061.28 and loss2: 0.00\n",
      "Epoch [826], train_loss: 2069.11 with loss1: 2069.11 and loss2: 0.00\n",
      "Epoch [827], train_loss: 2075.28 with loss1: 2075.28 and loss2: 0.00\n",
      "Epoch [828], train_loss: 2071.68 with loss1: 2071.68 and loss2: 0.00\n",
      "Epoch [829], train_loss: 2074.68 with loss1: 2074.68 and loss2: 0.00\n",
      "Epoch [830], train_loss: 2066.24 with loss1: 2066.24 and loss2: 0.00\n",
      "Epoch [831], train_loss: 2061.96 with loss1: 2061.96 and loss2: 0.00\n",
      "Epoch [832], train_loss: 2076.41 with loss1: 2076.41 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [833], train_loss: 2058.21 with loss1: 2058.21 and loss2: 0.00\n",
      "Epoch [834], train_loss: 2084.41 with loss1: 2084.41 and loss2: 0.00\n",
      "Epoch [835], train_loss: 2092.58 with loss1: 2092.58 and loss2: 0.00\n",
      "Epoch [836], train_loss: 2069.58 with loss1: 2069.58 and loss2: 0.00\n",
      "Epoch [837], train_loss: 2061.66 with loss1: 2061.66 and loss2: 0.00\n",
      "Epoch [838], train_loss: 2049.09 with loss1: 2049.09 and loss2: 0.00\n",
      "Epoch [839], train_loss: 2051.08 with loss1: 2051.08 and loss2: 0.00\n",
      "Epoch [840], train_loss: 2063.81 with loss1: 2063.81 and loss2: 0.00\n",
      "Epoch [841], train_loss: 2031.84 with loss1: 2031.84 and loss2: 0.00\n",
      "Epoch [842], train_loss: 2060.20 with loss1: 2060.20 and loss2: 0.00\n",
      "Epoch [843], train_loss: 2031.71 with loss1: 2031.71 and loss2: 0.00\n",
      "Epoch [844], train_loss: 2032.79 with loss1: 2032.79 and loss2: 0.00\n",
      "Epoch [845], train_loss: 2038.73 with loss1: 2038.73 and loss2: 0.00\n",
      "Epoch [846], train_loss: 2027.00 with loss1: 2027.00 and loss2: 0.00\n",
      "Epoch [847], train_loss: 2026.05 with loss1: 2026.05 and loss2: 0.00\n",
      "Epoch [848], train_loss: 2025.39 with loss1: 2025.39 and loss2: 0.00\n",
      "Epoch [849], train_loss: 2061.44 with loss1: 2061.44 and loss2: 0.00\n",
      "Epoch [850], train_loss: 2050.14 with loss1: 2050.14 and loss2: 0.00\n",
      "Epoch [851], train_loss: 2044.88 with loss1: 2044.88 and loss2: 0.00\n",
      "Epoch [852], train_loss: 2022.30 with loss1: 2022.30 and loss2: 0.00\n",
      "Epoch [853], train_loss: 2042.69 with loss1: 2042.69 and loss2: 0.00\n",
      "Epoch [854], train_loss: 2029.62 with loss1: 2029.62 and loss2: 0.00\n",
      "Epoch [855], train_loss: 2012.51 with loss1: 2012.51 and loss2: 0.00\n",
      "Epoch [856], train_loss: 2029.44 with loss1: 2029.44 and loss2: 0.00\n",
      "Epoch [857], train_loss: 2034.80 with loss1: 2034.80 and loss2: 0.00\n",
      "Epoch [858], train_loss: 2008.31 with loss1: 2008.31 and loss2: 0.00\n",
      "Epoch [859], train_loss: 2030.90 with loss1: 2030.90 and loss2: 0.00\n",
      "Epoch [860], train_loss: 2019.20 with loss1: 2019.20 and loss2: 0.00\n",
      "Epoch [861], train_loss: 2028.21 with loss1: 2028.21 and loss2: 0.00\n",
      "Epoch [862], train_loss: 2027.85 with loss1: 2027.85 and loss2: 0.00\n",
      "Epoch [863], train_loss: 2017.90 with loss1: 2017.90 and loss2: 0.00\n",
      "Epoch [864], train_loss: 2021.93 with loss1: 2021.93 and loss2: 0.00\n",
      "Epoch [865], train_loss: 2035.57 with loss1: 2035.57 and loss2: 0.00\n",
      "Epoch [866], train_loss: 2037.11 with loss1: 2037.11 and loss2: 0.00\n",
      "Epoch [867], train_loss: 2026.06 with loss1: 2026.06 and loss2: 0.00\n",
      "Epoch [868], train_loss: 2025.01 with loss1: 2025.01 and loss2: 0.00\n",
      "Epoch [869], train_loss: 2044.70 with loss1: 2044.70 and loss2: 0.00\n",
      "Epoch [870], train_loss: 2001.24 with loss1: 2001.24 and loss2: 0.00\n",
      "Epoch [871], train_loss: 1992.23 with loss1: 1992.23 and loss2: 0.00\n",
      "Epoch [872], train_loss: 2018.52 with loss1: 2018.52 and loss2: 0.00\n",
      "Epoch [873], train_loss: 2001.74 with loss1: 2001.74 and loss2: 0.00\n",
      "Epoch [874], train_loss: 1990.45 with loss1: 1990.45 and loss2: 0.00\n",
      "Epoch [875], train_loss: 2002.19 with loss1: 2002.19 and loss2: 0.00\n",
      "Epoch [876], train_loss: 2010.79 with loss1: 2010.79 and loss2: 0.00\n",
      "Epoch [877], train_loss: 1987.93 with loss1: 1987.93 and loss2: 0.00\n",
      "Epoch [878], train_loss: 1993.55 with loss1: 1993.55 and loss2: 0.00\n",
      "Epoch [879], train_loss: 1980.77 with loss1: 1980.77 and loss2: 0.00\n",
      "Epoch [880], train_loss: 1980.40 with loss1: 1980.40 and loss2: 0.00\n",
      "Epoch [881], train_loss: 1981.44 with loss1: 1981.44 and loss2: 0.00\n",
      "Epoch [882], train_loss: 1974.10 with loss1: 1974.10 and loss2: 0.00\n",
      "Epoch [883], train_loss: 1971.88 with loss1: 1971.88 and loss2: 0.00\n",
      "Epoch [884], train_loss: 1990.12 with loss1: 1990.12 and loss2: 0.00\n",
      "Epoch [885], train_loss: 1981.55 with loss1: 1981.55 and loss2: 0.00\n",
      "Epoch [886], train_loss: 1973.47 with loss1: 1973.47 and loss2: 0.00\n",
      "Epoch [887], train_loss: 1951.05 with loss1: 1951.05 and loss2: 0.00\n",
      "Epoch [888], train_loss: 1976.22 with loss1: 1976.22 and loss2: 0.00\n",
      "Epoch [889], train_loss: 1959.76 with loss1: 1959.76 and loss2: 0.00\n",
      "Epoch [890], train_loss: 1958.47 with loss1: 1958.47 and loss2: 0.00\n",
      "Epoch [891], train_loss: 1969.20 with loss1: 1969.20 and loss2: 0.00\n",
      "Epoch [892], train_loss: 1961.66 with loss1: 1961.66 and loss2: 0.00\n",
      "Epoch [893], train_loss: 1958.51 with loss1: 1958.51 and loss2: 0.00\n",
      "Epoch [894], train_loss: 1953.89 with loss1: 1953.89 and loss2: 0.00\n",
      "Epoch [895], train_loss: 1973.13 with loss1: 1973.13 and loss2: 0.00\n",
      "Epoch [896], train_loss: 1960.53 with loss1: 1960.53 and loss2: 0.00\n",
      "Epoch [897], train_loss: 1950.16 with loss1: 1950.16 and loss2: 0.00\n",
      "Epoch [898], train_loss: 1946.47 with loss1: 1946.47 and loss2: 0.00\n",
      "Epoch [899], train_loss: 1970.58 with loss1: 1970.58 and loss2: 0.00\n",
      "Epoch [900], train_loss: 1949.02 with loss1: 1949.02 and loss2: 0.00\n",
      "Epoch [901], train_loss: 1938.34 with loss1: 1938.34 and loss2: 0.00\n",
      "Epoch [902], train_loss: 1970.27 with loss1: 1970.27 and loss2: 0.00\n",
      "Epoch [903], train_loss: 1961.69 with loss1: 1961.69 and loss2: 0.00\n",
      "Epoch [904], train_loss: 1951.43 with loss1: 1951.43 and loss2: 0.00\n",
      "Epoch [905], train_loss: 1947.48 with loss1: 1947.48 and loss2: 0.00\n",
      "Epoch [906], train_loss: 1954.69 with loss1: 1954.69 and loss2: 0.00\n",
      "Epoch [907], train_loss: 1944.84 with loss1: 1944.84 and loss2: 0.00\n",
      "Epoch [908], train_loss: 1953.06 with loss1: 1953.06 and loss2: 0.00\n",
      "Epoch [909], train_loss: 1950.46 with loss1: 1950.46 and loss2: 0.00\n",
      "Epoch [910], train_loss: 1959.63 with loss1: 1959.63 and loss2: 0.00\n",
      "Epoch [911], train_loss: 1983.39 with loss1: 1983.39 and loss2: 0.00\n",
      "Epoch [912], train_loss: 1951.99 with loss1: 1951.99 and loss2: 0.00\n",
      "Epoch [913], train_loss: 1936.23 with loss1: 1936.23 and loss2: 0.00\n",
      "Epoch [914], train_loss: 1963.93 with loss1: 1963.93 and loss2: 0.00\n",
      "Epoch [915], train_loss: 1944.31 with loss1: 1944.31 and loss2: 0.00\n",
      "Epoch [916], train_loss: 1939.83 with loss1: 1939.83 and loss2: 0.00\n",
      "Epoch [917], train_loss: 1943.47 with loss1: 1943.47 and loss2: 0.00\n",
      "Epoch [918], train_loss: 1943.10 with loss1: 1943.10 and loss2: 0.00\n",
      "Epoch [919], train_loss: 1933.37 with loss1: 1933.37 and loss2: 0.00\n",
      "Epoch [920], train_loss: 1933.54 with loss1: 1933.54 and loss2: 0.00\n",
      "Epoch [921], train_loss: 1936.91 with loss1: 1936.91 and loss2: 0.00\n",
      "Epoch [922], train_loss: 1937.91 with loss1: 1937.91 and loss2: 0.00\n",
      "Epoch [923], train_loss: 1943.31 with loss1: 1943.31 and loss2: 0.00\n",
      "Epoch [924], train_loss: 1924.44 with loss1: 1924.44 and loss2: 0.00\n",
      "Epoch [925], train_loss: 1917.83 with loss1: 1917.83 and loss2: 0.00\n",
      "Epoch [926], train_loss: 1927.81 with loss1: 1927.81 and loss2: 0.00\n",
      "Epoch [927], train_loss: 1916.51 with loss1: 1916.51 and loss2: 0.00\n",
      "Epoch [928], train_loss: 1897.26 with loss1: 1897.26 and loss2: 0.00\n",
      "Epoch [929], train_loss: 1906.82 with loss1: 1906.82 and loss2: 0.00\n",
      "Epoch [930], train_loss: 1909.48 with loss1: 1909.48 and loss2: 0.00\n",
      "Epoch [931], train_loss: 1913.06 with loss1: 1913.06 and loss2: 0.00\n",
      "Epoch [932], train_loss: 1906.55 with loss1: 1906.55 and loss2: 0.00\n",
      "Epoch [933], train_loss: 1934.44 with loss1: 1934.44 and loss2: 0.00\n",
      "Epoch [934], train_loss: 1914.35 with loss1: 1914.35 and loss2: 0.00\n",
      "Epoch [935], train_loss: 1901.72 with loss1: 1901.72 and loss2: 0.00\n",
      "Epoch [936], train_loss: 1901.75 with loss1: 1901.75 and loss2: 0.00\n",
      "Epoch [937], train_loss: 1904.04 with loss1: 1904.04 and loss2: 0.00\n",
      "Epoch [938], train_loss: 1917.85 with loss1: 1917.85 and loss2: 0.00\n",
      "Epoch [939], train_loss: 1883.32 with loss1: 1883.32 and loss2: 0.00\n",
      "Epoch [940], train_loss: 1907.14 with loss1: 1907.14 and loss2: 0.00\n",
      "Epoch [941], train_loss: 1910.83 with loss1: 1910.83 and loss2: 0.00\n",
      "Epoch [942], train_loss: 1898.20 with loss1: 1898.20 and loss2: 0.00\n",
      "Epoch [943], train_loss: 1896.77 with loss1: 1896.77 and loss2: 0.00\n",
      "Epoch [944], train_loss: 1899.03 with loss1: 1899.03 and loss2: 0.00\n",
      "Epoch [945], train_loss: 1896.57 with loss1: 1896.57 and loss2: 0.00\n",
      "Epoch [946], train_loss: 1891.39 with loss1: 1891.39 and loss2: 0.00\n",
      "Epoch [947], train_loss: 1891.51 with loss1: 1891.51 and loss2: 0.00\n",
      "Epoch [948], train_loss: 1920.98 with loss1: 1920.98 and loss2: 0.00\n",
      "Epoch [949], train_loss: 1915.09 with loss1: 1915.09 and loss2: 0.00\n",
      "Epoch [950], train_loss: 1899.76 with loss1: 1899.76 and loss2: 0.00\n",
      "Epoch [951], train_loss: 1902.90 with loss1: 1902.90 and loss2: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [952], train_loss: 1909.26 with loss1: 1909.26 and loss2: 0.00\n",
      "Epoch [953], train_loss: 1900.89 with loss1: 1900.89 and loss2: 0.00\n",
      "Epoch [954], train_loss: 1901.34 with loss1: 1901.34 and loss2: 0.00\n",
      "Epoch [955], train_loss: 1896.60 with loss1: 1896.60 and loss2: 0.00\n",
      "Epoch [956], train_loss: 1898.15 with loss1: 1898.15 and loss2: 0.00\n",
      "Epoch [957], train_loss: 1876.33 with loss1: 1876.33 and loss2: 0.00\n",
      "Epoch [958], train_loss: 1886.69 with loss1: 1886.69 and loss2: 0.00\n",
      "Epoch [959], train_loss: 1892.40 with loss1: 1892.40 and loss2: 0.00\n",
      "Epoch [960], train_loss: 1892.04 with loss1: 1892.04 and loss2: 0.00\n",
      "Epoch [961], train_loss: 1908.44 with loss1: 1908.44 and loss2: 0.00\n",
      "Epoch [962], train_loss: 1889.61 with loss1: 1889.61 and loss2: 0.00\n",
      "Epoch [963], train_loss: 1877.12 with loss1: 1877.12 and loss2: 0.00\n",
      "Epoch [964], train_loss: 1895.16 with loss1: 1895.16 and loss2: 0.00\n",
      "Epoch [965], train_loss: 1876.57 with loss1: 1876.57 and loss2: 0.00\n",
      "Epoch [966], train_loss: 1889.81 with loss1: 1889.81 and loss2: 0.00\n",
      "Epoch [967], train_loss: 1889.40 with loss1: 1889.40 and loss2: 0.00\n",
      "Epoch [968], train_loss: 1888.85 with loss1: 1888.85 and loss2: 0.00\n",
      "Epoch [969], train_loss: 1884.83 with loss1: 1884.83 and loss2: 0.00\n",
      "Epoch [970], train_loss: 1872.77 with loss1: 1872.77 and loss2: 0.00\n",
      "Epoch [971], train_loss: 1885.50 with loss1: 1885.50 and loss2: 0.00\n",
      "Epoch [972], train_loss: 1897.13 with loss1: 1897.13 and loss2: 0.00\n",
      "Epoch [973], train_loss: 1881.69 with loss1: 1881.69 and loss2: 0.00\n",
      "Epoch [974], train_loss: 1879.18 with loss1: 1879.18 and loss2: 0.00\n",
      "Epoch [975], train_loss: 1880.54 with loss1: 1880.54 and loss2: 0.00\n",
      "Epoch [976], train_loss: 1869.69 with loss1: 1869.69 and loss2: 0.00\n",
      "Epoch [977], train_loss: 1883.17 with loss1: 1883.17 and loss2: 0.00\n",
      "Epoch [978], train_loss: 1863.44 with loss1: 1863.44 and loss2: 0.00\n",
      "Epoch [979], train_loss: 1860.82 with loss1: 1860.82 and loss2: 0.00\n",
      "Epoch [980], train_loss: 1862.82 with loss1: 1862.82 and loss2: 0.00\n",
      "Epoch [981], train_loss: 1866.15 with loss1: 1866.15 and loss2: 0.00\n",
      "Epoch [982], train_loss: 1870.14 with loss1: 1870.14 and loss2: 0.00\n",
      "Epoch [983], train_loss: 1849.48 with loss1: 1849.48 and loss2: 0.00\n",
      "Epoch [984], train_loss: 1857.20 with loss1: 1857.20 and loss2: 0.00\n",
      "Epoch [985], train_loss: 1829.73 with loss1: 1829.73 and loss2: 0.00\n",
      "Epoch [986], train_loss: 1844.45 with loss1: 1844.45 and loss2: 0.00\n",
      "Epoch [987], train_loss: 1845.81 with loss1: 1845.81 and loss2: 0.00\n",
      "Epoch [988], train_loss: 1848.78 with loss1: 1848.78 and loss2: 0.00\n",
      "Epoch [989], train_loss: 1841.82 with loss1: 1841.82 and loss2: 0.00\n",
      "Epoch [990], train_loss: 1838.48 with loss1: 1838.48 and loss2: 0.00\n",
      "Epoch [991], train_loss: 1859.60 with loss1: 1859.60 and loss2: 0.00\n",
      "Epoch [992], train_loss: 1844.12 with loss1: 1844.12 and loss2: 0.00\n",
      "Epoch [993], train_loss: 1849.16 with loss1: 1849.16 and loss2: 0.00\n",
      "Epoch [994], train_loss: 1829.17 with loss1: 1829.17 and loss2: 0.00\n",
      "Epoch [995], train_loss: 1833.84 with loss1: 1833.84 and loss2: 0.00\n",
      "Epoch [996], train_loss: 1829.24 with loss1: 1829.24 and loss2: 0.00\n",
      "Epoch [997], train_loss: 1818.80 with loss1: 1818.80 and loss2: 0.00\n",
      "Epoch [998], train_loss: 1818.37 with loss1: 1818.37 and loss2: 0.00\n",
      "Epoch [999], train_loss: 1815.86 with loss1: 1815.86 and loss2: 0.00\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# model version 3.5\n",
    "# delete loss2\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history = fit(epochs=1000, lr=1e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fee4ad3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 344594.56 with loss1: 343991.06 and loss2: 603.49\n",
      "Epoch [1], train_loss: 97680.33 with loss1: 97122.01 and loss2: 558.32\n",
      "Epoch [2], train_loss: 134971.22 with loss1: 134445.17 and loss2: 526.05\n",
      "Epoch [3], train_loss: 55012.54 with loss1: 54404.86 and loss2: 607.68\n",
      "Epoch [4], train_loss: 40242.79 with loss1: 39676.96 and loss2: 565.83\n",
      "Epoch [5], train_loss: 35231.79 with loss1: 34620.36 and loss2: 611.42\n",
      "Epoch [6], train_loss: 34301.95 with loss1: 33679.62 and loss2: 622.33\n",
      "Epoch [7], train_loss: 24158.23 with loss1: 23509.59 and loss2: 648.64\n",
      "Epoch [8], train_loss: 22778.75 with loss1: 22123.98 and loss2: 654.76\n",
      "Epoch [9], train_loss: 16671.33 with loss1: 15965.41 and loss2: 705.92\n",
      "Epoch [10], train_loss: 15318.68 with loss1: 14595.30 and loss2: 723.38\n",
      "Epoch [11], train_loss: 13296.25 with loss1: 12530.77 and loss2: 765.48\n",
      "Epoch [12], train_loss: 12570.10 with loss1: 11781.57 and loss2: 788.53\n",
      "Epoch [13], train_loss: 11830.20 with loss1: 11009.23 and loss2: 820.96\n",
      "Epoch [14], train_loss: 11313.98 with loss1: 10491.23 and loss2: 822.75\n",
      "Epoch [15], train_loss: 11158.61 with loss1: 10284.53 and loss2: 874.08\n",
      "Epoch [16], train_loss: 11011.88 with loss1: 10162.33 and loss2: 849.56\n",
      "Epoch [17], train_loss: 11020.85 with loss1: 10129.96 and loss2: 890.90\n",
      "Epoch [18], train_loss: 11041.14 with loss1: 10163.37 and loss2: 877.77\n",
      "Epoch [19], train_loss: 10654.55 with loss1: 9787.86 and loss2: 866.69\n",
      "Epoch [20], train_loss: 10603.01 with loss1: 9739.75 and loss2: 863.26\n",
      "Epoch [21], train_loss: 10060.56 with loss1: 9187.13 and loss2: 873.43\n",
      "Epoch [22], train_loss: 9916.93 with loss1: 9077.95 and loss2: 838.98\n",
      "Epoch [23], train_loss: 9636.30 with loss1: 8783.75 and loss2: 852.55\n",
      "Epoch [24], train_loss: 9449.72 with loss1: 8599.16 and loss2: 850.56\n",
      "Epoch [25], train_loss: 9061.05 with loss1: 8212.68 and loss2: 848.37\n",
      "Epoch [26], train_loss: 9033.12 with loss1: 8207.42 and loss2: 825.70\n",
      "Epoch [27], train_loss: 8684.78 with loss1: 7847.58 and loss2: 837.20\n",
      "Epoch [28], train_loss: 8631.35 with loss1: 7822.39 and loss2: 808.96\n",
      "Epoch [29], train_loss: 8323.20 with loss1: 7513.75 and loss2: 809.45\n",
      "Epoch [30], train_loss: 8341.51 with loss1: 7538.75 and loss2: 802.77\n",
      "Epoch [31], train_loss: 8023.78 with loss1: 7226.22 and loss2: 797.56\n",
      "Epoch [32], train_loss: 8053.82 with loss1: 7255.83 and loss2: 797.99\n",
      "Epoch [33], train_loss: 7837.68 with loss1: 7055.50 and loss2: 782.18\n",
      "Epoch [34], train_loss: 7845.64 with loss1: 7060.82 and loss2: 784.82\n",
      "Epoch [35], train_loss: 7629.97 with loss1: 6858.59 and loss2: 771.38\n",
      "Epoch [36], train_loss: 7653.93 with loss1: 6890.40 and loss2: 763.54\n",
      "Epoch [37], train_loss: 7478.02 with loss1: 6721.32 and loss2: 756.70\n",
      "Epoch [38], train_loss: 7488.96 with loss1: 6732.77 and loss2: 756.20\n",
      "Epoch [39], train_loss: 7293.35 with loss1: 6556.13 and loss2: 737.23\n",
      "Epoch [40], train_loss: 7348.01 with loss1: 6601.14 and loss2: 746.87\n",
      "Epoch [41], train_loss: 7144.00 with loss1: 6403.62 and loss2: 740.39\n",
      "Epoch [42], train_loss: 7213.59 with loss1: 6483.25 and loss2: 730.34\n",
      "Epoch [43], train_loss: 7055.34 with loss1: 6341.50 and loss2: 713.84\n",
      "Epoch [44], train_loss: 7064.00 with loss1: 6355.46 and loss2: 708.54\n",
      "Epoch [45], train_loss: 6932.44 with loss1: 6224.14 and loss2: 708.30\n",
      "Epoch [46], train_loss: 6970.22 with loss1: 6265.28 and loss2: 704.94\n",
      "Epoch [47], train_loss: 6823.12 with loss1: 6126.40 and loss2: 696.72\n",
      "Epoch [48], train_loss: 6873.33 with loss1: 6183.89 and loss2: 689.43\n",
      "Epoch [49], train_loss: 6665.31 with loss1: 5971.83 and loss2: 693.47\n",
      "Epoch [50], train_loss: 6714.62 with loss1: 6026.62 and loss2: 688.00\n",
      "Epoch [51], train_loss: 6576.80 with loss1: 5897.91 and loss2: 678.89\n",
      "Epoch [52], train_loss: 6646.32 with loss1: 5970.43 and loss2: 675.89\n",
      "Epoch [53], train_loss: 6446.68 with loss1: 5779.17 and loss2: 667.51\n",
      "Epoch [54], train_loss: 6494.33 with loss1: 5829.95 and loss2: 664.38\n",
      "Epoch [55], train_loss: 6412.28 with loss1: 5756.81 and loss2: 655.47\n",
      "Epoch [56], train_loss: 6431.68 with loss1: 5773.46 and loss2: 658.22\n",
      "Epoch [57], train_loss: 6329.73 with loss1: 5686.57 and loss2: 643.16\n",
      "Epoch [58], train_loss: 6340.30 with loss1: 5682.59 and loss2: 657.71\n",
      "Epoch [59], train_loss: 6248.52 with loss1: 5605.01 and loss2: 643.52\n",
      "Epoch [60], train_loss: 6297.47 with loss1: 5670.22 and loss2: 627.25\n",
      "Epoch [61], train_loss: 6183.72 with loss1: 5548.33 and loss2: 635.39\n",
      "Epoch [62], train_loss: 6187.32 with loss1: 5562.18 and loss2: 625.14\n",
      "Epoch [63], train_loss: 6118.11 with loss1: 5497.42 and loss2: 620.69\n",
      "Epoch [64], train_loss: 6174.66 with loss1: 5562.74 and loss2: 611.92\n",
      "Epoch [65], train_loss: 6079.90 with loss1: 5465.69 and loss2: 614.22\n",
      "Epoch [66], train_loss: 6097.59 with loss1: 5491.27 and loss2: 606.33\n",
      "Epoch [67], train_loss: 5995.33 with loss1: 5388.53 and loss2: 606.79\n",
      "Epoch [68], train_loss: 6032.72 with loss1: 5432.32 and loss2: 600.40\n",
      "Epoch [69], train_loss: 5941.60 with loss1: 5347.33 and loss2: 594.27\n",
      "Epoch [70], train_loss: 6021.18 with loss1: 5422.85 and loss2: 598.33\n",
      "Epoch [71], train_loss: 5892.00 with loss1: 5306.96 and loss2: 585.05\n",
      "Epoch [72], train_loss: 5970.53 with loss1: 5380.70 and loss2: 589.82\n",
      "Epoch [73], train_loss: 5862.03 with loss1: 5280.88 and loss2: 581.15\n",
      "Epoch [74], train_loss: 5923.23 with loss1: 5337.03 and loss2: 586.20\n",
      "Epoch [75], train_loss: 5794.02 with loss1: 5215.39 and loss2: 578.63\n",
      "Epoch [76], train_loss: 5892.38 with loss1: 5321.77 and loss2: 570.61\n",
      "Epoch [77], train_loss: 5751.07 with loss1: 5186.98 and loss2: 564.08\n",
      "Epoch [78], train_loss: 5891.80 with loss1: 5327.22 and loss2: 564.58\n",
      "Epoch [79], train_loss: 5757.43 with loss1: 5198.35 and loss2: 559.08\n",
      "Epoch [80], train_loss: 5864.98 with loss1: 5310.24 and loss2: 554.74\n",
      "Epoch [81], train_loss: 5652.15 with loss1: 5095.98 and loss2: 556.17\n",
      "Epoch [82], train_loss: 5689.43 with loss1: 5141.98 and loss2: 547.45\n",
      "Epoch [83], train_loss: 5460.59 with loss1: 4909.48 and loss2: 551.11\n",
      "Epoch [84], train_loss: 5404.11 with loss1: 4863.54 and loss2: 540.57\n",
      "Epoch [85], train_loss: 5243.44 with loss1: 4707.49 and loss2: 535.95\n",
      "Epoch [86], train_loss: 5248.99 with loss1: 4715.79 and loss2: 533.20\n",
      "Epoch [87], train_loss: 5140.99 with loss1: 4603.96 and loss2: 537.03\n",
      "Epoch [88], train_loss: 5140.71 with loss1: 4609.93 and loss2: 530.78\n",
      "Epoch [89], train_loss: 5059.01 with loss1: 4534.12 and loss2: 524.90\n",
      "Epoch [90], train_loss: 5106.36 with loss1: 4585.30 and loss2: 521.06\n",
      "Epoch [91], train_loss: 5111.99 with loss1: 4595.54 and loss2: 516.44\n",
      "Epoch [92], train_loss: 5220.94 with loss1: 4701.41 and loss2: 519.53\n",
      "Epoch [93], train_loss: 5196.88 with loss1: 4687.15 and loss2: 509.73\n",
      "Epoch [94], train_loss: 5371.47 with loss1: 4853.54 and loss2: 517.93\n",
      "Epoch [95], train_loss: 5359.02 with loss1: 4847.85 and loss2: 511.18\n",
      "Epoch [96], train_loss: 5548.23 with loss1: 5031.83 and loss2: 516.41\n",
      "Epoch [97], train_loss: 5459.28 with loss1: 4949.92 and loss2: 509.36\n",
      "Epoch [98], train_loss: 5643.17 with loss1: 5138.35 and loss2: 504.82\n",
      "Epoch [99], train_loss: 5506.69 with loss1: 5005.15 and loss2: 501.53\n",
      "Epoch [100], train_loss: 5670.50 with loss1: 5171.85 and loss2: 498.65\n",
      "Epoch [101], train_loss: 5463.49 with loss1: 4965.15 and loss2: 498.34\n",
      "Epoch [102], train_loss: 5599.65 with loss1: 5103.21 and loss2: 496.43\n",
      "Epoch [103], train_loss: 5278.16 with loss1: 4780.70 and loss2: 497.46\n",
      "Epoch [104], train_loss: 5279.39 with loss1: 4790.05 and loss2: 489.34\n",
      "Epoch [105], train_loss: 5018.96 with loss1: 4527.35 and loss2: 491.62\n",
      "Epoch [106], train_loss: 5014.06 with loss1: 4524.96 and loss2: 489.11\n",
      "Epoch [107], train_loss: 4845.38 with loss1: 4365.34 and loss2: 480.04\n",
      "Epoch [108], train_loss: 4814.22 with loss1: 4335.86 and loss2: 478.36\n",
      "Epoch [109], train_loss: 4726.76 with loss1: 4246.46 and loss2: 480.30\n",
      "Epoch [110], train_loss: 4696.75 with loss1: 4222.64 and loss2: 474.12\n",
      "Epoch [111], train_loss: 4677.27 with loss1: 4207.19 and loss2: 470.08\n",
      "Epoch [112], train_loss: 4678.52 with loss1: 4206.96 and loss2: 471.55\n",
      "Epoch [113], train_loss: 4639.61 with loss1: 4171.18 and loss2: 468.44\n",
      "Epoch [114], train_loss: 4702.89 with loss1: 4231.45 and loss2: 471.44\n",
      "Epoch [115], train_loss: 4661.51 with loss1: 4196.87 and loss2: 464.65\n",
      "Epoch [116], train_loss: 4745.46 with loss1: 4275.67 and loss2: 469.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [117], train_loss: 4706.33 with loss1: 4244.85 and loss2: 461.48\n",
      "Epoch [118], train_loss: 4816.00 with loss1: 4349.44 and loss2: 466.57\n",
      "Epoch [119], train_loss: 4755.45 with loss1: 4290.17 and loss2: 465.28\n",
      "Epoch [120], train_loss: 4874.64 with loss1: 4407.32 and loss2: 467.32\n",
      "Epoch [121], train_loss: 4775.48 with loss1: 4315.68 and loss2: 459.80\n",
      "Epoch [122], train_loss: 4836.26 with loss1: 4373.79 and loss2: 462.47\n",
      "Epoch [123], train_loss: 4750.83 with loss1: 4294.28 and loss2: 456.55\n",
      "Epoch [124], train_loss: 4810.16 with loss1: 4345.75 and loss2: 464.42\n",
      "Epoch [125], train_loss: 4675.61 with loss1: 4221.84 and loss2: 453.77\n",
      "Epoch [126], train_loss: 4730.63 with loss1: 4273.59 and loss2: 457.04\n",
      "Epoch [127], train_loss: 4593.29 with loss1: 4141.89 and loss2: 451.40\n",
      "Epoch [128], train_loss: 4664.68 with loss1: 4212.03 and loss2: 452.65\n",
      "Epoch [129], train_loss: 4533.65 with loss1: 4089.48 and loss2: 444.16\n",
      "Epoch [130], train_loss: 4604.20 with loss1: 4152.01 and loss2: 452.19\n",
      "Epoch [131], train_loss: 4432.59 with loss1: 3980.12 and loss2: 452.47\n",
      "Epoch [132], train_loss: 4470.45 with loss1: 4019.36 and loss2: 451.09\n",
      "Epoch [133], train_loss: 4326.63 with loss1: 3883.00 and loss2: 443.64\n",
      "Epoch [134], train_loss: 4346.85 with loss1: 3894.28 and loss2: 452.58\n",
      "Epoch [135], train_loss: 4219.17 with loss1: 3777.79 and loss2: 441.38\n",
      "Epoch [136], train_loss: 4268.53 with loss1: 3823.49 and loss2: 445.04\n",
      "Epoch [137], train_loss: 4120.24 with loss1: 3679.44 and loss2: 440.80\n",
      "Epoch [138], train_loss: 4158.51 with loss1: 3709.60 and loss2: 448.91\n",
      "Epoch [139], train_loss: 4039.83 with loss1: 3600.85 and loss2: 438.98\n",
      "Epoch [140], train_loss: 4080.72 with loss1: 3638.23 and loss2: 442.49\n",
      "Epoch [141], train_loss: 3970.35 with loss1: 3533.60 and loss2: 436.75\n",
      "Epoch [142], train_loss: 3988.49 with loss1: 3550.30 and loss2: 438.19\n",
      "Epoch [143], train_loss: 3910.21 with loss1: 3475.83 and loss2: 434.38\n",
      "Epoch [144], train_loss: 3945.04 with loss1: 3513.69 and loss2: 431.34\n",
      "Epoch [145], train_loss: 3871.19 with loss1: 3438.57 and loss2: 432.63\n",
      "Epoch [146], train_loss: 3929.67 with loss1: 3500.01 and loss2: 429.65\n",
      "Epoch [147], train_loss: 3866.01 with loss1: 3438.60 and loss2: 427.40\n",
      "Epoch [148], train_loss: 3890.62 with loss1: 3457.42 and loss2: 433.21\n",
      "Epoch [149], train_loss: 3828.09 with loss1: 3402.35 and loss2: 425.73\n",
      "Epoch [150], train_loss: 3914.37 with loss1: 3488.06 and loss2: 426.32\n",
      "Epoch [151], train_loss: 3878.21 with loss1: 3450.95 and loss2: 427.26\n",
      "Epoch [152], train_loss: 3978.70 with loss1: 3550.02 and loss2: 428.68\n",
      "Epoch [153], train_loss: 3931.19 with loss1: 3500.72 and loss2: 430.47\n",
      "Epoch [154], train_loss: 3959.84 with loss1: 3536.17 and loss2: 423.66\n",
      "Epoch [155], train_loss: 3897.13 with loss1: 3473.76 and loss2: 423.38\n",
      "Epoch [156], train_loss: 3962.06 with loss1: 3540.19 and loss2: 421.86\n",
      "Epoch [157], train_loss: 3912.87 with loss1: 3497.62 and loss2: 415.24\n",
      "Epoch [158], train_loss: 4007.67 with loss1: 3585.73 and loss2: 421.94\n",
      "Epoch [159], train_loss: 3868.46 with loss1: 3449.77 and loss2: 418.68\n",
      "Epoch [160], train_loss: 3935.78 with loss1: 3515.50 and loss2: 420.29\n",
      "Epoch [161], train_loss: 3804.04 with loss1: 3382.53 and loss2: 421.51\n",
      "Epoch [162], train_loss: 3853.48 with loss1: 3435.48 and loss2: 417.99\n",
      "Epoch [163], train_loss: 3716.74 with loss1: 3305.80 and loss2: 410.93\n",
      "Epoch [164], train_loss: 3731.87 with loss1: 3316.27 and loss2: 415.60\n",
      "Epoch [165], train_loss: 3619.52 with loss1: 3203.98 and loss2: 415.54\n",
      "Epoch [166], train_loss: 3641.54 with loss1: 3224.36 and loss2: 417.18\n",
      "Epoch [167], train_loss: 3548.59 with loss1: 3138.23 and loss2: 410.36\n",
      "Epoch [168], train_loss: 3548.98 with loss1: 3136.43 and loss2: 412.55\n",
      "Epoch [169], train_loss: 3488.93 with loss1: 3078.99 and loss2: 409.94\n",
      "Epoch [170], train_loss: 3523.33 with loss1: 3114.02 and loss2: 409.31\n",
      "Epoch [171], train_loss: 3468.06 with loss1: 3056.70 and loss2: 411.36\n",
      "Epoch [172], train_loss: 3480.27 with loss1: 3070.80 and loss2: 409.46\n",
      "Epoch [173], train_loss: 3429.16 with loss1: 3019.79 and loss2: 409.37\n",
      "Epoch [174], train_loss: 3482.15 with loss1: 3075.91 and loss2: 406.24\n",
      "Epoch [175], train_loss: 3449.37 with loss1: 3043.83 and loss2: 405.54\n",
      "Epoch [176], train_loss: 3490.93 with loss1: 3086.11 and loss2: 404.83\n",
      "Epoch [177], train_loss: 3437.23 with loss1: 3034.48 and loss2: 402.75\n",
      "Epoch [178], train_loss: 3482.07 with loss1: 3076.65 and loss2: 405.42\n",
      "Epoch [179], train_loss: 3426.31 with loss1: 3019.15 and loss2: 407.16\n",
      "Epoch [180], train_loss: 3461.67 with loss1: 3054.41 and loss2: 407.26\n",
      "Epoch [181], train_loss: 3384.28 with loss1: 2980.12 and loss2: 404.16\n",
      "Epoch [182], train_loss: 3430.18 with loss1: 3027.16 and loss2: 403.02\n",
      "Epoch [183], train_loss: 3349.57 with loss1: 2947.96 and loss2: 401.61\n",
      "Epoch [184], train_loss: 3391.93 with loss1: 2986.68 and loss2: 405.25\n",
      "Epoch [185], train_loss: 3360.55 with loss1: 2957.93 and loss2: 402.62\n",
      "Epoch [186], train_loss: 3394.53 with loss1: 2990.95 and loss2: 403.58\n",
      "Epoch [187], train_loss: 3349.08 with loss1: 2945.32 and loss2: 403.76\n",
      "Epoch [188], train_loss: 3432.04 with loss1: 3028.35 and loss2: 403.69\n",
      "Epoch [189], train_loss: 3381.83 with loss1: 2980.29 and loss2: 401.55\n",
      "Epoch [190], train_loss: 3449.47 with loss1: 3043.88 and loss2: 405.59\n",
      "Epoch [191], train_loss: 3400.34 with loss1: 3001.08 and loss2: 399.26\n",
      "Epoch [192], train_loss: 3475.43 with loss1: 3072.01 and loss2: 403.42\n",
      "Epoch [193], train_loss: 3393.50 with loss1: 2987.42 and loss2: 406.08\n",
      "Epoch [194], train_loss: 3430.55 with loss1: 3033.30 and loss2: 397.25\n",
      "Epoch [195], train_loss: 3322.96 with loss1: 2920.45 and loss2: 402.51\n",
      "Epoch [196], train_loss: 3373.74 with loss1: 2970.38 and loss2: 403.36\n",
      "Epoch [197], train_loss: 3255.92 with loss1: 2852.43 and loss2: 403.49\n",
      "Epoch [198], train_loss: 3265.05 with loss1: 2865.77 and loss2: 399.27\n",
      "Epoch [199], train_loss: 3178.52 with loss1: 2785.01 and loss2: 393.51\n",
      "Epoch [200], train_loss: 3182.06 with loss1: 2790.76 and loss2: 391.30\n",
      "Epoch [201], train_loss: 3110.56 with loss1: 2721.06 and loss2: 389.49\n",
      "Epoch [202], train_loss: 3160.25 with loss1: 2769.49 and loss2: 390.76\n",
      "Epoch [203], train_loss: 3091.35 with loss1: 2703.28 and loss2: 388.07\n",
      "Epoch [204], train_loss: 3117.92 with loss1: 2725.51 and loss2: 392.41\n",
      "Epoch [205], train_loss: 3047.31 with loss1: 2655.13 and loss2: 392.18\n",
      "Epoch [206], train_loss: 3048.81 with loss1: 2660.15 and loss2: 388.66\n",
      "Epoch [207], train_loss: 2973.87 with loss1: 2585.98 and loss2: 387.89\n",
      "Epoch [208], train_loss: 2977.25 with loss1: 2589.84 and loss2: 387.41\n",
      "Epoch [209], train_loss: 2902.74 with loss1: 2517.71 and loss2: 385.04\n",
      "Epoch [210], train_loss: 2906.32 with loss1: 2521.43 and loss2: 384.89\n",
      "Epoch [211], train_loss: 2854.41 with loss1: 2472.67 and loss2: 381.73\n",
      "Epoch [212], train_loss: 2859.36 with loss1: 2470.57 and loss2: 388.79\n",
      "Epoch [213], train_loss: 2847.14 with loss1: 2464.08 and loss2: 383.06\n",
      "Epoch [214], train_loss: 2850.68 with loss1: 2467.14 and loss2: 383.54\n",
      "Epoch [215], train_loss: 2821.85 with loss1: 2437.64 and loss2: 384.21\n",
      "Epoch [216], train_loss: 2845.92 with loss1: 2462.60 and loss2: 383.32\n",
      "Epoch [217], train_loss: 2836.27 with loss1: 2458.22 and loss2: 378.04\n",
      "Epoch [218], train_loss: 2875.53 with loss1: 2497.20 and loss2: 378.33\n",
      "Epoch [219], train_loss: 2880.85 with loss1: 2497.30 and loss2: 383.55\n",
      "Epoch [220], train_loss: 2938.88 with loss1: 2560.40 and loss2: 378.49\n",
      "Epoch [221], train_loss: 2896.19 with loss1: 2513.91 and loss2: 382.28\n",
      "Epoch [222], train_loss: 2949.38 with loss1: 2571.47 and loss2: 377.91\n",
      "Epoch [223], train_loss: 2908.95 with loss1: 2532.33 and loss2: 376.62\n",
      "Epoch [224], train_loss: 2958.10 with loss1: 2580.14 and loss2: 377.96\n",
      "Epoch [225], train_loss: 2942.10 with loss1: 2568.18 and loss2: 373.92\n",
      "Epoch [226], train_loss: 3015.17 with loss1: 2639.46 and loss2: 375.71\n",
      "Epoch [227], train_loss: 2988.76 with loss1: 2616.48 and loss2: 372.28\n",
      "Epoch [228], train_loss: 3048.84 with loss1: 2675.64 and loss2: 373.21\n",
      "Epoch [229], train_loss: 3012.34 with loss1: 2642.67 and loss2: 369.67\n",
      "Epoch [230], train_loss: 3075.20 with loss1: 2702.76 and loss2: 372.44\n",
      "Epoch [231], train_loss: 3006.56 with loss1: 2638.13 and loss2: 368.43\n",
      "Epoch [232], train_loss: 3067.10 with loss1: 2700.71 and loss2: 366.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [233], train_loss: 2991.09 with loss1: 2625.70 and loss2: 365.39\n",
      "Epoch [234], train_loss: 3040.25 with loss1: 2672.92 and loss2: 367.33\n",
      "Epoch [235], train_loss: 2955.59 with loss1: 2590.78 and loss2: 364.81\n",
      "Epoch [236], train_loss: 3012.16 with loss1: 2648.06 and loss2: 364.10\n",
      "Epoch [237], train_loss: 2925.71 with loss1: 2565.95 and loss2: 359.76\n",
      "Epoch [238], train_loss: 2967.53 with loss1: 2606.40 and loss2: 361.13\n",
      "Epoch [239], train_loss: 2870.71 with loss1: 2509.86 and loss2: 360.85\n",
      "Epoch [240], train_loss: 2909.81 with loss1: 2548.61 and loss2: 361.20\n",
      "Epoch [241], train_loss: 2825.64 with loss1: 2465.85 and loss2: 359.79\n",
      "Epoch [242], train_loss: 2835.18 with loss1: 2475.91 and loss2: 359.28\n",
      "Epoch [243], train_loss: 2764.94 with loss1: 2405.42 and loss2: 359.52\n",
      "Epoch [244], train_loss: 2757.89 with loss1: 2399.63 and loss2: 358.26\n",
      "Epoch [245], train_loss: 2686.97 with loss1: 2330.10 and loss2: 356.87\n",
      "Epoch [246], train_loss: 2694.38 with loss1: 2335.27 and loss2: 359.11\n",
      "Epoch [247], train_loss: 2642.01 with loss1: 2284.62 and loss2: 357.39\n",
      "Epoch [248], train_loss: 2645.91 with loss1: 2291.77 and loss2: 354.14\n",
      "Epoch [249], train_loss: 2606.64 with loss1: 2251.11 and loss2: 355.52\n",
      "Epoch [250], train_loss: 2618.52 with loss1: 2264.13 and loss2: 354.39\n",
      "Epoch [251], train_loss: 2584.23 with loss1: 2231.16 and loss2: 353.08\n",
      "Epoch [252], train_loss: 2590.59 with loss1: 2237.73 and loss2: 352.86\n",
      "Epoch [253], train_loss: 2584.71 with loss1: 2229.62 and loss2: 355.08\n",
      "Epoch [254], train_loss: 2576.87 with loss1: 2226.06 and loss2: 350.81\n",
      "Epoch [255], train_loss: 2559.56 with loss1: 2207.44 and loss2: 352.12\n",
      "Epoch [256], train_loss: 2576.61 with loss1: 2225.31 and loss2: 351.30\n",
      "Epoch [257], train_loss: 2557.43 with loss1: 2206.30 and loss2: 351.13\n",
      "Epoch [258], train_loss: 2592.32 with loss1: 2240.12 and loss2: 352.19\n",
      "Epoch [259], train_loss: 2582.48 with loss1: 2233.95 and loss2: 348.53\n",
      "Epoch [260], train_loss: 2596.82 with loss1: 2246.98 and loss2: 349.84\n",
      "Epoch [261], train_loss: 2567.59 with loss1: 2220.27 and loss2: 347.32\n",
      "Epoch [262], train_loss: 2606.71 with loss1: 2258.23 and loss2: 348.48\n",
      "Epoch [263], train_loss: 2584.94 with loss1: 2237.84 and loss2: 347.10\n",
      "Epoch [264], train_loss: 2629.04 with loss1: 2278.75 and loss2: 350.29\n",
      "Epoch [265], train_loss: 2586.96 with loss1: 2240.58 and loss2: 346.38\n",
      "Epoch [266], train_loss: 2616.80 with loss1: 2268.26 and loss2: 348.54\n",
      "Epoch [267], train_loss: 2594.93 with loss1: 2249.62 and loss2: 345.31\n",
      "Epoch [268], train_loss: 2633.24 with loss1: 2286.41 and loss2: 346.84\n",
      "Epoch [269], train_loss: 2603.24 with loss1: 2257.32 and loss2: 345.92\n",
      "Epoch [270], train_loss: 2608.48 with loss1: 2265.60 and loss2: 342.88\n",
      "Epoch [271], train_loss: 2584.65 with loss1: 2241.94 and loss2: 342.71\n",
      "Epoch [272], train_loss: 2622.74 with loss1: 2278.75 and loss2: 343.99\n",
      "Epoch [273], train_loss: 2590.36 with loss1: 2249.21 and loss2: 341.15\n",
      "Epoch [274], train_loss: 2623.53 with loss1: 2280.40 and loss2: 343.13\n",
      "Epoch [275], train_loss: 2571.05 with loss1: 2231.50 and loss2: 339.55\n",
      "Epoch [276], train_loss: 2624.96 with loss1: 2283.88 and loss2: 341.09\n",
      "Epoch [277], train_loss: 2586.32 with loss1: 2247.36 and loss2: 338.96\n",
      "Epoch [278], train_loss: 2617.70 with loss1: 2281.12 and loss2: 336.58\n",
      "Epoch [279], train_loss: 2578.64 with loss1: 2242.28 and loss2: 336.36\n",
      "Epoch [280], train_loss: 2622.90 with loss1: 2281.56 and loss2: 341.34\n",
      "Epoch [281], train_loss: 2566.60 with loss1: 2226.85 and loss2: 339.75\n",
      "Epoch [282], train_loss: 2607.23 with loss1: 2269.72 and loss2: 337.51\n",
      "Epoch [283], train_loss: 2553.80 with loss1: 2216.72 and loss2: 337.08\n",
      "Epoch [284], train_loss: 2571.05 with loss1: 2234.31 and loss2: 336.74\n",
      "Epoch [285], train_loss: 2509.54 with loss1: 2172.19 and loss2: 337.35\n",
      "Epoch [286], train_loss: 2533.86 with loss1: 2195.47 and loss2: 338.39\n",
      "Epoch [287], train_loss: 2494.26 with loss1: 2158.09 and loss2: 336.17\n",
      "Epoch [288], train_loss: 2506.85 with loss1: 2169.32 and loss2: 337.53\n",
      "Epoch [289], train_loss: 2481.79 with loss1: 2147.73 and loss2: 334.06\n",
      "Epoch [290], train_loss: 2499.43 with loss1: 2166.69 and loss2: 332.74\n",
      "Epoch [291], train_loss: 2477.16 with loss1: 2145.75 and loss2: 331.41\n",
      "Epoch [292], train_loss: 2500.69 with loss1: 2169.15 and loss2: 331.54\n",
      "Epoch [293], train_loss: 2480.93 with loss1: 2149.19 and loss2: 331.74\n",
      "Epoch [294], train_loss: 2508.83 with loss1: 2178.59 and loss2: 330.24\n",
      "Epoch [295], train_loss: 2473.33 with loss1: 2142.96 and loss2: 330.38\n",
      "Epoch [296], train_loss: 2518.24 with loss1: 2188.44 and loss2: 329.80\n",
      "Epoch [297], train_loss: 2482.82 with loss1: 2151.86 and loss2: 330.96\n",
      "Epoch [298], train_loss: 2530.32 with loss1: 2201.98 and loss2: 328.34\n",
      "Epoch [299], train_loss: 2495.58 with loss1: 2166.72 and loss2: 328.86\n",
      "Epoch [300], train_loss: 2538.47 with loss1: 2210.06 and loss2: 328.41\n",
      "Epoch [301], train_loss: 2476.07 with loss1: 2147.75 and loss2: 328.32\n",
      "Epoch [302], train_loss: 2510.50 with loss1: 2182.86 and loss2: 327.64\n",
      "Epoch [303], train_loss: 2436.21 with loss1: 2110.57 and loss2: 325.63\n",
      "Epoch [304], train_loss: 2443.51 with loss1: 2117.15 and loss2: 326.36\n",
      "Epoch [305], train_loss: 2391.36 with loss1: 2066.78 and loss2: 324.58\n",
      "Epoch [306], train_loss: 2395.61 with loss1: 2068.97 and loss2: 326.64\n",
      "Epoch [307], train_loss: 2348.27 with loss1: 2023.75 and loss2: 324.52\n",
      "Epoch [308], train_loss: 2349.06 with loss1: 2028.45 and loss2: 320.61\n",
      "Epoch [309], train_loss: 2297.74 with loss1: 1976.11 and loss2: 321.63\n",
      "Epoch [310], train_loss: 2297.32 with loss1: 1977.84 and loss2: 319.47\n",
      "Epoch [311], train_loss: 2266.17 with loss1: 1945.03 and loss2: 321.14\n",
      "Epoch [312], train_loss: 2270.78 with loss1: 1949.13 and loss2: 321.65\n",
      "Epoch [313], train_loss: 2243.26 with loss1: 1923.48 and loss2: 319.78\n",
      "Epoch [314], train_loss: 2261.43 with loss1: 1941.40 and loss2: 320.03\n",
      "Epoch [315], train_loss: 2255.42 with loss1: 1934.72 and loss2: 320.70\n",
      "Epoch [316], train_loss: 2267.64 with loss1: 1949.00 and loss2: 318.64\n",
      "Epoch [317], train_loss: 2243.93 with loss1: 1925.86 and loss2: 318.06\n",
      "Epoch [318], train_loss: 2263.76 with loss1: 1947.42 and loss2: 316.34\n",
      "Epoch [319], train_loss: 2252.01 with loss1: 1933.98 and loss2: 318.03\n",
      "Epoch [320], train_loss: 2265.92 with loss1: 1950.73 and loss2: 315.20\n",
      "Epoch [321], train_loss: 2260.60 with loss1: 1944.42 and loss2: 316.18\n",
      "Epoch [322], train_loss: 2299.79 with loss1: 1983.43 and loss2: 316.36\n",
      "Epoch [323], train_loss: 2281.66 with loss1: 1966.59 and loss2: 315.07\n",
      "Epoch [324], train_loss: 2311.85 with loss1: 1997.03 and loss2: 314.81\n",
      "Epoch [325], train_loss: 2326.45 with loss1: 2011.23 and loss2: 315.22\n",
      "Epoch [326], train_loss: 2364.35 with loss1: 2050.67 and loss2: 313.68\n",
      "Epoch [327], train_loss: 2341.67 with loss1: 2028.46 and loss2: 313.21\n",
      "Epoch [328], train_loss: 2393.82 with loss1: 2077.69 and loss2: 316.14\n",
      "Epoch [329], train_loss: 2353.62 with loss1: 2041.01 and loss2: 312.60\n",
      "Epoch [330], train_loss: 2407.63 with loss1: 2097.63 and loss2: 310.00\n",
      "Epoch [331], train_loss: 2365.84 with loss1: 2052.03 and loss2: 313.81\n",
      "Epoch [332], train_loss: 2398.68 with loss1: 2085.92 and loss2: 312.75\n",
      "Epoch [333], train_loss: 2343.82 with loss1: 2033.87 and loss2: 309.94\n",
      "Epoch [334], train_loss: 2381.70 with loss1: 2071.68 and loss2: 310.02\n",
      "Epoch [335], train_loss: 2314.34 with loss1: 2002.52 and loss2: 311.82\n",
      "Epoch [336], train_loss: 2351.01 with loss1: 2040.27 and loss2: 310.73\n",
      "Epoch [337], train_loss: 2267.85 with loss1: 1959.40 and loss2: 308.45\n",
      "Epoch [338], train_loss: 2288.86 with loss1: 1979.45 and loss2: 309.41\n",
      "Epoch [339], train_loss: 2239.37 with loss1: 1933.63 and loss2: 305.74\n",
      "Epoch [340], train_loss: 2247.62 with loss1: 1941.31 and loss2: 306.31\n",
      "Epoch [341], train_loss: 2225.26 with loss1: 1918.82 and loss2: 306.44\n",
      "Epoch [342], train_loss: 2222.02 with loss1: 1916.24 and loss2: 305.79\n",
      "Epoch [343], train_loss: 2181.40 with loss1: 1876.58 and loss2: 304.82\n",
      "Epoch [344], train_loss: 2204.24 with loss1: 1898.04 and loss2: 306.20\n",
      "Epoch [345], train_loss: 2173.84 with loss1: 1869.81 and loss2: 304.03\n",
      "Epoch [346], train_loss: 2183.63 with loss1: 1878.27 and loss2: 305.36\n",
      "Epoch [347], train_loss: 2159.13 with loss1: 1853.77 and loss2: 305.35\n",
      "Epoch [348], train_loss: 2184.84 with loss1: 1880.51 and loss2: 304.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [349], train_loss: 2172.01 with loss1: 1867.97 and loss2: 304.04\n",
      "Epoch [350], train_loss: 2184.43 with loss1: 1880.30 and loss2: 304.14\n",
      "Epoch [351], train_loss: 2174.25 with loss1: 1872.86 and loss2: 301.39\n",
      "Epoch [352], train_loss: 2200.57 with loss1: 1894.85 and loss2: 305.73\n",
      "Epoch [353], train_loss: 2171.43 with loss1: 1870.43 and loss2: 301.01\n",
      "Epoch [354], train_loss: 2201.15 with loss1: 1899.15 and loss2: 302.00\n",
      "Epoch [355], train_loss: 2171.21 with loss1: 1872.25 and loss2: 298.96\n",
      "Epoch [356], train_loss: 2205.59 with loss1: 1903.69 and loss2: 301.89\n",
      "Epoch [357], train_loss: 2185.73 with loss1: 1884.25 and loss2: 301.48\n",
      "Epoch [358], train_loss: 2211.06 with loss1: 1910.22 and loss2: 300.83\n",
      "Epoch [359], train_loss: 2197.60 with loss1: 1900.03 and loss2: 297.57\n",
      "Epoch [360], train_loss: 2217.16 with loss1: 1917.93 and loss2: 299.23\n",
      "Epoch [361], train_loss: 2186.75 with loss1: 1888.64 and loss2: 298.11\n",
      "Epoch [362], train_loss: 2233.10 with loss1: 1933.80 and loss2: 299.30\n",
      "Epoch [363], train_loss: 2187.06 with loss1: 1889.91 and loss2: 297.15\n",
      "Epoch [364], train_loss: 2209.63 with loss1: 1912.19 and loss2: 297.44\n",
      "Epoch [365], train_loss: 2167.45 with loss1: 1871.94 and loss2: 295.51\n",
      "Epoch [366], train_loss: 2189.37 with loss1: 1890.25 and loss2: 299.13\n",
      "Epoch [367], train_loss: 2155.56 with loss1: 1858.72 and loss2: 296.84\n",
      "Epoch [368], train_loss: 2164.86 with loss1: 1867.51 and loss2: 297.35\n",
      "Epoch [369], train_loss: 2125.06 with loss1: 1830.72 and loss2: 294.34\n",
      "Epoch [370], train_loss: 2147.22 with loss1: 1851.92 and loss2: 295.30\n",
      "Epoch [371], train_loss: 2104.64 with loss1: 1811.74 and loss2: 292.90\n",
      "Epoch [372], train_loss: 2120.37 with loss1: 1826.03 and loss2: 294.34\n",
      "Epoch [373], train_loss: 2094.76 with loss1: 1802.65 and loss2: 292.11\n",
      "Epoch [374], train_loss: 2112.61 with loss1: 1817.84 and loss2: 294.77\n",
      "Epoch [375], train_loss: 2078.27 with loss1: 1786.88 and loss2: 291.39\n",
      "Epoch [376], train_loss: 2107.95 with loss1: 1814.40 and loss2: 293.54\n",
      "Epoch [377], train_loss: 2091.72 with loss1: 1798.99 and loss2: 292.73\n",
      "Epoch [378], train_loss: 2099.84 with loss1: 1807.33 and loss2: 292.51\n",
      "Epoch [379], train_loss: 2070.69 with loss1: 1780.12 and loss2: 290.58\n",
      "Epoch [380], train_loss: 2093.77 with loss1: 1804.45 and loss2: 289.32\n",
      "Epoch [381], train_loss: 2065.01 with loss1: 1775.17 and loss2: 289.84\n",
      "Epoch [382], train_loss: 2085.24 with loss1: 1794.97 and loss2: 290.27\n",
      "Epoch [383], train_loss: 2060.09 with loss1: 1771.22 and loss2: 288.87\n",
      "Epoch [384], train_loss: 2087.64 with loss1: 1795.05 and loss2: 292.60\n",
      "Epoch [385], train_loss: 2050.45 with loss1: 1760.93 and loss2: 289.52\n",
      "Epoch [386], train_loss: 2056.16 with loss1: 1766.82 and loss2: 289.33\n",
      "Epoch [387], train_loss: 2037.97 with loss1: 1750.86 and loss2: 287.11\n",
      "Epoch [388], train_loss: 2053.14 with loss1: 1762.63 and loss2: 290.51\n",
      "Epoch [389], train_loss: 2022.90 with loss1: 1736.33 and loss2: 286.57\n",
      "Epoch [390], train_loss: 2039.30 with loss1: 1750.70 and loss2: 288.60\n",
      "Epoch [391], train_loss: 2013.76 with loss1: 1727.26 and loss2: 286.50\n",
      "Epoch [392], train_loss: 2029.29 with loss1: 1741.44 and loss2: 287.85\n",
      "Epoch [393], train_loss: 2005.96 with loss1: 1719.89 and loss2: 286.07\n",
      "Epoch [394], train_loss: 2030.79 with loss1: 1742.73 and loss2: 288.06\n",
      "Epoch [395], train_loss: 2009.82 with loss1: 1725.23 and loss2: 284.59\n",
      "Epoch [396], train_loss: 2043.13 with loss1: 1755.35 and loss2: 287.78\n",
      "Epoch [397], train_loss: 2024.13 with loss1: 1741.85 and loss2: 282.28\n",
      "Epoch [398], train_loss: 2047.69 with loss1: 1762.86 and loss2: 284.84\n",
      "Epoch [399], train_loss: 2036.25 with loss1: 1752.76 and loss2: 283.49\n",
      "Epoch [400], train_loss: 2057.93 with loss1: 1774.06 and loss2: 283.87\n",
      "Epoch [401], train_loss: 2044.89 with loss1: 1760.51 and loss2: 284.38\n",
      "Epoch [402], train_loss: 2076.27 with loss1: 1792.92 and loss2: 283.36\n",
      "Epoch [403], train_loss: 2057.13 with loss1: 1773.97 and loss2: 283.16\n",
      "Epoch [404], train_loss: 2096.32 with loss1: 1812.45 and loss2: 283.87\n",
      "Epoch [405], train_loss: 2053.59 with loss1: 1769.26 and loss2: 284.34\n",
      "Epoch [406], train_loss: 2077.57 with loss1: 1794.27 and loss2: 283.31\n",
      "Epoch [407], train_loss: 2036.20 with loss1: 1753.95 and loss2: 282.25\n",
      "Epoch [408], train_loss: 2046.50 with loss1: 1764.50 and loss2: 281.99\n",
      "Epoch [409], train_loss: 2007.74 with loss1: 1726.68 and loss2: 281.06\n",
      "Epoch [410], train_loss: 2022.97 with loss1: 1741.37 and loss2: 281.60\n",
      "Epoch [411], train_loss: 1982.96 with loss1: 1701.36 and loss2: 281.60\n",
      "Epoch [412], train_loss: 1988.85 with loss1: 1708.99 and loss2: 279.86\n",
      "Epoch [413], train_loss: 1946.53 with loss1: 1668.28 and loss2: 278.26\n",
      "Epoch [414], train_loss: 1963.78 with loss1: 1683.66 and loss2: 280.12\n",
      "Epoch [415], train_loss: 1925.76 with loss1: 1646.80 and loss2: 278.96\n",
      "Epoch [416], train_loss: 1937.49 with loss1: 1659.31 and loss2: 278.18\n",
      "Epoch [417], train_loss: 1912.41 with loss1: 1634.71 and loss2: 277.70\n",
      "Epoch [418], train_loss: 1917.93 with loss1: 1641.06 and loss2: 276.86\n",
      "Epoch [419], train_loss: 1906.40 with loss1: 1631.34 and loss2: 275.07\n",
      "Epoch [420], train_loss: 1904.59 with loss1: 1626.30 and loss2: 278.29\n",
      "Epoch [421], train_loss: 1893.59 with loss1: 1618.12 and loss2: 275.48\n",
      "Epoch [422], train_loss: 1912.79 with loss1: 1637.70 and loss2: 275.08\n",
      "Epoch [423], train_loss: 1902.29 with loss1: 1626.70 and loss2: 275.59\n",
      "Epoch [424], train_loss: 1916.24 with loss1: 1640.65 and loss2: 275.59\n",
      "Epoch [425], train_loss: 1916.30 with loss1: 1642.63 and loss2: 273.67\n",
      "Epoch [426], train_loss: 1932.12 with loss1: 1657.84 and loss2: 274.28\n",
      "Epoch [427], train_loss: 1926.40 with loss1: 1652.66 and loss2: 273.74\n",
      "Epoch [428], train_loss: 1951.37 with loss1: 1676.40 and loss2: 274.97\n",
      "Epoch [429], train_loss: 1938.83 with loss1: 1667.10 and loss2: 271.73\n",
      "Epoch [430], train_loss: 1965.73 with loss1: 1691.65 and loss2: 274.08\n",
      "Epoch [431], train_loss: 1954.65 with loss1: 1681.81 and loss2: 272.84\n",
      "Epoch [432], train_loss: 1984.20 with loss1: 1711.45 and loss2: 272.75\n",
      "Epoch [433], train_loss: 1970.27 with loss1: 1700.51 and loss2: 269.76\n",
      "Epoch [434], train_loss: 2006.97 with loss1: 1735.88 and loss2: 271.09\n",
      "Epoch [435], train_loss: 1983.70 with loss1: 1710.75 and loss2: 272.95\n",
      "Epoch [436], train_loss: 2021.88 with loss1: 1749.08 and loss2: 272.80\n",
      "Epoch [437], train_loss: 1999.61 with loss1: 1729.66 and loss2: 269.95\n",
      "Epoch [438], train_loss: 2039.28 with loss1: 1768.35 and loss2: 270.93\n",
      "Epoch [439], train_loss: 2005.30 with loss1: 1736.63 and loss2: 268.67\n",
      "Epoch [440], train_loss: 2033.57 with loss1: 1763.56 and loss2: 270.01\n",
      "Epoch [441], train_loss: 2002.99 with loss1: 1735.16 and loss2: 267.82\n",
      "Epoch [442], train_loss: 2023.49 with loss1: 1755.81 and loss2: 267.68\n",
      "Epoch [443], train_loss: 1981.42 with loss1: 1716.26 and loss2: 265.17\n",
      "Epoch [444], train_loss: 2006.86 with loss1: 1738.12 and loss2: 268.74\n",
      "Epoch [445], train_loss: 1964.49 with loss1: 1698.13 and loss2: 266.35\n",
      "Epoch [446], train_loss: 1975.67 with loss1: 1708.61 and loss2: 267.06\n",
      "Epoch [447], train_loss: 1931.74 with loss1: 1665.15 and loss2: 266.58\n",
      "Epoch [448], train_loss: 1937.32 with loss1: 1669.59 and loss2: 267.73\n",
      "Epoch [449], train_loss: 1887.20 with loss1: 1623.46 and loss2: 263.75\n",
      "Epoch [450], train_loss: 1892.33 with loss1: 1625.18 and loss2: 267.15\n",
      "Epoch [451], train_loss: 1859.29 with loss1: 1594.42 and loss2: 264.87\n",
      "Epoch [452], train_loss: 1868.98 with loss1: 1603.80 and loss2: 265.17\n",
      "Epoch [453], train_loss: 1840.95 with loss1: 1577.44 and loss2: 263.51\n",
      "Epoch [454], train_loss: 1842.55 with loss1: 1578.63 and loss2: 263.93\n",
      "Epoch [455], train_loss: 1821.86 with loss1: 1557.13 and loss2: 264.73\n",
      "Epoch [456], train_loss: 1837.37 with loss1: 1570.74 and loss2: 266.63\n",
      "Epoch [457], train_loss: 1824.81 with loss1: 1562.26 and loss2: 262.55\n",
      "Epoch [458], train_loss: 1822.46 with loss1: 1557.88 and loss2: 264.59\n",
      "Epoch [459], train_loss: 1805.05 with loss1: 1543.67 and loss2: 261.39\n",
      "Epoch [460], train_loss: 1818.14 with loss1: 1555.56 and loss2: 262.58\n",
      "Epoch [461], train_loss: 1814.55 with loss1: 1553.36 and loss2: 261.19\n",
      "Epoch [462], train_loss: 1823.51 with loss1: 1559.99 and loss2: 263.52\n",
      "Epoch [463], train_loss: 1807.61 with loss1: 1546.04 and loss2: 261.57\n",
      "Epoch [464], train_loss: 1834.12 with loss1: 1571.93 and loss2: 262.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [465], train_loss: 1822.10 with loss1: 1561.87 and loss2: 260.23\n",
      "Epoch [466], train_loss: 1856.05 with loss1: 1594.39 and loss2: 261.66\n",
      "Epoch [467], train_loss: 1837.77 with loss1: 1577.81 and loss2: 259.96\n",
      "Epoch [468], train_loss: 1860.36 with loss1: 1598.73 and loss2: 261.63\n",
      "Epoch [469], train_loss: 1837.30 with loss1: 1576.31 and loss2: 260.98\n",
      "Epoch [470], train_loss: 1856.95 with loss1: 1596.73 and loss2: 260.22\n",
      "Epoch [471], train_loss: 1844.06 with loss1: 1585.34 and loss2: 258.73\n",
      "Epoch [472], train_loss: 1874.68 with loss1: 1614.75 and loss2: 259.92\n",
      "Epoch [473], train_loss: 1844.93 with loss1: 1587.35 and loss2: 257.58\n",
      "Epoch [474], train_loss: 1874.14 with loss1: 1614.14 and loss2: 260.00\n",
      "Epoch [475], train_loss: 1853.82 with loss1: 1596.26 and loss2: 257.56\n",
      "Epoch [476], train_loss: 1880.07 with loss1: 1621.73 and loss2: 258.34\n",
      "Epoch [477], train_loss: 1857.72 with loss1: 1600.15 and loss2: 257.57\n",
      "Epoch [478], train_loss: 1867.97 with loss1: 1610.48 and loss2: 257.49\n",
      "Epoch [479], train_loss: 1836.79 with loss1: 1579.56 and loss2: 257.24\n",
      "Epoch [480], train_loss: 1848.71 with loss1: 1591.86 and loss2: 256.85\n",
      "Epoch [481], train_loss: 1821.88 with loss1: 1564.90 and loss2: 256.98\n",
      "Epoch [482], train_loss: 1825.13 with loss1: 1566.72 and loss2: 258.40\n",
      "Epoch [483], train_loss: 1793.99 with loss1: 1540.23 and loss2: 253.76\n",
      "Epoch [484], train_loss: 1802.65 with loss1: 1546.28 and loss2: 256.36\n",
      "Epoch [485], train_loss: 1773.65 with loss1: 1520.09 and loss2: 253.56\n",
      "Epoch [486], train_loss: 1785.46 with loss1: 1529.63 and loss2: 255.83\n",
      "Epoch [487], train_loss: 1769.42 with loss1: 1514.99 and loss2: 254.43\n",
      "Epoch [488], train_loss: 1778.38 with loss1: 1522.46 and loss2: 255.92\n",
      "Epoch [489], train_loss: 1754.33 with loss1: 1500.70 and loss2: 253.63\n",
      "Epoch [490], train_loss: 1762.83 with loss1: 1507.84 and loss2: 254.99\n",
      "Epoch [491], train_loss: 1754.30 with loss1: 1501.05 and loss2: 253.24\n",
      "Epoch [492], train_loss: 1761.53 with loss1: 1508.83 and loss2: 252.71\n",
      "Epoch [493], train_loss: 1758.67 with loss1: 1505.74 and loss2: 252.93\n",
      "Epoch [494], train_loss: 1771.70 with loss1: 1518.27 and loss2: 253.43\n",
      "Epoch [495], train_loss: 1761.70 with loss1: 1508.91 and loss2: 252.79\n",
      "Epoch [496], train_loss: 1775.22 with loss1: 1520.36 and loss2: 254.86\n",
      "Epoch [497], train_loss: 1760.52 with loss1: 1510.00 and loss2: 250.52\n",
      "Epoch [498], train_loss: 1784.58 with loss1: 1532.55 and loss2: 252.03\n",
      "Epoch [499], train_loss: 1768.68 with loss1: 1518.30 and loss2: 250.39\n",
      "Epoch [500], train_loss: 1788.08 with loss1: 1536.14 and loss2: 251.95\n",
      "Epoch [501], train_loss: 1772.78 with loss1: 1523.02 and loss2: 249.76\n",
      "Epoch [502], train_loss: 1802.93 with loss1: 1552.43 and loss2: 250.50\n",
      "Epoch [503], train_loss: 1783.45 with loss1: 1535.18 and loss2: 248.27\n",
      "Epoch [504], train_loss: 1803.10 with loss1: 1552.65 and loss2: 250.45\n",
      "Epoch [505], train_loss: 1776.81 with loss1: 1528.08 and loss2: 248.73\n",
      "Epoch [506], train_loss: 1804.07 with loss1: 1553.96 and loss2: 250.11\n",
      "Epoch [507], train_loss: 1781.30 with loss1: 1532.36 and loss2: 248.94\n",
      "Epoch [508], train_loss: 1807.43 with loss1: 1558.07 and loss2: 249.36\n",
      "Epoch [509], train_loss: 1790.29 with loss1: 1541.82 and loss2: 248.47\n",
      "Epoch [510], train_loss: 1820.82 with loss1: 1571.47 and loss2: 249.35\n",
      "Epoch [511], train_loss: 1793.56 with loss1: 1546.20 and loss2: 247.36\n",
      "Epoch [512], train_loss: 1812.40 with loss1: 1564.42 and loss2: 247.98\n",
      "Epoch [513], train_loss: 1789.63 with loss1: 1542.68 and loss2: 246.94\n",
      "Epoch [514], train_loss: 1807.79 with loss1: 1559.22 and loss2: 248.57\n",
      "Epoch [515], train_loss: 1770.17 with loss1: 1524.44 and loss2: 245.73\n",
      "Epoch [516], train_loss: 1791.41 with loss1: 1543.63 and loss2: 247.78\n",
      "Epoch [517], train_loss: 1765.04 with loss1: 1519.02 and loss2: 246.02\n",
      "Epoch [518], train_loss: 1769.01 with loss1: 1521.98 and loss2: 247.03\n",
      "Epoch [519], train_loss: 1738.08 with loss1: 1492.58 and loss2: 245.49\n",
      "Epoch [520], train_loss: 1746.95 with loss1: 1500.38 and loss2: 246.57\n",
      "Epoch [521], train_loss: 1718.12 with loss1: 1473.68 and loss2: 244.44\n",
      "Epoch [522], train_loss: 1728.01 with loss1: 1482.03 and loss2: 245.98\n",
      "Epoch [523], train_loss: 1701.45 with loss1: 1456.98 and loss2: 244.47\n",
      "Epoch [524], train_loss: 1712.13 with loss1: 1467.69 and loss2: 244.44\n",
      "Epoch [525], train_loss: 1687.91 with loss1: 1444.42 and loss2: 243.49\n",
      "Epoch [526], train_loss: 1690.84 with loss1: 1446.01 and loss2: 244.83\n",
      "Epoch [527], train_loss: 1680.91 with loss1: 1438.11 and loss2: 242.81\n",
      "Epoch [528], train_loss: 1686.91 with loss1: 1444.07 and loss2: 242.84\n",
      "Epoch [529], train_loss: 1660.75 with loss1: 1418.03 and loss2: 242.73\n",
      "Epoch [530], train_loss: 1669.02 with loss1: 1425.34 and loss2: 243.68\n",
      "Epoch [531], train_loss: 1668.25 with loss1: 1425.86 and loss2: 242.39\n",
      "Epoch [532], train_loss: 1676.23 with loss1: 1432.20 and loss2: 244.03\n",
      "Epoch [533], train_loss: 1666.63 with loss1: 1424.06 and loss2: 242.57\n",
      "Epoch [534], train_loss: 1666.94 with loss1: 1424.74 and loss2: 242.20\n",
      "Epoch [535], train_loss: 1652.96 with loss1: 1410.64 and loss2: 242.32\n",
      "Epoch [536], train_loss: 1671.64 with loss1: 1429.38 and loss2: 242.26\n",
      "Epoch [537], train_loss: 1655.17 with loss1: 1414.73 and loss2: 240.44\n",
      "Epoch [538], train_loss: 1681.16 with loss1: 1439.46 and loss2: 241.70\n",
      "Epoch [539], train_loss: 1662.42 with loss1: 1421.71 and loss2: 240.71\n",
      "Epoch [540], train_loss: 1673.41 with loss1: 1431.23 and loss2: 242.19\n",
      "Epoch [541], train_loss: 1663.32 with loss1: 1423.09 and loss2: 240.24\n",
      "Epoch [542], train_loss: 1680.28 with loss1: 1439.31 and loss2: 240.97\n",
      "Epoch [543], train_loss: 1658.62 with loss1: 1418.87 and loss2: 239.75\n",
      "Epoch [544], train_loss: 1667.71 with loss1: 1427.90 and loss2: 239.81\n",
      "Epoch [545], train_loss: 1645.48 with loss1: 1405.45 and loss2: 240.03\n",
      "Epoch [546], train_loss: 1660.87 with loss1: 1419.88 and loss2: 240.99\n",
      "Epoch [547], train_loss: 1633.30 with loss1: 1395.18 and loss2: 238.12\n",
      "Epoch [548], train_loss: 1649.34 with loss1: 1408.92 and loss2: 240.42\n",
      "Epoch [549], train_loss: 1629.65 with loss1: 1391.93 and loss2: 237.72\n",
      "Epoch [550], train_loss: 1636.96 with loss1: 1398.81 and loss2: 238.15\n",
      "Epoch [551], train_loss: 1616.48 with loss1: 1379.83 and loss2: 236.65\n",
      "Epoch [552], train_loss: 1626.83 with loss1: 1389.06 and loss2: 237.77\n",
      "Epoch [553], train_loss: 1617.25 with loss1: 1380.12 and loss2: 237.13\n",
      "Epoch [554], train_loss: 1623.53 with loss1: 1385.22 and loss2: 238.31\n",
      "Epoch [555], train_loss: 1617.40 with loss1: 1380.27 and loss2: 237.13\n",
      "Epoch [556], train_loss: 1628.21 with loss1: 1389.96 and loss2: 238.24\n",
      "Epoch [557], train_loss: 1608.91 with loss1: 1372.41 and loss2: 236.50\n",
      "Epoch [558], train_loss: 1625.08 with loss1: 1387.34 and loss2: 237.74\n",
      "Epoch [559], train_loss: 1611.81 with loss1: 1377.22 and loss2: 234.60\n",
      "Epoch [560], train_loss: 1631.14 with loss1: 1394.49 and loss2: 236.65\n",
      "Epoch [561], train_loss: 1621.11 with loss1: 1387.00 and loss2: 234.12\n",
      "Epoch [562], train_loss: 1637.82 with loss1: 1401.03 and loss2: 236.79\n",
      "Epoch [563], train_loss: 1615.30 with loss1: 1381.89 and loss2: 233.41\n",
      "Epoch [564], train_loss: 1640.33 with loss1: 1404.95 and loss2: 235.37\n",
      "Epoch [565], train_loss: 1625.70 with loss1: 1392.03 and loss2: 233.67\n",
      "Epoch [566], train_loss: 1641.30 with loss1: 1406.24 and loss2: 235.05\n",
      "Epoch [567], train_loss: 1642.65 with loss1: 1408.87 and loss2: 233.78\n",
      "Epoch [568], train_loss: 1659.16 with loss1: 1423.42 and loss2: 235.74\n",
      "Epoch [569], train_loss: 1639.11 with loss1: 1405.97 and loss2: 233.14\n",
      "Epoch [570], train_loss: 1663.95 with loss1: 1428.39 and loss2: 235.56\n",
      "Epoch [571], train_loss: 1641.31 with loss1: 1409.47 and loss2: 231.85\n",
      "Epoch [572], train_loss: 1655.91 with loss1: 1421.38 and loss2: 234.53\n",
      "Epoch [573], train_loss: 1641.03 with loss1: 1409.62 and loss2: 231.40\n",
      "Epoch [574], train_loss: 1654.79 with loss1: 1421.91 and loss2: 232.88\n",
      "Epoch [575], train_loss: 1642.04 with loss1: 1410.56 and loss2: 231.48\n",
      "Epoch [576], train_loss: 1653.68 with loss1: 1420.92 and loss2: 232.77\n",
      "Epoch [577], train_loss: 1641.33 with loss1: 1410.09 and loss2: 231.25\n",
      "Epoch [578], train_loss: 1658.14 with loss1: 1425.06 and loss2: 233.08\n",
      "Epoch [579], train_loss: 1629.30 with loss1: 1399.57 and loss2: 229.72\n",
      "Epoch [580], train_loss: 1650.81 with loss1: 1418.16 and loss2: 232.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [581], train_loss: 1633.80 with loss1: 1404.47 and loss2: 229.32\n",
      "Epoch [582], train_loss: 1645.88 with loss1: 1414.06 and loss2: 231.82\n",
      "Epoch [583], train_loss: 1629.28 with loss1: 1399.28 and loss2: 230.00\n",
      "Epoch [584], train_loss: 1641.90 with loss1: 1410.34 and loss2: 231.56\n",
      "Epoch [585], train_loss: 1618.27 with loss1: 1390.05 and loss2: 228.22\n",
      "Epoch [586], train_loss: 1625.26 with loss1: 1394.21 and loss2: 231.05\n",
      "Epoch [587], train_loss: 1605.89 with loss1: 1377.65 and loss2: 228.24\n",
      "Epoch [588], train_loss: 1618.19 with loss1: 1387.40 and loss2: 230.79\n",
      "Epoch [589], train_loss: 1599.18 with loss1: 1370.98 and loss2: 228.21\n",
      "Epoch [590], train_loss: 1612.04 with loss1: 1383.32 and loss2: 228.72\n",
      "Epoch [591], train_loss: 1589.07 with loss1: 1361.36 and loss2: 227.72\n",
      "Epoch [592], train_loss: 1599.66 with loss1: 1370.64 and loss2: 229.03\n",
      "Epoch [593], train_loss: 1580.74 with loss1: 1352.21 and loss2: 228.52\n",
      "Epoch [594], train_loss: 1586.75 with loss1: 1358.20 and loss2: 228.55\n",
      "Epoch [595], train_loss: 1566.14 with loss1: 1339.12 and loss2: 227.02\n",
      "Epoch [596], train_loss: 1576.52 with loss1: 1347.86 and loss2: 228.66\n",
      "Epoch [597], train_loss: 1562.12 with loss1: 1335.76 and loss2: 226.36\n",
      "Epoch [598], train_loss: 1567.59 with loss1: 1338.99 and loss2: 228.61\n",
      "Epoch [599], train_loss: 1553.64 with loss1: 1326.95 and loss2: 226.69\n",
      "Epoch [600], train_loss: 1551.68 with loss1: 1324.08 and loss2: 227.59\n",
      "Epoch [601], train_loss: 1540.22 with loss1: 1313.73 and loss2: 226.49\n",
      "Epoch [602], train_loss: 1543.51 with loss1: 1316.23 and loss2: 227.27\n",
      "Epoch [603], train_loss: 1534.57 with loss1: 1308.97 and loss2: 225.59\n",
      "Epoch [604], train_loss: 1546.37 with loss1: 1319.39 and loss2: 226.97\n",
      "Epoch [605], train_loss: 1530.18 with loss1: 1303.57 and loss2: 226.61\n",
      "Epoch [606], train_loss: 1533.77 with loss1: 1308.31 and loss2: 225.46\n",
      "Epoch [607], train_loss: 1533.91 with loss1: 1308.70 and loss2: 225.21\n",
      "Epoch [608], train_loss: 1539.67 with loss1: 1313.72 and loss2: 225.96\n",
      "Epoch [609], train_loss: 1535.56 with loss1: 1311.15 and loss2: 224.41\n",
      "Epoch [610], train_loss: 1546.47 with loss1: 1320.94 and loss2: 225.53\n",
      "Epoch [611], train_loss: 1536.63 with loss1: 1311.46 and loss2: 225.17\n",
      "Epoch [612], train_loss: 1541.57 with loss1: 1316.52 and loss2: 225.05\n",
      "Epoch [613], train_loss: 1535.25 with loss1: 1310.90 and loss2: 224.35\n",
      "Epoch [614], train_loss: 1541.72 with loss1: 1317.27 and loss2: 224.45\n",
      "Epoch [615], train_loss: 1525.39 with loss1: 1301.73 and loss2: 223.66\n",
      "Epoch [616], train_loss: 1538.52 with loss1: 1314.49 and loss2: 224.03\n",
      "Epoch [617], train_loss: 1522.64 with loss1: 1299.85 and loss2: 222.80\n",
      "Epoch [618], train_loss: 1542.87 with loss1: 1318.92 and loss2: 223.95\n",
      "Epoch [619], train_loss: 1521.94 with loss1: 1299.27 and loss2: 222.67\n",
      "Epoch [620], train_loss: 1532.06 with loss1: 1308.86 and loss2: 223.20\n",
      "Epoch [621], train_loss: 1517.35 with loss1: 1295.44 and loss2: 221.90\n",
      "Epoch [622], train_loss: 1533.59 with loss1: 1310.34 and loss2: 223.25\n",
      "Epoch [623], train_loss: 1514.47 with loss1: 1292.68 and loss2: 221.79\n",
      "Epoch [624], train_loss: 1527.41 with loss1: 1306.11 and loss2: 221.30\n",
      "Epoch [625], train_loss: 1518.56 with loss1: 1297.07 and loss2: 221.49\n",
      "Epoch [626], train_loss: 1522.19 with loss1: 1300.50 and loss2: 221.70\n",
      "Epoch [627], train_loss: 1510.98 with loss1: 1290.42 and loss2: 220.56\n",
      "Epoch [628], train_loss: 1527.64 with loss1: 1305.67 and loss2: 221.97\n",
      "Epoch [629], train_loss: 1510.34 with loss1: 1289.23 and loss2: 221.11\n",
      "Epoch [630], train_loss: 1516.53 with loss1: 1294.19 and loss2: 222.34\n",
      "Epoch [631], train_loss: 1502.21 with loss1: 1281.70 and loss2: 220.51\n",
      "Epoch [632], train_loss: 1521.59 with loss1: 1300.76 and loss2: 220.84\n",
      "Epoch [633], train_loss: 1508.82 with loss1: 1288.68 and loss2: 220.14\n",
      "Epoch [634], train_loss: 1528.59 with loss1: 1308.09 and loss2: 220.49\n",
      "Epoch [635], train_loss: 1518.83 with loss1: 1300.43 and loss2: 218.40\n",
      "Epoch [636], train_loss: 1537.50 with loss1: 1318.43 and loss2: 219.07\n",
      "Epoch [637], train_loss: 1518.30 with loss1: 1298.70 and loss2: 219.60\n",
      "Epoch [638], train_loss: 1539.39 with loss1: 1320.60 and loss2: 218.78\n",
      "Epoch [639], train_loss: 1518.93 with loss1: 1300.00 and loss2: 218.93\n",
      "Epoch [640], train_loss: 1529.15 with loss1: 1309.00 and loss2: 220.16\n",
      "Epoch [641], train_loss: 1502.44 with loss1: 1284.91 and loss2: 217.53\n",
      "Epoch [642], train_loss: 1515.67 with loss1: 1295.85 and loss2: 219.82\n",
      "Epoch [643], train_loss: 1490.83 with loss1: 1272.77 and loss2: 218.06\n",
      "Epoch [644], train_loss: 1496.37 with loss1: 1277.78 and loss2: 218.59\n",
      "Epoch [645], train_loss: 1477.43 with loss1: 1259.80 and loss2: 217.62\n",
      "Epoch [646], train_loss: 1474.71 with loss1: 1256.30 and loss2: 218.41\n",
      "Epoch [647], train_loss: 1460.00 with loss1: 1242.75 and loss2: 217.24\n",
      "Epoch [648], train_loss: 1470.06 with loss1: 1251.53 and loss2: 218.53\n",
      "Epoch [649], train_loss: 1459.98 with loss1: 1243.84 and loss2: 216.14\n",
      "Epoch [650], train_loss: 1463.42 with loss1: 1245.26 and loss2: 218.17\n",
      "Epoch [651], train_loss: 1448.71 with loss1: 1233.43 and loss2: 215.28\n",
      "Epoch [652], train_loss: 1456.41 with loss1: 1239.21 and loss2: 217.20\n",
      "Epoch [653], train_loss: 1441.10 with loss1: 1225.21 and loss2: 215.88\n",
      "Epoch [654], train_loss: 1449.06 with loss1: 1232.49 and loss2: 216.58\n",
      "Epoch [655], train_loss: 1436.46 with loss1: 1221.69 and loss2: 214.77\n",
      "Epoch [656], train_loss: 1440.72 with loss1: 1224.37 and loss2: 216.35\n",
      "Epoch [657], train_loss: 1439.90 with loss1: 1224.89 and loss2: 215.02\n",
      "Epoch [658], train_loss: 1443.82 with loss1: 1228.52 and loss2: 215.30\n",
      "Epoch [659], train_loss: 1431.25 with loss1: 1217.25 and loss2: 214.00\n",
      "Epoch [660], train_loss: 1440.48 with loss1: 1224.26 and loss2: 216.22\n",
      "Epoch [661], train_loss: 1430.54 with loss1: 1215.73 and loss2: 214.81\n",
      "Epoch [662], train_loss: 1448.80 with loss1: 1232.76 and loss2: 216.04\n",
      "Epoch [663], train_loss: 1436.63 with loss1: 1222.10 and loss2: 214.54\n",
      "Epoch [664], train_loss: 1453.16 with loss1: 1238.13 and loss2: 215.03\n",
      "Epoch [665], train_loss: 1436.85 with loss1: 1223.69 and loss2: 213.16\n",
      "Epoch [666], train_loss: 1449.49 with loss1: 1235.12 and loss2: 214.37\n",
      "Epoch [667], train_loss: 1441.91 with loss1: 1228.41 and loss2: 213.49\n",
      "Epoch [668], train_loss: 1458.95 with loss1: 1244.84 and loss2: 214.11\n",
      "Epoch [669], train_loss: 1444.33 with loss1: 1231.54 and loss2: 212.79\n",
      "Epoch [670], train_loss: 1454.83 with loss1: 1241.11 and loss2: 213.72\n",
      "Epoch [671], train_loss: 1448.75 with loss1: 1236.92 and loss2: 211.83\n",
      "Epoch [672], train_loss: 1456.68 with loss1: 1242.61 and loss2: 214.06\n",
      "Epoch [673], train_loss: 1444.94 with loss1: 1233.22 and loss2: 211.72\n",
      "Epoch [674], train_loss: 1456.78 with loss1: 1243.11 and loss2: 213.67\n",
      "Epoch [675], train_loss: 1452.55 with loss1: 1241.26 and loss2: 211.29\n",
      "Epoch [676], train_loss: 1461.67 with loss1: 1248.41 and loss2: 213.27\n",
      "Epoch [677], train_loss: 1451.51 with loss1: 1240.16 and loss2: 211.35\n",
      "Epoch [678], train_loss: 1455.21 with loss1: 1243.19 and loss2: 212.02\n",
      "Epoch [679], train_loss: 1442.33 with loss1: 1232.33 and loss2: 209.99\n",
      "Epoch [680], train_loss: 1452.03 with loss1: 1239.96 and loss2: 212.07\n",
      "Epoch [681], train_loss: 1435.55 with loss1: 1225.19 and loss2: 210.36\n",
      "Epoch [682], train_loss: 1438.85 with loss1: 1227.44 and loss2: 211.41\n",
      "Epoch [683], train_loss: 1425.76 with loss1: 1215.85 and loss2: 209.90\n",
      "Epoch [684], train_loss: 1424.77 with loss1: 1213.04 and loss2: 211.73\n",
      "Epoch [685], train_loss: 1415.54 with loss1: 1206.93 and loss2: 208.62\n",
      "Epoch [686], train_loss: 1417.28 with loss1: 1206.02 and loss2: 211.26\n",
      "Epoch [687], train_loss: 1401.49 with loss1: 1191.80 and loss2: 209.69\n",
      "Epoch [688], train_loss: 1401.00 with loss1: 1191.27 and loss2: 209.72\n",
      "Epoch [689], train_loss: 1394.31 with loss1: 1184.77 and loss2: 209.54\n",
      "Epoch [690], train_loss: 1400.47 with loss1: 1190.66 and loss2: 209.81\n",
      "Epoch [691], train_loss: 1390.42 with loss1: 1181.30 and loss2: 209.12\n",
      "Epoch [692], train_loss: 1395.54 with loss1: 1185.74 and loss2: 209.80\n",
      "Epoch [693], train_loss: 1387.66 with loss1: 1179.94 and loss2: 207.72\n",
      "Epoch [694], train_loss: 1399.58 with loss1: 1190.88 and loss2: 208.70\n",
      "Epoch [695], train_loss: 1390.68 with loss1: 1183.18 and loss2: 207.50\n",
      "Epoch [696], train_loss: 1394.84 with loss1: 1186.32 and loss2: 208.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [697], train_loss: 1384.80 with loss1: 1177.52 and loss2: 207.28\n",
      "Epoch [698], train_loss: 1397.69 with loss1: 1189.60 and loss2: 208.09\n",
      "Epoch [699], train_loss: 1391.55 with loss1: 1184.08 and loss2: 207.47\n",
      "Epoch [700], train_loss: 1406.08 with loss1: 1198.94 and loss2: 207.14\n",
      "Epoch [701], train_loss: 1407.40 with loss1: 1199.18 and loss2: 208.23\n",
      "Epoch [702], train_loss: 1415.39 with loss1: 1207.78 and loss2: 207.62\n",
      "Epoch [703], train_loss: 1407.23 with loss1: 1200.22 and loss2: 207.00\n",
      "Epoch [704], train_loss: 1429.26 with loss1: 1221.52 and loss2: 207.74\n",
      "Epoch [705], train_loss: 1418.13 with loss1: 1211.34 and loss2: 206.79\n",
      "Epoch [706], train_loss: 1437.32 with loss1: 1229.21 and loss2: 208.10\n",
      "Epoch [707], train_loss: 1434.26 with loss1: 1227.96 and loss2: 206.31\n",
      "Epoch [708], train_loss: 1460.01 with loss1: 1252.68 and loss2: 207.32\n",
      "Epoch [709], train_loss: 1465.76 with loss1: 1258.45 and loss2: 207.31\n",
      "Epoch [710], train_loss: 1486.69 with loss1: 1279.73 and loss2: 206.96\n",
      "Epoch [711], train_loss: 1479.81 with loss1: 1274.19 and loss2: 205.63\n",
      "Epoch [712], train_loss: 1509.16 with loss1: 1302.48 and loss2: 206.68\n",
      "Epoch [713], train_loss: 1494.33 with loss1: 1288.80 and loss2: 205.53\n",
      "Epoch [714], train_loss: 1538.50 with loss1: 1331.23 and loss2: 207.27\n",
      "Epoch [715], train_loss: 1514.22 with loss1: 1308.95 and loss2: 205.27\n",
      "Epoch [716], train_loss: 1536.57 with loss1: 1330.97 and loss2: 205.60\n",
      "Epoch [717], train_loss: 1504.67 with loss1: 1299.24 and loss2: 205.44\n",
      "Epoch [718], train_loss: 1514.41 with loss1: 1308.49 and loss2: 205.92\n",
      "Epoch [719], train_loss: 1477.06 with loss1: 1271.71 and loss2: 205.35\n",
      "Epoch [720], train_loss: 1479.15 with loss1: 1273.43 and loss2: 205.72\n",
      "Epoch [721], train_loss: 1446.24 with loss1: 1241.89 and loss2: 204.35\n",
      "Epoch [722], train_loss: 1446.31 with loss1: 1240.84 and loss2: 205.46\n",
      "Epoch [723], train_loss: 1408.69 with loss1: 1204.06 and loss2: 204.62\n",
      "Epoch [724], train_loss: 1406.53 with loss1: 1201.43 and loss2: 205.10\n",
      "Epoch [725], train_loss: 1378.46 with loss1: 1174.37 and loss2: 204.09\n",
      "Epoch [726], train_loss: 1379.85 with loss1: 1175.71 and loss2: 204.13\n",
      "Epoch [727], train_loss: 1358.80 with loss1: 1154.74 and loss2: 204.06\n",
      "Epoch [728], train_loss: 1355.62 with loss1: 1151.12 and loss2: 204.50\n",
      "Epoch [729], train_loss: 1340.43 with loss1: 1137.48 and loss2: 202.94\n",
      "Epoch [730], train_loss: 1347.61 with loss1: 1143.97 and loss2: 203.64\n",
      "Epoch [731], train_loss: 1341.26 with loss1: 1139.09 and loss2: 202.17\n",
      "Epoch [732], train_loss: 1337.99 with loss1: 1134.86 and loss2: 203.14\n",
      "Epoch [733], train_loss: 1328.86 with loss1: 1126.14 and loss2: 202.72\n",
      "Epoch [734], train_loss: 1336.97 with loss1: 1134.09 and loss2: 202.88\n",
      "Epoch [735], train_loss: 1328.60 with loss1: 1126.13 and loss2: 202.47\n",
      "Epoch [736], train_loss: 1342.62 with loss1: 1140.36 and loss2: 202.26\n",
      "Epoch [737], train_loss: 1331.61 with loss1: 1130.22 and loss2: 201.39\n",
      "Epoch [738], train_loss: 1347.30 with loss1: 1144.66 and loss2: 202.65\n",
      "Epoch [739], train_loss: 1334.67 with loss1: 1133.52 and loss2: 201.15\n",
      "Epoch [740], train_loss: 1346.69 with loss1: 1144.86 and loss2: 201.82\n",
      "Epoch [741], train_loss: 1340.49 with loss1: 1139.48 and loss2: 201.01\n",
      "Epoch [742], train_loss: 1343.78 with loss1: 1143.47 and loss2: 200.31\n",
      "Epoch [743], train_loss: 1334.40 with loss1: 1134.61 and loss2: 199.79\n",
      "Epoch [744], train_loss: 1343.34 with loss1: 1142.50 and loss2: 200.84\n",
      "Epoch [745], train_loss: 1324.43 with loss1: 1124.15 and loss2: 200.28\n",
      "Epoch [746], train_loss: 1339.54 with loss1: 1138.94 and loss2: 200.60\n",
      "Epoch [747], train_loss: 1316.54 with loss1: 1116.25 and loss2: 200.29\n",
      "Epoch [748], train_loss: 1332.70 with loss1: 1132.23 and loss2: 200.47\n",
      "Epoch [749], train_loss: 1315.90 with loss1: 1116.57 and loss2: 199.33\n",
      "Epoch [750], train_loss: 1318.56 with loss1: 1119.08 and loss2: 199.48\n",
      "Epoch [751], train_loss: 1307.79 with loss1: 1108.93 and loss2: 198.86\n",
      "Epoch [752], train_loss: 1316.46 with loss1: 1117.11 and loss2: 199.35\n",
      "Epoch [753], train_loss: 1301.26 with loss1: 1102.81 and loss2: 198.44\n",
      "Epoch [754], train_loss: 1315.79 with loss1: 1116.30 and loss2: 199.50\n",
      "Epoch [755], train_loss: 1306.59 with loss1: 1108.61 and loss2: 197.98\n",
      "Epoch [756], train_loss: 1304.48 with loss1: 1105.63 and loss2: 198.85\n",
      "Epoch [757], train_loss: 1297.80 with loss1: 1099.46 and loss2: 198.34\n",
      "Epoch [758], train_loss: 1306.40 with loss1: 1107.82 and loss2: 198.58\n",
      "Epoch [759], train_loss: 1298.51 with loss1: 1100.59 and loss2: 197.92\n",
      "Epoch [760], train_loss: 1311.23 with loss1: 1112.27 and loss2: 198.96\n",
      "Epoch [761], train_loss: 1306.32 with loss1: 1107.60 and loss2: 198.72\n",
      "Epoch [762], train_loss: 1312.52 with loss1: 1113.62 and loss2: 198.90\n",
      "Epoch [763], train_loss: 1305.76 with loss1: 1109.12 and loss2: 196.64\n",
      "Epoch [764], train_loss: 1316.00 with loss1: 1117.02 and loss2: 198.98\n",
      "Epoch [765], train_loss: 1311.19 with loss1: 1114.40 and loss2: 196.79\n",
      "Epoch [766], train_loss: 1319.00 with loss1: 1121.29 and loss2: 197.70\n",
      "Epoch [767], train_loss: 1319.31 with loss1: 1122.50 and loss2: 196.82\n",
      "Epoch [768], train_loss: 1331.91 with loss1: 1134.53 and loss2: 197.37\n",
      "Epoch [769], train_loss: 1326.73 with loss1: 1131.41 and loss2: 195.32\n",
      "Epoch [770], train_loss: 1341.52 with loss1: 1143.27 and loss2: 198.25\n",
      "Epoch [771], train_loss: 1331.66 with loss1: 1135.07 and loss2: 196.59\n",
      "Epoch [772], train_loss: 1345.37 with loss1: 1148.43 and loss2: 196.93\n",
      "Epoch [773], train_loss: 1337.23 with loss1: 1141.36 and loss2: 195.88\n",
      "Epoch [774], train_loss: 1351.49 with loss1: 1154.48 and loss2: 197.01\n",
      "Epoch [775], train_loss: 1339.12 with loss1: 1142.92 and loss2: 196.20\n",
      "Epoch [776], train_loss: 1347.87 with loss1: 1152.20 and loss2: 195.68\n",
      "Epoch [777], train_loss: 1342.92 with loss1: 1147.66 and loss2: 195.25\n",
      "Epoch [778], train_loss: 1352.68 with loss1: 1156.29 and loss2: 196.39\n",
      "Epoch [779], train_loss: 1336.59 with loss1: 1141.49 and loss2: 195.09\n",
      "Epoch [780], train_loss: 1345.85 with loss1: 1150.09 and loss2: 195.77\n",
      "Epoch [781], train_loss: 1337.26 with loss1: 1142.56 and loss2: 194.71\n",
      "Epoch [782], train_loss: 1346.07 with loss1: 1150.58 and loss2: 195.49\n",
      "Epoch [783], train_loss: 1330.99 with loss1: 1137.02 and loss2: 193.97\n",
      "Epoch [784], train_loss: 1339.46 with loss1: 1144.18 and loss2: 195.28\n",
      "Epoch [785], train_loss: 1330.73 with loss1: 1136.81 and loss2: 193.92\n",
      "Epoch [786], train_loss: 1339.47 with loss1: 1144.57 and loss2: 194.90\n",
      "Epoch [787], train_loss: 1325.22 with loss1: 1130.98 and loss2: 194.24\n",
      "Epoch [788], train_loss: 1329.79 with loss1: 1135.91 and loss2: 193.88\n",
      "Epoch [789], train_loss: 1317.24 with loss1: 1123.46 and loss2: 193.78\n",
      "Epoch [790], train_loss: 1324.88 with loss1: 1131.16 and loss2: 193.72\n",
      "Epoch [791], train_loss: 1306.93 with loss1: 1113.63 and loss2: 193.30\n",
      "Epoch [792], train_loss: 1309.75 with loss1: 1115.96 and loss2: 193.80\n",
      "Epoch [793], train_loss: 1297.04 with loss1: 1104.37 and loss2: 192.66\n",
      "Epoch [794], train_loss: 1301.64 with loss1: 1108.34 and loss2: 193.30\n",
      "Epoch [795], train_loss: 1289.96 with loss1: 1097.05 and loss2: 192.91\n",
      "Epoch [796], train_loss: 1286.59 with loss1: 1093.37 and loss2: 193.22\n",
      "Epoch [797], train_loss: 1275.46 with loss1: 1083.71 and loss2: 191.75\n",
      "Epoch [798], train_loss: 1285.08 with loss1: 1092.25 and loss2: 192.83\n",
      "Epoch [799], train_loss: 1269.20 with loss1: 1077.08 and loss2: 192.12\n",
      "Epoch [800], train_loss: 1275.54 with loss1: 1083.57 and loss2: 191.97\n",
      "Epoch [801], train_loss: 1267.62 with loss1: 1076.20 and loss2: 191.42\n",
      "Epoch [802], train_loss: 1276.26 with loss1: 1082.91 and loss2: 193.35\n",
      "Epoch [803], train_loss: 1260.18 with loss1: 1068.56 and loss2: 191.62\n",
      "Epoch [804], train_loss: 1265.45 with loss1: 1073.68 and loss2: 191.77\n",
      "Epoch [805], train_loss: 1251.16 with loss1: 1059.36 and loss2: 191.80\n",
      "Epoch [806], train_loss: 1253.19 with loss1: 1061.70 and loss2: 191.49\n",
      "Epoch [807], train_loss: 1249.51 with loss1: 1059.19 and loss2: 190.32\n",
      "Epoch [808], train_loss: 1254.42 with loss1: 1063.48 and loss2: 190.94\n",
      "Epoch [809], train_loss: 1250.12 with loss1: 1059.91 and loss2: 190.22\n",
      "Epoch [810], train_loss: 1252.50 with loss1: 1061.00 and loss2: 191.50\n",
      "Epoch [811], train_loss: 1249.99 with loss1: 1058.90 and loss2: 191.09\n",
      "Epoch [812], train_loss: 1257.60 with loss1: 1066.88 and loss2: 190.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [813], train_loss: 1254.54 with loss1: 1064.32 and loss2: 190.22\n",
      "Epoch [814], train_loss: 1256.29 with loss1: 1066.43 and loss2: 189.86\n",
      "Epoch [815], train_loss: 1251.11 with loss1: 1061.28 and loss2: 189.83\n",
      "Epoch [816], train_loss: 1261.22 with loss1: 1070.92 and loss2: 190.30\n",
      "Epoch [817], train_loss: 1257.99 with loss1: 1068.59 and loss2: 189.40\n",
      "Epoch [818], train_loss: 1264.60 with loss1: 1074.92 and loss2: 189.68\n",
      "Epoch [819], train_loss: 1259.49 with loss1: 1070.24 and loss2: 189.25\n",
      "Epoch [820], train_loss: 1269.21 with loss1: 1078.91 and loss2: 190.29\n",
      "Epoch [821], train_loss: 1259.28 with loss1: 1070.16 and loss2: 189.12\n",
      "Epoch [822], train_loss: 1262.98 with loss1: 1073.65 and loss2: 189.34\n",
      "Epoch [823], train_loss: 1255.27 with loss1: 1066.44 and loss2: 188.83\n",
      "Epoch [824], train_loss: 1268.69 with loss1: 1080.04 and loss2: 188.65\n",
      "Epoch [825], train_loss: 1261.93 with loss1: 1073.41 and loss2: 188.52\n",
      "Epoch [826], train_loss: 1270.35 with loss1: 1081.39 and loss2: 188.96\n",
      "Epoch [827], train_loss: 1263.98 with loss1: 1075.52 and loss2: 188.46\n",
      "Epoch [828], train_loss: 1274.73 with loss1: 1086.01 and loss2: 188.72\n",
      "Epoch [829], train_loss: 1267.36 with loss1: 1079.59 and loss2: 187.77\n",
      "Epoch [830], train_loss: 1276.07 with loss1: 1086.34 and loss2: 189.72\n",
      "Epoch [831], train_loss: 1264.89 with loss1: 1076.34 and loss2: 188.55\n",
      "Epoch [832], train_loss: 1277.33 with loss1: 1087.30 and loss2: 190.03\n",
      "Epoch [833], train_loss: 1269.92 with loss1: 1081.53 and loss2: 188.40\n",
      "Epoch [834], train_loss: 1278.60 with loss1: 1089.80 and loss2: 188.80\n",
      "Epoch [835], train_loss: 1274.43 with loss1: 1086.02 and loss2: 188.41\n",
      "Epoch [836], train_loss: 1281.25 with loss1: 1093.89 and loss2: 187.36\n",
      "Epoch [837], train_loss: 1274.59 with loss1: 1086.56 and loss2: 188.03\n",
      "Epoch [838], train_loss: 1285.35 with loss1: 1097.91 and loss2: 187.44\n",
      "Epoch [839], train_loss: 1280.59 with loss1: 1094.14 and loss2: 186.45\n",
      "Epoch [840], train_loss: 1296.19 with loss1: 1107.38 and loss2: 188.81\n",
      "Epoch [841], train_loss: 1282.67 with loss1: 1095.94 and loss2: 186.73\n",
      "Epoch [842], train_loss: 1298.71 with loss1: 1110.66 and loss2: 188.05\n",
      "Epoch [843], train_loss: 1284.35 with loss1: 1097.48 and loss2: 186.88\n",
      "Epoch [844], train_loss: 1293.87 with loss1: 1106.77 and loss2: 187.10\n",
      "Epoch [845], train_loss: 1274.77 with loss1: 1087.80 and loss2: 186.97\n",
      "Epoch [846], train_loss: 1281.57 with loss1: 1093.69 and loss2: 187.88\n",
      "Epoch [847], train_loss: 1261.96 with loss1: 1075.55 and loss2: 186.41\n",
      "Epoch [848], train_loss: 1269.26 with loss1: 1082.73 and loss2: 186.53\n",
      "Epoch [849], train_loss: 1240.71 with loss1: 1054.29 and loss2: 186.42\n",
      "Epoch [850], train_loss: 1243.18 with loss1: 1055.56 and loss2: 187.62\n",
      "Epoch [851], train_loss: 1225.55 with loss1: 1040.05 and loss2: 185.50\n",
      "Epoch [852], train_loss: 1228.81 with loss1: 1042.28 and loss2: 186.52\n",
      "Epoch [853], train_loss: 1207.79 with loss1: 1022.42 and loss2: 185.37\n",
      "Epoch [854], train_loss: 1220.36 with loss1: 1034.56 and loss2: 185.79\n",
      "Epoch [855], train_loss: 1199.23 with loss1: 1013.91 and loss2: 185.31\n",
      "Epoch [856], train_loss: 1204.58 with loss1: 1018.95 and loss2: 185.63\n",
      "Epoch [857], train_loss: 1184.11 with loss1: 999.64 and loss2: 184.47\n",
      "Epoch [858], train_loss: 1186.85 with loss1: 1002.60 and loss2: 184.25\n",
      "Epoch [859], train_loss: 1180.36 with loss1: 994.44 and loss2: 185.92\n",
      "Epoch [860], train_loss: 1191.72 with loss1: 1007.08 and loss2: 184.64\n",
      "Epoch [861], train_loss: 1177.52 with loss1: 992.33 and loss2: 185.19\n",
      "Epoch [862], train_loss: 1187.93 with loss1: 1003.25 and loss2: 184.68\n",
      "Epoch [863], train_loss: 1178.20 with loss1: 994.66 and loss2: 183.54\n",
      "Epoch [864], train_loss: 1180.57 with loss1: 996.46 and loss2: 184.11\n",
      "Epoch [865], train_loss: 1169.48 with loss1: 986.13 and loss2: 183.35\n",
      "Epoch [866], train_loss: 1180.56 with loss1: 997.02 and loss2: 183.54\n",
      "Epoch [867], train_loss: 1176.77 with loss1: 993.51 and loss2: 183.26\n",
      "Epoch [868], train_loss: 1188.44 with loss1: 1004.67 and loss2: 183.78\n",
      "Epoch [869], train_loss: 1182.34 with loss1: 999.96 and loss2: 182.38\n",
      "Epoch [870], train_loss: 1190.85 with loss1: 1007.11 and loss2: 183.74\n",
      "Epoch [871], train_loss: 1185.79 with loss1: 1002.90 and loss2: 182.89\n",
      "Epoch [872], train_loss: 1196.44 with loss1: 1013.27 and loss2: 183.16\n",
      "Epoch [873], train_loss: 1190.58 with loss1: 1008.39 and loss2: 182.19\n",
      "Epoch [874], train_loss: 1206.14 with loss1: 1023.12 and loss2: 183.02\n",
      "Epoch [875], train_loss: 1197.65 with loss1: 1015.55 and loss2: 182.10\n",
      "Epoch [876], train_loss: 1211.32 with loss1: 1028.06 and loss2: 183.25\n",
      "Epoch [877], train_loss: 1203.77 with loss1: 1021.69 and loss2: 182.08\n",
      "Epoch [878], train_loss: 1224.86 with loss1: 1042.41 and loss2: 182.45\n",
      "Epoch [879], train_loss: 1209.54 with loss1: 1028.80 and loss2: 180.74\n",
      "Epoch [880], train_loss: 1223.55 with loss1: 1042.16 and loss2: 181.39\n",
      "Epoch [881], train_loss: 1210.52 with loss1: 1030.17 and loss2: 180.35\n",
      "Epoch [882], train_loss: 1227.86 with loss1: 1046.61 and loss2: 181.25\n",
      "Epoch [883], train_loss: 1213.74 with loss1: 1033.19 and loss2: 180.55\n",
      "Epoch [884], train_loss: 1227.57 with loss1: 1045.82 and loss2: 181.76\n",
      "Epoch [885], train_loss: 1218.98 with loss1: 1038.13 and loss2: 180.85\n",
      "Epoch [886], train_loss: 1231.99 with loss1: 1051.24 and loss2: 180.75\n",
      "Epoch [887], train_loss: 1216.71 with loss1: 1036.81 and loss2: 179.89\n",
      "Epoch [888], train_loss: 1229.69 with loss1: 1048.38 and loss2: 181.31\n",
      "Epoch [889], train_loss: 1216.26 with loss1: 1037.06 and loss2: 179.20\n",
      "Epoch [890], train_loss: 1234.93 with loss1: 1054.62 and loss2: 180.31\n",
      "Epoch [891], train_loss: 1220.19 with loss1: 1039.95 and loss2: 180.25\n",
      "Epoch [892], train_loss: 1228.73 with loss1: 1048.48 and loss2: 180.25\n",
      "Epoch [893], train_loss: 1218.70 with loss1: 1039.22 and loss2: 179.48\n",
      "Epoch [894], train_loss: 1226.62 with loss1: 1046.22 and loss2: 180.39\n",
      "Epoch [895], train_loss: 1211.80 with loss1: 1032.76 and loss2: 179.04\n",
      "Epoch [896], train_loss: 1228.45 with loss1: 1048.17 and loss2: 180.28\n",
      "Epoch [897], train_loss: 1212.48 with loss1: 1033.35 and loss2: 179.13\n",
      "Epoch [898], train_loss: 1219.86 with loss1: 1040.40 and loss2: 179.46\n",
      "Epoch [899], train_loss: 1206.59 with loss1: 1028.40 and loss2: 178.20\n",
      "Epoch [900], train_loss: 1219.74 with loss1: 1040.25 and loss2: 179.49\n",
      "Epoch [901], train_loss: 1209.02 with loss1: 1030.35 and loss2: 178.67\n",
      "Epoch [902], train_loss: 1208.85 with loss1: 1029.28 and loss2: 179.56\n",
      "Epoch [903], train_loss: 1199.03 with loss1: 1021.21 and loss2: 177.82\n",
      "Epoch [904], train_loss: 1211.01 with loss1: 1031.80 and loss2: 179.21\n",
      "Epoch [905], train_loss: 1191.98 with loss1: 1013.65 and loss2: 178.33\n",
      "Epoch [906], train_loss: 1199.90 with loss1: 1020.30 and loss2: 179.60\n",
      "Epoch [907], train_loss: 1188.68 with loss1: 1011.12 and loss2: 177.56\n",
      "Epoch [908], train_loss: 1199.48 with loss1: 1020.51 and loss2: 178.98\n",
      "Epoch [909], train_loss: 1184.28 with loss1: 1007.22 and loss2: 177.06\n",
      "Epoch [910], train_loss: 1187.53 with loss1: 1009.72 and loss2: 177.81\n",
      "Epoch [911], train_loss: 1172.41 with loss1: 995.01 and loss2: 177.40\n",
      "Epoch [912], train_loss: 1174.62 with loss1: 996.97 and loss2: 177.65\n",
      "Epoch [913], train_loss: 1164.41 with loss1: 986.92 and loss2: 177.48\n",
      "Epoch [914], train_loss: 1169.94 with loss1: 991.40 and loss2: 178.54\n",
      "Epoch [915], train_loss: 1164.12 with loss1: 986.79 and loss2: 177.33\n",
      "Epoch [916], train_loss: 1165.78 with loss1: 988.20 and loss2: 177.58\n",
      "Epoch [917], train_loss: 1154.95 with loss1: 978.16 and loss2: 176.79\n",
      "Epoch [918], train_loss: 1156.26 with loss1: 978.59 and loss2: 177.68\n",
      "Epoch [919], train_loss: 1150.16 with loss1: 974.16 and loss2: 176.00\n",
      "Epoch [920], train_loss: 1151.11 with loss1: 974.09 and loss2: 177.03\n",
      "Epoch [921], train_loss: 1141.53 with loss1: 965.89 and loss2: 175.63\n",
      "Epoch [922], train_loss: 1145.54 with loss1: 969.31 and loss2: 176.23\n",
      "Epoch [923], train_loss: 1136.34 with loss1: 960.83 and loss2: 175.50\n",
      "Epoch [924], train_loss: 1138.06 with loss1: 961.14 and loss2: 176.92\n",
      "Epoch [925], train_loss: 1133.24 with loss1: 957.67 and loss2: 175.56\n",
      "Epoch [926], train_loss: 1139.95 with loss1: 963.18 and loss2: 176.77\n",
      "Epoch [927], train_loss: 1132.40 with loss1: 956.57 and loss2: 175.83\n",
      "Epoch [928], train_loss: 1138.96 with loss1: 963.46 and loss2: 175.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [929], train_loss: 1138.10 with loss1: 962.54 and loss2: 175.56\n",
      "Epoch [930], train_loss: 1143.60 with loss1: 968.40 and loss2: 175.20\n",
      "Epoch [931], train_loss: 1142.34 with loss1: 967.08 and loss2: 175.27\n",
      "Epoch [932], train_loss: 1150.68 with loss1: 974.92 and loss2: 175.76\n",
      "Epoch [933], train_loss: 1143.14 with loss1: 969.24 and loss2: 173.90\n",
      "Epoch [934], train_loss: 1156.30 with loss1: 980.76 and loss2: 175.54\n",
      "Epoch [935], train_loss: 1154.12 with loss1: 979.24 and loss2: 174.88\n",
      "Epoch [936], train_loss: 1165.25 with loss1: 990.15 and loss2: 175.10\n",
      "Epoch [937], train_loss: 1161.34 with loss1: 986.26 and loss2: 175.08\n",
      "Epoch [938], train_loss: 1164.53 with loss1: 989.76 and loss2: 174.77\n",
      "Epoch [939], train_loss: 1166.63 with loss1: 991.19 and loss2: 175.44\n",
      "Epoch [940], train_loss: 1167.90 with loss1: 993.57 and loss2: 174.33\n",
      "Epoch [941], train_loss: 1161.68 with loss1: 987.65 and loss2: 174.03\n",
      "Epoch [942], train_loss: 1161.86 with loss1: 986.97 and loss2: 174.89\n",
      "Epoch [943], train_loss: 1149.08 with loss1: 973.39 and loss2: 175.69\n",
      "Epoch [944], train_loss: 1152.14 with loss1: 977.72 and loss2: 174.41\n",
      "Epoch [945], train_loss: 1140.02 with loss1: 966.45 and loss2: 173.57\n",
      "Epoch [946], train_loss: 1139.34 with loss1: 965.56 and loss2: 173.78\n",
      "Epoch [947], train_loss: 1127.79 with loss1: 954.51 and loss2: 173.29\n",
      "Epoch [948], train_loss: 1129.89 with loss1: 956.19 and loss2: 173.70\n",
      "Epoch [949], train_loss: 1124.04 with loss1: 951.40 and loss2: 172.64\n",
      "Epoch [950], train_loss: 1125.40 with loss1: 951.14 and loss2: 174.26\n",
      "Epoch [951], train_loss: 1114.69 with loss1: 941.96 and loss2: 172.73\n",
      "Epoch [952], train_loss: 1119.44 with loss1: 945.97 and loss2: 173.47\n",
      "Epoch [953], train_loss: 1113.53 with loss1: 939.74 and loss2: 173.79\n",
      "Epoch [954], train_loss: 1114.34 with loss1: 941.73 and loss2: 172.61\n",
      "Epoch [955], train_loss: 1109.46 with loss1: 937.38 and loss2: 172.08\n",
      "Epoch [956], train_loss: 1109.98 with loss1: 937.37 and loss2: 172.61\n",
      "Epoch [957], train_loss: 1113.34 with loss1: 941.05 and loss2: 172.29\n",
      "Epoch [958], train_loss: 1113.82 with loss1: 941.10 and loss2: 172.73\n",
      "Epoch [959], train_loss: 1106.76 with loss1: 934.25 and loss2: 172.51\n",
      "Epoch [960], train_loss: 1110.06 with loss1: 937.68 and loss2: 172.38\n",
      "Epoch [961], train_loss: 1105.71 with loss1: 934.06 and loss2: 171.65\n",
      "Epoch [962], train_loss: 1114.02 with loss1: 941.65 and loss2: 172.37\n",
      "Epoch [963], train_loss: 1108.58 with loss1: 936.80 and loss2: 171.77\n",
      "Epoch [964], train_loss: 1117.06 with loss1: 945.89 and loss2: 171.16\n",
      "Epoch [965], train_loss: 1110.35 with loss1: 938.88 and loss2: 171.47\n",
      "Epoch [966], train_loss: 1123.09 with loss1: 950.98 and loss2: 172.11\n",
      "Epoch [967], train_loss: 1109.95 with loss1: 939.09 and loss2: 170.86\n",
      "Epoch [968], train_loss: 1123.75 with loss1: 952.49 and loss2: 171.26\n",
      "Epoch [969], train_loss: 1109.79 with loss1: 939.12 and loss2: 170.67\n",
      "Epoch [970], train_loss: 1121.66 with loss1: 950.38 and loss2: 171.28\n",
      "Epoch [971], train_loss: 1115.18 with loss1: 944.54 and loss2: 170.64\n",
      "Epoch [972], train_loss: 1130.19 with loss1: 958.76 and loss2: 171.43\n",
      "Epoch [973], train_loss: 1115.08 with loss1: 944.83 and loss2: 170.25\n",
      "Epoch [974], train_loss: 1132.53 with loss1: 960.89 and loss2: 171.64\n",
      "Epoch [975], train_loss: 1118.67 with loss1: 948.23 and loss2: 170.44\n",
      "Epoch [976], train_loss: 1132.31 with loss1: 961.72 and loss2: 170.59\n",
      "Epoch [977], train_loss: 1121.96 with loss1: 952.29 and loss2: 169.67\n",
      "Epoch [978], train_loss: 1133.10 with loss1: 963.08 and loss2: 170.02\n",
      "Epoch [979], train_loss: 1121.07 with loss1: 951.16 and loss2: 169.91\n",
      "Epoch [980], train_loss: 1142.37 with loss1: 971.17 and loss2: 171.21\n",
      "Epoch [981], train_loss: 1125.93 with loss1: 955.88 and loss2: 170.05\n",
      "Epoch [982], train_loss: 1133.96 with loss1: 963.69 and loss2: 170.27\n",
      "Epoch [983], train_loss: 1123.77 with loss1: 954.58 and loss2: 169.19\n",
      "Epoch [984], train_loss: 1140.44 with loss1: 969.39 and loss2: 171.05\n",
      "Epoch [985], train_loss: 1123.64 with loss1: 954.03 and loss2: 169.61\n",
      "Epoch [986], train_loss: 1134.52 with loss1: 964.78 and loss2: 169.74\n",
      "Epoch [987], train_loss: 1124.45 with loss1: 955.17 and loss2: 169.27\n",
      "Epoch [988], train_loss: 1131.35 with loss1: 962.22 and loss2: 169.13\n",
      "Epoch [989], train_loss: 1117.47 with loss1: 948.58 and loss2: 168.90\n",
      "Epoch [990], train_loss: 1124.54 with loss1: 954.79 and loss2: 169.75\n",
      "Epoch [991], train_loss: 1111.67 with loss1: 943.81 and loss2: 167.86\n",
      "Epoch [992], train_loss: 1125.66 with loss1: 957.10 and loss2: 168.56\n",
      "Epoch [993], train_loss: 1114.51 with loss1: 945.64 and loss2: 168.86\n",
      "Epoch [994], train_loss: 1130.53 with loss1: 961.33 and loss2: 169.20\n",
      "Epoch [995], train_loss: 1113.96 with loss1: 945.60 and loss2: 168.36\n",
      "Epoch [996], train_loss: 1121.67 with loss1: 953.35 and loss2: 168.31\n",
      "Epoch [997], train_loss: 1116.51 with loss1: 948.61 and loss2: 167.90\n",
      "Epoch [998], train_loss: 1126.35 with loss1: 957.38 and loss2: 168.97\n",
      "Epoch [999], train_loss: 1117.04 with loss1: 949.56 and loss2: 167.48\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# model version 3.5\n",
    "# with loss2\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history = fit(epochs=1000, lr=5e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "64b09f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 1124.87 with loss1: 956.06 and loss2: 168.81\n",
      "Epoch [1], train_loss: 1118.14 with loss1: 950.29 and loss2: 167.85\n",
      "Epoch [2], train_loss: 1128.26 with loss1: 959.87 and loss2: 168.39\n",
      "Epoch [3], train_loss: 1120.25 with loss1: 952.13 and loss2: 168.12\n",
      "Epoch [4], train_loss: 1126.45 with loss1: 958.06 and loss2: 168.39\n",
      "Epoch [5], train_loss: 1119.86 with loss1: 952.50 and loss2: 167.36\n",
      "Epoch [6], train_loss: 1134.44 with loss1: 966.71 and loss2: 167.73\n",
      "Epoch [7], train_loss: 1124.53 with loss1: 957.22 and loss2: 167.31\n",
      "Epoch [8], train_loss: 1133.58 with loss1: 965.99 and loss2: 167.58\n",
      "Epoch [9], train_loss: 1123.92 with loss1: 956.54 and loss2: 167.39\n",
      "Epoch [10], train_loss: 1138.55 with loss1: 971.04 and loss2: 167.51\n",
      "Epoch [11], train_loss: 1131.37 with loss1: 965.12 and loss2: 166.25\n",
      "Epoch [12], train_loss: 1140.10 with loss1: 973.37 and loss2: 166.73\n",
      "Epoch [13], train_loss: 1126.99 with loss1: 960.71 and loss2: 166.28\n",
      "Epoch [14], train_loss: 1133.74 with loss1: 966.77 and loss2: 166.97\n",
      "Epoch [15], train_loss: 1122.46 with loss1: 956.42 and loss2: 166.04\n",
      "Epoch [16], train_loss: 1130.90 with loss1: 964.26 and loss2: 166.64\n",
      "Epoch [17], train_loss: 1122.07 with loss1: 956.87 and loss2: 165.20\n",
      "Epoch [18], train_loss: 1129.02 with loss1: 962.26 and loss2: 166.77\n",
      "Epoch [19], train_loss: 1115.40 with loss1: 949.94 and loss2: 165.46\n",
      "Epoch [20], train_loss: 1127.26 with loss1: 960.55 and loss2: 166.71\n",
      "Epoch [21], train_loss: 1112.60 with loss1: 946.87 and loss2: 165.73\n",
      "Epoch [22], train_loss: 1113.69 with loss1: 947.37 and loss2: 166.32\n",
      "Epoch [23], train_loss: 1096.12 with loss1: 931.27 and loss2: 164.86\n",
      "Epoch [24], train_loss: 1099.28 with loss1: 932.81 and loss2: 166.48\n",
      "Epoch [25], train_loss: 1088.43 with loss1: 923.52 and loss2: 164.91\n",
      "Epoch [26], train_loss: 1091.63 with loss1: 926.38 and loss2: 165.25\n",
      "Epoch [27], train_loss: 1078.82 with loss1: 914.21 and loss2: 164.62\n",
      "Epoch [28], train_loss: 1082.91 with loss1: 917.10 and loss2: 165.81\n",
      "Epoch [29], train_loss: 1072.02 with loss1: 907.54 and loss2: 164.48\n",
      "Epoch [30], train_loss: 1069.70 with loss1: 904.70 and loss2: 165.00\n",
      "Epoch [31], train_loss: 1058.71 with loss1: 894.39 and loss2: 164.32\n",
      "Epoch [32], train_loss: 1058.68 with loss1: 894.14 and loss2: 164.53\n",
      "Epoch [33], train_loss: 1051.35 with loss1: 887.03 and loss2: 164.31\n",
      "Epoch [34], train_loss: 1052.69 with loss1: 887.76 and loss2: 164.93\n",
      "Epoch [35], train_loss: 1044.13 with loss1: 880.09 and loss2: 164.04\n",
      "Epoch [36], train_loss: 1047.46 with loss1: 882.72 and loss2: 164.74\n",
      "Epoch [37], train_loss: 1038.76 with loss1: 874.98 and loss2: 163.78\n",
      "Epoch [38], train_loss: 1036.27 with loss1: 871.64 and loss2: 164.63\n",
      "Epoch [39], train_loss: 1034.00 with loss1: 870.37 and loss2: 163.63\n",
      "Epoch [40], train_loss: 1034.99 with loss1: 870.67 and loss2: 164.31\n",
      "Epoch [41], train_loss: 1030.06 with loss1: 866.83 and loss2: 163.23\n",
      "Epoch [42], train_loss: 1030.09 with loss1: 866.00 and loss2: 164.09\n",
      "Epoch [43], train_loss: 1024.51 with loss1: 861.47 and loss2: 163.04\n",
      "Epoch [44], train_loss: 1029.92 with loss1: 866.19 and loss2: 163.73\n",
      "Epoch [45], train_loss: 1022.13 with loss1: 859.76 and loss2: 162.37\n",
      "Epoch [46], train_loss: 1026.52 with loss1: 863.84 and loss2: 162.67\n",
      "Epoch [47], train_loss: 1026.04 with loss1: 863.08 and loss2: 162.95\n",
      "Epoch [48], train_loss: 1029.44 with loss1: 866.28 and loss2: 163.17\n",
      "Epoch [49], train_loss: 1031.12 with loss1: 868.37 and loss2: 162.75\n",
      "Epoch [50], train_loss: 1035.43 with loss1: 871.73 and loss2: 163.70\n",
      "Epoch [51], train_loss: 1029.55 with loss1: 867.81 and loss2: 161.74\n",
      "Epoch [52], train_loss: 1036.42 with loss1: 873.94 and loss2: 162.48\n",
      "Epoch [53], train_loss: 1033.57 with loss1: 871.73 and loss2: 161.84\n",
      "Epoch [54], train_loss: 1040.48 with loss1: 877.90 and loss2: 162.58\n",
      "Epoch [55], train_loss: 1034.45 with loss1: 872.73 and loss2: 161.72\n",
      "Epoch [56], train_loss: 1046.45 with loss1: 883.89 and loss2: 162.56\n",
      "Epoch [57], train_loss: 1041.67 with loss1: 879.94 and loss2: 161.73\n",
      "Epoch [58], train_loss: 1050.57 with loss1: 888.32 and loss2: 162.25\n",
      "Epoch [59], train_loss: 1043.00 with loss1: 881.65 and loss2: 161.35\n",
      "Epoch [60], train_loss: 1053.83 with loss1: 891.83 and loss2: 162.00\n",
      "Epoch [61], train_loss: 1049.03 with loss1: 887.83 and loss2: 161.20\n",
      "Epoch [62], train_loss: 1056.79 with loss1: 894.46 and loss2: 162.34\n",
      "Epoch [63], train_loss: 1054.30 with loss1: 893.18 and loss2: 161.12\n",
      "Epoch [64], train_loss: 1059.65 with loss1: 898.32 and loss2: 161.33\n",
      "Epoch [65], train_loss: 1051.63 with loss1: 891.38 and loss2: 160.25\n",
      "Epoch [66], train_loss: 1067.64 with loss1: 905.85 and loss2: 161.79\n",
      "Epoch [67], train_loss: 1061.40 with loss1: 901.19 and loss2: 160.21\n",
      "Epoch [68], train_loss: 1072.03 with loss1: 910.68 and loss2: 161.35\n",
      "Epoch [69], train_loss: 1075.97 with loss1: 915.46 and loss2: 160.51\n",
      "Epoch [70], train_loss: 1079.73 with loss1: 918.92 and loss2: 160.81\n",
      "Epoch [71], train_loss: 1066.53 with loss1: 906.31 and loss2: 160.22\n",
      "Epoch [72], train_loss: 1080.01 with loss1: 919.05 and loss2: 160.96\n",
      "Epoch [73], train_loss: 1070.63 with loss1: 910.27 and loss2: 160.37\n",
      "Epoch [74], train_loss: 1076.84 with loss1: 916.30 and loss2: 160.54\n",
      "Epoch [75], train_loss: 1070.94 with loss1: 911.85 and loss2: 159.09\n",
      "Epoch [76], train_loss: 1080.09 with loss1: 920.38 and loss2: 159.71\n",
      "Epoch [77], train_loss: 1072.03 with loss1: 912.80 and loss2: 159.23\n",
      "Epoch [78], train_loss: 1084.64 with loss1: 924.36 and loss2: 160.28\n",
      "Epoch [79], train_loss: 1069.13 with loss1: 909.90 and loss2: 159.23\n",
      "Epoch [80], train_loss: 1082.42 with loss1: 922.17 and loss2: 160.25\n",
      "Epoch [81], train_loss: 1068.49 with loss1: 909.67 and loss2: 158.82\n",
      "Epoch [82], train_loss: 1081.60 with loss1: 921.42 and loss2: 160.18\n",
      "Epoch [83], train_loss: 1070.69 with loss1: 911.41 and loss2: 159.28\n",
      "Epoch [84], train_loss: 1082.00 with loss1: 922.03 and loss2: 159.96\n",
      "Epoch [85], train_loss: 1070.28 with loss1: 911.42 and loss2: 158.86\n",
      "Epoch [86], train_loss: 1081.05 with loss1: 921.96 and loss2: 159.09\n",
      "Epoch [87], train_loss: 1071.37 with loss1: 912.21 and loss2: 159.16\n",
      "Epoch [88], train_loss: 1076.54 with loss1: 916.87 and loss2: 159.67\n",
      "Epoch [89], train_loss: 1064.89 with loss1: 906.28 and loss2: 158.61\n",
      "Epoch [90], train_loss: 1072.59 with loss1: 913.83 and loss2: 158.76\n",
      "Epoch [91], train_loss: 1058.73 with loss1: 900.56 and loss2: 158.17\n",
      "Epoch [92], train_loss: 1068.32 with loss1: 908.76 and loss2: 159.55\n",
      "Epoch [93], train_loss: 1056.80 with loss1: 899.20 and loss2: 157.61\n",
      "Epoch [94], train_loss: 1060.77 with loss1: 902.17 and loss2: 158.60\n",
      "Epoch [95], train_loss: 1052.27 with loss1: 894.80 and loss2: 157.46\n",
      "Epoch [96], train_loss: 1058.29 with loss1: 900.23 and loss2: 158.05\n",
      "Epoch [97], train_loss: 1048.17 with loss1: 890.36 and loss2: 157.81\n",
      "Epoch [98], train_loss: 1054.09 with loss1: 895.37 and loss2: 158.72\n",
      "Epoch [99], train_loss: 1043.12 with loss1: 886.03 and loss2: 157.09\n",
      "Epoch [100], train_loss: 1053.72 with loss1: 895.57 and loss2: 158.15\n",
      "Epoch [101], train_loss: 1041.29 with loss1: 884.96 and loss2: 156.33\n",
      "Epoch [102], train_loss: 1045.65 with loss1: 887.90 and loss2: 157.75\n",
      "Epoch [103], train_loss: 1040.69 with loss1: 884.00 and loss2: 156.69\n",
      "Epoch [104], train_loss: 1042.42 with loss1: 884.06 and loss2: 158.37\n",
      "Epoch [105], train_loss: 1031.57 with loss1: 874.81 and loss2: 156.76\n",
      "Epoch [106], train_loss: 1034.62 with loss1: 877.44 and loss2: 157.19\n",
      "Epoch [107], train_loss: 1027.85 with loss1: 871.68 and loss2: 156.17\n",
      "Epoch [108], train_loss: 1032.47 with loss1: 875.54 and loss2: 156.93\n",
      "Epoch [109], train_loss: 1024.77 with loss1: 868.56 and loss2: 156.21\n",
      "Epoch [110], train_loss: 1029.53 with loss1: 873.26 and loss2: 156.27\n",
      "Epoch [111], train_loss: 1019.55 with loss1: 863.16 and loss2: 156.39\n",
      "Epoch [112], train_loss: 1024.21 with loss1: 866.44 and loss2: 157.77\n",
      "Epoch [113], train_loss: 1014.06 with loss1: 858.05 and loss2: 156.00\n",
      "Epoch [114], train_loss: 1016.43 with loss1: 859.99 and loss2: 156.43\n",
      "Epoch [115], train_loss: 1010.89 with loss1: 855.24 and loss2: 155.65\n",
      "Epoch [116], train_loss: 1014.86 with loss1: 858.35 and loss2: 156.52\n",
      "Epoch [117], train_loss: 1007.27 with loss1: 850.92 and loss2: 156.35\n",
      "Epoch [118], train_loss: 1011.10 with loss1: 855.14 and loss2: 155.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [119], train_loss: 1001.73 with loss1: 845.78 and loss2: 155.95\n",
      "Epoch [120], train_loss: 1003.14 with loss1: 847.36 and loss2: 155.78\n",
      "Epoch [121], train_loss: 998.40 with loss1: 842.50 and loss2: 155.90\n",
      "Epoch [122], train_loss: 1002.04 with loss1: 846.58 and loss2: 155.46\n",
      "Epoch [123], train_loss: 994.17 with loss1: 838.90 and loss2: 155.27\n",
      "Epoch [124], train_loss: 1000.95 with loss1: 844.82 and loss2: 156.13\n",
      "Epoch [125], train_loss: 995.44 with loss1: 839.73 and loss2: 155.71\n",
      "Epoch [126], train_loss: 997.48 with loss1: 841.41 and loss2: 156.07\n",
      "Epoch [127], train_loss: 991.01 with loss1: 836.31 and loss2: 154.70\n",
      "Epoch [128], train_loss: 996.73 with loss1: 840.99 and loss2: 155.75\n",
      "Epoch [129], train_loss: 989.01 with loss1: 834.05 and loss2: 154.96\n",
      "Epoch [130], train_loss: 991.23 with loss1: 836.21 and loss2: 155.01\n",
      "Epoch [131], train_loss: 988.42 with loss1: 833.31 and loss2: 155.11\n",
      "Epoch [132], train_loss: 991.76 with loss1: 836.63 and loss2: 155.14\n",
      "Epoch [133], train_loss: 986.31 with loss1: 831.71 and loss2: 154.60\n",
      "Epoch [134], train_loss: 991.60 with loss1: 836.30 and loss2: 155.30\n",
      "Epoch [135], train_loss: 991.24 with loss1: 836.62 and loss2: 154.62\n",
      "Epoch [136], train_loss: 994.49 with loss1: 839.34 and loss2: 155.14\n",
      "Epoch [137], train_loss: 991.94 with loss1: 837.39 and loss2: 154.55\n",
      "Epoch [138], train_loss: 1000.27 with loss1: 845.16 and loss2: 155.11\n",
      "Epoch [139], train_loss: 997.00 with loss1: 842.31 and loss2: 154.69\n",
      "Epoch [140], train_loss: 1005.97 with loss1: 851.86 and loss2: 154.11\n",
      "Epoch [141], train_loss: 1003.40 with loss1: 848.58 and loss2: 154.82\n",
      "Epoch [142], train_loss: 1014.22 with loss1: 859.18 and loss2: 155.04\n",
      "Epoch [143], train_loss: 1011.16 with loss1: 857.19 and loss2: 153.97\n",
      "Epoch [144], train_loss: 1019.76 with loss1: 865.53 and loss2: 154.23\n",
      "Epoch [145], train_loss: 1011.41 with loss1: 857.23 and loss2: 154.18\n",
      "Epoch [146], train_loss: 1018.48 with loss1: 863.57 and loss2: 154.91\n",
      "Epoch [147], train_loss: 1014.09 with loss1: 860.04 and loss2: 154.05\n",
      "Epoch [148], train_loss: 1020.13 with loss1: 866.34 and loss2: 153.79\n",
      "Epoch [149], train_loss: 1014.31 with loss1: 861.12 and loss2: 153.19\n",
      "Epoch [150], train_loss: 1022.73 with loss1: 868.72 and loss2: 154.01\n",
      "Epoch [151], train_loss: 1016.71 with loss1: 863.10 and loss2: 153.61\n",
      "Epoch [152], train_loss: 1020.35 with loss1: 866.30 and loss2: 154.05\n",
      "Epoch [153], train_loss: 1012.51 with loss1: 858.88 and loss2: 153.62\n",
      "Epoch [154], train_loss: 1017.64 with loss1: 863.93 and loss2: 153.71\n",
      "Epoch [155], train_loss: 1006.31 with loss1: 854.09 and loss2: 152.23\n",
      "Epoch [156], train_loss: 1013.39 with loss1: 860.16 and loss2: 153.23\n",
      "Epoch [157], train_loss: 1003.38 with loss1: 850.26 and loss2: 153.12\n",
      "Epoch [158], train_loss: 1008.35 with loss1: 855.11 and loss2: 153.23\n",
      "Epoch [159], train_loss: 1000.10 with loss1: 847.48 and loss2: 152.62\n",
      "Epoch [160], train_loss: 1004.96 with loss1: 851.22 and loss2: 153.74\n",
      "Epoch [161], train_loss: 990.36 with loss1: 838.53 and loss2: 151.83\n",
      "Epoch [162], train_loss: 996.72 with loss1: 843.42 and loss2: 153.29\n",
      "Epoch [163], train_loss: 985.85 with loss1: 833.38 and loss2: 152.48\n",
      "Epoch [164], train_loss: 988.44 with loss1: 835.82 and loss2: 152.61\n",
      "Epoch [165], train_loss: 979.00 with loss1: 825.73 and loss2: 153.27\n",
      "Epoch [166], train_loss: 984.85 with loss1: 832.53 and loss2: 152.32\n",
      "Epoch [167], train_loss: 973.71 with loss1: 822.21 and loss2: 151.51\n",
      "Epoch [168], train_loss: 980.12 with loss1: 828.22 and loss2: 151.91\n",
      "Epoch [169], train_loss: 970.73 with loss1: 819.18 and loss2: 151.55\n",
      "Epoch [170], train_loss: 975.22 with loss1: 822.71 and loss2: 152.50\n",
      "Epoch [171], train_loss: 969.22 with loss1: 817.83 and loss2: 151.39\n",
      "Epoch [172], train_loss: 972.22 with loss1: 819.94 and loss2: 152.28\n",
      "Epoch [173], train_loss: 967.05 with loss1: 815.85 and loss2: 151.20\n",
      "Epoch [174], train_loss: 972.70 with loss1: 820.76 and loss2: 151.94\n",
      "Epoch [175], train_loss: 965.77 with loss1: 815.06 and loss2: 150.70\n",
      "Epoch [176], train_loss: 970.35 with loss1: 818.15 and loss2: 152.19\n",
      "Epoch [177], train_loss: 968.77 with loss1: 817.35 and loss2: 151.42\n",
      "Epoch [178], train_loss: 973.79 with loss1: 822.86 and loss2: 150.93\n",
      "Epoch [179], train_loss: 970.24 with loss1: 819.26 and loss2: 150.98\n",
      "Epoch [180], train_loss: 977.96 with loss1: 826.25 and loss2: 151.71\n",
      "Epoch [181], train_loss: 972.51 with loss1: 821.81 and loss2: 150.70\n",
      "Epoch [182], train_loss: 976.23 with loss1: 825.10 and loss2: 151.13\n",
      "Epoch [183], train_loss: 971.09 with loss1: 820.48 and loss2: 150.62\n",
      "Epoch [184], train_loss: 980.43 with loss1: 829.24 and loss2: 151.18\n",
      "Epoch [185], train_loss: 971.26 with loss1: 821.00 and loss2: 150.26\n",
      "Epoch [186], train_loss: 979.94 with loss1: 828.88 and loss2: 151.06\n",
      "Epoch [187], train_loss: 975.27 with loss1: 824.73 and loss2: 150.53\n",
      "Epoch [188], train_loss: 981.87 with loss1: 831.26 and loss2: 150.60\n",
      "Epoch [189], train_loss: 976.63 with loss1: 826.68 and loss2: 149.96\n",
      "Epoch [190], train_loss: 988.26 with loss1: 837.68 and loss2: 150.58\n",
      "Epoch [191], train_loss: 980.89 with loss1: 831.43 and loss2: 149.46\n",
      "Epoch [192], train_loss: 986.33 with loss1: 836.07 and loss2: 150.25\n",
      "Epoch [193], train_loss: 983.81 with loss1: 833.99 and loss2: 149.82\n",
      "Epoch [194], train_loss: 987.77 with loss1: 837.76 and loss2: 150.01\n",
      "Epoch [195], train_loss: 978.64 with loss1: 829.51 and loss2: 149.13\n",
      "Epoch [196], train_loss: 985.89 with loss1: 836.44 and loss2: 149.44\n",
      "Epoch [197], train_loss: 979.67 with loss1: 830.31 and loss2: 149.36\n",
      "Epoch [198], train_loss: 985.05 with loss1: 834.99 and loss2: 150.05\n",
      "Epoch [199], train_loss: 980.02 with loss1: 831.22 and loss2: 148.80\n",
      "Epoch [200], train_loss: 988.19 with loss1: 838.43 and loss2: 149.76\n",
      "Epoch [201], train_loss: 989.94 with loss1: 840.92 and loss2: 149.02\n",
      "Epoch [202], train_loss: 984.49 with loss1: 834.99 and loss2: 149.50\n",
      "Epoch [203], train_loss: 974.32 with loss1: 825.86 and loss2: 148.46\n",
      "Epoch [204], train_loss: 981.72 with loss1: 832.58 and loss2: 149.14\n",
      "Epoch [205], train_loss: 975.72 with loss1: 827.06 and loss2: 148.66\n",
      "Epoch [206], train_loss: 982.37 with loss1: 832.84 and loss2: 149.53\n",
      "Epoch [207], train_loss: 973.97 with loss1: 825.42 and loss2: 148.54\n",
      "Epoch [208], train_loss: 979.55 with loss1: 830.06 and loss2: 149.49\n",
      "Epoch [209], train_loss: 976.04 with loss1: 827.33 and loss2: 148.71\n",
      "Epoch [210], train_loss: 974.88 with loss1: 825.94 and loss2: 148.93\n",
      "Epoch [211], train_loss: 968.73 with loss1: 821.14 and loss2: 147.59\n",
      "Epoch [212], train_loss: 971.41 with loss1: 822.28 and loss2: 149.12\n",
      "Epoch [213], train_loss: 963.94 with loss1: 816.24 and loss2: 147.69\n",
      "Epoch [214], train_loss: 969.91 with loss1: 821.18 and loss2: 148.73\n",
      "Epoch [215], train_loss: 961.09 with loss1: 814.24 and loss2: 146.85\n",
      "Epoch [216], train_loss: 968.87 with loss1: 820.70 and loss2: 148.18\n",
      "Epoch [217], train_loss: 963.40 with loss1: 815.39 and loss2: 148.02\n",
      "Epoch [218], train_loss: 967.40 with loss1: 819.03 and loss2: 148.38\n",
      "Epoch [219], train_loss: 963.61 with loss1: 816.13 and loss2: 147.48\n",
      "Epoch [220], train_loss: 970.51 with loss1: 822.24 and loss2: 148.27\n",
      "Epoch [221], train_loss: 965.83 with loss1: 819.19 and loss2: 146.65\n",
      "Epoch [222], train_loss: 973.80 with loss1: 826.29 and loss2: 147.52\n",
      "Epoch [223], train_loss: 970.49 with loss1: 823.28 and loss2: 147.21\n",
      "Epoch [224], train_loss: 977.40 with loss1: 829.75 and loss2: 147.66\n",
      "Epoch [225], train_loss: 971.91 with loss1: 824.71 and loss2: 147.21\n",
      "Epoch [226], train_loss: 980.96 with loss1: 833.94 and loss2: 147.02\n",
      "Epoch [227], train_loss: 975.28 with loss1: 828.06 and loss2: 147.21\n",
      "Epoch [228], train_loss: 983.93 with loss1: 836.36 and loss2: 147.56\n",
      "Epoch [229], train_loss: 973.52 with loss1: 826.90 and loss2: 146.62\n",
      "Epoch [230], train_loss: 982.87 with loss1: 835.86 and loss2: 147.01\n",
      "Epoch [231], train_loss: 974.29 with loss1: 827.84 and loss2: 146.45\n",
      "Epoch [232], train_loss: 980.82 with loss1: 833.87 and loss2: 146.95\n",
      "Epoch [233], train_loss: 973.10 with loss1: 826.60 and loss2: 146.50\n",
      "Epoch [234], train_loss: 973.32 with loss1: 826.05 and loss2: 147.27\n",
      "Epoch [235], train_loss: 962.06 with loss1: 816.34 and loss2: 145.72\n",
      "Epoch [236], train_loss: 961.49 with loss1: 815.10 and loss2: 146.39\n",
      "Epoch [237], train_loss: 949.75 with loss1: 804.15 and loss2: 145.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [238], train_loss: 954.58 with loss1: 808.71 and loss2: 145.87\n",
      "Epoch [239], train_loss: 945.33 with loss1: 799.90 and loss2: 145.43\n",
      "Epoch [240], train_loss: 947.80 with loss1: 801.01 and loss2: 146.79\n",
      "Epoch [241], train_loss: 935.32 with loss1: 790.10 and loss2: 145.22\n",
      "Epoch [242], train_loss: 939.01 with loss1: 792.91 and loss2: 146.10\n",
      "Epoch [243], train_loss: 933.05 with loss1: 787.82 and loss2: 145.23\n",
      "Epoch [244], train_loss: 935.85 with loss1: 790.06 and loss2: 145.79\n",
      "Epoch [245], train_loss: 926.22 with loss1: 781.20 and loss2: 145.02\n",
      "Epoch [246], train_loss: 927.60 with loss1: 781.23 and loss2: 146.37\n",
      "Epoch [247], train_loss: 921.51 with loss1: 775.58 and loss2: 145.94\n",
      "Epoch [248], train_loss: 927.82 with loss1: 782.03 and loss2: 145.79\n",
      "Epoch [249], train_loss: 914.83 with loss1: 769.93 and loss2: 144.90\n",
      "Epoch [250], train_loss: 920.04 with loss1: 774.20 and loss2: 145.84\n",
      "Epoch [251], train_loss: 913.25 with loss1: 768.81 and loss2: 144.44\n",
      "Epoch [252], train_loss: 912.49 with loss1: 767.22 and loss2: 145.27\n",
      "Epoch [253], train_loss: 907.54 with loss1: 763.16 and loss2: 144.39\n",
      "Epoch [254], train_loss: 922.66 with loss1: 777.71 and loss2: 144.95\n",
      "Epoch [255], train_loss: 911.04 with loss1: 766.23 and loss2: 144.81\n",
      "Epoch [256], train_loss: 912.65 with loss1: 768.02 and loss2: 144.63\n",
      "Epoch [257], train_loss: 906.56 with loss1: 762.02 and loss2: 144.53\n",
      "Epoch [258], train_loss: 911.16 with loss1: 766.67 and loss2: 144.49\n",
      "Epoch [259], train_loss: 907.52 with loss1: 762.97 and loss2: 144.54\n",
      "Epoch [260], train_loss: 910.56 with loss1: 765.96 and loss2: 144.60\n",
      "Epoch [261], train_loss: 910.51 with loss1: 765.87 and loss2: 144.64\n",
      "Epoch [262], train_loss: 911.38 with loss1: 766.88 and loss2: 144.50\n",
      "Epoch [263], train_loss: 905.21 with loss1: 761.09 and loss2: 144.12\n",
      "Epoch [264], train_loss: 913.04 with loss1: 768.76 and loss2: 144.28\n",
      "Epoch [265], train_loss: 911.35 with loss1: 767.35 and loss2: 144.00\n",
      "Epoch [266], train_loss: 913.81 with loss1: 768.92 and loss2: 144.89\n",
      "Epoch [267], train_loss: 911.43 with loss1: 766.43 and loss2: 145.00\n",
      "Epoch [268], train_loss: 916.02 with loss1: 772.29 and loss2: 143.73\n",
      "Epoch [269], train_loss: 910.94 with loss1: 767.01 and loss2: 143.93\n",
      "Epoch [270], train_loss: 920.69 with loss1: 776.70 and loss2: 143.99\n",
      "Epoch [271], train_loss: 915.58 with loss1: 771.65 and loss2: 143.94\n",
      "Epoch [272], train_loss: 923.20 with loss1: 779.31 and loss2: 143.89\n",
      "Epoch [273], train_loss: 915.55 with loss1: 772.09 and loss2: 143.47\n",
      "Epoch [274], train_loss: 919.56 with loss1: 776.05 and loss2: 143.52\n",
      "Epoch [275], train_loss: 919.76 with loss1: 776.67 and loss2: 143.08\n",
      "Epoch [276], train_loss: 921.46 with loss1: 777.59 and loss2: 143.88\n",
      "Epoch [277], train_loss: 917.69 with loss1: 774.56 and loss2: 143.13\n",
      "Epoch [278], train_loss: 923.42 with loss1: 780.18 and loss2: 143.24\n",
      "Epoch [279], train_loss: 916.92 with loss1: 774.08 and loss2: 142.84\n",
      "Epoch [280], train_loss: 924.50 with loss1: 781.13 and loss2: 143.37\n",
      "Epoch [281], train_loss: 917.69 with loss1: 774.70 and loss2: 142.99\n",
      "Epoch [282], train_loss: 924.15 with loss1: 781.49 and loss2: 142.65\n",
      "Epoch [283], train_loss: 918.93 with loss1: 776.01 and loss2: 142.92\n",
      "Epoch [284], train_loss: 923.78 with loss1: 780.74 and loss2: 143.04\n",
      "Epoch [285], train_loss: 917.86 with loss1: 775.10 and loss2: 142.76\n",
      "Epoch [286], train_loss: 920.69 with loss1: 778.23 and loss2: 142.45\n",
      "Epoch [287], train_loss: 911.69 with loss1: 769.72 and loss2: 141.96\n",
      "Epoch [288], train_loss: 916.75 with loss1: 773.62 and loss2: 143.13\n",
      "Epoch [289], train_loss: 909.58 with loss1: 767.32 and loss2: 142.26\n",
      "Epoch [290], train_loss: 918.86 with loss1: 776.72 and loss2: 142.15\n",
      "Epoch [291], train_loss: 909.84 with loss1: 767.47 and loss2: 142.37\n",
      "Epoch [292], train_loss: 916.31 with loss1: 774.08 and loss2: 142.23\n",
      "Epoch [293], train_loss: 910.36 with loss1: 768.91 and loss2: 141.45\n",
      "Epoch [294], train_loss: 915.05 with loss1: 773.23 and loss2: 141.82\n",
      "Epoch [295], train_loss: 909.83 with loss1: 768.19 and loss2: 141.63\n",
      "Epoch [296], train_loss: 913.17 with loss1: 771.34 and loss2: 141.82\n",
      "Epoch [297], train_loss: 905.90 with loss1: 764.57 and loss2: 141.33\n",
      "Epoch [298], train_loss: 909.48 with loss1: 767.63 and loss2: 141.86\n",
      "Epoch [299], train_loss: 906.21 with loss1: 765.12 and loss2: 141.08\n",
      "Epoch [300], train_loss: 913.41 with loss1: 771.44 and loss2: 141.97\n",
      "Epoch [301], train_loss: 906.49 with loss1: 765.13 and loss2: 141.36\n",
      "Epoch [302], train_loss: 915.57 with loss1: 774.09 and loss2: 141.48\n",
      "Epoch [303], train_loss: 908.49 with loss1: 767.62 and loss2: 140.87\n",
      "Epoch [304], train_loss: 916.25 with loss1: 774.57 and loss2: 141.68\n",
      "Epoch [305], train_loss: 911.37 with loss1: 770.39 and loss2: 140.98\n",
      "Epoch [306], train_loss: 920.16 with loss1: 779.03 and loss2: 141.14\n",
      "Epoch [307], train_loss: 912.15 with loss1: 771.80 and loss2: 140.34\n",
      "Epoch [308], train_loss: 918.78 with loss1: 777.63 and loss2: 141.15\n",
      "Epoch [309], train_loss: 914.23 with loss1: 773.41 and loss2: 140.83\n",
      "Epoch [310], train_loss: 922.13 with loss1: 780.87 and loss2: 141.26\n",
      "Epoch [311], train_loss: 919.20 with loss1: 778.03 and loss2: 141.16\n",
      "Epoch [312], train_loss: 921.36 with loss1: 780.85 and loss2: 140.51\n",
      "Epoch [313], train_loss: 920.76 with loss1: 780.70 and loss2: 140.07\n",
      "Epoch [314], train_loss: 928.29 with loss1: 787.19 and loss2: 141.10\n",
      "Epoch [315], train_loss: 924.39 with loss1: 784.72 and loss2: 139.67\n",
      "Epoch [316], train_loss: 932.83 with loss1: 791.58 and loss2: 141.25\n",
      "Epoch [317], train_loss: 921.75 with loss1: 782.45 and loss2: 139.30\n",
      "Epoch [318], train_loss: 931.96 with loss1: 791.02 and loss2: 140.94\n",
      "Epoch [319], train_loss: 924.01 with loss1: 784.29 and loss2: 139.72\n",
      "Epoch [320], train_loss: 935.36 with loss1: 794.99 and loss2: 140.37\n",
      "Epoch [321], train_loss: 929.94 with loss1: 789.83 and loss2: 140.11\n",
      "Epoch [322], train_loss: 935.45 with loss1: 795.58 and loss2: 139.87\n",
      "Epoch [323], train_loss: 931.76 with loss1: 791.70 and loss2: 140.06\n",
      "Epoch [324], train_loss: 941.55 with loss1: 801.04 and loss2: 140.50\n",
      "Epoch [325], train_loss: 932.87 with loss1: 793.44 and loss2: 139.43\n",
      "Epoch [326], train_loss: 941.13 with loss1: 801.10 and loss2: 140.04\n",
      "Epoch [327], train_loss: 929.13 with loss1: 790.27 and loss2: 138.86\n",
      "Epoch [328], train_loss: 937.20 with loss1: 797.65 and loss2: 139.55\n",
      "Epoch [329], train_loss: 927.16 with loss1: 787.87 and loss2: 139.29\n",
      "Epoch [330], train_loss: 937.58 with loss1: 797.93 and loss2: 139.66\n",
      "Epoch [331], train_loss: 928.78 with loss1: 789.60 and loss2: 139.18\n",
      "Epoch [332], train_loss: 935.80 with loss1: 795.78 and loss2: 140.02\n",
      "Epoch [333], train_loss: 930.34 with loss1: 791.55 and loss2: 138.80\n",
      "Epoch [334], train_loss: 934.88 with loss1: 795.86 and loss2: 139.02\n",
      "Epoch [335], train_loss: 929.02 with loss1: 789.85 and loss2: 139.17\n",
      "Epoch [336], train_loss: 932.51 with loss1: 793.18 and loss2: 139.33\n",
      "Epoch [337], train_loss: 925.52 with loss1: 786.70 and loss2: 138.82\n",
      "Epoch [338], train_loss: 927.67 with loss1: 788.55 and loss2: 139.12\n",
      "Epoch [339], train_loss: 915.66 with loss1: 777.11 and loss2: 138.56\n",
      "Epoch [340], train_loss: 926.09 with loss1: 786.76 and loss2: 139.34\n",
      "Epoch [341], train_loss: 912.75 with loss1: 774.09 and loss2: 138.66\n",
      "Epoch [342], train_loss: 916.18 with loss1: 777.51 and loss2: 138.67\n",
      "Epoch [343], train_loss: 907.35 with loss1: 769.25 and loss2: 138.10\n",
      "Epoch [344], train_loss: 912.86 with loss1: 774.48 and loss2: 138.38\n",
      "Epoch [345], train_loss: 897.96 with loss1: 759.97 and loss2: 137.99\n",
      "Epoch [346], train_loss: 897.00 with loss1: 757.93 and loss2: 139.08\n",
      "Epoch [347], train_loss: 888.57 with loss1: 750.52 and loss2: 138.05\n",
      "Epoch [348], train_loss: 889.86 with loss1: 751.42 and loss2: 138.45\n",
      "Epoch [349], train_loss: 883.93 with loss1: 745.70 and loss2: 138.23\n",
      "Epoch [350], train_loss: 886.75 with loss1: 748.54 and loss2: 138.21\n",
      "Epoch [351], train_loss: 876.45 with loss1: 738.59 and loss2: 137.86\n",
      "Epoch [352], train_loss: 883.41 with loss1: 744.70 and loss2: 138.71\n",
      "Epoch [353], train_loss: 873.59 with loss1: 736.49 and loss2: 137.10\n",
      "Epoch [354], train_loss: 872.17 with loss1: 734.10 and loss2: 138.06\n",
      "Epoch [355], train_loss: 866.93 with loss1: 729.34 and loss2: 137.60\n",
      "Epoch [356], train_loss: 868.97 with loss1: 731.21 and loss2: 137.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [357], train_loss: 867.47 with loss1: 730.25 and loss2: 137.22\n",
      "Epoch [358], train_loss: 864.68 with loss1: 726.53 and loss2: 138.15\n",
      "Epoch [359], train_loss: 859.03 with loss1: 722.13 and loss2: 136.90\n",
      "Epoch [360], train_loss: 861.41 with loss1: 723.76 and loss2: 137.65\n",
      "Epoch [361], train_loss: 857.36 with loss1: 720.71 and loss2: 136.64\n",
      "Epoch [362], train_loss: 858.21 with loss1: 720.98 and loss2: 137.23\n",
      "Epoch [363], train_loss: 856.88 with loss1: 719.99 and loss2: 136.89\n",
      "Epoch [364], train_loss: 862.69 with loss1: 725.46 and loss2: 137.23\n",
      "Epoch [365], train_loss: 862.67 with loss1: 725.68 and loss2: 136.99\n",
      "Epoch [366], train_loss: 864.79 with loss1: 727.54 and loss2: 137.25\n",
      "Epoch [367], train_loss: 865.11 with loss1: 728.05 and loss2: 137.06\n",
      "Epoch [368], train_loss: 866.86 with loss1: 728.95 and loss2: 137.91\n",
      "Epoch [369], train_loss: 862.08 with loss1: 725.87 and loss2: 136.21\n",
      "Epoch [370], train_loss: 865.67 with loss1: 728.72 and loss2: 136.95\n",
      "Epoch [371], train_loss: 864.41 with loss1: 727.97 and loss2: 136.45\n",
      "Epoch [372], train_loss: 865.32 with loss1: 729.07 and loss2: 136.25\n",
      "Epoch [373], train_loss: 867.76 with loss1: 731.05 and loss2: 136.71\n",
      "Epoch [374], train_loss: 872.39 with loss1: 736.03 and loss2: 136.35\n",
      "Epoch [375], train_loss: 869.26 with loss1: 732.52 and loss2: 136.74\n",
      "Epoch [376], train_loss: 872.67 with loss1: 736.25 and loss2: 136.42\n",
      "Epoch [377], train_loss: 870.12 with loss1: 733.96 and loss2: 136.16\n",
      "Epoch [378], train_loss: 877.18 with loss1: 740.90 and loss2: 136.28\n",
      "Epoch [379], train_loss: 871.67 with loss1: 735.59 and loss2: 136.08\n",
      "Epoch [380], train_loss: 877.11 with loss1: 741.40 and loss2: 135.71\n",
      "Epoch [381], train_loss: 871.77 with loss1: 735.78 and loss2: 135.99\n",
      "Epoch [382], train_loss: 875.43 with loss1: 739.27 and loss2: 136.16\n",
      "Epoch [383], train_loss: 870.34 with loss1: 734.35 and loss2: 135.99\n",
      "Epoch [384], train_loss: 874.98 with loss1: 739.04 and loss2: 135.94\n",
      "Epoch [385], train_loss: 869.23 with loss1: 733.61 and loss2: 135.62\n",
      "Epoch [386], train_loss: 873.43 with loss1: 737.47 and loss2: 135.96\n",
      "Epoch [387], train_loss: 869.14 with loss1: 733.80 and loss2: 135.34\n",
      "Epoch [388], train_loss: 873.11 with loss1: 737.25 and loss2: 135.86\n",
      "Epoch [389], train_loss: 863.99 with loss1: 729.07 and loss2: 134.92\n",
      "Epoch [390], train_loss: 870.04 with loss1: 734.60 and loss2: 135.45\n",
      "Epoch [391], train_loss: 864.83 with loss1: 729.80 and loss2: 135.03\n",
      "Epoch [392], train_loss: 869.98 with loss1: 734.14 and loss2: 135.85\n",
      "Epoch [393], train_loss: 861.31 with loss1: 726.87 and loss2: 134.43\n",
      "Epoch [394], train_loss: 865.52 with loss1: 729.92 and loss2: 135.60\n",
      "Epoch [395], train_loss: 860.71 with loss1: 726.15 and loss2: 134.56\n",
      "Epoch [396], train_loss: 866.91 with loss1: 731.66 and loss2: 135.25\n",
      "Epoch [397], train_loss: 859.55 with loss1: 725.40 and loss2: 134.15\n",
      "Epoch [398], train_loss: 866.15 with loss1: 731.27 and loss2: 134.88\n",
      "Epoch [399], train_loss: 862.42 with loss1: 728.09 and loss2: 134.33\n",
      "Epoch [400], train_loss: 866.73 with loss1: 732.11 and loss2: 134.63\n",
      "Epoch [401], train_loss: 860.40 with loss1: 726.36 and loss2: 134.04\n",
      "Epoch [402], train_loss: 868.35 with loss1: 734.09 and loss2: 134.26\n",
      "Epoch [403], train_loss: 861.25 with loss1: 726.93 and loss2: 134.33\n",
      "Epoch [404], train_loss: 871.71 with loss1: 736.69 and loss2: 135.01\n",
      "Epoch [405], train_loss: 860.80 with loss1: 726.88 and loss2: 133.92\n",
      "Epoch [406], train_loss: 870.40 with loss1: 735.93 and loss2: 134.48\n",
      "Epoch [407], train_loss: 870.74 with loss1: 736.89 and loss2: 133.85\n",
      "Epoch [408], train_loss: 878.68 with loss1: 744.21 and loss2: 134.46\n",
      "Epoch [409], train_loss: 867.90 with loss1: 734.38 and loss2: 133.52\n",
      "Epoch [410], train_loss: 879.44 with loss1: 744.31 and loss2: 135.13\n",
      "Epoch [411], train_loss: 875.31 with loss1: 741.65 and loss2: 133.66\n",
      "Epoch [412], train_loss: 885.67 with loss1: 751.40 and loss2: 134.27\n",
      "Epoch [413], train_loss: 878.46 with loss1: 745.30 and loss2: 133.16\n",
      "Epoch [414], train_loss: 892.43 with loss1: 758.61 and loss2: 133.83\n",
      "Epoch [415], train_loss: 885.12 with loss1: 751.73 and loss2: 133.39\n",
      "Epoch [416], train_loss: 898.08 with loss1: 764.48 and loss2: 133.60\n",
      "Epoch [417], train_loss: 891.93 with loss1: 757.98 and loss2: 133.95\n",
      "Epoch [418], train_loss: 901.52 with loss1: 768.04 and loss2: 133.47\n",
      "Epoch [419], train_loss: 895.39 with loss1: 762.30 and loss2: 133.09\n",
      "Epoch [420], train_loss: 905.80 with loss1: 772.32 and loss2: 133.48\n",
      "Epoch [421], train_loss: 900.09 with loss1: 767.37 and loss2: 132.72\n",
      "Epoch [422], train_loss: 909.71 with loss1: 776.48 and loss2: 133.24\n",
      "Epoch [423], train_loss: 902.98 with loss1: 770.14 and loss2: 132.84\n",
      "Epoch [424], train_loss: 908.62 with loss1: 774.86 and loss2: 133.76\n",
      "Epoch [425], train_loss: 895.09 with loss1: 762.67 and loss2: 132.41\n",
      "Epoch [426], train_loss: 907.43 with loss1: 774.10 and loss2: 133.33\n",
      "Epoch [427], train_loss: 893.59 with loss1: 760.67 and loss2: 132.92\n",
      "Epoch [428], train_loss: 900.91 with loss1: 767.66 and loss2: 133.24\n",
      "Epoch [429], train_loss: 888.65 with loss1: 756.12 and loss2: 132.53\n",
      "Epoch [430], train_loss: 892.39 with loss1: 759.22 and loss2: 133.17\n",
      "Epoch [431], train_loss: 880.16 with loss1: 747.08 and loss2: 133.08\n",
      "Epoch [432], train_loss: 878.87 with loss1: 746.15 and loss2: 132.73\n",
      "Epoch [433], train_loss: 869.16 with loss1: 737.05 and loss2: 132.10\n",
      "Epoch [434], train_loss: 873.11 with loss1: 739.97 and loss2: 133.14\n",
      "Epoch [435], train_loss: 867.64 with loss1: 735.29 and loss2: 132.35\n",
      "Epoch [436], train_loss: 866.35 with loss1: 733.88 and loss2: 132.47\n",
      "Epoch [437], train_loss: 860.31 with loss1: 728.09 and loss2: 132.22\n",
      "Epoch [438], train_loss: 862.06 with loss1: 728.88 and loss2: 133.17\n",
      "Epoch [439], train_loss: 854.32 with loss1: 722.17 and loss2: 132.15\n",
      "Epoch [440], train_loss: 852.68 with loss1: 720.17 and loss2: 132.51\n",
      "Epoch [441], train_loss: 846.47 with loss1: 714.10 and loss2: 132.37\n",
      "Epoch [442], train_loss: 849.34 with loss1: 717.12 and loss2: 132.23\n",
      "Epoch [443], train_loss: 844.78 with loss1: 713.12 and loss2: 131.66\n",
      "Epoch [444], train_loss: 844.26 with loss1: 712.28 and loss2: 131.98\n",
      "Epoch [445], train_loss: 838.59 with loss1: 706.94 and loss2: 131.65\n",
      "Epoch [446], train_loss: 840.44 with loss1: 708.18 and loss2: 132.26\n",
      "Epoch [447], train_loss: 835.55 with loss1: 703.77 and loss2: 131.78\n",
      "Epoch [448], train_loss: 841.31 with loss1: 709.35 and loss2: 131.95\n",
      "Epoch [449], train_loss: 834.25 with loss1: 702.10 and loss2: 132.15\n",
      "Epoch [450], train_loss: 837.97 with loss1: 706.39 and loss2: 131.58\n",
      "Epoch [451], train_loss: 832.06 with loss1: 700.93 and loss2: 131.13\n",
      "Epoch [452], train_loss: 835.01 with loss1: 703.17 and loss2: 131.85\n",
      "Epoch [453], train_loss: 829.23 with loss1: 698.58 and loss2: 130.65\n",
      "Epoch [454], train_loss: 832.10 with loss1: 700.49 and loss2: 131.60\n",
      "Epoch [455], train_loss: 833.28 with loss1: 702.64 and loss2: 130.64\n",
      "Epoch [456], train_loss: 830.24 with loss1: 699.29 and loss2: 130.95\n",
      "Epoch [457], train_loss: 834.08 with loss1: 703.27 and loss2: 130.81\n",
      "Epoch [458], train_loss: 836.01 with loss1: 704.00 and loss2: 132.01\n",
      "Epoch [459], train_loss: 832.97 with loss1: 701.90 and loss2: 131.07\n",
      "Epoch [460], train_loss: 835.10 with loss1: 703.68 and loss2: 131.42\n",
      "Epoch [461], train_loss: 832.49 with loss1: 702.19 and loss2: 130.30\n",
      "Epoch [462], train_loss: 834.27 with loss1: 703.60 and loss2: 130.67\n",
      "Epoch [463], train_loss: 835.48 with loss1: 704.95 and loss2: 130.52\n",
      "Epoch [464], train_loss: 836.87 with loss1: 705.71 and loss2: 131.17\n",
      "Epoch [465], train_loss: 836.30 with loss1: 705.72 and loss2: 130.58\n",
      "Epoch [466], train_loss: 838.61 with loss1: 707.91 and loss2: 130.69\n",
      "Epoch [467], train_loss: 837.22 with loss1: 706.73 and loss2: 130.48\n",
      "Epoch [468], train_loss: 841.31 with loss1: 710.87 and loss2: 130.44\n",
      "Epoch [469], train_loss: 838.50 with loss1: 707.89 and loss2: 130.61\n",
      "Epoch [470], train_loss: 843.65 with loss1: 713.37 and loss2: 130.28\n",
      "Epoch [471], train_loss: 837.41 with loss1: 707.04 and loss2: 130.37\n",
      "Epoch [472], train_loss: 841.08 with loss1: 710.23 and loss2: 130.85\n",
      "Epoch [473], train_loss: 837.94 with loss1: 708.06 and loss2: 129.88\n",
      "Epoch [474], train_loss: 837.25 with loss1: 707.31 and loss2: 129.94\n",
      "Epoch [475], train_loss: 833.47 with loss1: 703.72 and loss2: 129.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [476], train_loss: 837.48 with loss1: 707.17 and loss2: 130.31\n",
      "Epoch [477], train_loss: 828.61 with loss1: 699.17 and loss2: 129.45\n",
      "Epoch [478], train_loss: 832.85 with loss1: 702.66 and loss2: 130.19\n",
      "Epoch [479], train_loss: 824.76 with loss1: 695.31 and loss2: 129.46\n",
      "Epoch [480], train_loss: 828.65 with loss1: 699.33 and loss2: 129.32\n",
      "Epoch [481], train_loss: 824.65 with loss1: 695.41 and loss2: 129.24\n",
      "Epoch [482], train_loss: 826.05 with loss1: 696.14 and loss2: 129.91\n",
      "Epoch [483], train_loss: 826.78 with loss1: 697.32 and loss2: 129.47\n",
      "Epoch [484], train_loss: 823.66 with loss1: 693.93 and loss2: 129.74\n",
      "Epoch [485], train_loss: 818.58 with loss1: 689.26 and loss2: 129.33\n",
      "Epoch [486], train_loss: 821.96 with loss1: 692.47 and loss2: 129.49\n",
      "Epoch [487], train_loss: 817.28 with loss1: 688.29 and loss2: 128.99\n",
      "Epoch [488], train_loss: 821.96 with loss1: 692.50 and loss2: 129.46\n",
      "Epoch [489], train_loss: 817.90 with loss1: 689.23 and loss2: 128.67\n",
      "Epoch [490], train_loss: 822.71 with loss1: 693.62 and loss2: 129.10\n",
      "Epoch [491], train_loss: 817.92 with loss1: 689.56 and loss2: 128.36\n",
      "Epoch [492], train_loss: 820.52 with loss1: 691.58 and loss2: 128.94\n",
      "Epoch [493], train_loss: 819.02 with loss1: 690.34 and loss2: 128.68\n",
      "Epoch [494], train_loss: 820.97 with loss1: 692.26 and loss2: 128.71\n",
      "Epoch [495], train_loss: 821.44 with loss1: 692.62 and loss2: 128.82\n",
      "Epoch [496], train_loss: 823.22 with loss1: 694.06 and loss2: 129.16\n",
      "Epoch [497], train_loss: 822.20 with loss1: 693.99 and loss2: 128.21\n",
      "Epoch [498], train_loss: 827.06 with loss1: 698.36 and loss2: 128.70\n",
      "Epoch [499], train_loss: 821.59 with loss1: 692.71 and loss2: 128.88\n",
      "Epoch [500], train_loss: 831.24 with loss1: 702.36 and loss2: 128.87\n",
      "Epoch [501], train_loss: 823.20 with loss1: 695.24 and loss2: 127.96\n",
      "Epoch [502], train_loss: 828.75 with loss1: 700.13 and loss2: 128.61\n",
      "Epoch [503], train_loss: 827.96 with loss1: 699.72 and loss2: 128.24\n",
      "Epoch [504], train_loss: 834.12 with loss1: 705.92 and loss2: 128.20\n",
      "Epoch [505], train_loss: 829.67 with loss1: 701.85 and loss2: 127.82\n",
      "Epoch [506], train_loss: 836.77 with loss1: 708.45 and loss2: 128.33\n",
      "Epoch [507], train_loss: 833.90 with loss1: 705.70 and loss2: 128.19\n",
      "Epoch [508], train_loss: 837.62 with loss1: 709.11 and loss2: 128.51\n",
      "Epoch [509], train_loss: 834.64 with loss1: 706.90 and loss2: 127.74\n",
      "Epoch [510], train_loss: 839.82 with loss1: 711.77 and loss2: 128.05\n",
      "Epoch [511], train_loss: 833.84 with loss1: 706.59 and loss2: 127.25\n",
      "Epoch [512], train_loss: 842.81 with loss1: 714.40 and loss2: 128.40\n",
      "Epoch [513], train_loss: 833.98 with loss1: 706.81 and loss2: 127.18\n",
      "Epoch [514], train_loss: 841.61 with loss1: 713.94 and loss2: 127.67\n",
      "Epoch [515], train_loss: 836.10 with loss1: 708.73 and loss2: 127.37\n",
      "Epoch [516], train_loss: 841.54 with loss1: 713.42 and loss2: 128.12\n",
      "Epoch [517], train_loss: 834.54 with loss1: 707.59 and loss2: 126.95\n",
      "Epoch [518], train_loss: 843.44 with loss1: 715.74 and loss2: 127.71\n",
      "Epoch [519], train_loss: 831.30 with loss1: 704.29 and loss2: 127.01\n",
      "Epoch [520], train_loss: 842.81 with loss1: 715.33 and loss2: 127.48\n",
      "Epoch [521], train_loss: 831.50 with loss1: 704.42 and loss2: 127.07\n",
      "Epoch [522], train_loss: 840.81 with loss1: 713.71 and loss2: 127.09\n",
      "Epoch [523], train_loss: 827.61 with loss1: 701.03 and loss2: 126.58\n",
      "Epoch [524], train_loss: 834.72 with loss1: 707.36 and loss2: 127.36\n",
      "Epoch [525], train_loss: 827.06 with loss1: 700.62 and loss2: 126.44\n",
      "Epoch [526], train_loss: 826.63 with loss1: 699.93 and loss2: 126.70\n",
      "Epoch [527], train_loss: 822.33 with loss1: 695.49 and loss2: 126.84\n",
      "Epoch [528], train_loss: 825.19 with loss1: 698.07 and loss2: 127.12\n",
      "Epoch [529], train_loss: 818.33 with loss1: 691.54 and loss2: 126.79\n",
      "Epoch [530], train_loss: 816.58 with loss1: 689.25 and loss2: 127.33\n",
      "Epoch [531], train_loss: 807.42 with loss1: 681.38 and loss2: 126.04\n",
      "Epoch [532], train_loss: 808.74 with loss1: 682.14 and loss2: 126.59\n",
      "Epoch [533], train_loss: 804.72 with loss1: 678.80 and loss2: 125.92\n",
      "Epoch [534], train_loss: 809.22 with loss1: 682.86 and loss2: 126.36\n",
      "Epoch [535], train_loss: 803.10 with loss1: 677.01 and loss2: 126.09\n",
      "Epoch [536], train_loss: 805.71 with loss1: 679.25 and loss2: 126.46\n",
      "Epoch [537], train_loss: 798.51 with loss1: 672.36 and loss2: 126.14\n",
      "Epoch [538], train_loss: 804.26 with loss1: 677.45 and loss2: 126.80\n",
      "Epoch [539], train_loss: 797.50 with loss1: 671.66 and loss2: 125.84\n",
      "Epoch [540], train_loss: 797.84 with loss1: 672.16 and loss2: 125.68\n",
      "Epoch [541], train_loss: 797.28 with loss1: 671.82 and loss2: 125.46\n",
      "Epoch [542], train_loss: 801.01 with loss1: 674.72 and loss2: 126.30\n",
      "Epoch [543], train_loss: 795.60 with loss1: 669.91 and loss2: 125.69\n",
      "Epoch [544], train_loss: 800.33 with loss1: 674.09 and loss2: 126.23\n",
      "Epoch [545], train_loss: 797.71 with loss1: 672.31 and loss2: 125.40\n",
      "Epoch [546], train_loss: 798.33 with loss1: 672.48 and loss2: 125.86\n",
      "Epoch [547], train_loss: 794.27 with loss1: 669.09 and loss2: 125.19\n",
      "Epoch [548], train_loss: 796.61 with loss1: 670.76 and loss2: 125.85\n",
      "Epoch [549], train_loss: 794.76 with loss1: 669.66 and loss2: 125.09\n",
      "Epoch [550], train_loss: 797.36 with loss1: 671.59 and loss2: 125.76\n",
      "Epoch [551], train_loss: 794.13 with loss1: 668.89 and loss2: 125.24\n",
      "Epoch [552], train_loss: 796.33 with loss1: 670.93 and loss2: 125.41\n",
      "Epoch [553], train_loss: 798.61 with loss1: 673.68 and loss2: 124.93\n",
      "Epoch [554], train_loss: 802.18 with loss1: 676.60 and loss2: 125.58\n",
      "Epoch [555], train_loss: 798.43 with loss1: 673.74 and loss2: 124.69\n",
      "Epoch [556], train_loss: 800.64 with loss1: 674.88 and loss2: 125.76\n",
      "Epoch [557], train_loss: 797.59 with loss1: 672.51 and loss2: 125.08\n",
      "Epoch [558], train_loss: 804.13 with loss1: 678.61 and loss2: 125.51\n",
      "Epoch [559], train_loss: 798.71 with loss1: 674.25 and loss2: 124.46\n",
      "Epoch [560], train_loss: 803.97 with loss1: 678.82 and loss2: 125.15\n",
      "Epoch [561], train_loss: 801.27 with loss1: 676.57 and loss2: 124.71\n",
      "Epoch [562], train_loss: 811.58 with loss1: 686.13 and loss2: 125.45\n",
      "Epoch [563], train_loss: 803.08 with loss1: 678.27 and loss2: 124.81\n",
      "Epoch [564], train_loss: 811.50 with loss1: 686.32 and loss2: 125.19\n",
      "Epoch [565], train_loss: 806.49 with loss1: 681.87 and loss2: 124.62\n",
      "Epoch [566], train_loss: 812.38 with loss1: 687.76 and loss2: 124.62\n",
      "Epoch [567], train_loss: 806.71 with loss1: 682.49 and loss2: 124.23\n",
      "Epoch [568], train_loss: 812.63 with loss1: 688.16 and loss2: 124.47\n",
      "Epoch [569], train_loss: 807.03 with loss1: 682.93 and loss2: 124.10\n",
      "Epoch [570], train_loss: 814.71 with loss1: 689.88 and loss2: 124.83\n",
      "Epoch [571], train_loss: 813.09 with loss1: 689.17 and loss2: 123.92\n",
      "Epoch [572], train_loss: 815.35 with loss1: 690.80 and loss2: 124.56\n",
      "Epoch [573], train_loss: 808.14 with loss1: 684.27 and loss2: 123.87\n",
      "Epoch [574], train_loss: 814.05 with loss1: 689.26 and loss2: 124.80\n",
      "Epoch [575], train_loss: 806.12 with loss1: 682.34 and loss2: 123.78\n",
      "Epoch [576], train_loss: 809.59 with loss1: 685.02 and loss2: 124.58\n",
      "Epoch [577], train_loss: 809.12 with loss1: 685.44 and loss2: 123.68\n",
      "Epoch [578], train_loss: 806.81 with loss1: 682.73 and loss2: 124.07\n",
      "Epoch [579], train_loss: 799.73 with loss1: 675.97 and loss2: 123.75\n",
      "Epoch [580], train_loss: 803.44 with loss1: 679.16 and loss2: 124.28\n",
      "Epoch [581], train_loss: 797.69 with loss1: 674.46 and loss2: 123.23\n",
      "Epoch [582], train_loss: 800.42 with loss1: 676.49 and loss2: 123.93\n",
      "Epoch [583], train_loss: 794.72 with loss1: 671.50 and loss2: 123.22\n",
      "Epoch [584], train_loss: 805.76 with loss1: 681.92 and loss2: 123.84\n",
      "Epoch [585], train_loss: 799.33 with loss1: 675.15 and loss2: 124.18\n",
      "Epoch [586], train_loss: 799.29 with loss1: 675.30 and loss2: 124.00\n",
      "Epoch [587], train_loss: 793.31 with loss1: 670.29 and loss2: 123.01\n",
      "Epoch [588], train_loss: 796.37 with loss1: 672.36 and loss2: 124.01\n",
      "Epoch [589], train_loss: 792.20 with loss1: 669.24 and loss2: 122.96\n",
      "Epoch [590], train_loss: 794.72 with loss1: 670.99 and loss2: 123.73\n",
      "Epoch [591], train_loss: 790.86 with loss1: 667.36 and loss2: 123.50\n",
      "Epoch [592], train_loss: 794.65 with loss1: 671.91 and loss2: 122.74\n",
      "Epoch [593], train_loss: 787.35 with loss1: 664.51 and loss2: 122.84\n",
      "Epoch [594], train_loss: 791.25 with loss1: 667.91 and loss2: 123.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [595], train_loss: 785.32 with loss1: 662.68 and loss2: 122.64\n",
      "Epoch [596], train_loss: 788.02 with loss1: 664.97 and loss2: 123.05\n",
      "Epoch [597], train_loss: 786.61 with loss1: 663.67 and loss2: 122.94\n",
      "Epoch [598], train_loss: 787.26 with loss1: 664.26 and loss2: 123.00\n",
      "Epoch [599], train_loss: 783.90 with loss1: 661.72 and loss2: 122.19\n",
      "Epoch [600], train_loss: 785.91 with loss1: 663.11 and loss2: 122.81\n",
      "Epoch [601], train_loss: 777.99 with loss1: 655.38 and loss2: 122.61\n",
      "Epoch [602], train_loss: 783.33 with loss1: 660.84 and loss2: 122.49\n",
      "Epoch [603], train_loss: 777.93 with loss1: 655.47 and loss2: 122.46\n",
      "Epoch [604], train_loss: 781.60 with loss1: 659.05 and loss2: 122.55\n",
      "Epoch [605], train_loss: 776.77 with loss1: 654.46 and loss2: 122.31\n",
      "Epoch [606], train_loss: 784.22 with loss1: 661.17 and loss2: 123.04\n",
      "Epoch [607], train_loss: 778.35 with loss1: 656.17 and loss2: 122.18\n",
      "Epoch [608], train_loss: 784.27 with loss1: 661.67 and loss2: 122.60\n",
      "Epoch [609], train_loss: 778.55 with loss1: 656.88 and loss2: 121.66\n",
      "Epoch [610], train_loss: 785.67 with loss1: 663.81 and loss2: 121.86\n",
      "Epoch [611], train_loss: 782.88 with loss1: 660.58 and loss2: 122.30\n",
      "Epoch [612], train_loss: 790.52 with loss1: 668.16 and loss2: 122.36\n",
      "Epoch [613], train_loss: 785.67 with loss1: 664.51 and loss2: 121.17\n",
      "Epoch [614], train_loss: 795.86 with loss1: 674.04 and loss2: 121.82\n",
      "Epoch [615], train_loss: 786.83 with loss1: 664.96 and loss2: 121.87\n",
      "Epoch [616], train_loss: 791.05 with loss1: 668.76 and loss2: 122.30\n",
      "Epoch [617], train_loss: 788.74 with loss1: 666.73 and loss2: 122.01\n",
      "Epoch [618], train_loss: 792.98 with loss1: 670.62 and loss2: 122.36\n",
      "Epoch [619], train_loss: 791.17 with loss1: 669.28 and loss2: 121.88\n",
      "Epoch [620], train_loss: 797.19 with loss1: 674.99 and loss2: 122.20\n",
      "Epoch [621], train_loss: 792.72 with loss1: 671.65 and loss2: 121.07\n",
      "Epoch [622], train_loss: 794.40 with loss1: 672.26 and loss2: 122.15\n",
      "Epoch [623], train_loss: 789.66 with loss1: 668.17 and loss2: 121.49\n",
      "Epoch [624], train_loss: 796.42 with loss1: 674.58 and loss2: 121.84\n",
      "Epoch [625], train_loss: 789.30 with loss1: 668.07 and loss2: 121.23\n",
      "Epoch [626], train_loss: 795.20 with loss1: 673.58 and loss2: 121.62\n",
      "Epoch [627], train_loss: 787.48 with loss1: 666.45 and loss2: 121.03\n",
      "Epoch [628], train_loss: 792.80 with loss1: 671.45 and loss2: 121.35\n",
      "Epoch [629], train_loss: 786.83 with loss1: 665.98 and loss2: 120.85\n",
      "Epoch [630], train_loss: 789.17 with loss1: 667.71 and loss2: 121.46\n",
      "Epoch [631], train_loss: 783.46 with loss1: 662.52 and loss2: 120.94\n",
      "Epoch [632], train_loss: 787.84 with loss1: 666.66 and loss2: 121.19\n",
      "Epoch [633], train_loss: 785.24 with loss1: 664.04 and loss2: 121.21\n",
      "Epoch [634], train_loss: 792.17 with loss1: 670.97 and loss2: 121.21\n",
      "Epoch [635], train_loss: 785.92 with loss1: 665.22 and loss2: 120.70\n",
      "Epoch [636], train_loss: 788.82 with loss1: 667.65 and loss2: 121.17\n",
      "Epoch [637], train_loss: 786.58 with loss1: 665.97 and loss2: 120.61\n",
      "Epoch [638], train_loss: 790.97 with loss1: 669.78 and loss2: 121.19\n",
      "Epoch [639], train_loss: 788.72 with loss1: 667.77 and loss2: 120.95\n",
      "Epoch [640], train_loss: 792.06 with loss1: 671.16 and loss2: 120.89\n",
      "Epoch [641], train_loss: 790.41 with loss1: 668.80 and loss2: 121.61\n",
      "Epoch [642], train_loss: 794.62 with loss1: 673.67 and loss2: 120.95\n",
      "Epoch [643], train_loss: 790.48 with loss1: 669.51 and loss2: 120.97\n",
      "Epoch [644], train_loss: 798.41 with loss1: 677.37 and loss2: 121.04\n",
      "Epoch [645], train_loss: 792.85 with loss1: 672.85 and loss2: 119.99\n",
      "Epoch [646], train_loss: 803.75 with loss1: 682.92 and loss2: 120.84\n",
      "Epoch [647], train_loss: 798.68 with loss1: 678.95 and loss2: 119.72\n",
      "Epoch [648], train_loss: 813.36 with loss1: 692.73 and loss2: 120.63\n",
      "Epoch [649], train_loss: 808.50 with loss1: 688.34 and loss2: 120.17\n",
      "Epoch [650], train_loss: 820.64 with loss1: 700.13 and loss2: 120.51\n",
      "Epoch [651], train_loss: 816.02 with loss1: 695.98 and loss2: 120.04\n",
      "Epoch [652], train_loss: 828.58 with loss1: 708.09 and loss2: 120.48\n",
      "Epoch [653], train_loss: 823.09 with loss1: 703.54 and loss2: 119.55\n",
      "Epoch [654], train_loss: 841.78 with loss1: 721.69 and loss2: 120.10\n",
      "Epoch [655], train_loss: 834.61 with loss1: 714.43 and loss2: 120.17\n",
      "Epoch [656], train_loss: 850.67 with loss1: 730.43 and loss2: 120.24\n",
      "Epoch [657], train_loss: 843.06 with loss1: 723.06 and loss2: 120.00\n",
      "Epoch [658], train_loss: 855.68 with loss1: 735.75 and loss2: 119.93\n",
      "Epoch [659], train_loss: 841.69 with loss1: 722.17 and loss2: 119.51\n",
      "Epoch [660], train_loss: 853.63 with loss1: 733.73 and loss2: 119.91\n",
      "Epoch [661], train_loss: 837.93 with loss1: 718.41 and loss2: 119.52\n",
      "Epoch [662], train_loss: 844.33 with loss1: 724.70 and loss2: 119.63\n",
      "Epoch [663], train_loss: 826.53 with loss1: 707.16 and loss2: 119.37\n",
      "Epoch [664], train_loss: 826.39 with loss1: 706.88 and loss2: 119.51\n",
      "Epoch [665], train_loss: 813.76 with loss1: 693.97 and loss2: 119.79\n",
      "Epoch [666], train_loss: 811.16 with loss1: 691.71 and loss2: 119.45\n",
      "Epoch [667], train_loss: 793.64 with loss1: 674.24 and loss2: 119.40\n",
      "Epoch [668], train_loss: 794.46 with loss1: 675.36 and loss2: 119.10\n",
      "Epoch [669], train_loss: 784.25 with loss1: 664.88 and loss2: 119.38\n",
      "Epoch [670], train_loss: 785.23 with loss1: 665.88 and loss2: 119.35\n",
      "Epoch [671], train_loss: 772.95 with loss1: 653.77 and loss2: 119.19\n",
      "Epoch [672], train_loss: 770.49 with loss1: 651.07 and loss2: 119.41\n",
      "Epoch [673], train_loss: 760.90 with loss1: 641.98 and loss2: 118.92\n",
      "Epoch [674], train_loss: 759.79 with loss1: 640.26 and loss2: 119.53\n",
      "Epoch [675], train_loss: 749.90 with loss1: 631.03 and loss2: 118.86\n",
      "Epoch [676], train_loss: 754.88 with loss1: 635.72 and loss2: 119.16\n",
      "Epoch [677], train_loss: 746.57 with loss1: 627.54 and loss2: 119.03\n",
      "Epoch [678], train_loss: 746.85 with loss1: 627.67 and loss2: 119.18\n",
      "Epoch [679], train_loss: 740.51 with loss1: 621.95 and loss2: 118.56\n",
      "Epoch [680], train_loss: 742.49 with loss1: 623.82 and loss2: 118.67\n",
      "Epoch [681], train_loss: 734.33 with loss1: 615.94 and loss2: 118.39\n",
      "Epoch [682], train_loss: 737.64 with loss1: 618.90 and loss2: 118.74\n",
      "Epoch [683], train_loss: 731.77 with loss1: 613.02 and loss2: 118.75\n",
      "Epoch [684], train_loss: 730.12 with loss1: 611.99 and loss2: 118.13\n",
      "Epoch [685], train_loss: 726.73 with loss1: 608.64 and loss2: 118.09\n",
      "Epoch [686], train_loss: 727.74 with loss1: 609.28 and loss2: 118.46\n",
      "Epoch [687], train_loss: 722.71 with loss1: 604.40 and loss2: 118.31\n",
      "Epoch [688], train_loss: 723.95 with loss1: 605.61 and loss2: 118.34\n",
      "Epoch [689], train_loss: 720.18 with loss1: 602.31 and loss2: 117.87\n",
      "Epoch [690], train_loss: 726.20 with loss1: 607.04 and loss2: 119.16\n",
      "Epoch [691], train_loss: 720.47 with loss1: 602.71 and loss2: 117.76\n",
      "Epoch [692], train_loss: 720.26 with loss1: 602.04 and loss2: 118.22\n",
      "Epoch [693], train_loss: 720.88 with loss1: 603.32 and loss2: 117.55\n",
      "Epoch [694], train_loss: 722.49 with loss1: 603.95 and loss2: 118.54\n",
      "Epoch [695], train_loss: 718.55 with loss1: 600.79 and loss2: 117.76\n",
      "Epoch [696], train_loss: 721.92 with loss1: 604.01 and loss2: 117.91\n",
      "Epoch [697], train_loss: 722.68 with loss1: 605.00 and loss2: 117.67\n",
      "Epoch [698], train_loss: 723.65 with loss1: 605.59 and loss2: 118.06\n",
      "Epoch [699], train_loss: 722.05 with loss1: 604.41 and loss2: 117.65\n",
      "Epoch [700], train_loss: 724.77 with loss1: 607.27 and loss2: 117.50\n",
      "Epoch [701], train_loss: 721.47 with loss1: 603.96 and loss2: 117.51\n",
      "Epoch [702], train_loss: 731.97 with loss1: 614.41 and loss2: 117.56\n",
      "Epoch [703], train_loss: 722.36 with loss1: 604.85 and loss2: 117.51\n",
      "Epoch [704], train_loss: 727.34 with loss1: 609.69 and loss2: 117.65\n",
      "Epoch [705], train_loss: 724.69 with loss1: 607.21 and loss2: 117.48\n",
      "Epoch [706], train_loss: 729.88 with loss1: 612.35 and loss2: 117.53\n",
      "Epoch [707], train_loss: 727.90 with loss1: 611.08 and loss2: 116.82\n",
      "Epoch [708], train_loss: 736.29 with loss1: 618.84 and loss2: 117.45\n",
      "Epoch [709], train_loss: 733.82 with loss1: 616.64 and loss2: 117.18\n",
      "Epoch [710], train_loss: 738.29 with loss1: 621.16 and loss2: 117.13\n",
      "Epoch [711], train_loss: 737.68 with loss1: 620.71 and loss2: 116.97\n",
      "Epoch [712], train_loss: 744.48 with loss1: 626.77 and loss2: 117.71\n",
      "Epoch [713], train_loss: 744.38 with loss1: 627.67 and loss2: 116.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [714], train_loss: 752.02 with loss1: 635.11 and loss2: 116.91\n",
      "Epoch [715], train_loss: 749.67 with loss1: 632.93 and loss2: 116.74\n",
      "Epoch [716], train_loss: 758.55 with loss1: 641.22 and loss2: 117.33\n",
      "Epoch [717], train_loss: 751.90 with loss1: 635.17 and loss2: 116.74\n",
      "Epoch [718], train_loss: 761.30 with loss1: 644.19 and loss2: 117.11\n",
      "Epoch [719], train_loss: 757.95 with loss1: 641.11 and loss2: 116.84\n",
      "Epoch [720], train_loss: 760.65 with loss1: 644.00 and loss2: 116.65\n",
      "Epoch [721], train_loss: 756.28 with loss1: 639.76 and loss2: 116.52\n",
      "Epoch [722], train_loss: 763.95 with loss1: 647.12 and loss2: 116.83\n",
      "Epoch [723], train_loss: 758.13 with loss1: 641.81 and loss2: 116.33\n",
      "Epoch [724], train_loss: 763.32 with loss1: 646.86 and loss2: 116.47\n",
      "Epoch [725], train_loss: 758.27 with loss1: 642.12 and loss2: 116.15\n",
      "Epoch [726], train_loss: 770.10 with loss1: 653.36 and loss2: 116.74\n",
      "Epoch [727], train_loss: 762.52 with loss1: 646.24 and loss2: 116.28\n",
      "Epoch [728], train_loss: 770.89 with loss1: 654.53 and loss2: 116.36\n",
      "Epoch [729], train_loss: 767.33 with loss1: 651.41 and loss2: 115.92\n",
      "Epoch [730], train_loss: 772.59 with loss1: 656.20 and loss2: 116.39\n",
      "Epoch [731], train_loss: 768.26 with loss1: 652.26 and loss2: 116.00\n",
      "Epoch [732], train_loss: 775.63 with loss1: 659.38 and loss2: 116.24\n",
      "Epoch [733], train_loss: 770.38 with loss1: 654.42 and loss2: 115.96\n",
      "Epoch [734], train_loss: 780.40 with loss1: 663.88 and loss2: 116.52\n",
      "Epoch [735], train_loss: 774.18 with loss1: 658.20 and loss2: 115.98\n",
      "Epoch [736], train_loss: 782.79 with loss1: 666.58 and loss2: 116.22\n",
      "Epoch [737], train_loss: 775.42 with loss1: 659.85 and loss2: 115.57\n",
      "Epoch [738], train_loss: 784.91 with loss1: 668.55 and loss2: 116.36\n",
      "Epoch [739], train_loss: 779.25 with loss1: 663.25 and loss2: 116.00\n",
      "Epoch [740], train_loss: 787.39 with loss1: 670.92 and loss2: 116.47\n",
      "Epoch [741], train_loss: 781.01 with loss1: 665.33 and loss2: 115.68\n",
      "Epoch [742], train_loss: 787.66 with loss1: 672.09 and loss2: 115.57\n",
      "Epoch [743], train_loss: 780.90 with loss1: 665.36 and loss2: 115.54\n",
      "Epoch [744], train_loss: 791.59 with loss1: 675.61 and loss2: 115.98\n",
      "Epoch [745], train_loss: 782.32 with loss1: 667.01 and loss2: 115.31\n",
      "Epoch [746], train_loss: 791.46 with loss1: 675.72 and loss2: 115.74\n",
      "Epoch [747], train_loss: 781.73 with loss1: 666.65 and loss2: 115.08\n",
      "Epoch [748], train_loss: 794.47 with loss1: 678.62 and loss2: 115.85\n",
      "Epoch [749], train_loss: 783.52 with loss1: 668.94 and loss2: 114.58\n",
      "Epoch [750], train_loss: 797.17 with loss1: 682.02 and loss2: 115.15\n",
      "Epoch [751], train_loss: 793.75 with loss1: 678.83 and loss2: 114.91\n",
      "Epoch [752], train_loss: 804.83 with loss1: 689.60 and loss2: 115.23\n",
      "Epoch [753], train_loss: 794.65 with loss1: 679.68 and loss2: 114.96\n",
      "Epoch [754], train_loss: 804.70 with loss1: 689.63 and loss2: 115.07\n",
      "Epoch [755], train_loss: 795.52 with loss1: 680.21 and loss2: 115.31\n",
      "Epoch [756], train_loss: 806.80 with loss1: 691.57 and loss2: 115.23\n",
      "Epoch [757], train_loss: 795.06 with loss1: 680.56 and loss2: 114.50\n",
      "Epoch [758], train_loss: 802.56 with loss1: 687.71 and loss2: 114.86\n",
      "Epoch [759], train_loss: 791.96 with loss1: 676.59 and loss2: 115.37\n",
      "Epoch [760], train_loss: 797.74 with loss1: 682.72 and loss2: 115.01\n",
      "Epoch [761], train_loss: 787.76 with loss1: 672.74 and loss2: 115.02\n",
      "Epoch [762], train_loss: 793.16 with loss1: 677.49 and loss2: 115.67\n",
      "Epoch [763], train_loss: 773.55 with loss1: 659.17 and loss2: 114.38\n",
      "Epoch [764], train_loss: 775.00 with loss1: 660.01 and loss2: 114.99\n",
      "Epoch [765], train_loss: 763.62 with loss1: 649.16 and loss2: 114.46\n",
      "Epoch [766], train_loss: 768.84 with loss1: 653.12 and loss2: 115.72\n",
      "Epoch [767], train_loss: 753.32 with loss1: 638.08 and loss2: 115.23\n",
      "Epoch [768], train_loss: 752.55 with loss1: 638.10 and loss2: 114.45\n",
      "Epoch [769], train_loss: 742.00 with loss1: 627.83 and loss2: 114.17\n",
      "Epoch [770], train_loss: 742.70 with loss1: 627.44 and loss2: 115.25\n",
      "Epoch [771], train_loss: 732.63 with loss1: 618.24 and loss2: 114.39\n",
      "Epoch [772], train_loss: 731.40 with loss1: 617.23 and loss2: 114.17\n",
      "Epoch [773], train_loss: 723.15 with loss1: 608.72 and loss2: 114.42\n",
      "Epoch [774], train_loss: 721.30 with loss1: 607.24 and loss2: 114.06\n",
      "Epoch [775], train_loss: 713.50 with loss1: 599.84 and loss2: 113.66\n",
      "Epoch [776], train_loss: 715.35 with loss1: 601.31 and loss2: 114.03\n",
      "Epoch [777], train_loss: 710.17 with loss1: 596.15 and loss2: 114.02\n",
      "Epoch [778], train_loss: 711.80 with loss1: 597.91 and loss2: 113.89\n",
      "Epoch [779], train_loss: 705.80 with loss1: 591.85 and loss2: 113.95\n",
      "Epoch [780], train_loss: 707.87 with loss1: 593.77 and loss2: 114.10\n",
      "Epoch [781], train_loss: 704.63 with loss1: 591.09 and loss2: 113.54\n",
      "Epoch [782], train_loss: 703.94 with loss1: 589.78 and loss2: 114.17\n",
      "Epoch [783], train_loss: 699.89 with loss1: 586.47 and loss2: 113.43\n",
      "Epoch [784], train_loss: 702.33 with loss1: 588.54 and loss2: 113.79\n",
      "Epoch [785], train_loss: 697.27 with loss1: 583.44 and loss2: 113.83\n",
      "Epoch [786], train_loss: 699.30 with loss1: 585.24 and loss2: 114.06\n",
      "Epoch [787], train_loss: 696.95 with loss1: 583.63 and loss2: 113.31\n",
      "Epoch [788], train_loss: 699.23 with loss1: 585.31 and loss2: 113.91\n",
      "Epoch [789], train_loss: 694.24 with loss1: 581.43 and loss2: 112.81\n",
      "Epoch [790], train_loss: 701.35 with loss1: 587.86 and loss2: 113.49\n",
      "Epoch [791], train_loss: 698.92 with loss1: 585.61 and loss2: 113.31\n",
      "Epoch [792], train_loss: 700.15 with loss1: 586.31 and loss2: 113.83\n",
      "Epoch [793], train_loss: 698.48 with loss1: 585.42 and loss2: 113.06\n",
      "Epoch [794], train_loss: 701.45 with loss1: 588.06 and loss2: 113.39\n",
      "Epoch [795], train_loss: 699.19 with loss1: 585.71 and loss2: 113.48\n",
      "Epoch [796], train_loss: 701.90 with loss1: 588.18 and loss2: 113.72\n",
      "Epoch [797], train_loss: 702.72 with loss1: 589.81 and loss2: 112.90\n",
      "Epoch [798], train_loss: 704.94 with loss1: 591.76 and loss2: 113.18\n",
      "Epoch [799], train_loss: 704.69 with loss1: 591.94 and loss2: 112.75\n",
      "Epoch [800], train_loss: 707.76 with loss1: 594.72 and loss2: 113.04\n",
      "Epoch [801], train_loss: 704.39 with loss1: 591.55 and loss2: 112.84\n",
      "Epoch [802], train_loss: 709.97 with loss1: 597.10 and loss2: 112.86\n",
      "Epoch [803], train_loss: 712.06 with loss1: 599.36 and loss2: 112.70\n",
      "Epoch [804], train_loss: 717.99 with loss1: 605.07 and loss2: 112.92\n",
      "Epoch [805], train_loss: 711.37 with loss1: 599.11 and loss2: 112.26\n",
      "Epoch [806], train_loss: 717.00 with loss1: 604.08 and loss2: 112.91\n",
      "Epoch [807], train_loss: 716.51 with loss1: 604.07 and loss2: 112.44\n",
      "Epoch [808], train_loss: 722.91 with loss1: 610.20 and loss2: 112.71\n",
      "Epoch [809], train_loss: 721.61 with loss1: 608.97 and loss2: 112.64\n",
      "Epoch [810], train_loss: 725.89 with loss1: 612.77 and loss2: 113.12\n",
      "Epoch [811], train_loss: 723.81 with loss1: 611.72 and loss2: 112.09\n",
      "Epoch [812], train_loss: 731.96 with loss1: 619.05 and loss2: 112.91\n",
      "Epoch [813], train_loss: 730.11 with loss1: 617.93 and loss2: 112.18\n",
      "Epoch [814], train_loss: 734.80 with loss1: 622.17 and loss2: 112.63\n",
      "Epoch [815], train_loss: 733.26 with loss1: 621.10 and loss2: 112.16\n",
      "Epoch [816], train_loss: 739.74 with loss1: 627.27 and loss2: 112.47\n",
      "Epoch [817], train_loss: 731.32 with loss1: 619.10 and loss2: 112.22\n",
      "Epoch [818], train_loss: 738.02 with loss1: 625.37 and loss2: 112.65\n",
      "Epoch [819], train_loss: 733.44 with loss1: 621.48 and loss2: 111.95\n",
      "Epoch [820], train_loss: 739.30 with loss1: 626.88 and loss2: 112.43\n",
      "Epoch [821], train_loss: 734.97 with loss1: 622.99 and loss2: 111.98\n",
      "Epoch [822], train_loss: 740.61 with loss1: 628.36 and loss2: 112.25\n",
      "Epoch [823], train_loss: 732.17 with loss1: 620.61 and loss2: 111.55\n",
      "Epoch [824], train_loss: 736.64 with loss1: 624.50 and loss2: 112.14\n",
      "Epoch [825], train_loss: 731.06 with loss1: 619.48 and loss2: 111.58\n",
      "Epoch [826], train_loss: 736.34 with loss1: 623.78 and loss2: 112.56\n",
      "Epoch [827], train_loss: 731.67 with loss1: 619.88 and loss2: 111.79\n",
      "Epoch [828], train_loss: 735.15 with loss1: 622.83 and loss2: 112.32\n",
      "Epoch [829], train_loss: 729.95 with loss1: 618.44 and loss2: 111.51\n",
      "Epoch [830], train_loss: 735.42 with loss1: 623.69 and loss2: 111.74\n",
      "Epoch [831], train_loss: 728.26 with loss1: 616.77 and loss2: 111.48\n",
      "Epoch [832], train_loss: 732.34 with loss1: 620.63 and loss2: 111.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [833], train_loss: 726.16 with loss1: 614.97 and loss2: 111.19\n",
      "Epoch [834], train_loss: 732.37 with loss1: 620.23 and loss2: 112.13\n",
      "Epoch [835], train_loss: 726.77 with loss1: 615.31 and loss2: 111.46\n",
      "Epoch [836], train_loss: 731.25 with loss1: 619.75 and loss2: 111.50\n",
      "Epoch [837], train_loss: 726.21 with loss1: 615.31 and loss2: 110.90\n",
      "Epoch [838], train_loss: 731.25 with loss1: 619.72 and loss2: 111.53\n",
      "Epoch [839], train_loss: 728.45 with loss1: 617.59 and loss2: 110.86\n",
      "Epoch [840], train_loss: 727.13 with loss1: 616.02 and loss2: 111.11\n",
      "Epoch [841], train_loss: 724.13 with loss1: 613.09 and loss2: 111.04\n",
      "Epoch [842], train_loss: 726.05 with loss1: 614.79 and loss2: 111.26\n",
      "Epoch [843], train_loss: 724.15 with loss1: 613.51 and loss2: 110.64\n",
      "Epoch [844], train_loss: 723.33 with loss1: 612.17 and loss2: 111.16\n",
      "Epoch [845], train_loss: 718.80 with loss1: 608.27 and loss2: 110.53\n",
      "Epoch [846], train_loss: 723.73 with loss1: 612.71 and loss2: 111.02\n",
      "Epoch [847], train_loss: 716.88 with loss1: 606.56 and loss2: 110.32\n",
      "Epoch [848], train_loss: 720.44 with loss1: 609.40 and loss2: 111.04\n",
      "Epoch [849], train_loss: 715.58 with loss1: 604.35 and loss2: 111.23\n",
      "Epoch [850], train_loss: 721.41 with loss1: 610.42 and loss2: 110.99\n",
      "Epoch [851], train_loss: 715.53 with loss1: 604.91 and loss2: 110.62\n",
      "Epoch [852], train_loss: 716.23 with loss1: 605.32 and loss2: 110.91\n",
      "Epoch [853], train_loss: 714.73 with loss1: 604.48 and loss2: 110.26\n",
      "Epoch [854], train_loss: 717.38 with loss1: 606.63 and loss2: 110.74\n",
      "Epoch [855], train_loss: 712.30 with loss1: 601.39 and loss2: 110.91\n",
      "Epoch [856], train_loss: 715.92 with loss1: 605.23 and loss2: 110.68\n",
      "Epoch [857], train_loss: 710.12 with loss1: 599.62 and loss2: 110.50\n",
      "Epoch [858], train_loss: 715.77 with loss1: 605.35 and loss2: 110.42\n",
      "Epoch [859], train_loss: 711.47 with loss1: 601.29 and loss2: 110.18\n",
      "Epoch [860], train_loss: 713.08 with loss1: 602.37 and loss2: 110.71\n",
      "Epoch [861], train_loss: 709.63 with loss1: 599.71 and loss2: 109.92\n",
      "Epoch [862], train_loss: 715.16 with loss1: 604.55 and loss2: 110.61\n",
      "Epoch [863], train_loss: 710.00 with loss1: 599.68 and loss2: 110.32\n",
      "Epoch [864], train_loss: 713.58 with loss1: 603.13 and loss2: 110.45\n",
      "Epoch [865], train_loss: 708.24 with loss1: 598.23 and loss2: 110.01\n",
      "Epoch [866], train_loss: 714.34 with loss1: 604.30 and loss2: 110.04\n",
      "Epoch [867], train_loss: 711.85 with loss1: 601.94 and loss2: 109.90\n",
      "Epoch [868], train_loss: 713.33 with loss1: 603.21 and loss2: 110.11\n",
      "Epoch [869], train_loss: 714.66 with loss1: 604.84 and loss2: 109.81\n",
      "Epoch [870], train_loss: 714.04 with loss1: 603.78 and loss2: 110.26\n",
      "Epoch [871], train_loss: 711.58 with loss1: 602.35 and loss2: 109.23\n",
      "Epoch [872], train_loss: 713.56 with loss1: 603.62 and loss2: 109.94\n",
      "Epoch [873], train_loss: 711.49 with loss1: 602.01 and loss2: 109.48\n",
      "Epoch [874], train_loss: 714.00 with loss1: 603.77 and loss2: 110.23\n",
      "Epoch [875], train_loss: 708.66 with loss1: 598.94 and loss2: 109.72\n",
      "Epoch [876], train_loss: 714.71 with loss1: 604.85 and loss2: 109.86\n",
      "Epoch [877], train_loss: 710.74 with loss1: 601.31 and loss2: 109.42\n",
      "Epoch [878], train_loss: 712.94 with loss1: 603.25 and loss2: 109.69\n",
      "Epoch [879], train_loss: 710.03 with loss1: 600.48 and loss2: 109.55\n",
      "Epoch [880], train_loss: 717.47 with loss1: 607.81 and loss2: 109.66\n",
      "Epoch [881], train_loss: 710.96 with loss1: 601.60 and loss2: 109.37\n",
      "Epoch [882], train_loss: 712.78 with loss1: 603.09 and loss2: 109.69\n",
      "Epoch [883], train_loss: 709.14 with loss1: 599.82 and loss2: 109.32\n",
      "Epoch [884], train_loss: 712.03 with loss1: 602.29 and loss2: 109.74\n",
      "Epoch [885], train_loss: 707.55 with loss1: 598.44 and loss2: 109.11\n",
      "Epoch [886], train_loss: 715.49 with loss1: 606.08 and loss2: 109.41\n",
      "Epoch [887], train_loss: 710.42 with loss1: 601.49 and loss2: 108.94\n",
      "Epoch [888], train_loss: 713.63 with loss1: 604.30 and loss2: 109.33\n",
      "Epoch [889], train_loss: 709.39 with loss1: 600.28 and loss2: 109.11\n",
      "Epoch [890], train_loss: 713.41 with loss1: 603.63 and loss2: 109.78\n",
      "Epoch [891], train_loss: 707.83 with loss1: 598.90 and loss2: 108.93\n",
      "Epoch [892], train_loss: 712.64 with loss1: 603.51 and loss2: 109.13\n",
      "Epoch [893], train_loss: 709.76 with loss1: 600.78 and loss2: 108.98\n",
      "Epoch [894], train_loss: 708.93 with loss1: 599.73 and loss2: 109.20\n",
      "Epoch [895], train_loss: 705.14 with loss1: 596.61 and loss2: 108.53\n",
      "Epoch [896], train_loss: 706.34 with loss1: 597.11 and loss2: 109.23\n",
      "Epoch [897], train_loss: 705.35 with loss1: 596.72 and loss2: 108.63\n",
      "Epoch [898], train_loss: 705.93 with loss1: 596.52 and loss2: 109.41\n",
      "Epoch [899], train_loss: 701.73 with loss1: 593.17 and loss2: 108.56\n",
      "Epoch [900], train_loss: 704.41 with loss1: 595.42 and loss2: 108.98\n",
      "Epoch [901], train_loss: 701.51 with loss1: 593.09 and loss2: 108.42\n",
      "Epoch [902], train_loss: 703.37 with loss1: 594.38 and loss2: 108.99\n",
      "Epoch [903], train_loss: 703.29 with loss1: 594.74 and loss2: 108.55\n",
      "Epoch [904], train_loss: 706.12 with loss1: 597.38 and loss2: 108.74\n",
      "Epoch [905], train_loss: 697.85 with loss1: 589.56 and loss2: 108.29\n",
      "Epoch [906], train_loss: 702.59 with loss1: 593.90 and loss2: 108.68\n",
      "Epoch [907], train_loss: 698.86 with loss1: 590.80 and loss2: 108.06\n",
      "Epoch [908], train_loss: 704.45 with loss1: 595.70 and loss2: 108.75\n",
      "Epoch [909], train_loss: 697.74 with loss1: 589.45 and loss2: 108.29\n",
      "Epoch [910], train_loss: 702.53 with loss1: 594.17 and loss2: 108.36\n",
      "Epoch [911], train_loss: 700.72 with loss1: 592.89 and loss2: 107.82\n",
      "Epoch [912], train_loss: 703.75 with loss1: 595.59 and loss2: 108.16\n",
      "Epoch [913], train_loss: 706.04 with loss1: 598.43 and loss2: 107.61\n",
      "Epoch [914], train_loss: 708.07 with loss1: 599.70 and loss2: 108.37\n",
      "Epoch [915], train_loss: 704.48 with loss1: 596.83 and loss2: 107.64\n",
      "Epoch [916], train_loss: 708.95 with loss1: 600.98 and loss2: 107.97\n",
      "Epoch [917], train_loss: 707.69 with loss1: 599.97 and loss2: 107.72\n",
      "Epoch [918], train_loss: 713.79 with loss1: 605.51 and loss2: 108.28\n",
      "Epoch [919], train_loss: 711.41 with loss1: 603.47 and loss2: 107.94\n",
      "Epoch [920], train_loss: 719.58 with loss1: 611.50 and loss2: 108.08\n",
      "Epoch [921], train_loss: 716.68 with loss1: 608.53 and loss2: 108.15\n",
      "Epoch [922], train_loss: 725.84 with loss1: 617.40 and loss2: 108.44\n",
      "Epoch [923], train_loss: 720.07 with loss1: 612.73 and loss2: 107.35\n",
      "Epoch [924], train_loss: 729.34 with loss1: 621.22 and loss2: 108.12\n",
      "Epoch [925], train_loss: 722.29 with loss1: 614.72 and loss2: 107.57\n",
      "Epoch [926], train_loss: 733.74 with loss1: 625.63 and loss2: 108.11\n",
      "Epoch [927], train_loss: 722.43 with loss1: 614.84 and loss2: 107.59\n",
      "Epoch [928], train_loss: 726.45 with loss1: 618.62 and loss2: 107.83\n",
      "Epoch [929], train_loss: 720.47 with loss1: 612.86 and loss2: 107.61\n",
      "Epoch [930], train_loss: 726.26 with loss1: 618.44 and loss2: 107.82\n",
      "Epoch [931], train_loss: 713.95 with loss1: 606.83 and loss2: 107.12\n",
      "Epoch [932], train_loss: 715.22 with loss1: 607.52 and loss2: 107.70\n",
      "Epoch [933], train_loss: 705.26 with loss1: 597.80 and loss2: 107.46\n",
      "Epoch [934], train_loss: 703.69 with loss1: 596.49 and loss2: 107.21\n",
      "Epoch [935], train_loss: 701.20 with loss1: 593.76 and loss2: 107.44\n",
      "Epoch [936], train_loss: 695.59 with loss1: 588.44 and loss2: 107.15\n",
      "Epoch [937], train_loss: 687.02 with loss1: 580.01 and loss2: 107.02\n",
      "Epoch [938], train_loss: 687.42 with loss1: 580.04 and loss2: 107.37\n",
      "Epoch [939], train_loss: 682.64 with loss1: 575.73 and loss2: 106.90\n",
      "Epoch [940], train_loss: 680.30 with loss1: 572.52 and loss2: 107.78\n",
      "Epoch [941], train_loss: 672.78 with loss1: 565.77 and loss2: 107.01\n",
      "Epoch [942], train_loss: 674.19 with loss1: 567.01 and loss2: 107.18\n",
      "Epoch [943], train_loss: 670.27 with loss1: 563.08 and loss2: 107.19\n",
      "Epoch [944], train_loss: 674.14 with loss1: 566.82 and loss2: 107.32\n",
      "Epoch [945], train_loss: 666.20 with loss1: 559.16 and loss2: 107.04\n",
      "Epoch [946], train_loss: 664.60 with loss1: 557.73 and loss2: 106.87\n",
      "Epoch [947], train_loss: 660.38 with loss1: 553.54 and loss2: 106.84\n",
      "Epoch [948], train_loss: 663.16 with loss1: 556.11 and loss2: 107.05\n",
      "Epoch [949], train_loss: 659.83 with loss1: 553.01 and loss2: 106.82\n",
      "Epoch [950], train_loss: 658.16 with loss1: 551.28 and loss2: 106.88\n",
      "Epoch [951], train_loss: 657.29 with loss1: 551.02 and loss2: 106.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [952], train_loss: 658.32 with loss1: 551.28 and loss2: 107.04\n",
      "Epoch [953], train_loss: 657.23 with loss1: 551.00 and loss2: 106.22\n",
      "Epoch [954], train_loss: 656.11 with loss1: 549.52 and loss2: 106.59\n",
      "Epoch [955], train_loss: 656.38 with loss1: 549.91 and loss2: 106.46\n",
      "Epoch [956], train_loss: 653.43 with loss1: 547.07 and loss2: 106.36\n",
      "Epoch [957], train_loss: 653.44 with loss1: 547.17 and loss2: 106.27\n",
      "Epoch [958], train_loss: 657.37 with loss1: 551.05 and loss2: 106.32\n",
      "Epoch [959], train_loss: 654.81 with loss1: 548.32 and loss2: 106.49\n",
      "Epoch [960], train_loss: 656.79 with loss1: 550.26 and loss2: 106.53\n",
      "Epoch [961], train_loss: 658.03 with loss1: 551.63 and loss2: 106.39\n",
      "Epoch [962], train_loss: 662.57 with loss1: 555.75 and loss2: 106.82\n",
      "Epoch [963], train_loss: 660.28 with loss1: 554.33 and loss2: 105.95\n",
      "Epoch [964], train_loss: 661.69 with loss1: 555.07 and loss2: 106.62\n",
      "Epoch [965], train_loss: 664.44 with loss1: 558.34 and loss2: 106.10\n",
      "Epoch [966], train_loss: 665.23 with loss1: 558.85 and loss2: 106.37\n",
      "Epoch [967], train_loss: 667.97 with loss1: 562.60 and loss2: 105.37\n",
      "Epoch [968], train_loss: 669.19 with loss1: 563.18 and loss2: 106.02\n",
      "Epoch [969], train_loss: 670.35 with loss1: 564.77 and loss2: 105.58\n",
      "Epoch [970], train_loss: 675.08 with loss1: 568.89 and loss2: 106.19\n",
      "Epoch [971], train_loss: 678.13 with loss1: 572.40 and loss2: 105.73\n",
      "Epoch [972], train_loss: 679.72 with loss1: 573.37 and loss2: 106.35\n",
      "Epoch [973], train_loss: 682.63 with loss1: 577.22 and loss2: 105.41\n",
      "Epoch [974], train_loss: 683.20 with loss1: 577.28 and loss2: 105.92\n",
      "Epoch [975], train_loss: 685.05 with loss1: 579.24 and loss2: 105.81\n",
      "Epoch [976], train_loss: 686.60 with loss1: 580.67 and loss2: 105.92\n",
      "Epoch [977], train_loss: 688.15 with loss1: 582.24 and loss2: 105.92\n",
      "Epoch [978], train_loss: 689.54 with loss1: 583.73 and loss2: 105.81\n",
      "Epoch [979], train_loss: 689.72 with loss1: 584.29 and loss2: 105.43\n",
      "Epoch [980], train_loss: 693.14 with loss1: 587.53 and loss2: 105.61\n",
      "Epoch [981], train_loss: 696.59 with loss1: 590.76 and loss2: 105.84\n",
      "Epoch [982], train_loss: 696.51 with loss1: 590.61 and loss2: 105.89\n",
      "Epoch [983], train_loss: 697.11 with loss1: 591.90 and loss2: 105.21\n",
      "Epoch [984], train_loss: 700.29 with loss1: 594.68 and loss2: 105.60\n",
      "Epoch [985], train_loss: 700.41 with loss1: 594.81 and loss2: 105.60\n",
      "Epoch [986], train_loss: 702.20 with loss1: 596.06 and loss2: 106.14\n",
      "Epoch [987], train_loss: 699.04 with loss1: 594.05 and loss2: 104.99\n",
      "Epoch [988], train_loss: 701.57 with loss1: 595.98 and loss2: 105.60\n",
      "Epoch [989], train_loss: 700.85 with loss1: 595.18 and loss2: 105.67\n",
      "Epoch [990], train_loss: 698.72 with loss1: 593.19 and loss2: 105.53\n",
      "Epoch [991], train_loss: 696.86 with loss1: 591.61 and loss2: 105.25\n",
      "Epoch [992], train_loss: 695.90 with loss1: 590.29 and loss2: 105.61\n",
      "Epoch [993], train_loss: 694.65 with loss1: 589.77 and loss2: 104.89\n",
      "Epoch [994], train_loss: 697.04 with loss1: 591.46 and loss2: 105.58\n",
      "Epoch [995], train_loss: 691.85 with loss1: 587.08 and loss2: 104.78\n",
      "Epoch [996], train_loss: 691.39 with loss1: 585.74 and loss2: 105.64\n",
      "Epoch [997], train_loss: 688.60 with loss1: 583.59 and loss2: 105.01\n",
      "Epoch [998], train_loss: 686.40 with loss1: 581.14 and loss2: 105.26\n",
      "Epoch [999], train_loss: 683.63 with loss1: 578.73 and loss2: 104.91\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# model version 3.5\n",
    "# with loss2\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history = fit(epochs=1000, lr=5e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5f9cb588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 378940.03 with loss1: 373303.91, loss2: 625.83 and loss3: 5010.29\n",
      "Epoch [1], train_loss: 166990.52 with loss1: 161371.38, loss2: 608.90 and loss3: 5010.23\n",
      "Epoch [2], train_loss: 140083.45 with loss1: 134216.44, loss2: 856.84 and loss3: 5010.17\n",
      "Epoch [3], train_loss: 193912.08 with loss1: 187932.34, loss2: 969.63 and loss3: 5010.11\n",
      "Epoch [4], train_loss: 103601.93 with loss1: 97910.79, loss2: 681.09 and loss3: 5010.05\n",
      "Epoch [5], train_loss: 103475.23 with loss1: 97834.99, loss2: 630.25 and loss3: 5009.99\n",
      "Epoch [6], train_loss: 103376.92 with loss1: 97769.99, loss2: 597.00 and loss3: 5009.93\n",
      "Epoch [7], train_loss: 103323.93 with loss1: 97712.94, loss2: 601.13 and loss3: 5009.87\n",
      "Epoch [8], train_loss: 103266.63 with loss1: 97638.03, loss2: 618.79 and loss3: 5009.81\n",
      "Epoch [9], train_loss: 103173.91 with loss1: 97567.29, loss2: 596.87 and loss3: 5009.75\n",
      "Epoch [10], train_loss: 103069.88 with loss1: 97491.64, loss2: 568.54 and loss3: 5009.70\n",
      "Epoch [11], train_loss: 103032.62 with loss1: 97401.29, loss2: 621.69 and loss3: 5009.64\n",
      "Epoch [12], train_loss: 102879.59 with loss1: 97322.13, loss2: 547.88 and loss3: 5009.58\n",
      "Epoch [13], train_loss: 102779.62 with loss1: 97201.52, loss2: 568.59 and loss3: 5009.52\n",
      "Epoch [14], train_loss: 102631.51 with loss1: 97039.59, loss2: 582.45 and loss3: 5009.47\n",
      "Epoch [15], train_loss: 102407.87 with loss1: 96863.92, loss2: 534.54 and loss3: 5009.41\n",
      "Epoch [16], train_loss: 102214.04 with loss1: 96645.40, loss2: 559.29 and loss3: 5009.35\n",
      "Epoch [17], train_loss: 101893.07 with loss1: 96340.38, loss2: 543.40 and loss3: 5009.30\n",
      "Epoch [18], train_loss: 101493.81 with loss1: 95915.91, loss2: 568.66 and loss3: 5009.24\n",
      "Epoch [19], train_loss: 100912.07 with loss1: 95373.65, loss2: 529.24 and loss3: 5009.18\n",
      "Epoch [20], train_loss: 100038.40 with loss1: 94503.62, loss2: 525.65 and loss3: 5009.12\n",
      "Epoch [21], train_loss: 98581.16 with loss1: 93064.79, loss2: 507.30 and loss3: 5009.07\n",
      "Epoch [22], train_loss: 95883.81 with loss1: 90350.90, loss2: 523.90 and loss3: 5009.01\n",
      "Epoch [23], train_loss: 89415.82 with loss1: 83902.52, loss2: 504.34 and loss3: 5008.96\n",
      "Epoch [24], train_loss: 71598.06 with loss1: 66051.87, loss2: 537.30 and loss3: 5008.90\n",
      "Epoch [25], train_loss: 45238.49 with loss1: 39664.04, loss2: 565.61 and loss3: 5008.85\n",
      "Epoch [26], train_loss: 27975.80 with loss1: 22380.61, loss2: 586.40 and loss3: 5008.79\n",
      "Epoch [27], train_loss: 21826.46 with loss1: 16194.34, loss2: 623.38 and loss3: 5008.74\n",
      "Epoch [28], train_loss: 19037.23 with loss1: 13340.08, loss2: 688.47 and loss3: 5008.68\n",
      "Epoch [29], train_loss: 17701.66 with loss1: 11968.90, loss2: 724.14 and loss3: 5008.63\n",
      "Epoch [30], train_loss: 19499.09 with loss1: 13693.86, loss2: 796.66 and loss3: 5008.57\n",
      "Epoch [31], train_loss: 33826.62 with loss1: 27999.61, loss2: 818.49 and loss3: 5008.52\n",
      "Epoch [32], train_loss: 22674.07 with loss1: 16747.58, loss2: 918.02 and loss3: 5008.47\n",
      "Epoch [33], train_loss: 19130.00 with loss1: 13251.71, loss2: 869.88 and loss3: 5008.41\n",
      "Epoch [34], train_loss: 15831.57 with loss1: 9960.52, loss2: 862.69 and loss3: 5008.36\n",
      "Epoch [35], train_loss: 15103.47 with loss1: 9269.96, loss2: 825.21 and loss3: 5008.31\n",
      "Epoch [36], train_loss: 14549.61 with loss1: 8708.50, loss2: 832.86 and loss3: 5008.25\n",
      "Epoch [37], train_loss: 14186.62 with loss1: 8358.35, loss2: 820.07 and loss3: 5008.20\n",
      "Epoch [38], train_loss: 13878.92 with loss1: 8043.46, loss2: 827.32 and loss3: 5008.15\n",
      "Epoch [39], train_loss: 13619.39 with loss1: 7798.12, loss2: 813.18 and loss3: 5008.10\n",
      "Epoch [40], train_loss: 13397.43 with loss1: 7576.91, loss2: 812.48 and loss3: 5008.04\n",
      "Epoch [41], train_loss: 13248.99 with loss1: 7440.11, loss2: 800.89 and loss3: 5007.99\n",
      "Epoch [42], train_loss: 13226.04 with loss1: 7417.55, loss2: 800.56 and loss3: 5007.94\n",
      "Epoch [43], train_loss: 13321.07 with loss1: 7516.06, loss2: 797.12 and loss3: 5007.89\n",
      "Epoch [44], train_loss: 13696.98 with loss1: 7883.94, loss2: 805.20 and loss3: 5007.83\n",
      "Epoch [45], train_loss: 14671.40 with loss1: 8866.39, loss2: 797.23 and loss3: 5007.78\n",
      "Epoch [46], train_loss: 14768.94 with loss1: 8958.31, loss2: 802.90 and loss3: 5007.73\n",
      "Epoch [47], train_loss: 15962.29 with loss1: 10183.25, loss2: 771.36 and loss3: 5007.68\n",
      "Epoch [48], train_loss: 13986.93 with loss1: 8188.93, loss2: 790.37 and loss3: 5007.62\n",
      "Epoch [49], train_loss: 13743.34 with loss1: 7975.19, loss2: 760.58 and loss3: 5007.57\n",
      "Epoch [50], train_loss: 12967.06 with loss1: 7189.59, loss2: 769.95 and loss3: 5007.52\n",
      "Epoch [51], train_loss: 12803.91 with loss1: 7056.21, loss2: 740.23 and loss3: 5007.47\n",
      "Epoch [52], train_loss: 12530.33 with loss1: 6779.28, loss2: 743.63 and loss3: 5007.42\n",
      "Epoch [53], train_loss: 12534.41 with loss1: 6795.61, loss2: 731.44 and loss3: 5007.37\n",
      "Epoch [54], train_loss: 12385.48 with loss1: 6653.95, loss2: 724.21 and loss3: 5007.31\n",
      "Epoch [55], train_loss: 12392.87 with loss1: 6679.69, loss2: 705.92 and loss3: 5007.26\n",
      "Epoch [56], train_loss: 12269.31 with loss1: 6567.46, loss2: 694.64 and loss3: 5007.21\n",
      "Epoch [57], train_loss: 12384.87 with loss1: 6685.26, loss2: 692.45 and loss3: 5007.16\n",
      "Epoch [58], train_loss: 12274.13 with loss1: 6571.00, loss2: 696.02 and loss3: 5007.11\n",
      "Epoch [59], train_loss: 12412.71 with loss1: 6726.80, loss2: 678.86 and loss3: 5007.05\n",
      "Epoch [60], train_loss: 12235.10 with loss1: 6535.13, loss2: 692.97 and loss3: 5007.00\n",
      "Epoch [61], train_loss: 12394.36 with loss1: 6716.85, loss2: 670.56 and loss3: 5006.95\n",
      "Epoch [62], train_loss: 12155.82 with loss1: 6473.88, loss2: 675.04 and loss3: 5006.90\n",
      "Epoch [63], train_loss: 12236.01 with loss1: 6578.42, loss2: 650.75 and loss3: 5006.85\n",
      "Epoch [64], train_loss: 12028.94 with loss1: 6362.09, loss2: 660.05 and loss3: 5006.79\n",
      "Epoch [65], train_loss: 12059.98 with loss1: 6403.03, loss2: 650.21 and loss3: 5006.74\n",
      "Epoch [66], train_loss: 11823.46 with loss1: 6167.04, loss2: 649.72 and loss3: 5006.69\n",
      "Epoch [67], train_loss: 11865.91 with loss1: 6221.86, loss2: 637.41 and loss3: 5006.64\n",
      "Epoch [68], train_loss: 11653.90 with loss1: 6004.16, loss2: 643.15 and loss3: 5006.59\n",
      "Epoch [69], train_loss: 11668.14 with loss1: 6036.80, loss2: 624.80 and loss3: 5006.54\n",
      "Epoch [70], train_loss: 11495.14 with loss1: 5865.40, loss2: 623.25 and loss3: 5006.49\n",
      "Epoch [71], train_loss: 11529.46 with loss1: 5919.23, loss2: 603.79 and loss3: 5006.43\n",
      "Epoch [72], train_loss: 11411.14 with loss1: 5788.24, loss2: 616.52 and loss3: 5006.38\n",
      "Epoch [73], train_loss: 11392.45 with loss1: 5791.15, loss2: 594.97 and loss3: 5006.33\n",
      "Epoch [74], train_loss: 11292.08 with loss1: 5692.93, loss2: 592.87 and loss3: 5006.28\n",
      "Epoch [75], train_loss: 11316.27 with loss1: 5722.62, loss2: 587.42 and loss3: 5006.23\n",
      "Epoch [76], train_loss: 11214.10 with loss1: 5626.73, loss2: 581.19 and loss3: 5006.18\n",
      "Epoch [77], train_loss: 11278.62 with loss1: 5700.23, loss2: 572.28 and loss3: 5006.12\n",
      "Epoch [78], train_loss: 11182.44 with loss1: 5600.04, loss2: 576.32 and loss3: 5006.07\n",
      "Epoch [79], train_loss: 11255.32 with loss1: 5681.40, loss2: 567.91 and loss3: 5006.02\n",
      "Epoch [80], train_loss: 11140.63 with loss1: 5571.99, loss2: 562.67 and loss3: 5005.97\n",
      "Epoch [81], train_loss: 11159.88 with loss1: 5589.92, loss2: 564.04 and loss3: 5005.92\n",
      "Epoch [82], train_loss: 11025.68 with loss1: 5463.21, loss2: 556.61 and loss3: 5005.87\n",
      "Epoch [83], train_loss: 11070.04 with loss1: 5509.62, loss2: 554.61 and loss3: 5005.82\n",
      "Epoch [84], train_loss: 10951.25 with loss1: 5396.67, loss2: 548.81 and loss3: 5005.76\n",
      "Epoch [85], train_loss: 10984.21 with loss1: 5428.08, loss2: 550.42 and loss3: 5005.71\n",
      "Epoch [86], train_loss: 10837.96 with loss1: 5288.68, loss2: 543.62 and loss3: 5005.66\n",
      "Epoch [87], train_loss: 10913.13 with loss1: 5364.50, loss2: 543.02 and loss3: 5005.61\n",
      "Epoch [88], train_loss: 10737.61 with loss1: 5204.66, loss2: 527.39 and loss3: 5005.56\n",
      "Epoch [89], train_loss: 10779.30 with loss1: 5240.07, loss2: 533.73 and loss3: 5005.51\n",
      "Epoch [90], train_loss: 10663.83 with loss1: 5139.39, loss2: 518.99 and loss3: 5005.46\n",
      "Epoch [91], train_loss: 10739.64 with loss1: 5209.74, loss2: 524.50 and loss3: 5005.40\n",
      "Epoch [92], train_loss: 10610.31 with loss1: 5088.86, loss2: 516.09 and loss3: 5005.35\n",
      "Epoch [93], train_loss: 10643.70 with loss1: 5127.23, loss2: 511.17 and loss3: 5005.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94], train_loss: 10520.30 with loss1: 5003.04, loss2: 512.02 and loss3: 5005.25\n",
      "Epoch [95], train_loss: 10585.89 with loss1: 5070.79, loss2: 509.90 and loss3: 5005.20\n",
      "Epoch [96], train_loss: 10446.88 with loss1: 4938.68, loss2: 503.05 and loss3: 5005.15\n",
      "Epoch [97], train_loss: 10528.93 with loss1: 5018.45, loss2: 505.38 and loss3: 5005.10\n",
      "Epoch [98], train_loss: 10409.98 with loss1: 4907.67, loss2: 497.27 and loss3: 5005.04\n",
      "Epoch [99], train_loss: 10449.02 with loss1: 4941.46, loss2: 502.57 and loss3: 5004.99\n",
      "Epoch [100], train_loss: 10326.02 with loss1: 4835.09, loss2: 485.99 and loss3: 5004.94\n",
      "Epoch [101], train_loss: 10385.46 with loss1: 4877.93, loss2: 502.63 and loss3: 5004.89\n",
      "Epoch [102], train_loss: 10246.02 with loss1: 4768.95, loss2: 472.23 and loss3: 5004.84\n",
      "Epoch [103], train_loss: 10293.60 with loss1: 4793.87, loss2: 494.95 and loss3: 5004.79\n",
      "Epoch [104], train_loss: 10164.42 with loss1: 4689.90, loss2: 469.78 and loss3: 5004.74\n",
      "Epoch [105], train_loss: 10231.76 with loss1: 4741.44, loss2: 485.64 and loss3: 5004.68\n",
      "Epoch [106], train_loss: 10112.15 with loss1: 4644.58, loss2: 462.94 and loss3: 5004.63\n",
      "Epoch [107], train_loss: 10147.04 with loss1: 4665.19, loss2: 477.27 and loss3: 5004.58\n",
      "Epoch [108], train_loss: 10044.09 with loss1: 4566.18, loss2: 473.38 and loss3: 5004.53\n",
      "Epoch [109], train_loss: 10064.36 with loss1: 4596.18, loss2: 463.70 and loss3: 5004.48\n",
      "Epoch [110], train_loss: 9982.12 with loss1: 4511.11, loss2: 466.59 and loss3: 5004.43\n",
      "Epoch [111], train_loss: 10010.41 with loss1: 4547.28, loss2: 458.75 and loss3: 5004.38\n",
      "Epoch [112], train_loss: 9907.49 with loss1: 4445.05, loss2: 458.12 and loss3: 5004.33\n",
      "Epoch [113], train_loss: 10001.28 with loss1: 4537.51, loss2: 459.49 and loss3: 5004.27\n",
      "Epoch [114], train_loss: 9909.96 with loss1: 4455.55, loss2: 450.20 and loss3: 5004.22\n",
      "Epoch [115], train_loss: 9972.03 with loss1: 4504.25, loss2: 463.61 and loss3: 5004.17\n",
      "Epoch [116], train_loss: 9840.81 with loss1: 4392.62, loss2: 444.07 and loss3: 5004.12\n",
      "Epoch [117], train_loss: 9945.14 with loss1: 4476.01, loss2: 465.06 and loss3: 5004.07\n",
      "Epoch [118], train_loss: 9789.59 with loss1: 4347.88, loss2: 437.69 and loss3: 5004.02\n",
      "Epoch [119], train_loss: 9850.82 with loss1: 4382.19, loss2: 464.66 and loss3: 5003.97\n",
      "Epoch [120], train_loss: 9688.97 with loss1: 4253.50, loss2: 431.55 and loss3: 5003.91\n",
      "Epoch [121], train_loss: 9691.51 with loss1: 4235.02, loss2: 452.63 and loss3: 5003.86\n",
      "Epoch [122], train_loss: 9540.88 with loss1: 4103.51, loss2: 433.56 and loss3: 5003.81\n",
      "Epoch [123], train_loss: 9556.54 with loss1: 4111.74, loss2: 441.04 and loss3: 5003.76\n",
      "Epoch [124], train_loss: 9474.49 with loss1: 4038.78, loss2: 432.00 and loss3: 5003.71\n",
      "Epoch [125], train_loss: 9467.21 with loss1: 4029.22, loss2: 434.33 and loss3: 5003.66\n",
      "Epoch [126], train_loss: 9407.90 with loss1: 3980.03, loss2: 424.27 and loss3: 5003.61\n",
      "Epoch [127], train_loss: 9442.94 with loss1: 4008.06, loss2: 431.32 and loss3: 5003.56\n",
      "Epoch [128], train_loss: 9376.10 with loss1: 3945.82, loss2: 426.77 and loss3: 5003.50\n",
      "Epoch [129], train_loss: 9436.50 with loss1: 4010.06, loss2: 422.99 and loss3: 5003.45\n",
      "Epoch [130], train_loss: 9380.31 with loss1: 3953.71, loss2: 423.19 and loss3: 5003.40\n",
      "Epoch [131], train_loss: 9461.14 with loss1: 4033.79, loss2: 424.00 and loss3: 5003.35\n",
      "Epoch [132], train_loss: 9374.46 with loss1: 3945.07, loss2: 426.09 and loss3: 5003.30\n",
      "Epoch [133], train_loss: 9431.18 with loss1: 4016.04, loss2: 411.89 and loss3: 5003.25\n",
      "Epoch [134], train_loss: 9325.70 with loss1: 3898.64, loss2: 423.86 and loss3: 5003.20\n",
      "Epoch [135], train_loss: 9361.13 with loss1: 3934.13, loss2: 423.86 and loss3: 5003.15\n",
      "Epoch [136], train_loss: 9264.51 with loss1: 3840.12, loss2: 421.30 and loss3: 5003.09\n",
      "Epoch [137], train_loss: 9292.29 with loss1: 3864.28, loss2: 424.97 and loss3: 5003.04\n",
      "Epoch [138], train_loss: 9194.16 with loss1: 3776.23, loss2: 414.94 and loss3: 5002.99\n",
      "Epoch [139], train_loss: 9248.61 with loss1: 3818.19, loss2: 427.48 and loss3: 5002.94\n",
      "Epoch [140], train_loss: 9146.96 with loss1: 3731.61, loss2: 412.46 and loss3: 5002.89\n",
      "Epoch [141], train_loss: 9179.77 with loss1: 3755.46, loss2: 421.47 and loss3: 5002.84\n",
      "Epoch [142], train_loss: 9094.00 with loss1: 3685.80, loss2: 405.41 and loss3: 5002.79\n",
      "Epoch [143], train_loss: 9111.43 with loss1: 3680.19, loss2: 428.51 and loss3: 5002.74\n",
      "Epoch [144], train_loss: 9006.27 with loss1: 3603.93, loss2: 399.66 and loss3: 5002.69\n",
      "Epoch [145], train_loss: 9019.39 with loss1: 3603.31, loss2: 413.45 and loss3: 5002.63\n",
      "Epoch [146], train_loss: 8936.14 with loss1: 3525.03, loss2: 408.53 and loss3: 5002.58\n",
      "Epoch [147], train_loss: 8913.97 with loss1: 3500.65, loss2: 410.78 and loss3: 5002.53\n",
      "Epoch [148], train_loss: 8837.79 with loss1: 3427.50, loss2: 407.81 and loss3: 5002.48\n",
      "Epoch [149], train_loss: 8838.47 with loss1: 3427.93, loss2: 408.10 and loss3: 5002.43\n",
      "Epoch [150], train_loss: 8792.43 with loss1: 3383.69, loss2: 406.36 and loss3: 5002.38\n",
      "Epoch [151], train_loss: 8776.48 with loss1: 3369.70, loss2: 404.45 and loss3: 5002.33\n",
      "Epoch [152], train_loss: 8743.95 with loss1: 3339.17, loss2: 402.51 and loss3: 5002.28\n",
      "Epoch [153], train_loss: 8761.27 with loss1: 3355.21, loss2: 403.83 and loss3: 5002.23\n",
      "Epoch [154], train_loss: 8725.80 with loss1: 3317.05, loss2: 406.57 and loss3: 5002.17\n",
      "Epoch [155], train_loss: 8766.14 with loss1: 3364.27, loss2: 399.75 and loss3: 5002.12\n",
      "Epoch [156], train_loss: 8738.82 with loss1: 3332.18, loss2: 404.57 and loss3: 5002.07\n",
      "Epoch [157], train_loss: 8770.34 with loss1: 3370.27, loss2: 398.05 and loss3: 5002.02\n",
      "Epoch [158], train_loss: 8759.51 with loss1: 3353.51, loss2: 404.03 and loss3: 5001.97\n",
      "Epoch [159], train_loss: 8798.41 with loss1: 3400.08, loss2: 396.40 and loss3: 5001.92\n",
      "Epoch [160], train_loss: 8753.31 with loss1: 3350.39, loss2: 401.05 and loss3: 5001.87\n",
      "Epoch [161], train_loss: 8798.06 with loss1: 3397.89, loss2: 398.36 and loss3: 5001.82\n",
      "Epoch [162], train_loss: 8742.26 with loss1: 3339.43, loss2: 401.06 and loss3: 5001.77\n",
      "Epoch [163], train_loss: 8788.82 with loss1: 3392.33, loss2: 394.78 and loss3: 5001.72\n",
      "Epoch [164], train_loss: 8708.36 with loss1: 3307.46, loss2: 399.23 and loss3: 5001.66\n",
      "Epoch [165], train_loss: 8773.52 with loss1: 3377.41, loss2: 394.50 and loss3: 5001.61\n",
      "Epoch [166], train_loss: 8694.08 with loss1: 3291.06, loss2: 401.45 and loss3: 5001.56\n",
      "Epoch [167], train_loss: 8713.23 with loss1: 3319.72, loss2: 392.00 and loss3: 5001.51\n",
      "Epoch [168], train_loss: 8622.76 with loss1: 3224.95, loss2: 396.35 and loss3: 5001.46\n",
      "Epoch [169], train_loss: 8636.51 with loss1: 3239.62, loss2: 395.48 and loss3: 5001.41\n",
      "Epoch [170], train_loss: 8562.08 with loss1: 3166.25, loss2: 394.47 and loss3: 5001.36\n",
      "Epoch [171], train_loss: 8567.01 with loss1: 3170.08, loss2: 395.62 and loss3: 5001.31\n",
      "Epoch [172], train_loss: 8490.90 with loss1: 3099.39, loss2: 390.26 and loss3: 5001.26\n",
      "Epoch [173], train_loss: 8514.44 with loss1: 3119.06, loss2: 394.18 and loss3: 5001.21\n",
      "Epoch [174], train_loss: 8447.99 with loss1: 3058.62, loss2: 388.21 and loss3: 5001.15\n",
      "Epoch [175], train_loss: 8448.78 with loss1: 3055.09, loss2: 392.58 and loss3: 5001.10\n",
      "Epoch [176], train_loss: 8398.63 with loss1: 3011.11, loss2: 386.47 and loss3: 5001.05\n",
      "Epoch [177], train_loss: 8426.02 with loss1: 3039.25, loss2: 385.77 and loss3: 5001.00\n",
      "Epoch [178], train_loss: 8387.65 with loss1: 3002.25, loss2: 384.45 and loss3: 5000.95\n",
      "Epoch [179], train_loss: 8400.92 with loss1: 3015.49, loss2: 384.53 and loss3: 5000.90\n",
      "Epoch [180], train_loss: 8354.89 with loss1: 2972.80, loss2: 381.24 and loss3: 5000.85\n",
      "Epoch [181], train_loss: 8367.13 with loss1: 2983.43, loss2: 382.90 and loss3: 5000.80\n",
      "Epoch [182], train_loss: 8327.21 with loss1: 2948.10, loss2: 378.36 and loss3: 5000.75\n",
      "Epoch [183], train_loss: 8338.25 with loss1: 2958.23, loss2: 379.33 and loss3: 5000.70\n",
      "Epoch [184], train_loss: 8298.37 with loss1: 2919.24, loss2: 378.49 and loss3: 5000.65\n",
      "Epoch [185], train_loss: 8326.07 with loss1: 2949.25, loss2: 376.23 and loss3: 5000.59\n",
      "Epoch [186], train_loss: 8305.42 with loss1: 2927.25, loss2: 377.63 and loss3: 5000.54\n",
      "Epoch [187], train_loss: 8338.75 with loss1: 2958.58, loss2: 379.68 and loss3: 5000.49\n",
      "Epoch [188], train_loss: 8310.22 with loss1: 2931.91, loss2: 377.86 and loss3: 5000.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [189], train_loss: 8317.73 with loss1: 2938.89, loss2: 378.45 and loss3: 5000.39\n",
      "Epoch [190], train_loss: 8278.22 with loss1: 2904.47, loss2: 373.41 and loss3: 5000.34\n",
      "Epoch [191], train_loss: 8302.88 with loss1: 2923.53, loss2: 379.06 and loss3: 5000.29\n",
      "Epoch [192], train_loss: 8260.01 with loss1: 2885.31, loss2: 374.46 and loss3: 5000.24\n",
      "Epoch [193], train_loss: 8308.82 with loss1: 2931.21, loss2: 377.43 and loss3: 5000.19\n",
      "Epoch [194], train_loss: 8264.04 with loss1: 2890.74, loss2: 373.16 and loss3: 5000.14\n",
      "Epoch [195], train_loss: 8274.94 with loss1: 2905.11, loss2: 369.74 and loss3: 5000.09\n",
      "Epoch [196], train_loss: 8232.48 with loss1: 2859.76, loss2: 372.68 and loss3: 5000.04\n",
      "Epoch [197], train_loss: 8258.12 with loss1: 2892.01, loss2: 366.13 and loss3: 4999.98\n",
      "Epoch [198], train_loss: 8201.91 with loss1: 2831.90, loss2: 370.08 and loss3: 4999.93\n",
      "Epoch [199], train_loss: 8227.79 with loss1: 2857.29, loss2: 370.62 and loss3: 4999.88\n",
      "Epoch [200], train_loss: 8177.63 with loss1: 2812.05, loss2: 365.75 and loss3: 4999.83\n",
      "Epoch [201], train_loss: 8196.56 with loss1: 2826.99, loss2: 369.79 and loss3: 4999.78\n",
      "Epoch [202], train_loss: 8150.47 with loss1: 2784.94, loss2: 365.80 and loss3: 4999.73\n",
      "Epoch [203], train_loss: 8164.50 with loss1: 2798.22, loss2: 366.60 and loss3: 4999.68\n",
      "Epoch [204], train_loss: 8128.71 with loss1: 2763.37, loss2: 365.71 and loss3: 4999.63\n",
      "Epoch [205], train_loss: 8154.53 with loss1: 2789.16, loss2: 365.79 and loss3: 4999.58\n",
      "Epoch [206], train_loss: 8104.58 with loss1: 2741.08, loss2: 363.98 and loss3: 4999.53\n",
      "Epoch [207], train_loss: 8131.60 with loss1: 2770.77, loss2: 361.36 and loss3: 4999.48\n",
      "Epoch [208], train_loss: 8096.76 with loss1: 2733.83, loss2: 363.50 and loss3: 4999.43\n",
      "Epoch [209], train_loss: 8119.71 with loss1: 2762.45, loss2: 357.88 and loss3: 4999.38\n",
      "Epoch [210], train_loss: 8085.25 with loss1: 2725.50, loss2: 360.43 and loss3: 4999.32\n",
      "Epoch [211], train_loss: 8102.80 with loss1: 2742.69, loss2: 360.84 and loss3: 4999.27\n",
      "Epoch [212], train_loss: 8048.48 with loss1: 2690.18, loss2: 359.07 and loss3: 4999.22\n",
      "Epoch [213], train_loss: 8098.09 with loss1: 2741.37, loss2: 357.55 and loss3: 4999.17\n",
      "Epoch [214], train_loss: 8057.90 with loss1: 2702.43, loss2: 356.35 and loss3: 4999.12\n",
      "Epoch [215], train_loss: 8074.63 with loss1: 2722.12, loss2: 353.44 and loss3: 4999.07\n",
      "Epoch [216], train_loss: 8017.12 with loss1: 2664.40, loss2: 353.70 and loss3: 4999.02\n",
      "Epoch [217], train_loss: 8042.59 with loss1: 2687.56, loss2: 356.06 and loss3: 4998.97\n",
      "Epoch [218], train_loss: 7999.88 with loss1: 2648.09, loss2: 352.87 and loss3: 4998.92\n",
      "Epoch [219], train_loss: 8020.82 with loss1: 2670.17, loss2: 351.78 and loss3: 4998.87\n",
      "Epoch [220], train_loss: 7973.22 with loss1: 2624.55, loss2: 349.85 and loss3: 4998.82\n",
      "Epoch [221], train_loss: 8004.61 with loss1: 2651.11, loss2: 354.74 and loss3: 4998.77\n",
      "Epoch [222], train_loss: 7959.62 with loss1: 2611.33, loss2: 349.58 and loss3: 4998.72\n",
      "Epoch [223], train_loss: 7980.94 with loss1: 2632.35, loss2: 349.92 and loss3: 4998.67\n",
      "Epoch [224], train_loss: 7952.56 with loss1: 2604.19, loss2: 349.76 and loss3: 4998.61\n",
      "Epoch [225], train_loss: 7977.15 with loss1: 2626.36, loss2: 352.23 and loss3: 4998.56\n",
      "Epoch [226], train_loss: 7931.13 with loss1: 2584.87, loss2: 347.75 and loss3: 4998.51\n",
      "Epoch [227], train_loss: 7946.62 with loss1: 2599.16, loss2: 349.01 and loss3: 4998.46\n",
      "Epoch [228], train_loss: 7908.26 with loss1: 2561.51, loss2: 348.34 and loss3: 4998.41\n",
      "Epoch [229], train_loss: 7941.22 with loss1: 2598.04, loss2: 344.81 and loss3: 4998.36\n",
      "Epoch [230], train_loss: 7902.99 with loss1: 2557.04, loss2: 347.64 and loss3: 4998.31\n",
      "Epoch [231], train_loss: 7924.51 with loss1: 2582.91, loss2: 343.34 and loss3: 4998.26\n",
      "Epoch [232], train_loss: 7900.22 with loss1: 2557.05, loss2: 344.96 and loss3: 4998.21\n",
      "Epoch [233], train_loss: 7909.08 with loss1: 2571.00, loss2: 339.92 and loss3: 4998.16\n",
      "Epoch [234], train_loss: 7873.78 with loss1: 2536.35, loss2: 339.32 and loss3: 4998.11\n",
      "Epoch [235], train_loss: 7899.72 with loss1: 2561.18, loss2: 340.48 and loss3: 4998.06\n",
      "Epoch [236], train_loss: 7862.09 with loss1: 2523.48, loss2: 340.61 and loss3: 4998.01\n",
      "Epoch [237], train_loss: 7873.16 with loss1: 2536.45, loss2: 338.76 and loss3: 4997.95\n",
      "Epoch [238], train_loss: 7833.14 with loss1: 2497.47, loss2: 337.77 and loss3: 4997.90\n",
      "Epoch [239], train_loss: 7865.61 with loss1: 2531.61, loss2: 336.15 and loss3: 4997.85\n",
      "Epoch [240], train_loss: 7824.00 with loss1: 2488.58, loss2: 337.62 and loss3: 4997.80\n",
      "Epoch [241], train_loss: 7833.46 with loss1: 2498.25, loss2: 337.46 and loss3: 4997.75\n",
      "Epoch [242], train_loss: 7796.21 with loss1: 2465.26, loss2: 333.24 and loss3: 4997.70\n",
      "Epoch [243], train_loss: 7807.22 with loss1: 2475.48, loss2: 334.09 and loss3: 4997.65\n",
      "Epoch [244], train_loss: 7775.57 with loss1: 2447.17, loss2: 330.79 and loss3: 4997.60\n",
      "Epoch [245], train_loss: 7797.98 with loss1: 2467.67, loss2: 332.76 and loss3: 4997.55\n",
      "Epoch [246], train_loss: 7770.67 with loss1: 2442.21, loss2: 330.96 and loss3: 4997.50\n",
      "Epoch [247], train_loss: 7769.59 with loss1: 2437.66, loss2: 334.48 and loss3: 4997.45\n",
      "Epoch [248], train_loss: 7742.47 with loss1: 2415.64, loss2: 329.44 and loss3: 4997.40\n",
      "Epoch [249], train_loss: 7761.08 with loss1: 2429.69, loss2: 334.04 and loss3: 4997.35\n",
      "Epoch [250], train_loss: 7735.44 with loss1: 2408.58, loss2: 329.56 and loss3: 4997.29\n",
      "Epoch [251], train_loss: 7748.06 with loss1: 2421.95, loss2: 328.87 and loss3: 4997.24\n",
      "Epoch [252], train_loss: 7733.24 with loss1: 2406.82, loss2: 329.23 and loss3: 4997.19\n",
      "Epoch [253], train_loss: 7756.09 with loss1: 2429.80, loss2: 329.14 and loss3: 4997.14\n",
      "Epoch [254], train_loss: 7736.02 with loss1: 2409.21, loss2: 329.72 and loss3: 4997.09\n",
      "Epoch [255], train_loss: 7757.43 with loss1: 2431.41, loss2: 328.98 and loss3: 4997.04\n",
      "Epoch [256], train_loss: 7734.65 with loss1: 2412.71, loss2: 324.95 and loss3: 4996.99\n",
      "Epoch [257], train_loss: 7758.96 with loss1: 2434.77, loss2: 327.25 and loss3: 4996.94\n",
      "Epoch [258], train_loss: 7721.66 with loss1: 2401.93, loss2: 322.84 and loss3: 4996.89\n",
      "Epoch [259], train_loss: 7736.21 with loss1: 2412.98, loss2: 326.39 and loss3: 4996.84\n",
      "Epoch [260], train_loss: 7711.30 with loss1: 2389.03, loss2: 325.48 and loss3: 4996.79\n",
      "Epoch [261], train_loss: 7729.39 with loss1: 2410.28, loss2: 322.38 and loss3: 4996.74\n",
      "Epoch [262], train_loss: 7684.94 with loss1: 2367.62, loss2: 320.63 and loss3: 4996.69\n",
      "Epoch [263], train_loss: 7713.62 with loss1: 2394.33, loss2: 322.65 and loss3: 4996.64\n",
      "Epoch [264], train_loss: 7666.06 with loss1: 2349.99, loss2: 319.48 and loss3: 4996.59\n",
      "Epoch [265], train_loss: 7684.36 with loss1: 2366.87, loss2: 320.96 and loss3: 4996.54\n",
      "Epoch [266], train_loss: 7649.19 with loss1: 2335.00, loss2: 317.70 and loss3: 4996.48\n",
      "Epoch [267], train_loss: 7666.89 with loss1: 2351.59, loss2: 318.87 and loss3: 4996.43\n",
      "Epoch [268], train_loss: 7643.17 with loss1: 2330.14, loss2: 316.66 and loss3: 4996.38\n",
      "Epoch [269], train_loss: 7653.00 with loss1: 2337.33, loss2: 319.33 and loss3: 4996.33\n",
      "Epoch [270], train_loss: 7613.73 with loss1: 2302.67, loss2: 314.78 and loss3: 4996.28\n",
      "Epoch [271], train_loss: 7633.53 with loss1: 2319.37, loss2: 317.92 and loss3: 4996.23\n",
      "Epoch [272], train_loss: 7608.69 with loss1: 2298.31, loss2: 314.20 and loss3: 4996.18\n",
      "Epoch [273], train_loss: 7618.52 with loss1: 2306.84, loss2: 315.55 and loss3: 4996.13\n",
      "Epoch [274], train_loss: 7585.04 with loss1: 2277.62, loss2: 311.34 and loss3: 4996.08\n",
      "Epoch [275], train_loss: 7598.65 with loss1: 2291.32, loss2: 311.30 and loss3: 4996.03\n",
      "Epoch [276], train_loss: 7571.31 with loss1: 2262.56, loss2: 312.78 and loss3: 4995.98\n",
      "Epoch [277], train_loss: 7587.05 with loss1: 2279.29, loss2: 311.83 and loss3: 4995.93\n",
      "Epoch [278], train_loss: 7559.43 with loss1: 2255.46, loss2: 308.10 and loss3: 4995.88\n",
      "Epoch [279], train_loss: 7584.29 with loss1: 2276.49, loss2: 311.97 and loss3: 4995.83\n",
      "Epoch [280], train_loss: 7552.97 with loss1: 2246.46, loss2: 310.74 and loss3: 4995.78\n",
      "Epoch [281], train_loss: 7571.00 with loss1: 2266.21, loss2: 309.06 and loss3: 4995.73\n",
      "Epoch [282], train_loss: 7542.28 with loss1: 2236.68, loss2: 309.92 and loss3: 4995.67\n",
      "Epoch [283], train_loss: 7551.83 with loss1: 2247.55, loss2: 308.65 and loss3: 4995.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [284], train_loss: 7533.23 with loss1: 2232.82, loss2: 304.84 and loss3: 4995.57\n",
      "Epoch [285], train_loss: 7535.83 with loss1: 2232.97, loss2: 307.33 and loss3: 4995.52\n",
      "Epoch [286], train_loss: 7516.20 with loss1: 2214.18, loss2: 306.55 and loss3: 4995.47\n",
      "Epoch [287], train_loss: 7528.02 with loss1: 2225.56, loss2: 307.04 and loss3: 4995.42\n",
      "Epoch [288], train_loss: 7511.79 with loss1: 2212.97, loss2: 303.45 and loss3: 4995.37\n",
      "Epoch [289], train_loss: 7526.97 with loss1: 2226.17, loss2: 305.48 and loss3: 4995.32\n",
      "Epoch [290], train_loss: 7499.91 with loss1: 2201.99, loss2: 302.66 and loss3: 4995.27\n",
      "Epoch [291], train_loss: 7534.39 with loss1: 2235.72, loss2: 303.45 and loss3: 4995.22\n",
      "Epoch [292], train_loss: 7511.51 with loss1: 2211.33, loss2: 305.01 and loss3: 4995.17\n",
      "Epoch [293], train_loss: 7526.74 with loss1: 2229.01, loss2: 302.61 and loss3: 4995.12\n",
      "Epoch [294], train_loss: 7496.95 with loss1: 2201.77, loss2: 300.12 and loss3: 4995.07\n",
      "Epoch [295], train_loss: 7518.32 with loss1: 2221.12, loss2: 302.18 and loss3: 4995.02\n",
      "Epoch [296], train_loss: 7488.51 with loss1: 2191.14, loss2: 302.40 and loss3: 4994.96\n",
      "Epoch [297], train_loss: 7502.23 with loss1: 2205.32, loss2: 302.00 and loss3: 4994.92\n",
      "Epoch [298], train_loss: 7478.70 with loss1: 2183.20, loss2: 300.64 and loss3: 4994.86\n",
      "Epoch [299], train_loss: 7502.62 with loss1: 2206.72, loss2: 301.08 and loss3: 4994.81\n",
      "Epoch [300], train_loss: 7482.16 with loss1: 2189.53, loss2: 297.87 and loss3: 4994.76\n",
      "Epoch [301], train_loss: 7492.99 with loss1: 2196.65, loss2: 301.62 and loss3: 4994.71\n",
      "Epoch [302], train_loss: 7454.90 with loss1: 2161.87, loss2: 298.37 and loss3: 4994.66\n",
      "Epoch [303], train_loss: 7468.92 with loss1: 2176.39, loss2: 297.92 and loss3: 4994.61\n",
      "Epoch [304], train_loss: 7450.14 with loss1: 2160.79, loss2: 294.79 and loss3: 4994.56\n",
      "Epoch [305], train_loss: 7466.26 with loss1: 2171.16, loss2: 300.59 and loss3: 4994.51\n",
      "Epoch [306], train_loss: 7443.07 with loss1: 2154.94, loss2: 293.67 and loss3: 4994.46\n",
      "Epoch [307], train_loss: 7451.80 with loss1: 2158.67, loss2: 298.72 and loss3: 4994.41\n",
      "Epoch [308], train_loss: 7421.52 with loss1: 2133.84, loss2: 293.32 and loss3: 4994.36\n",
      "Epoch [309], train_loss: 7445.64 with loss1: 2153.94, loss2: 297.39 and loss3: 4994.31\n",
      "Epoch [310], train_loss: 7410.87 with loss1: 2125.78, loss2: 290.84 and loss3: 4994.26\n",
      "Epoch [311], train_loss: 7421.62 with loss1: 2132.75, loss2: 294.65 and loss3: 4994.21\n",
      "Epoch [312], train_loss: 7391.92 with loss1: 2104.09, loss2: 293.66 and loss3: 4994.16\n",
      "Epoch [313], train_loss: 7406.49 with loss1: 2118.81, loss2: 293.58 and loss3: 4994.10\n",
      "Epoch [314], train_loss: 7385.43 with loss1: 2100.46, loss2: 290.92 and loss3: 4994.05\n",
      "Epoch [315], train_loss: 7391.86 with loss1: 2104.35, loss2: 293.50 and loss3: 4994.00\n",
      "Epoch [316], train_loss: 7366.05 with loss1: 2081.35, loss2: 290.75 and loss3: 4993.95\n",
      "Epoch [317], train_loss: 7378.84 with loss1: 2091.76, loss2: 293.17 and loss3: 4993.90\n",
      "Epoch [318], train_loss: 7352.51 with loss1: 2071.46, loss2: 287.20 and loss3: 4993.85\n",
      "Epoch [319], train_loss: 7367.75 with loss1: 2081.93, loss2: 292.02 and loss3: 4993.80\n",
      "Epoch [320], train_loss: 7331.41 with loss1: 2050.27, loss2: 287.40 and loss3: 4993.75\n",
      "Epoch [321], train_loss: 7348.19 with loss1: 2063.36, loss2: 291.12 and loss3: 4993.70\n",
      "Epoch [322], train_loss: 7323.36 with loss1: 2043.57, loss2: 286.14 and loss3: 4993.65\n",
      "Epoch [323], train_loss: 7335.99 with loss1: 2055.72, loss2: 286.67 and loss3: 4993.60\n",
      "Epoch [324], train_loss: 7329.28 with loss1: 2050.97, loss2: 284.76 and loss3: 4993.55\n",
      "Epoch [325], train_loss: 7347.25 with loss1: 2064.67, loss2: 289.08 and loss3: 4993.50\n",
      "Epoch [326], train_loss: 7321.10 with loss1: 2043.55, loss2: 284.11 and loss3: 4993.45\n",
      "Epoch [327], train_loss: 7340.23 with loss1: 2061.55, loss2: 285.29 and loss3: 4993.40\n",
      "Epoch [328], train_loss: 7316.78 with loss1: 2038.47, loss2: 284.96 and loss3: 4993.35\n",
      "Epoch [329], train_loss: 7331.64 with loss1: 2051.43, loss2: 286.91 and loss3: 4993.30\n",
      "Epoch [330], train_loss: 7315.81 with loss1: 2041.03, loss2: 281.53 and loss3: 4993.25\n",
      "Epoch [331], train_loss: 7330.19 with loss1: 2051.36, loss2: 285.63 and loss3: 4993.19\n",
      "Epoch [332], train_loss: 7305.45 with loss1: 2030.11, loss2: 282.20 and loss3: 4993.14\n",
      "Epoch [333], train_loss: 7326.88 with loss1: 2049.51, loss2: 284.27 and loss3: 4993.09\n",
      "Epoch [334], train_loss: 7308.37 with loss1: 2035.27, loss2: 280.06 and loss3: 4993.04\n",
      "Epoch [335], train_loss: 7331.25 with loss1: 2054.69, loss2: 283.57 and loss3: 4992.99\n",
      "Epoch [336], train_loss: 7301.96 with loss1: 2028.32, loss2: 280.70 and loss3: 4992.94\n",
      "Epoch [337], train_loss: 7322.48 with loss1: 2048.84, loss2: 280.74 and loss3: 4992.89\n",
      "Epoch [338], train_loss: 7295.36 with loss1: 2023.44, loss2: 279.08 and loss3: 4992.84\n",
      "Epoch [339], train_loss: 7318.51 with loss1: 2045.39, loss2: 280.33 and loss3: 4992.79\n",
      "Epoch [340], train_loss: 7289.75 with loss1: 2017.35, loss2: 279.66 and loss3: 4992.74\n",
      "Epoch [341], train_loss: 7306.24 with loss1: 2034.45, loss2: 279.10 and loss3: 4992.69\n",
      "Epoch [342], train_loss: 7274.50 with loss1: 2006.19, loss2: 275.67 and loss3: 4992.64\n",
      "Epoch [343], train_loss: 7293.01 with loss1: 2020.71, loss2: 279.71 and loss3: 4992.59\n",
      "Epoch [344], train_loss: 7269.50 with loss1: 1998.90, loss2: 278.06 and loss3: 4992.54\n",
      "Epoch [345], train_loss: 7289.79 with loss1: 2017.91, loss2: 279.40 and loss3: 4992.49\n",
      "Epoch [346], train_loss: 7248.44 with loss1: 1979.42, loss2: 276.58 and loss3: 4992.44\n",
      "Epoch [347], train_loss: 7265.79 with loss1: 1996.17, loss2: 277.23 and loss3: 4992.39\n",
      "Epoch [348], train_loss: 7247.79 with loss1: 1980.60, loss2: 274.86 and loss3: 4992.33\n",
      "Epoch [349], train_loss: 7264.63 with loss1: 1995.59, loss2: 276.75 and loss3: 4992.28\n",
      "Epoch [350], train_loss: 7240.92 with loss1: 1975.68, loss2: 273.00 and loss3: 4992.23\n",
      "Epoch [351], train_loss: 7247.35 with loss1: 1979.45, loss2: 275.72 and loss3: 4992.18\n",
      "Epoch [352], train_loss: 7222.36 with loss1: 1957.00, loss2: 273.23 and loss3: 4992.13\n",
      "Epoch [353], train_loss: 7234.91 with loss1: 1967.78, loss2: 275.04 and loss3: 4992.08\n",
      "Epoch [354], train_loss: 7214.24 with loss1: 1949.46, loss2: 272.76 and loss3: 4992.03\n",
      "Epoch [355], train_loss: 7228.31 with loss1: 1962.22, loss2: 274.12 and loss3: 4991.98\n",
      "Epoch [356], train_loss: 7200.41 with loss1: 1936.39, loss2: 272.08 and loss3: 4991.93\n",
      "Epoch [357], train_loss: 7223.91 with loss1: 1958.36, loss2: 273.68 and loss3: 4991.88\n",
      "Epoch [358], train_loss: 7202.86 with loss1: 1939.20, loss2: 271.83 and loss3: 4991.83\n",
      "Epoch [359], train_loss: 7207.50 with loss1: 1943.84, loss2: 271.88 and loss3: 4991.78\n",
      "Epoch [360], train_loss: 7187.30 with loss1: 1925.76, loss2: 269.81 and loss3: 4991.73\n",
      "Epoch [361], train_loss: 7202.15 with loss1: 1939.50, loss2: 270.97 and loss3: 4991.68\n",
      "Epoch [362], train_loss: 7180.98 with loss1: 1919.03, loss2: 270.32 and loss3: 4991.63\n",
      "Epoch [363], train_loss: 7203.02 with loss1: 1939.58, loss2: 271.86 and loss3: 4991.58\n",
      "Epoch [364], train_loss: 7187.68 with loss1: 1925.41, loss2: 270.74 and loss3: 4991.53\n",
      "Epoch [365], train_loss: 7198.03 with loss1: 1936.62, loss2: 269.94 and loss3: 4991.48\n",
      "Epoch [366], train_loss: 7184.99 with loss1: 1922.34, loss2: 271.22 and loss3: 4991.42\n",
      "Epoch [367], train_loss: 7195.46 with loss1: 1935.56, loss2: 268.53 and loss3: 4991.37\n",
      "Epoch [368], train_loss: 7176.39 with loss1: 1917.37, loss2: 267.69 and loss3: 4991.32\n",
      "Epoch [369], train_loss: 7188.82 with loss1: 1930.05, loss2: 267.49 and loss3: 4991.27\n",
      "Epoch [370], train_loss: 7164.10 with loss1: 1905.29, loss2: 267.59 and loss3: 4991.22\n",
      "Epoch [371], train_loss: 7177.67 with loss1: 1918.10, loss2: 268.40 and loss3: 4991.17\n",
      "Epoch [372], train_loss: 7156.19 with loss1: 1901.13, loss2: 263.94 and loss3: 4991.12\n",
      "Epoch [373], train_loss: 7171.37 with loss1: 1912.59, loss2: 267.71 and loss3: 4991.07\n",
      "Epoch [374], train_loss: 7145.43 with loss1: 1890.32, loss2: 264.09 and loss3: 4991.02\n",
      "Epoch [375], train_loss: 7166.87 with loss1: 1909.80, loss2: 266.11 and loss3: 4990.97\n",
      "Epoch [376], train_loss: 7144.00 with loss1: 1888.22, loss2: 264.86 and loss3: 4990.92\n",
      "Epoch [377], train_loss: 7154.82 with loss1: 1898.18, loss2: 265.77 and loss3: 4990.87\n",
      "Epoch [378], train_loss: 7135.80 with loss1: 1881.93, loss2: 263.05 and loss3: 4990.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [379], train_loss: 7150.45 with loss1: 1895.19, loss2: 264.49 and loss3: 4990.77\n",
      "Epoch [380], train_loss: 7126.67 with loss1: 1872.31, loss2: 263.64 and loss3: 4990.72\n",
      "Epoch [381], train_loss: 7138.69 with loss1: 1883.34, loss2: 264.68 and loss3: 4990.67\n",
      "Epoch [382], train_loss: 7118.75 with loss1: 1866.00, loss2: 262.13 and loss3: 4990.62\n",
      "Epoch [383], train_loss: 7139.20 with loss1: 1884.84, loss2: 263.80 and loss3: 4990.57\n",
      "Epoch [384], train_loss: 7112.82 with loss1: 1858.66, loss2: 263.64 and loss3: 4990.52\n",
      "Epoch [385], train_loss: 7121.42 with loss1: 1869.30, loss2: 261.66 and loss3: 4990.46\n",
      "Epoch [386], train_loss: 7104.41 with loss1: 1855.32, loss2: 258.68 and loss3: 4990.41\n",
      "Epoch [387], train_loss: 7119.60 with loss1: 1866.61, loss2: 262.63 and loss3: 4990.36\n",
      "Epoch [388], train_loss: 7094.29 with loss1: 1845.57, loss2: 258.41 and loss3: 4990.31\n",
      "Epoch [389], train_loss: 7104.57 with loss1: 1853.12, loss2: 261.19 and loss3: 4990.26\n",
      "Epoch [390], train_loss: 7081.00 with loss1: 1834.45, loss2: 256.33 and loss3: 4990.21\n",
      "Epoch [391], train_loss: 7099.62 with loss1: 1848.81, loss2: 260.64 and loss3: 4990.16\n",
      "Epoch [392], train_loss: 7072.40 with loss1: 1824.41, loss2: 257.88 and loss3: 4990.11\n",
      "Epoch [393], train_loss: 7089.16 with loss1: 1840.08, loss2: 259.03 and loss3: 4990.06\n",
      "Epoch [394], train_loss: 7071.61 with loss1: 1824.66, loss2: 256.94 and loss3: 4990.01\n",
      "Epoch [395], train_loss: 7084.47 with loss1: 1835.94, loss2: 258.57 and loss3: 4989.96\n",
      "Epoch [396], train_loss: 7063.13 with loss1: 1815.41, loss2: 257.81 and loss3: 4989.91\n",
      "Epoch [397], train_loss: 7076.70 with loss1: 1829.30, loss2: 257.55 and loss3: 4989.86\n",
      "Epoch [398], train_loss: 7061.04 with loss1: 1816.29, loss2: 254.94 and loss3: 4989.81\n",
      "Epoch [399], train_loss: 7066.60 with loss1: 1820.78, loss2: 256.06 and loss3: 4989.76\n",
      "Epoch [400], train_loss: 7050.25 with loss1: 1804.89, loss2: 255.65 and loss3: 4989.71\n",
      "Epoch [401], train_loss: 7066.93 with loss1: 1819.15, loss2: 258.12 and loss3: 4989.66\n",
      "Epoch [402], train_loss: 7046.33 with loss1: 1800.96, loss2: 255.77 and loss3: 4989.61\n",
      "Epoch [403], train_loss: 7056.99 with loss1: 1812.35, loss2: 255.08 and loss3: 4989.56\n",
      "Epoch [404], train_loss: 7037.47 with loss1: 1793.47, loss2: 254.50 and loss3: 4989.50\n",
      "Epoch [405], train_loss: 7049.45 with loss1: 1802.30, loss2: 257.69 and loss3: 4989.45\n",
      "Epoch [406], train_loss: 7035.45 with loss1: 1792.83, loss2: 253.22 and loss3: 4989.40\n",
      "Epoch [407], train_loss: 7051.98 with loss1: 1806.36, loss2: 256.26 and loss3: 4989.35\n",
      "Epoch [408], train_loss: 7034.27 with loss1: 1792.26, loss2: 252.71 and loss3: 4989.30\n",
      "Epoch [409], train_loss: 7047.40 with loss1: 1802.64, loss2: 255.50 and loss3: 4989.25\n",
      "Epoch [410], train_loss: 7029.06 with loss1: 1788.81, loss2: 251.05 and loss3: 4989.20\n",
      "Epoch [411], train_loss: 7035.72 with loss1: 1793.19, loss2: 253.38 and loss3: 4989.15\n",
      "Epoch [412], train_loss: 7020.88 with loss1: 1781.35, loss2: 250.42 and loss3: 4989.10\n",
      "Epoch [413], train_loss: 7036.27 with loss1: 1794.72, loss2: 252.50 and loss3: 4989.05\n",
      "Epoch [414], train_loss: 7009.26 with loss1: 1770.05, loss2: 250.21 and loss3: 4989.00\n",
      "Epoch [415], train_loss: 7025.99 with loss1: 1785.21, loss2: 251.83 and loss3: 4988.95\n",
      "Epoch [416], train_loss: 7015.54 with loss1: 1777.70, loss2: 248.94 and loss3: 4988.90\n",
      "Epoch [417], train_loss: 7028.08 with loss1: 1788.08, loss2: 251.15 and loss3: 4988.85\n",
      "Epoch [418], train_loss: 7010.65 with loss1: 1772.05, loss2: 249.81 and loss3: 4988.80\n",
      "Epoch [419], train_loss: 7020.32 with loss1: 1779.67, loss2: 251.90 and loss3: 4988.75\n",
      "Epoch [420], train_loss: 7001.37 with loss1: 1763.86, loss2: 248.82 and loss3: 4988.70\n",
      "Epoch [421], train_loss: 7020.01 with loss1: 1781.88, loss2: 249.48 and loss3: 4988.65\n",
      "Epoch [422], train_loss: 6995.90 with loss1: 1760.29, loss2: 247.02 and loss3: 4988.60\n",
      "Epoch [423], train_loss: 7012.63 with loss1: 1771.64, loss2: 252.44 and loss3: 4988.54\n",
      "Epoch [424], train_loss: 6990.41 with loss1: 1754.44, loss2: 247.47 and loss3: 4988.49\n",
      "Epoch [425], train_loss: 7000.31 with loss1: 1762.41, loss2: 249.45 and loss3: 4988.44\n",
      "Epoch [426], train_loss: 6981.88 with loss1: 1747.51, loss2: 245.97 and loss3: 4988.39\n",
      "Epoch [427], train_loss: 6998.71 with loss1: 1761.72, loss2: 248.65 and loss3: 4988.34\n",
      "Epoch [428], train_loss: 6984.28 with loss1: 1748.28, loss2: 247.71 and loss3: 4988.29\n",
      "Epoch [429], train_loss: 6988.89 with loss1: 1752.68, loss2: 247.97 and loss3: 4988.24\n",
      "Epoch [430], train_loss: 6973.47 with loss1: 1740.37, loss2: 244.91 and loss3: 4988.19\n",
      "Epoch [431], train_loss: 6985.37 with loss1: 1750.38, loss2: 246.85 and loss3: 4988.14\n",
      "Epoch [432], train_loss: 6964.54 with loss1: 1731.58, loss2: 244.88 and loss3: 4988.09\n",
      "Epoch [433], train_loss: 6978.84 with loss1: 1744.08, loss2: 246.72 and loss3: 4988.04\n",
      "Epoch [434], train_loss: 6955.78 with loss1: 1720.54, loss2: 247.25 and loss3: 4987.99\n",
      "Epoch [435], train_loss: 6970.31 with loss1: 1736.42, loss2: 245.95 and loss3: 4987.94\n",
      "Epoch [436], train_loss: 6951.26 with loss1: 1720.90, loss2: 242.47 and loss3: 4987.89\n",
      "Epoch [437], train_loss: 6955.18 with loss1: 1720.50, loss2: 246.84 and loss3: 4987.84\n",
      "Epoch [438], train_loss: 6945.55 with loss1: 1714.80, loss2: 242.96 and loss3: 4987.79\n",
      "Epoch [439], train_loss: 6957.33 with loss1: 1723.44, loss2: 246.16 and loss3: 4987.74\n",
      "Epoch [440], train_loss: 6929.83 with loss1: 1699.66, loss2: 242.49 and loss3: 4987.69\n",
      "Epoch [441], train_loss: 6943.65 with loss1: 1711.48, loss2: 244.54 and loss3: 4987.64\n",
      "Epoch [442], train_loss: 6925.22 with loss1: 1696.39, loss2: 241.24 and loss3: 4987.58\n",
      "Epoch [443], train_loss: 6937.08 with loss1: 1706.10, loss2: 243.45 and loss3: 4987.53\n",
      "Epoch [444], train_loss: 6922.41 with loss1: 1692.78, loss2: 242.15 and loss3: 4987.48\n",
      "Epoch [445], train_loss: 6926.26 with loss1: 1696.48, loss2: 242.35 and loss3: 4987.43\n",
      "Epoch [446], train_loss: 6906.34 with loss1: 1678.07, loss2: 240.89 and loss3: 4987.38\n",
      "Epoch [447], train_loss: 6926.75 with loss1: 1696.78, loss2: 242.64 and loss3: 4987.33\n",
      "Epoch [448], train_loss: 6906.64 with loss1: 1678.16, loss2: 241.20 and loss3: 4987.28\n",
      "Epoch [449], train_loss: 6921.97 with loss1: 1693.06, loss2: 241.67 and loss3: 4987.23\n",
      "Epoch [450], train_loss: 6910.08 with loss1: 1683.88, loss2: 239.02 and loss3: 4987.18\n",
      "Epoch [451], train_loss: 6919.00 with loss1: 1690.89, loss2: 240.98 and loss3: 4987.13\n",
      "Epoch [452], train_loss: 6900.32 with loss1: 1673.67, loss2: 239.57 and loss3: 4987.08\n",
      "Epoch [453], train_loss: 6909.03 with loss1: 1680.70, loss2: 241.30 and loss3: 4987.03\n",
      "Epoch [454], train_loss: 6896.84 with loss1: 1671.88, loss2: 237.97 and loss3: 4986.98\n",
      "Epoch [455], train_loss: 6910.60 with loss1: 1683.05, loss2: 240.62 and loss3: 4986.93\n",
      "Epoch [456], train_loss: 6896.36 with loss1: 1672.55, loss2: 236.93 and loss3: 4986.88\n",
      "Epoch [457], train_loss: 6915.60 with loss1: 1688.86, loss2: 239.91 and loss3: 4986.83\n",
      "Epoch [458], train_loss: 6897.24 with loss1: 1674.64, loss2: 235.83 and loss3: 4986.78\n",
      "Epoch [459], train_loss: 6914.71 with loss1: 1689.16, loss2: 238.83 and loss3: 4986.73\n",
      "Epoch [460], train_loss: 6893.99 with loss1: 1669.99, loss2: 237.32 and loss3: 4986.68\n",
      "Epoch [461], train_loss: 6911.16 with loss1: 1685.17, loss2: 239.37 and loss3: 4986.63\n",
      "Epoch [462], train_loss: 6887.53 with loss1: 1665.31, loss2: 235.64 and loss3: 4986.58\n",
      "Epoch [463], train_loss: 6906.93 with loss1: 1681.87, loss2: 238.54 and loss3: 4986.52\n",
      "Epoch [464], train_loss: 6883.39 with loss1: 1661.31, loss2: 235.60 and loss3: 4986.47\n",
      "Epoch [465], train_loss: 6895.24 with loss1: 1670.96, loss2: 237.86 and loss3: 4986.42\n",
      "Epoch [466], train_loss: 6878.74 with loss1: 1656.95, loss2: 235.41 and loss3: 4986.37\n",
      "Epoch [467], train_loss: 6884.06 with loss1: 1661.85, loss2: 235.89 and loss3: 4986.32\n",
      "Epoch [468], train_loss: 6867.97 with loss1: 1647.22, loss2: 234.47 and loss3: 4986.27\n",
      "Epoch [469], train_loss: 6886.39 with loss1: 1663.86, loss2: 236.31 and loss3: 4986.22\n",
      "Epoch [470], train_loss: 6865.91 with loss1: 1644.62, loss2: 235.12 and loss3: 4986.17\n",
      "Epoch [471], train_loss: 6878.92 with loss1: 1655.70, loss2: 237.10 and loss3: 4986.12\n",
      "Epoch [472], train_loss: 6860.74 with loss1: 1641.68, loss2: 232.99 and loss3: 4986.07\n",
      "Epoch [473], train_loss: 6874.87 with loss1: 1652.62, loss2: 236.23 and loss3: 4986.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [474], train_loss: 6854.59 with loss1: 1636.72, loss2: 231.91 and loss3: 4985.97\n",
      "Epoch [475], train_loss: 6875.41 with loss1: 1655.35, loss2: 234.14 and loss3: 4985.92\n",
      "Epoch [476], train_loss: 6851.73 with loss1: 1632.85, loss2: 233.01 and loss3: 4985.87\n",
      "Epoch [477], train_loss: 6863.51 with loss1: 1643.55, loss2: 234.13 and loss3: 4985.82\n",
      "Epoch [478], train_loss: 6856.26 with loss1: 1636.04, loss2: 234.45 and loss3: 4985.77\n",
      "Epoch [479], train_loss: 6859.73 with loss1: 1640.61, loss2: 233.41 and loss3: 4985.72\n",
      "Epoch [480], train_loss: 6841.84 with loss1: 1624.12, loss2: 232.06 and loss3: 4985.67\n",
      "Epoch [481], train_loss: 6845.11 with loss1: 1624.31, loss2: 235.19 and loss3: 4985.62\n",
      "Epoch [482], train_loss: 6836.79 with loss1: 1620.14, loss2: 231.08 and loss3: 4985.57\n",
      "Epoch [483], train_loss: 6845.62 with loss1: 1629.19, loss2: 230.91 and loss3: 4985.52\n",
      "Epoch [484], train_loss: 6832.09 with loss1: 1615.87, loss2: 230.75 and loss3: 4985.46\n",
      "Epoch [485], train_loss: 6836.90 with loss1: 1619.49, loss2: 231.99 and loss3: 4985.41\n",
      "Epoch [486], train_loss: 6829.74 with loss1: 1615.37, loss2: 229.01 and loss3: 4985.36\n",
      "Epoch [487], train_loss: 6835.13 with loss1: 1619.11, loss2: 230.71 and loss3: 4985.31\n",
      "Epoch [488], train_loss: 6820.05 with loss1: 1605.14, loss2: 229.65 and loss3: 4985.26\n",
      "Epoch [489], train_loss: 6830.06 with loss1: 1612.99, loss2: 231.86 and loss3: 4985.21\n",
      "Epoch [490], train_loss: 6818.59 with loss1: 1603.00, loss2: 230.43 and loss3: 4985.16\n",
      "Epoch [491], train_loss: 6815.10 with loss1: 1598.80, loss2: 231.18 and loss3: 4985.11\n",
      "Epoch [492], train_loss: 6809.77 with loss1: 1597.72, loss2: 226.99 and loss3: 4985.06\n",
      "Epoch [493], train_loss: 6810.36 with loss1: 1595.67, loss2: 229.67 and loss3: 4985.01\n",
      "Epoch [494], train_loss: 6796.49 with loss1: 1584.46, loss2: 227.07 and loss3: 4984.96\n",
      "Epoch [495], train_loss: 6802.46 with loss1: 1586.75, loss2: 230.80 and loss3: 4984.91\n",
      "Epoch [496], train_loss: 6793.24 with loss1: 1580.78, loss2: 227.60 and loss3: 4984.86\n",
      "Epoch [497], train_loss: 6799.50 with loss1: 1586.38, loss2: 228.32 and loss3: 4984.81\n",
      "Epoch [498], train_loss: 6780.92 with loss1: 1570.20, loss2: 225.96 and loss3: 4984.76\n",
      "Epoch [499], train_loss: 6796.29 with loss1: 1582.04, loss2: 229.53 and loss3: 4984.71\n",
      "Epoch [500], train_loss: 6780.79 with loss1: 1570.64, loss2: 225.49 and loss3: 4984.66\n",
      "Epoch [501], train_loss: 6788.99 with loss1: 1576.72, loss2: 227.67 and loss3: 4984.61\n",
      "Epoch [502], train_loss: 6772.19 with loss1: 1562.38, loss2: 225.25 and loss3: 4984.56\n",
      "Epoch [503], train_loss: 6782.88 with loss1: 1570.75, loss2: 227.62 and loss3: 4984.51\n",
      "Epoch [504], train_loss: 6772.65 with loss1: 1563.02, loss2: 225.18 and loss3: 4984.46\n",
      "Epoch [505], train_loss: 6779.88 with loss1: 1568.86, loss2: 226.61 and loss3: 4984.41\n",
      "Epoch [506], train_loss: 6767.08 with loss1: 1558.23, loss2: 224.49 and loss3: 4984.36\n",
      "Epoch [507], train_loss: 6785.26 with loss1: 1574.35, loss2: 226.61 and loss3: 4984.31\n",
      "Epoch [508], train_loss: 6770.60 with loss1: 1561.12, loss2: 225.23 and loss3: 4984.25\n",
      "Epoch [509], train_loss: 6785.22 with loss1: 1574.59, loss2: 226.42 and loss3: 4984.20\n",
      "Epoch [510], train_loss: 6767.41 with loss1: 1559.25, loss2: 224.00 and loss3: 4984.15\n",
      "Epoch [511], train_loss: 6777.82 with loss1: 1566.88, loss2: 226.83 and loss3: 4984.10\n",
      "Epoch [512], train_loss: 6761.98 with loss1: 1555.09, loss2: 222.83 and loss3: 4984.05\n",
      "Epoch [513], train_loss: 6767.22 with loss1: 1558.67, loss2: 224.55 and loss3: 4984.00\n",
      "Epoch [514], train_loss: 6756.06 with loss1: 1550.41, loss2: 221.70 and loss3: 4983.95\n",
      "Epoch [515], train_loss: 6776.29 with loss1: 1569.08, loss2: 223.31 and loss3: 4983.90\n",
      "Epoch [516], train_loss: 6757.37 with loss1: 1551.59, loss2: 221.93 and loss3: 4983.85\n",
      "Epoch [517], train_loss: 6762.90 with loss1: 1554.94, loss2: 224.16 and loss3: 4983.80\n",
      "Epoch [518], train_loss: 6749.55 with loss1: 1543.79, loss2: 222.01 and loss3: 4983.75\n",
      "Epoch [519], train_loss: 6761.90 with loss1: 1554.53, loss2: 223.67 and loss3: 4983.70\n",
      "Epoch [520], train_loss: 6748.80 with loss1: 1544.33, loss2: 220.82 and loss3: 4983.65\n",
      "Epoch [521], train_loss: 6755.61 with loss1: 1548.80, loss2: 223.21 and loss3: 4983.60\n",
      "Epoch [522], train_loss: 6742.54 with loss1: 1537.76, loss2: 221.23 and loss3: 4983.55\n",
      "Epoch [523], train_loss: 6758.76 with loss1: 1552.98, loss2: 222.28 and loss3: 4983.50\n",
      "Epoch [524], train_loss: 6741.68 with loss1: 1538.42, loss2: 219.81 and loss3: 4983.45\n",
      "Epoch [525], train_loss: 6750.55 with loss1: 1544.69, loss2: 222.47 and loss3: 4983.40\n",
      "Epoch [526], train_loss: 6734.14 with loss1: 1531.98, loss2: 218.81 and loss3: 4983.35\n",
      "Epoch [527], train_loss: 6741.84 with loss1: 1538.02, loss2: 220.53 and loss3: 4983.30\n",
      "Epoch [528], train_loss: 6729.82 with loss1: 1526.88, loss2: 219.69 and loss3: 4983.25\n",
      "Epoch [529], train_loss: 6738.31 with loss1: 1534.14, loss2: 220.97 and loss3: 4983.20\n",
      "Epoch [530], train_loss: 6717.25 with loss1: 1515.69, loss2: 218.41 and loss3: 4983.15\n",
      "Epoch [531], train_loss: 6727.29 with loss1: 1523.34, loss2: 220.86 and loss3: 4983.10\n",
      "Epoch [532], train_loss: 6715.61 with loss1: 1514.59, loss2: 217.98 and loss3: 4983.05\n",
      "Epoch [533], train_loss: 6728.25 with loss1: 1525.94, loss2: 219.31 and loss3: 4983.00\n",
      "Epoch [534], train_loss: 6711.30 with loss1: 1511.15, loss2: 217.21 and loss3: 4982.94\n",
      "Epoch [535], train_loss: 6728.50 with loss1: 1525.13, loss2: 220.48 and loss3: 4982.89\n",
      "Epoch [536], train_loss: 6706.65 with loss1: 1507.25, loss2: 216.56 and loss3: 4982.84\n",
      "Epoch [537], train_loss: 6726.39 with loss1: 1524.55, loss2: 219.04 and loss3: 4982.79\n",
      "Epoch [538], train_loss: 6704.03 with loss1: 1503.69, loss2: 217.59 and loss3: 4982.74\n",
      "Epoch [539], train_loss: 6715.60 with loss1: 1514.14, loss2: 218.77 and loss3: 4982.69\n",
      "Epoch [540], train_loss: 6697.36 with loss1: 1498.21, loss2: 216.51 and loss3: 4982.64\n",
      "Epoch [541], train_loss: 6714.00 with loss1: 1512.83, loss2: 218.59 and loss3: 4982.59\n",
      "Epoch [542], train_loss: 6697.93 with loss1: 1500.08, loss2: 215.32 and loss3: 4982.54\n",
      "Epoch [543], train_loss: 6705.45 with loss1: 1505.70, loss2: 217.26 and loss3: 4982.49\n",
      "Epoch [544], train_loss: 6691.20 with loss1: 1492.79, loss2: 215.97 and loss3: 4982.44\n",
      "Epoch [545], train_loss: 6703.42 with loss1: 1504.56, loss2: 216.47 and loss3: 4982.39\n",
      "Epoch [546], train_loss: 6691.34 with loss1: 1493.81, loss2: 215.19 and loss3: 4982.34\n",
      "Epoch [547], train_loss: 6698.40 with loss1: 1499.64, loss2: 216.47 and loss3: 4982.29\n",
      "Epoch [548], train_loss: 6690.23 with loss1: 1493.61, loss2: 214.39 and loss3: 4982.24\n",
      "Epoch [549], train_loss: 6693.67 with loss1: 1494.48, loss2: 217.00 and loss3: 4982.19\n",
      "Epoch [550], train_loss: 6678.28 with loss1: 1482.57, loss2: 213.56 and loss3: 4982.14\n",
      "Epoch [551], train_loss: 6684.14 with loss1: 1486.44, loss2: 215.61 and loss3: 4982.09\n",
      "Epoch [552], train_loss: 6673.76 with loss1: 1479.86, loss2: 211.86 and loss3: 4982.04\n",
      "Epoch [553], train_loss: 6685.25 with loss1: 1487.67, loss2: 215.59 and loss3: 4981.99\n",
      "Epoch [554], train_loss: 6672.90 with loss1: 1477.82, loss2: 213.15 and loss3: 4981.94\n",
      "Epoch [555], train_loss: 6682.64 with loss1: 1485.63, loss2: 215.12 and loss3: 4981.89\n",
      "Epoch [556], train_loss: 6667.45 with loss1: 1473.35, loss2: 212.26 and loss3: 4981.84\n",
      "Epoch [557], train_loss: 6682.52 with loss1: 1486.18, loss2: 214.56 and loss3: 4981.79\n",
      "Epoch [558], train_loss: 6663.88 with loss1: 1469.73, loss2: 212.42 and loss3: 4981.74\n",
      "Epoch [559], train_loss: 6679.64 with loss1: 1484.21, loss2: 213.75 and loss3: 4981.68\n",
      "Epoch [560], train_loss: 6664.29 with loss1: 1471.17, loss2: 211.48 and loss3: 4981.63\n",
      "Epoch [561], train_loss: 6675.98 with loss1: 1480.66, loss2: 213.74 and loss3: 4981.58\n",
      "Epoch [562], train_loss: 6661.06 with loss1: 1468.36, loss2: 211.17 and loss3: 4981.53\n",
      "Epoch [563], train_loss: 6676.26 with loss1: 1481.52, loss2: 213.26 and loss3: 4981.48\n",
      "Epoch [564], train_loss: 6661.10 with loss1: 1469.41, loss2: 210.26 and loss3: 4981.43\n",
      "Epoch [565], train_loss: 6667.08 with loss1: 1472.56, loss2: 213.14 and loss3: 4981.38\n",
      "Epoch [566], train_loss: 6657.32 with loss1: 1463.73, loss2: 212.25 and loss3: 4981.33\n",
      "Epoch [567], train_loss: 6669.77 with loss1: 1476.09, loss2: 212.40 and loss3: 4981.28\n",
      "Epoch [568], train_loss: 6647.81 with loss1: 1457.67, loss2: 208.91 and loss3: 4981.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [569], train_loss: 6658.54 with loss1: 1465.32, loss2: 212.04 and loss3: 4981.18\n",
      "Epoch [570], train_loss: 6646.89 with loss1: 1456.04, loss2: 209.72 and loss3: 4981.13\n",
      "Epoch [571], train_loss: 6662.77 with loss1: 1470.33, loss2: 211.36 and loss3: 4981.08\n",
      "Epoch [572], train_loss: 6645.12 with loss1: 1455.27, loss2: 208.83 and loss3: 4981.03\n",
      "Epoch [573], train_loss: 6652.91 with loss1: 1459.94, loss2: 211.99 and loss3: 4980.98\n",
      "Epoch [574], train_loss: 6633.57 with loss1: 1443.78, loss2: 208.86 and loss3: 4980.93\n",
      "Epoch [575], train_loss: 6648.14 with loss1: 1456.64, loss2: 210.62 and loss3: 4980.88\n",
      "Epoch [576], train_loss: 6631.47 with loss1: 1441.74, loss2: 208.89 and loss3: 4980.83\n",
      "Epoch [577], train_loss: 6645.72 with loss1: 1453.59, loss2: 211.36 and loss3: 4980.78\n",
      "Epoch [578], train_loss: 6634.16 with loss1: 1444.77, loss2: 208.67 and loss3: 4980.73\n",
      "Epoch [579], train_loss: 6640.70 with loss1: 1450.01, loss2: 210.01 and loss3: 4980.68\n",
      "Epoch [580], train_loss: 6622.82 with loss1: 1434.43, loss2: 207.76 and loss3: 4980.63\n",
      "Epoch [581], train_loss: 6632.26 with loss1: 1441.52, loss2: 210.17 and loss3: 4980.58\n",
      "Epoch [582], train_loss: 6620.03 with loss1: 1432.00, loss2: 207.50 and loss3: 4980.53\n",
      "Epoch [583], train_loss: 6627.05 with loss1: 1437.60, loss2: 208.98 and loss3: 4980.48\n",
      "Epoch [584], train_loss: 6619.37 with loss1: 1431.19, loss2: 207.75 and loss3: 4980.43\n",
      "Epoch [585], train_loss: 6631.02 with loss1: 1441.03, loss2: 209.62 and loss3: 4980.38\n",
      "Epoch [586], train_loss: 6617.34 with loss1: 1430.17, loss2: 206.84 and loss3: 4980.33\n",
      "Epoch [587], train_loss: 6619.22 with loss1: 1429.84, loss2: 209.10 and loss3: 4980.27\n",
      "Epoch [588], train_loss: 6616.63 with loss1: 1429.23, loss2: 207.18 and loss3: 4980.22\n",
      "Epoch [589], train_loss: 6615.70 with loss1: 1427.04, loss2: 208.48 and loss3: 4980.17\n",
      "Epoch [590], train_loss: 6607.29 with loss1: 1421.25, loss2: 205.92 and loss3: 4980.12\n",
      "Epoch [591], train_loss: 6622.63 with loss1: 1433.76, loss2: 208.80 and loss3: 4980.07\n",
      "Epoch [592], train_loss: 6607.10 with loss1: 1422.41, loss2: 204.67 and loss3: 4980.02\n",
      "Epoch [593], train_loss: 6614.38 with loss1: 1427.11, loss2: 207.30 and loss3: 4979.97\n",
      "Epoch [594], train_loss: 6603.00 with loss1: 1417.77, loss2: 205.30 and loss3: 4979.92\n",
      "Epoch [595], train_loss: 6615.41 with loss1: 1427.14, loss2: 208.40 and loss3: 4979.87\n",
      "Epoch [596], train_loss: 6597.55 with loss1: 1412.82, loss2: 204.91 and loss3: 4979.82\n",
      "Epoch [597], train_loss: 6606.38 with loss1: 1419.45, loss2: 207.17 and loss3: 4979.77\n",
      "Epoch [598], train_loss: 6598.35 with loss1: 1414.80, loss2: 203.83 and loss3: 4979.72\n",
      "Epoch [599], train_loss: 6606.15 with loss1: 1420.19, loss2: 206.29 and loss3: 4979.67\n",
      "Epoch [600], train_loss: 6590.90 with loss1: 1406.92, loss2: 204.37 and loss3: 4979.62\n",
      "Epoch [601], train_loss: 6607.34 with loss1: 1421.89, loss2: 205.89 and loss3: 4979.57\n",
      "Epoch [602], train_loss: 6593.59 with loss1: 1410.09, loss2: 203.98 and loss3: 4979.52\n",
      "Epoch [603], train_loss: 6601.11 with loss1: 1416.96, loss2: 204.69 and loss3: 4979.47\n",
      "Epoch [604], train_loss: 6587.11 with loss1: 1404.34, loss2: 203.35 and loss3: 4979.42\n",
      "Epoch [605], train_loss: 6589.21 with loss1: 1404.67, loss2: 205.17 and loss3: 4979.37\n",
      "Epoch [606], train_loss: 6576.28 with loss1: 1394.04, loss2: 202.93 and loss3: 4979.32\n",
      "Epoch [607], train_loss: 6590.50 with loss1: 1406.31, loss2: 204.93 and loss3: 4979.27\n",
      "Epoch [608], train_loss: 6573.96 with loss1: 1391.78, loss2: 202.96 and loss3: 4979.22\n",
      "Epoch [609], train_loss: 6585.59 with loss1: 1401.67, loss2: 204.76 and loss3: 4979.17\n",
      "Epoch [610], train_loss: 6572.35 with loss1: 1390.47, loss2: 202.77 and loss3: 4979.12\n",
      "Epoch [611], train_loss: 6575.60 with loss1: 1393.07, loss2: 203.47 and loss3: 4979.07\n",
      "Epoch [612], train_loss: 6556.18 with loss1: 1375.55, loss2: 201.62 and loss3: 4979.02\n",
      "Epoch [613], train_loss: 6569.12 with loss1: 1385.91, loss2: 204.24 and loss3: 4978.97\n",
      "Epoch [614], train_loss: 6558.33 with loss1: 1377.68, loss2: 201.74 and loss3: 4978.92\n",
      "Epoch [615], train_loss: 6567.27 with loss1: 1384.87, loss2: 203.53 and loss3: 4978.86\n",
      "Epoch [616], train_loss: 6547.33 with loss1: 1368.26, loss2: 200.25 and loss3: 4978.81\n",
      "Epoch [617], train_loss: 6561.04 with loss1: 1379.14, loss2: 203.14 and loss3: 4978.76\n",
      "Epoch [618], train_loss: 6554.45 with loss1: 1375.45, loss2: 200.29 and loss3: 4978.71\n",
      "Epoch [619], train_loss: 6560.35 with loss1: 1379.97, loss2: 201.72 and loss3: 4978.66\n",
      "Epoch [620], train_loss: 6546.85 with loss1: 1367.36, loss2: 200.87 and loss3: 4978.61\n",
      "Epoch [621], train_loss: 6561.51 with loss1: 1381.31, loss2: 201.63 and loss3: 4978.56\n",
      "Epoch [622], train_loss: 6544.67 with loss1: 1365.67, loss2: 200.49 and loss3: 4978.51\n",
      "Epoch [623], train_loss: 6559.58 with loss1: 1380.17, loss2: 200.95 and loss3: 4978.46\n",
      "Epoch [624], train_loss: 6538.05 with loss1: 1359.45, loss2: 200.19 and loss3: 4978.41\n",
      "Epoch [625], train_loss: 6552.44 with loss1: 1372.53, loss2: 201.54 and loss3: 4978.36\n",
      "Epoch [626], train_loss: 6540.82 with loss1: 1363.94, loss2: 198.56 and loss3: 4978.31\n",
      "Epoch [627], train_loss: 6548.52 with loss1: 1369.91, loss2: 200.35 and loss3: 4978.26\n",
      "Epoch [628], train_loss: 6538.43 with loss1: 1362.11, loss2: 198.11 and loss3: 4978.21\n",
      "Epoch [629], train_loss: 6550.16 with loss1: 1371.05, loss2: 200.95 and loss3: 4978.16\n",
      "Epoch [630], train_loss: 6539.60 with loss1: 1364.43, loss2: 197.06 and loss3: 4978.11\n",
      "Epoch [631], train_loss: 6547.07 with loss1: 1369.89, loss2: 199.12 and loss3: 4978.06\n",
      "Epoch [632], train_loss: 6536.09 with loss1: 1360.22, loss2: 197.86 and loss3: 4978.01\n",
      "Epoch [633], train_loss: 6544.63 with loss1: 1367.00, loss2: 199.68 and loss3: 4977.96\n",
      "Epoch [634], train_loss: 6530.17 with loss1: 1355.11, loss2: 197.15 and loss3: 4977.91\n",
      "Epoch [635], train_loss: 6539.89 with loss1: 1362.57, loss2: 199.46 and loss3: 4977.86\n",
      "Epoch [636], train_loss: 6524.31 with loss1: 1349.20, loss2: 197.30 and loss3: 4977.81\n",
      "Epoch [637], train_loss: 6535.18 with loss1: 1358.15, loss2: 199.27 and loss3: 4977.76\n",
      "Epoch [638], train_loss: 6520.96 with loss1: 1346.32, loss2: 196.93 and loss3: 4977.71\n",
      "Epoch [639], train_loss: 6530.79 with loss1: 1355.16, loss2: 197.96 and loss3: 4977.66\n",
      "Epoch [640], train_loss: 6521.49 with loss1: 1346.98, loss2: 196.91 and loss3: 4977.61\n",
      "Epoch [641], train_loss: 6527.21 with loss1: 1352.01, loss2: 197.65 and loss3: 4977.56\n",
      "Epoch [642], train_loss: 6523.04 with loss1: 1349.48, loss2: 196.05 and loss3: 4977.51\n",
      "Epoch [643], train_loss: 6529.01 with loss1: 1353.40, loss2: 198.15 and loss3: 4977.46\n",
      "Epoch [644], train_loss: 6519.15 with loss1: 1345.31, loss2: 196.44 and loss3: 4977.41\n",
      "Epoch [645], train_loss: 6528.29 with loss1: 1353.19, loss2: 197.74 and loss3: 4977.35\n",
      "Epoch [646], train_loss: 6514.01 with loss1: 1341.18, loss2: 195.52 and loss3: 4977.30\n",
      "Epoch [647], train_loss: 6525.62 with loss1: 1350.22, loss2: 198.14 and loss3: 4977.25\n",
      "Epoch [648], train_loss: 6516.82 with loss1: 1343.01, loss2: 196.61 and loss3: 4977.20\n",
      "Epoch [649], train_loss: 6520.32 with loss1: 1346.06, loss2: 197.11 and loss3: 4977.15\n",
      "Epoch [650], train_loss: 6508.37 with loss1: 1336.69, loss2: 194.58 and loss3: 4977.10\n",
      "Epoch [651], train_loss: 6519.36 with loss1: 1345.32, loss2: 196.99 and loss3: 4977.05\n",
      "Epoch [652], train_loss: 6504.55 with loss1: 1334.26, loss2: 193.28 and loss3: 4977.00\n",
      "Epoch [653], train_loss: 6515.41 with loss1: 1342.07, loss2: 196.39 and loss3: 4976.95\n",
      "Epoch [654], train_loss: 6505.69 with loss1: 1333.74, loss2: 195.06 and loss3: 4976.90\n",
      "Epoch [655], train_loss: 6516.22 with loss1: 1343.36, loss2: 196.00 and loss3: 4976.85\n",
      "Epoch [656], train_loss: 6501.73 with loss1: 1332.63, loss2: 192.30 and loss3: 4976.80\n",
      "Epoch [657], train_loss: 6514.48 with loss1: 1342.48, loss2: 195.26 and loss3: 4976.75\n",
      "Epoch [658], train_loss: 6498.71 with loss1: 1327.75, loss2: 194.26 and loss3: 4976.70\n",
      "Epoch [659], train_loss: 6503.05 with loss1: 1331.97, loss2: 194.43 and loss3: 4976.65\n",
      "Epoch [660], train_loss: 6495.05 with loss1: 1325.17, loss2: 193.28 and loss3: 4976.60\n",
      "Epoch [661], train_loss: 6502.57 with loss1: 1331.72, loss2: 194.30 and loss3: 4976.55\n",
      "Epoch [662], train_loss: 6494.71 with loss1: 1324.95, loss2: 193.27 and loss3: 4976.50\n",
      "Epoch [663], train_loss: 6497.81 with loss1: 1327.31, loss2: 194.06 and loss3: 4976.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [664], train_loss: 6488.93 with loss1: 1319.91, loss2: 192.62 and loss3: 4976.40\n",
      "Epoch [665], train_loss: 6500.18 with loss1: 1329.66, loss2: 194.17 and loss3: 4976.35\n",
      "Epoch [666], train_loss: 6490.76 with loss1: 1321.39, loss2: 193.07 and loss3: 4976.30\n",
      "Epoch [667], train_loss: 6493.33 with loss1: 1323.34, loss2: 193.74 and loss3: 4976.25\n",
      "Epoch [668], train_loss: 6480.43 with loss1: 1312.73, loss2: 191.50 and loss3: 4976.20\n",
      "Epoch [669], train_loss: 6494.43 with loss1: 1324.05, loss2: 194.24 and loss3: 4976.15\n",
      "Epoch [670], train_loss: 6479.86 with loss1: 1312.05, loss2: 191.71 and loss3: 4976.10\n",
      "Epoch [671], train_loss: 6492.38 with loss1: 1322.36, loss2: 193.97 and loss3: 4976.05\n",
      "Epoch [672], train_loss: 6480.75 with loss1: 1314.00, loss2: 190.76 and loss3: 4976.00\n",
      "Epoch [673], train_loss: 6491.17 with loss1: 1321.83, loss2: 193.40 and loss3: 4975.95\n",
      "Epoch [674], train_loss: 6474.59 with loss1: 1308.13, loss2: 190.56 and loss3: 4975.90\n",
      "Epoch [675], train_loss: 6485.22 with loss1: 1316.15, loss2: 193.22 and loss3: 4975.85\n",
      "Epoch [676], train_loss: 6473.52 with loss1: 1305.83, loss2: 191.89 and loss3: 4975.79\n",
      "Epoch [677], train_loss: 6480.82 with loss1: 1312.66, loss2: 192.42 and loss3: 4975.74\n",
      "Epoch [678], train_loss: 6466.70 with loss1: 1301.31, loss2: 189.69 and loss3: 4975.69\n",
      "Epoch [679], train_loss: 6480.47 with loss1: 1312.54, loss2: 192.29 and loss3: 4975.64\n",
      "Epoch [680], train_loss: 6461.09 with loss1: 1295.48, loss2: 190.02 and loss3: 4975.59\n",
      "Epoch [681], train_loss: 6471.69 with loss1: 1303.12, loss2: 193.02 and loss3: 4975.54\n",
      "Epoch [682], train_loss: 6460.96 with loss1: 1295.84, loss2: 189.62 and loss3: 4975.49\n",
      "Epoch [683], train_loss: 6468.93 with loss1: 1302.76, loss2: 190.73 and loss3: 4975.44\n",
      "Epoch [684], train_loss: 6461.43 with loss1: 1296.43, loss2: 189.61 and loss3: 4975.39\n",
      "Epoch [685], train_loss: 6468.44 with loss1: 1302.38, loss2: 190.72 and loss3: 4975.34\n",
      "Epoch [686], train_loss: 6453.02 with loss1: 1289.26, loss2: 188.47 and loss3: 4975.29\n",
      "Epoch [687], train_loss: 6464.54 with loss1: 1299.35, loss2: 189.95 and loss3: 4975.24\n",
      "Epoch [688], train_loss: 6460.20 with loss1: 1295.92, loss2: 189.08 and loss3: 4975.19\n",
      "Epoch [689], train_loss: 6466.68 with loss1: 1300.90, loss2: 190.64 and loss3: 4975.14\n",
      "Epoch [690], train_loss: 6451.70 with loss1: 1288.38, loss2: 188.23 and loss3: 4975.09\n",
      "Epoch [691], train_loss: 6464.83 with loss1: 1299.89, loss2: 189.90 and loss3: 4975.04\n",
      "Epoch [692], train_loss: 6455.37 with loss1: 1292.11, loss2: 188.28 and loss3: 4974.99\n",
      "Epoch [693], train_loss: 6467.22 with loss1: 1302.90, loss2: 189.38 and loss3: 4974.94\n",
      "Epoch [694], train_loss: 6452.95 with loss1: 1289.89, loss2: 188.17 and loss3: 4974.89\n",
      "Epoch [695], train_loss: 6463.40 with loss1: 1299.49, loss2: 189.07 and loss3: 4974.84\n",
      "Epoch [696], train_loss: 6452.14 with loss1: 1289.32, loss2: 188.03 and loss3: 4974.79\n",
      "Epoch [697], train_loss: 6460.90 with loss1: 1298.20, loss2: 187.97 and loss3: 4974.74\n",
      "Epoch [698], train_loss: 6448.89 with loss1: 1286.18, loss2: 188.02 and loss3: 4974.69\n",
      "Epoch [699], train_loss: 6457.75 with loss1: 1294.53, loss2: 188.58 and loss3: 4974.64\n",
      "Epoch [700], train_loss: 6438.14 with loss1: 1276.40, loss2: 187.15 and loss3: 4974.59\n",
      "Epoch [701], train_loss: 6447.70 with loss1: 1285.09, loss2: 188.07 and loss3: 4974.54\n",
      "Epoch [702], train_loss: 6428.51 with loss1: 1267.52, loss2: 186.51 and loss3: 4974.49\n",
      "Epoch [703], train_loss: 6434.56 with loss1: 1271.28, loss2: 188.84 and loss3: 4974.44\n",
      "Epoch [704], train_loss: 6414.35 with loss1: 1253.25, loss2: 186.71 and loss3: 4974.39\n",
      "Epoch [705], train_loss: 6421.14 with loss1: 1257.89, loss2: 188.92 and loss3: 4974.34\n",
      "Epoch [706], train_loss: 6404.67 with loss1: 1244.73, loss2: 185.66 and loss3: 4974.29\n",
      "Epoch [707], train_loss: 6409.15 with loss1: 1248.43, loss2: 186.49 and loss3: 4974.23\n",
      "Epoch [708], train_loss: 6396.05 with loss1: 1235.87, loss2: 185.99 and loss3: 4974.18\n",
      "Epoch [709], train_loss: 6402.30 with loss1: 1240.57, loss2: 187.59 and loss3: 4974.13\n",
      "Epoch [710], train_loss: 6387.78 with loss1: 1228.82, loss2: 184.88 and loss3: 4974.08\n",
      "Epoch [711], train_loss: 6385.93 with loss1: 1226.47, loss2: 185.43 and loss3: 4974.03\n",
      "Epoch [712], train_loss: 6380.33 with loss1: 1221.55, loss2: 184.80 and loss3: 4973.98\n",
      "Epoch [713], train_loss: 6383.33 with loss1: 1222.83, loss2: 186.57 and loss3: 4973.93\n",
      "Epoch [714], train_loss: 6376.06 with loss1: 1217.73, loss2: 184.45 and loss3: 4973.88\n",
      "Epoch [715], train_loss: 6376.25 with loss1: 1216.76, loss2: 185.66 and loss3: 4973.83\n",
      "Epoch [716], train_loss: 6369.24 with loss1: 1211.61, loss2: 183.85 and loss3: 4973.78\n",
      "Epoch [717], train_loss: 6373.75 with loss1: 1215.12, loss2: 184.90 and loss3: 4973.73\n",
      "Epoch [718], train_loss: 6369.59 with loss1: 1211.75, loss2: 184.16 and loss3: 4973.68\n",
      "Epoch [719], train_loss: 6378.27 with loss1: 1220.01, loss2: 184.62 and loss3: 4973.63\n",
      "Epoch [720], train_loss: 6373.73 with loss1: 1216.55, loss2: 183.60 and loss3: 4973.58\n",
      "Epoch [721], train_loss: 6382.46 with loss1: 1224.31, loss2: 184.62 and loss3: 4973.53\n",
      "Epoch [722], train_loss: 6379.48 with loss1: 1222.50, loss2: 183.50 and loss3: 4973.48\n",
      "Epoch [723], train_loss: 6389.03 with loss1: 1230.45, loss2: 185.15 and loss3: 4973.43\n",
      "Epoch [724], train_loss: 6385.05 with loss1: 1228.37, loss2: 183.30 and loss3: 4973.38\n",
      "Epoch [725], train_loss: 6395.93 with loss1: 1238.11, loss2: 184.49 and loss3: 4973.33\n",
      "Epoch [726], train_loss: 6391.49 with loss1: 1235.56, loss2: 182.65 and loss3: 4973.28\n",
      "Epoch [727], train_loss: 6410.59 with loss1: 1253.27, loss2: 184.09 and loss3: 4973.23\n",
      "Epoch [728], train_loss: 6404.79 with loss1: 1249.62, loss2: 181.99 and loss3: 4973.18\n",
      "Epoch [729], train_loss: 6422.63 with loss1: 1265.42, loss2: 184.09 and loss3: 4973.13\n",
      "Epoch [730], train_loss: 6421.82 with loss1: 1266.31, loss2: 182.44 and loss3: 4973.08\n",
      "Epoch [731], train_loss: 6440.49 with loss1: 1283.32, loss2: 184.15 and loss3: 4973.03\n",
      "Epoch [732], train_loss: 6440.62 with loss1: 1284.14, loss2: 183.50 and loss3: 4972.98\n",
      "Epoch [733], train_loss: 6457.47 with loss1: 1301.46, loss2: 183.08 and loss3: 4972.93\n",
      "Epoch [734], train_loss: 6449.50 with loss1: 1295.07, loss2: 181.56 and loss3: 4972.88\n",
      "Epoch [735], train_loss: 6477.44 with loss1: 1321.36, loss2: 183.25 and loss3: 4972.83\n",
      "Epoch [736], train_loss: 6459.16 with loss1: 1304.65, loss2: 181.73 and loss3: 4972.78\n",
      "Epoch [737], train_loss: 6482.83 with loss1: 1326.65, loss2: 183.45 and loss3: 4972.73\n",
      "Epoch [738], train_loss: 6468.32 with loss1: 1314.39, loss2: 181.26 and loss3: 4972.67\n",
      "Epoch [739], train_loss: 6487.89 with loss1: 1332.41, loss2: 182.85 and loss3: 4972.62\n",
      "Epoch [740], train_loss: 6467.08 with loss1: 1314.01, loss2: 180.49 and loss3: 4972.57\n",
      "Epoch [741], train_loss: 6486.79 with loss1: 1331.68, loss2: 182.59 and loss3: 4972.52\n",
      "Epoch [742], train_loss: 6462.93 with loss1: 1310.65, loss2: 179.80 and loss3: 4972.47\n",
      "Epoch [743], train_loss: 6478.99 with loss1: 1324.00, loss2: 182.56 and loss3: 4972.42\n",
      "Epoch [744], train_loss: 6455.43 with loss1: 1301.67, loss2: 181.39 and loss3: 4972.37\n",
      "Epoch [745], train_loss: 6467.23 with loss1: 1312.13, loss2: 182.77 and loss3: 4972.32\n",
      "Epoch [746], train_loss: 6447.46 with loss1: 1294.58, loss2: 180.61 and loss3: 4972.27\n",
      "Epoch [747], train_loss: 6445.99 with loss1: 1291.05, loss2: 182.72 and loss3: 4972.22\n",
      "Epoch [748], train_loss: 6424.48 with loss1: 1273.08, loss2: 179.24 and loss3: 4972.17\n",
      "Epoch [749], train_loss: 6435.51 with loss1: 1281.97, loss2: 181.42 and loss3: 4972.12\n",
      "Epoch [750], train_loss: 6410.20 with loss1: 1259.16, loss2: 178.97 and loss3: 4972.07\n",
      "Epoch [751], train_loss: 6415.99 with loss1: 1262.80, loss2: 181.16 and loss3: 4972.02\n",
      "Epoch [752], train_loss: 6396.03 with loss1: 1245.71, loss2: 178.35 and loss3: 4971.97\n",
      "Epoch [753], train_loss: 6399.45 with loss1: 1247.55, loss2: 179.98 and loss3: 4971.92\n",
      "Epoch [754], train_loss: 6378.19 with loss1: 1227.30, loss2: 179.02 and loss3: 4971.87\n",
      "Epoch [755], train_loss: 6381.42 with loss1: 1229.42, loss2: 180.18 and loss3: 4971.82\n",
      "Epoch [756], train_loss: 6364.13 with loss1: 1213.65, loss2: 178.71 and loss3: 4971.77\n",
      "Epoch [757], train_loss: 6362.67 with loss1: 1211.36, loss2: 179.58 and loss3: 4971.72\n",
      "Epoch [758], train_loss: 6345.00 with loss1: 1196.10, loss2: 177.22 and loss3: 4971.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [759], train_loss: 6352.68 with loss1: 1200.76, loss2: 180.30 and loss3: 4971.62\n",
      "Epoch [760], train_loss: 6340.15 with loss1: 1191.44, loss2: 177.14 and loss3: 4971.57\n",
      "Epoch [761], train_loss: 6341.36 with loss1: 1190.26, loss2: 179.58 and loss3: 4971.52\n",
      "Epoch [762], train_loss: 6329.42 with loss1: 1180.95, loss2: 177.00 and loss3: 4971.47\n",
      "Epoch [763], train_loss: 6341.57 with loss1: 1190.16, loss2: 179.99 and loss3: 4971.42\n",
      "Epoch [764], train_loss: 6323.80 with loss1: 1175.16, loss2: 177.27 and loss3: 4971.37\n",
      "Epoch [765], train_loss: 6328.52 with loss1: 1177.98, loss2: 179.23 and loss3: 4971.32\n",
      "Epoch [766], train_loss: 6325.05 with loss1: 1176.34, loss2: 177.44 and loss3: 4971.27\n",
      "Epoch [767], train_loss: 6326.31 with loss1: 1176.32, loss2: 178.77 and loss3: 4971.22\n",
      "Epoch [768], train_loss: 6318.05 with loss1: 1171.43, loss2: 175.45 and loss3: 4971.17\n",
      "Epoch [769], train_loss: 6323.93 with loss1: 1174.90, loss2: 177.91 and loss3: 4971.12\n",
      "Epoch [770], train_loss: 6315.48 with loss1: 1167.46, loss2: 176.95 and loss3: 4971.07\n",
      "Epoch [771], train_loss: 6321.87 with loss1: 1172.06, loss2: 178.79 and loss3: 4971.01\n",
      "Epoch [772], train_loss: 6307.64 with loss1: 1160.77, loss2: 175.91 and loss3: 4970.96\n",
      "Epoch [773], train_loss: 6317.16 with loss1: 1167.82, loss2: 178.42 and loss3: 4970.91\n",
      "Epoch [774], train_loss: 6311.92 with loss1: 1164.99, loss2: 176.06 and loss3: 4970.86\n",
      "Epoch [775], train_loss: 6316.29 with loss1: 1168.69, loss2: 176.79 and loss3: 4970.81\n",
      "Epoch [776], train_loss: 6309.64 with loss1: 1164.08, loss2: 174.80 and loss3: 4970.76\n",
      "Epoch [777], train_loss: 6314.54 with loss1: 1167.35, loss2: 176.48 and loss3: 4970.71\n",
      "Epoch [778], train_loss: 6310.11 with loss1: 1164.33, loss2: 175.12 and loss3: 4970.66\n",
      "Epoch [779], train_loss: 6313.83 with loss1: 1166.43, loss2: 176.79 and loss3: 4970.61\n",
      "Epoch [780], train_loss: 6308.16 with loss1: 1162.46, loss2: 175.13 and loss3: 4970.56\n",
      "Epoch [781], train_loss: 6313.80 with loss1: 1166.56, loss2: 176.73 and loss3: 4970.51\n",
      "Epoch [782], train_loss: 6306.33 with loss1: 1160.92, loss2: 174.95 and loss3: 4970.46\n",
      "Epoch [783], train_loss: 6316.23 with loss1: 1170.85, loss2: 174.97 and loss3: 4970.41\n",
      "Epoch [784], train_loss: 6313.97 with loss1: 1168.92, loss2: 174.69 and loss3: 4970.36\n",
      "Epoch [785], train_loss: 6317.86 with loss1: 1172.68, loss2: 174.87 and loss3: 4970.31\n",
      "Epoch [786], train_loss: 6313.24 with loss1: 1169.59, loss2: 173.39 and loss3: 4970.26\n",
      "Epoch [787], train_loss: 6322.12 with loss1: 1177.21, loss2: 174.70 and loss3: 4970.21\n",
      "Epoch [788], train_loss: 6318.44 with loss1: 1174.86, loss2: 173.43 and loss3: 4970.16\n",
      "Epoch [789], train_loss: 6331.82 with loss1: 1186.42, loss2: 175.29 and loss3: 4970.11\n",
      "Epoch [790], train_loss: 6319.41 with loss1: 1176.05, loss2: 173.31 and loss3: 4970.06\n",
      "Epoch [791], train_loss: 6332.06 with loss1: 1187.45, loss2: 174.59 and loss3: 4970.01\n",
      "Epoch [792], train_loss: 6324.56 with loss1: 1181.49, loss2: 173.11 and loss3: 4969.96\n",
      "Epoch [793], train_loss: 6338.20 with loss1: 1193.18, loss2: 175.10 and loss3: 4969.91\n",
      "Epoch [794], train_loss: 6334.79 with loss1: 1191.96, loss2: 172.97 and loss3: 4969.86\n",
      "Epoch [795], train_loss: 6346.53 with loss1: 1202.37, loss2: 174.35 and loss3: 4969.81\n",
      "Epoch [796], train_loss: 6333.36 with loss1: 1190.60, loss2: 173.00 and loss3: 4969.76\n",
      "Epoch [797], train_loss: 6348.32 with loss1: 1204.34, loss2: 174.27 and loss3: 4969.71\n",
      "Epoch [798], train_loss: 6340.14 with loss1: 1197.83, loss2: 172.65 and loss3: 4969.66\n",
      "Epoch [799], train_loss: 6348.90 with loss1: 1205.13, loss2: 174.16 and loss3: 4969.61\n",
      "Epoch [800], train_loss: 6339.11 with loss1: 1198.38, loss2: 171.17 and loss3: 4969.56\n",
      "Epoch [801], train_loss: 6353.05 with loss1: 1210.11, loss2: 173.43 and loss3: 4969.51\n",
      "Epoch [802], train_loss: 6336.65 with loss1: 1194.71, loss2: 172.48 and loss3: 4969.46\n",
      "Epoch [803], train_loss: 6347.53 with loss1: 1205.32, loss2: 172.80 and loss3: 4969.40\n",
      "Epoch [804], train_loss: 6341.91 with loss1: 1200.76, loss2: 171.79 and loss3: 4969.35\n",
      "Epoch [805], train_loss: 6351.04 with loss1: 1207.58, loss2: 174.15 and loss3: 4969.30\n",
      "Epoch [806], train_loss: 6335.01 with loss1: 1193.71, loss2: 172.05 and loss3: 4969.25\n",
      "Epoch [807], train_loss: 6342.85 with loss1: 1200.53, loss2: 173.12 and loss3: 4969.20\n",
      "Epoch [808], train_loss: 6327.53 with loss1: 1187.46, loss2: 170.92 and loss3: 4969.15\n",
      "Epoch [809], train_loss: 6338.72 with loss1: 1196.75, loss2: 172.86 and loss3: 4969.10\n",
      "Epoch [810], train_loss: 6325.17 with loss1: 1185.44, loss2: 170.68 and loss3: 4969.05\n",
      "Epoch [811], train_loss: 6332.72 with loss1: 1191.21, loss2: 172.52 and loss3: 4969.00\n",
      "Epoch [812], train_loss: 6314.84 with loss1: 1174.17, loss2: 171.72 and loss3: 4968.95\n",
      "Epoch [813], train_loss: 6322.21 with loss1: 1180.91, loss2: 172.40 and loss3: 4968.90\n",
      "Epoch [814], train_loss: 6304.71 with loss1: 1166.03, loss2: 169.82 and loss3: 4968.85\n",
      "Epoch [815], train_loss: 6312.86 with loss1: 1172.95, loss2: 171.12 and loss3: 4968.80\n",
      "Epoch [816], train_loss: 6298.83 with loss1: 1160.04, loss2: 170.04 and loss3: 4968.75\n",
      "Epoch [817], train_loss: 6309.82 with loss1: 1168.82, loss2: 172.29 and loss3: 4968.70\n",
      "Epoch [818], train_loss: 6297.10 with loss1: 1157.86, loss2: 170.59 and loss3: 4968.65\n",
      "Epoch [819], train_loss: 6304.27 with loss1: 1164.44, loss2: 171.23 and loss3: 4968.60\n",
      "Epoch [820], train_loss: 6295.37 with loss1: 1158.17, loss2: 168.65 and loss3: 4968.55\n",
      "Epoch [821], train_loss: 6310.13 with loss1: 1170.35, loss2: 171.28 and loss3: 4968.50\n",
      "Epoch [822], train_loss: 6292.65 with loss1: 1155.48, loss2: 168.72 and loss3: 4968.45\n",
      "Epoch [823], train_loss: 6305.10 with loss1: 1165.09, loss2: 171.61 and loss3: 4968.40\n",
      "Epoch [824], train_loss: 6288.99 with loss1: 1151.12, loss2: 169.51 and loss3: 4968.35\n",
      "Epoch [825], train_loss: 6295.58 with loss1: 1156.81, loss2: 170.47 and loss3: 4968.30\n",
      "Epoch [826], train_loss: 6285.96 with loss1: 1148.44, loss2: 169.27 and loss3: 4968.25\n",
      "Epoch [827], train_loss: 6295.76 with loss1: 1157.30, loss2: 170.26 and loss3: 4968.20\n",
      "Epoch [828], train_loss: 6281.77 with loss1: 1145.57, loss2: 168.05 and loss3: 4968.15\n",
      "Epoch [829], train_loss: 6292.30 with loss1: 1153.69, loss2: 170.51 and loss3: 4968.10\n",
      "Epoch [830], train_loss: 6277.82 with loss1: 1140.79, loss2: 168.98 and loss3: 4968.05\n",
      "Epoch [831], train_loss: 6288.86 with loss1: 1151.29, loss2: 169.57 and loss3: 4968.00\n",
      "Epoch [832], train_loss: 6276.55 with loss1: 1139.33, loss2: 169.27 and loss3: 4967.95\n",
      "Epoch [833], train_loss: 6288.37 with loss1: 1149.36, loss2: 171.12 and loss3: 4967.90\n",
      "Epoch [834], train_loss: 6275.30 with loss1: 1139.37, loss2: 168.09 and loss3: 4967.85\n",
      "Epoch [835], train_loss: 6282.15 with loss1: 1145.23, loss2: 169.12 and loss3: 4967.80\n",
      "Epoch [836], train_loss: 6270.57 with loss1: 1134.30, loss2: 168.52 and loss3: 4967.75\n",
      "Epoch [837], train_loss: 6274.22 with loss1: 1137.72, loss2: 168.81 and loss3: 4967.69\n",
      "Epoch [838], train_loss: 6262.82 with loss1: 1127.79, loss2: 167.39 and loss3: 4967.64\n",
      "Epoch [839], train_loss: 6272.19 with loss1: 1136.36, loss2: 168.23 and loss3: 4967.59\n",
      "Epoch [840], train_loss: 6258.30 with loss1: 1122.95, loss2: 167.81 and loss3: 4967.54\n",
      "Epoch [841], train_loss: 6263.86 with loss1: 1128.29, loss2: 168.08 and loss3: 4967.49\n",
      "Epoch [842], train_loss: 6251.18 with loss1: 1116.54, loss2: 167.20 and loss3: 4967.44\n",
      "Epoch [843], train_loss: 6256.20 with loss1: 1120.84, loss2: 167.98 and loss3: 4967.39\n",
      "Epoch [844], train_loss: 6246.68 with loss1: 1112.04, loss2: 167.30 and loss3: 4967.34\n",
      "Epoch [845], train_loss: 6249.49 with loss1: 1114.03, loss2: 168.17 and loss3: 4967.29\n",
      "Epoch [846], train_loss: 6239.37 with loss1: 1105.71, loss2: 166.41 and loss3: 4967.24\n",
      "Epoch [847], train_loss: 6245.52 with loss1: 1111.30, loss2: 167.03 and loss3: 4967.19\n",
      "Epoch [848], train_loss: 6234.73 with loss1: 1099.96, loss2: 167.63 and loss3: 4967.14\n",
      "Epoch [849], train_loss: 6241.32 with loss1: 1106.25, loss2: 167.98 and loss3: 4967.09\n",
      "Epoch [850], train_loss: 6229.17 with loss1: 1095.66, loss2: 166.46 and loss3: 4967.04\n",
      "Epoch [851], train_loss: 6237.09 with loss1: 1102.75, loss2: 167.35 and loss3: 4966.99\n",
      "Epoch [852], train_loss: 6227.28 with loss1: 1094.67, loss2: 165.67 and loss3: 4966.94\n",
      "Epoch [853], train_loss: 6233.48 with loss1: 1099.25, loss2: 167.33 and loss3: 4966.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [854], train_loss: 6229.21 with loss1: 1096.72, loss2: 165.65 and loss3: 4966.84\n",
      "Epoch [855], train_loss: 6231.77 with loss1: 1097.24, loss2: 167.75 and loss3: 4966.79\n",
      "Epoch [856], train_loss: 6221.72 with loss1: 1089.36, loss2: 165.62 and loss3: 4966.74\n",
      "Epoch [857], train_loss: 6228.36 with loss1: 1094.71, loss2: 166.96 and loss3: 4966.69\n",
      "Epoch [858], train_loss: 6220.39 with loss1: 1089.10, loss2: 164.65 and loss3: 4966.64\n",
      "Epoch [859], train_loss: 6228.68 with loss1: 1096.24, loss2: 165.85 and loss3: 4966.59\n",
      "Epoch [860], train_loss: 6220.50 with loss1: 1089.36, loss2: 164.61 and loss3: 4966.54\n",
      "Epoch [861], train_loss: 6229.73 with loss1: 1097.10, loss2: 166.14 and loss3: 4966.49\n",
      "Epoch [862], train_loss: 6224.03 with loss1: 1092.56, loss2: 165.02 and loss3: 4966.44\n",
      "Epoch [863], train_loss: 6231.49 with loss1: 1099.29, loss2: 165.82 and loss3: 4966.39\n",
      "Epoch [864], train_loss: 6226.18 with loss1: 1095.74, loss2: 164.10 and loss3: 4966.34\n",
      "Epoch [865], train_loss: 6238.78 with loss1: 1106.89, loss2: 165.59 and loss3: 4966.29\n",
      "Epoch [866], train_loss: 6230.19 with loss1: 1099.97, loss2: 163.98 and loss3: 4966.24\n",
      "Epoch [867], train_loss: 6242.72 with loss1: 1111.44, loss2: 165.10 and loss3: 4966.19\n",
      "Epoch [868], train_loss: 6234.70 with loss1: 1103.86, loss2: 164.70 and loss3: 4966.14\n",
      "Epoch [869], train_loss: 6251.56 with loss1: 1119.57, loss2: 165.91 and loss3: 4966.09\n",
      "Epoch [870], train_loss: 6242.99 with loss1: 1113.51, loss2: 163.44 and loss3: 4966.04\n",
      "Epoch [871], train_loss: 6253.30 with loss1: 1122.80, loss2: 164.52 and loss3: 4965.99\n",
      "Epoch [872], train_loss: 6247.87 with loss1: 1117.44, loss2: 164.49 and loss3: 4965.94\n",
      "Epoch [873], train_loss: 6257.78 with loss1: 1127.22, loss2: 164.68 and loss3: 4965.88\n",
      "Epoch [874], train_loss: 6251.12 with loss1: 1122.02, loss2: 163.26 and loss3: 4965.83\n",
      "Epoch [875], train_loss: 6260.94 with loss1: 1129.42, loss2: 165.73 and loss3: 4965.78\n",
      "Epoch [876], train_loss: 6252.96 with loss1: 1124.47, loss2: 162.75 and loss3: 4965.73\n",
      "Epoch [877], train_loss: 6263.27 with loss1: 1133.24, loss2: 164.35 and loss3: 4965.68\n",
      "Epoch [878], train_loss: 6257.00 with loss1: 1129.34, loss2: 162.02 and loss3: 4965.63\n",
      "Epoch [879], train_loss: 6270.93 with loss1: 1141.34, loss2: 164.01 and loss3: 4965.58\n",
      "Epoch [880], train_loss: 6259.26 with loss1: 1132.46, loss2: 161.27 and loss3: 4965.53\n",
      "Epoch [881], train_loss: 6273.57 with loss1: 1143.36, loss2: 164.72 and loss3: 4965.48\n",
      "Epoch [882], train_loss: 6263.59 with loss1: 1135.52, loss2: 162.64 and loss3: 4965.43\n",
      "Epoch [883], train_loss: 6275.71 with loss1: 1146.97, loss2: 163.35 and loss3: 4965.38\n",
      "Epoch [884], train_loss: 6267.92 with loss1: 1140.82, loss2: 161.76 and loss3: 4965.33\n",
      "Epoch [885], train_loss: 6273.23 with loss1: 1144.00, loss2: 163.95 and loss3: 4965.28\n",
      "Epoch [886], train_loss: 6262.42 with loss1: 1136.35, loss2: 160.84 and loss3: 4965.23\n",
      "Epoch [887], train_loss: 6274.17 with loss1: 1145.34, loss2: 163.65 and loss3: 4965.18\n",
      "Epoch [888], train_loss: 6257.25 with loss1: 1130.45, loss2: 161.67 and loss3: 4965.13\n",
      "Epoch [889], train_loss: 6266.19 with loss1: 1138.78, loss2: 162.33 and loss3: 4965.08\n",
      "Epoch [890], train_loss: 6255.98 with loss1: 1129.72, loss2: 161.23 and loss3: 4965.03\n",
      "Epoch [891], train_loss: 6262.07 with loss1: 1134.08, loss2: 163.01 and loss3: 4964.98\n",
      "Epoch [892], train_loss: 6246.74 with loss1: 1120.84, loss2: 160.98 and loss3: 4964.93\n",
      "Epoch [893], train_loss: 6252.72 with loss1: 1126.17, loss2: 161.67 and loss3: 4964.88\n",
      "Epoch [894], train_loss: 6238.08 with loss1: 1111.95, loss2: 161.30 and loss3: 4964.83\n",
      "Epoch [895], train_loss: 6245.88 with loss1: 1118.73, loss2: 162.38 and loss3: 4964.78\n",
      "Epoch [896], train_loss: 6228.19 with loss1: 1102.66, loss2: 160.80 and loss3: 4964.73\n",
      "Epoch [897], train_loss: 6234.46 with loss1: 1107.48, loss2: 162.30 and loss3: 4964.68\n",
      "Epoch [898], train_loss: 6221.22 with loss1: 1096.26, loss2: 160.33 and loss3: 4964.63\n",
      "Epoch [899], train_loss: 6228.58 with loss1: 1101.74, loss2: 162.26 and loss3: 4964.58\n",
      "Epoch [900], train_loss: 6210.77 with loss1: 1086.29, loss2: 159.96 and loss3: 4964.53\n",
      "Epoch [901], train_loss: 6216.58 with loss1: 1090.92, loss2: 161.19 and loss3: 4964.48\n",
      "Epoch [902], train_loss: 6201.96 with loss1: 1077.30, loss2: 160.23 and loss3: 4964.43\n",
      "Epoch [903], train_loss: 6211.43 with loss1: 1085.75, loss2: 161.30 and loss3: 4964.38\n",
      "Epoch [904], train_loss: 6200.71 with loss1: 1076.62, loss2: 159.77 and loss3: 4964.33\n",
      "Epoch [905], train_loss: 6202.44 with loss1: 1077.46, loss2: 160.70 and loss3: 4964.28\n",
      "Epoch [906], train_loss: 6191.14 with loss1: 1067.57, loss2: 159.34 and loss3: 4964.23\n",
      "Epoch [907], train_loss: 6199.60 with loss1: 1074.44, loss2: 160.98 and loss3: 4964.18\n",
      "Epoch [908], train_loss: 6189.36 with loss1: 1066.05, loss2: 159.18 and loss3: 4964.13\n",
      "Epoch [909], train_loss: 6187.94 with loss1: 1063.64, loss2: 160.22 and loss3: 4964.08\n",
      "Epoch [910], train_loss: 6181.66 with loss1: 1058.87, loss2: 158.76 and loss3: 4964.03\n",
      "Epoch [911], train_loss: 6183.98 with loss1: 1059.83, loss2: 160.18 and loss3: 4963.98\n",
      "Epoch [912], train_loss: 6176.99 with loss1: 1053.89, loss2: 159.17 and loss3: 4963.92\n",
      "Epoch [913], train_loss: 6180.77 with loss1: 1057.07, loss2: 159.82 and loss3: 4963.88\n",
      "Epoch [914], train_loss: 6172.29 with loss1: 1049.10, loss2: 159.37 and loss3: 4963.82\n",
      "Epoch [915], train_loss: 6175.41 with loss1: 1052.50, loss2: 159.14 and loss3: 4963.77\n",
      "Epoch [916], train_loss: 6167.60 with loss1: 1045.47, loss2: 158.41 and loss3: 4963.72\n",
      "Epoch [917], train_loss: 6173.73 with loss1: 1050.67, loss2: 159.39 and loss3: 4963.67\n",
      "Epoch [918], train_loss: 6164.88 with loss1: 1043.13, loss2: 158.13 and loss3: 4963.62\n",
      "Epoch [919], train_loss: 6166.63 with loss1: 1042.42, loss2: 160.64 and loss3: 4963.57\n",
      "Epoch [920], train_loss: 6158.93 with loss1: 1037.64, loss2: 157.77 and loss3: 4963.52\n",
      "Epoch [921], train_loss: 6165.71 with loss1: 1043.95, loss2: 158.29 and loss3: 4963.47\n",
      "Epoch [922], train_loss: 6164.83 with loss1: 1044.02, loss2: 157.39 and loss3: 4963.42\n",
      "Epoch [923], train_loss: 6168.44 with loss1: 1046.30, loss2: 158.78 and loss3: 4963.37\n",
      "Epoch [924], train_loss: 6161.97 with loss1: 1041.11, loss2: 157.54 and loss3: 4963.32\n",
      "Epoch [925], train_loss: 6168.99 with loss1: 1047.13, loss2: 158.59 and loss3: 4963.27\n",
      "Epoch [926], train_loss: 6166.61 with loss1: 1045.94, loss2: 157.45 and loss3: 4963.22\n",
      "Epoch [927], train_loss: 6168.11 with loss1: 1046.49, loss2: 158.44 and loss3: 4963.17\n",
      "Epoch [928], train_loss: 6161.92 with loss1: 1041.79, loss2: 157.01 and loss3: 4963.12\n",
      "Epoch [929], train_loss: 6165.78 with loss1: 1044.06, loss2: 158.66 and loss3: 4963.07\n",
      "Epoch [930], train_loss: 6158.90 with loss1: 1039.31, loss2: 156.57 and loss3: 4963.02\n",
      "Epoch [931], train_loss: 6166.13 with loss1: 1045.11, loss2: 158.05 and loss3: 4962.97\n",
      "Epoch [932], train_loss: 6159.99 with loss1: 1040.13, loss2: 156.94 and loss3: 4962.92\n",
      "Epoch [933], train_loss: 6165.83 with loss1: 1044.47, loss2: 158.49 and loss3: 4962.87\n",
      "Epoch [934], train_loss: 6160.28 with loss1: 1040.96, loss2: 156.50 and loss3: 4962.82\n",
      "Epoch [935], train_loss: 6165.62 with loss1: 1045.15, loss2: 157.71 and loss3: 4962.77\n",
      "Epoch [936], train_loss: 6164.01 with loss1: 1045.01, loss2: 156.29 and loss3: 4962.72\n",
      "Epoch [937], train_loss: 6168.22 with loss1: 1048.13, loss2: 157.42 and loss3: 4962.67\n",
      "Epoch [938], train_loss: 6162.38 with loss1: 1044.20, loss2: 155.57 and loss3: 4962.62\n",
      "Epoch [939], train_loss: 6170.23 with loss1: 1050.36, loss2: 157.30 and loss3: 4962.57\n",
      "Epoch [940], train_loss: 6161.35 with loss1: 1043.30, loss2: 155.54 and loss3: 4962.52\n",
      "Epoch [941], train_loss: 6173.84 with loss1: 1054.32, loss2: 157.05 and loss3: 4962.47\n",
      "Epoch [942], train_loss: 6177.55 with loss1: 1059.65, loss2: 155.48 and loss3: 4962.42\n",
      "Epoch [943], train_loss: 6178.71 with loss1: 1058.87, loss2: 157.47 and loss3: 4962.37\n",
      "Epoch [944], train_loss: 6171.69 with loss1: 1054.31, loss2: 155.07 and loss3: 4962.32\n",
      "Epoch [945], train_loss: 6184.61 with loss1: 1065.80, loss2: 156.54 and loss3: 4962.27\n",
      "Epoch [946], train_loss: 6175.73 with loss1: 1058.50, loss2: 155.02 and loss3: 4962.22\n",
      "Epoch [947], train_loss: 6180.77 with loss1: 1062.99, loss2: 155.61 and loss3: 4962.17\n",
      "Epoch [948], train_loss: 6177.85 with loss1: 1061.35, loss2: 154.38 and loss3: 4962.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [949], train_loss: 6187.53 with loss1: 1068.97, loss2: 156.49 and loss3: 4962.07\n",
      "Epoch [950], train_loss: 6182.40 with loss1: 1065.93, loss2: 154.45 and loss3: 4962.02\n",
      "Epoch [951], train_loss: 6186.53 with loss1: 1067.88, loss2: 156.68 and loss3: 4961.97\n",
      "Epoch [952], train_loss: 6179.27 with loss1: 1063.08, loss2: 154.27 and loss3: 4961.92\n",
      "Epoch [953], train_loss: 6190.64 with loss1: 1072.58, loss2: 156.19 and loss3: 4961.86\n",
      "Epoch [954], train_loss: 6180.29 with loss1: 1062.77, loss2: 155.71 and loss3: 4961.81\n",
      "Epoch [955], train_loss: 6194.07 with loss1: 1076.65, loss2: 155.66 and loss3: 4961.76\n",
      "Epoch [956], train_loss: 6180.12 with loss1: 1064.62, loss2: 153.79 and loss3: 4961.71\n",
      "Epoch [957], train_loss: 6193.39 with loss1: 1075.34, loss2: 156.39 and loss3: 4961.66\n",
      "Epoch [958], train_loss: 6181.91 with loss1: 1065.97, loss2: 154.32 and loss3: 4961.61\n",
      "Epoch [959], train_loss: 6192.25 with loss1: 1075.05, loss2: 155.63 and loss3: 4961.56\n",
      "Epoch [960], train_loss: 6181.29 with loss1: 1065.96, loss2: 153.81 and loss3: 4961.51\n",
      "Epoch [961], train_loss: 6191.64 with loss1: 1074.43, loss2: 155.76 and loss3: 4961.46\n",
      "Epoch [962], train_loss: 6176.35 with loss1: 1061.80, loss2: 153.13 and loss3: 4961.41\n",
      "Epoch [963], train_loss: 6186.89 with loss1: 1071.38, loss2: 154.15 and loss3: 4961.36\n",
      "Epoch [964], train_loss: 6176.16 with loss1: 1061.61, loss2: 153.23 and loss3: 4961.31\n",
      "Epoch [965], train_loss: 6184.70 with loss1: 1069.01, loss2: 154.43 and loss3: 4961.26\n",
      "Epoch [966], train_loss: 6172.52 with loss1: 1058.26, loss2: 153.05 and loss3: 4961.21\n",
      "Epoch [967], train_loss: 6183.29 with loss1: 1067.94, loss2: 154.19 and loss3: 4961.16\n",
      "Epoch [968], train_loss: 6171.32 with loss1: 1057.19, loss2: 153.02 and loss3: 4961.11\n",
      "Epoch [969], train_loss: 6188.30 with loss1: 1073.19, loss2: 154.04 and loss3: 4961.06\n",
      "Epoch [970], train_loss: 6176.65 with loss1: 1060.25, loss2: 155.38 and loss3: 4961.01\n",
      "Epoch [971], train_loss: 6186.16 with loss1: 1071.43, loss2: 153.77 and loss3: 4960.96\n",
      "Epoch [972], train_loss: 6171.42 with loss1: 1058.78, loss2: 151.73 and loss3: 4960.91\n",
      "Epoch [973], train_loss: 6184.53 with loss1: 1069.48, loss2: 154.19 and loss3: 4960.86\n",
      "Epoch [974], train_loss: 6173.14 with loss1: 1059.55, loss2: 152.78 and loss3: 4960.81\n",
      "Epoch [975], train_loss: 6181.50 with loss1: 1067.30, loss2: 153.44 and loss3: 4960.76\n",
      "Epoch [976], train_loss: 6167.56 with loss1: 1054.20, loss2: 152.65 and loss3: 4960.71\n",
      "Epoch [977], train_loss: 6177.32 with loss1: 1063.22, loss2: 153.44 and loss3: 4960.66\n",
      "Epoch [978], train_loss: 6160.17 with loss1: 1047.27, loss2: 152.29 and loss3: 4960.61\n",
      "Epoch [979], train_loss: 6166.54 with loss1: 1053.11, loss2: 152.87 and loss3: 4960.56\n",
      "Epoch [980], train_loss: 6156.99 with loss1: 1044.27, loss2: 152.21 and loss3: 4960.51\n",
      "Epoch [981], train_loss: 6165.18 with loss1: 1051.13, loss2: 153.59 and loss3: 4960.46\n",
      "Epoch [982], train_loss: 6148.18 with loss1: 1035.59, loss2: 152.18 and loss3: 4960.41\n",
      "Epoch [983], train_loss: 6151.28 with loss1: 1038.55, loss2: 152.37 and loss3: 4960.36\n",
      "Epoch [984], train_loss: 6138.66 with loss1: 1026.14, loss2: 152.21 and loss3: 4960.31\n",
      "Epoch [985], train_loss: 6141.86 with loss1: 1029.71, loss2: 151.90 and loss3: 4960.26\n",
      "Epoch [986], train_loss: 6132.56 with loss1: 1021.00, loss2: 151.36 and loss3: 4960.21\n",
      "Epoch [987], train_loss: 6130.49 with loss1: 1017.93, loss2: 152.40 and loss3: 4960.16\n",
      "Epoch [988], train_loss: 6119.88 with loss1: 1008.43, loss2: 151.35 and loss3: 4960.11\n",
      "Epoch [989], train_loss: 6124.27 with loss1: 1011.83, loss2: 152.39 and loss3: 4960.06\n",
      "Epoch [990], train_loss: 6111.69 with loss1: 1000.94, loss2: 150.74 and loss3: 4960.01\n",
      "Epoch [991], train_loss: 6116.03 with loss1: 1004.44, loss2: 151.63 and loss3: 4959.96\n",
      "Epoch [992], train_loss: 6106.61 with loss1: 996.63, loss2: 150.07 and loss3: 4959.91\n",
      "Epoch [993], train_loss: 6112.69 with loss1: 1000.66, loss2: 152.18 and loss3: 4959.86\n",
      "Epoch [994], train_loss: 6106.78 with loss1: 996.73, loss2: 150.25 and loss3: 4959.81\n",
      "Epoch [995], train_loss: 6106.70 with loss1: 994.74, loss2: 152.20 and loss3: 4959.75\n",
      "Epoch [996], train_loss: 6098.51 with loss1: 988.39, loss2: 150.41 and loss3: 4959.71\n",
      "Epoch [997], train_loss: 6103.15 with loss1: 993.08, loss2: 150.41 and loss3: 4959.65\n",
      "Epoch [998], train_loss: 6096.78 with loss1: 986.72, loss2: 150.45 and loss3: 4959.60\n",
      "Epoch [999], train_loss: 6098.88 with loss1: 988.34, loss2: 150.98 and loss3: 4959.55\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# model version 3.5\n",
    "# with loss2 and loss3\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history = fit(epochs=1000, lr=5e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ff2d0608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 6089.68 with loss1: 980.02, loss2: 150.15 and loss3: 4959.50\n",
      "Epoch [1], train_loss: 6094.20 with loss1: 984.11, loss2: 150.64 and loss3: 4959.45\n",
      "Epoch [2], train_loss: 6090.55 with loss1: 981.64, loss2: 149.50 and loss3: 4959.40\n",
      "Epoch [3], train_loss: 6096.25 with loss1: 986.09, loss2: 150.80 and loss3: 4959.35\n",
      "Epoch [4], train_loss: 6086.63 with loss1: 978.20, loss2: 149.13 and loss3: 4959.30\n",
      "Epoch [5], train_loss: 6092.77 with loss1: 983.06, loss2: 150.46 and loss3: 4959.25\n",
      "Epoch [6], train_loss: 6085.84 with loss1: 977.25, loss2: 149.39 and loss3: 4959.20\n",
      "Epoch [7], train_loss: 6091.93 with loss1: 982.37, loss2: 150.40 and loss3: 4959.15\n",
      "Epoch [8], train_loss: 6086.69 with loss1: 978.47, loss2: 149.12 and loss3: 4959.10\n",
      "Epoch [9], train_loss: 6089.75 with loss1: 980.89, loss2: 149.81 and loss3: 4959.05\n",
      "Epoch [10], train_loss: 6083.53 with loss1: 975.20, loss2: 149.32 and loss3: 4959.00\n",
      "Epoch [11], train_loss: 6089.85 with loss1: 981.02, loss2: 149.88 and loss3: 4958.95\n",
      "Epoch [12], train_loss: 6084.45 with loss1: 977.08, loss2: 148.47 and loss3: 4958.90\n",
      "Epoch [13], train_loss: 6094.17 with loss1: 985.58, loss2: 149.74 and loss3: 4958.85\n",
      "Epoch [14], train_loss: 6087.92 with loss1: 980.86, loss2: 148.26 and loss3: 4958.80\n",
      "Epoch [15], train_loss: 6098.41 with loss1: 990.66, loss2: 148.99 and loss3: 4958.75\n",
      "Epoch [16], train_loss: 6094.67 with loss1: 987.63, loss2: 148.34 and loss3: 4958.70\n",
      "Epoch [17], train_loss: 6104.44 with loss1: 995.88, loss2: 149.91 and loss3: 4958.65\n",
      "Epoch [18], train_loss: 6102.91 with loss1: 996.43, loss2: 147.88 and loss3: 4958.60\n",
      "Epoch [19], train_loss: 6110.02 with loss1: 1002.70, loss2: 148.77 and loss3: 4958.55\n",
      "Epoch [20], train_loss: 6106.26 with loss1: 1000.06, loss2: 147.71 and loss3: 4958.50\n",
      "Epoch [21], train_loss: 6116.77 with loss1: 1009.63, loss2: 148.69 and loss3: 4958.45\n",
      "Epoch [22], train_loss: 6111.03 with loss1: 1003.89, loss2: 148.75 and loss3: 4958.40\n",
      "Epoch [23], train_loss: 6123.21 with loss1: 1016.24, loss2: 148.62 and loss3: 4958.35\n",
      "Epoch [24], train_loss: 6118.12 with loss1: 1011.33, loss2: 148.48 and loss3: 4958.30\n",
      "Epoch [25], train_loss: 6127.30 with loss1: 1019.74, loss2: 149.31 and loss3: 4958.25\n",
      "Epoch [26], train_loss: 6118.80 with loss1: 1013.05, loss2: 147.55 and loss3: 4958.20\n",
      "Epoch [27], train_loss: 6128.62 with loss1: 1021.59, loss2: 148.89 and loss3: 4958.15\n",
      "Epoch [28], train_loss: 6119.97 with loss1: 1014.08, loss2: 147.80 and loss3: 4958.10\n",
      "Epoch [29], train_loss: 6131.75 with loss1: 1025.26, loss2: 148.44 and loss3: 4958.05\n",
      "Epoch [30], train_loss: 6119.86 with loss1: 1015.70, loss2: 146.17 and loss3: 4958.00\n",
      "Epoch [31], train_loss: 6131.09 with loss1: 1024.72, loss2: 148.42 and loss3: 4957.95\n",
      "Epoch [32], train_loss: 6118.78 with loss1: 1014.26, loss2: 146.62 and loss3: 4957.90\n",
      "Epoch [33], train_loss: 6130.84 with loss1: 1025.48, loss2: 147.52 and loss3: 4957.85\n",
      "Epoch [34], train_loss: 6119.41 with loss1: 1015.09, loss2: 146.53 and loss3: 4957.80\n",
      "Epoch [35], train_loss: 6126.15 with loss1: 1020.01, loss2: 148.39 and loss3: 4957.75\n",
      "Epoch [36], train_loss: 6113.64 with loss1: 1010.00, loss2: 145.95 and loss3: 4957.70\n",
      "Epoch [37], train_loss: 6127.68 with loss1: 1022.06, loss2: 147.98 and loss3: 4957.65\n",
      "Epoch [38], train_loss: 6112.59 with loss1: 1009.25, loss2: 145.74 and loss3: 4957.60\n",
      "Epoch [39], train_loss: 6121.22 with loss1: 1016.21, loss2: 147.47 and loss3: 4957.55\n",
      "Epoch [40], train_loss: 6107.06 with loss1: 1003.33, loss2: 146.23 and loss3: 4957.50\n",
      "Epoch [41], train_loss: 6116.07 with loss1: 1011.32, loss2: 147.31 and loss3: 4957.45\n",
      "Epoch [42], train_loss: 6104.27 with loss1: 1001.07, loss2: 145.80 and loss3: 4957.40\n",
      "Epoch [43], train_loss: 6109.46 with loss1: 1004.83, loss2: 147.28 and loss3: 4957.34\n",
      "Epoch [44], train_loss: 6097.05 with loss1: 994.53, loss2: 145.22 and loss3: 4957.29\n",
      "Epoch [45], train_loss: 6105.33 with loss1: 1001.10, loss2: 146.98 and loss3: 4957.24\n",
      "Epoch [46], train_loss: 6092.68 with loss1: 990.36, loss2: 145.13 and loss3: 4957.19\n",
      "Epoch [47], train_loss: 6101.83 with loss1: 997.87, loss2: 146.81 and loss3: 4957.14\n",
      "Epoch [48], train_loss: 6089.04 with loss1: 986.62, loss2: 145.33 and loss3: 4957.09\n",
      "Epoch [49], train_loss: 6094.00 with loss1: 990.36, loss2: 146.60 and loss3: 4957.04\n",
      "Epoch [50], train_loss: 6082.93 with loss1: 980.85, loss2: 145.09 and loss3: 4956.99\n",
      "Epoch [51], train_loss: 6085.85 with loss1: 983.16, loss2: 145.75 and loss3: 4956.94\n",
      "Epoch [52], train_loss: 6075.97 with loss1: 974.53, loss2: 144.55 and loss3: 4956.89\n",
      "Epoch [53], train_loss: 6081.35 with loss1: 978.71, loss2: 145.80 and loss3: 4956.84\n",
      "Epoch [54], train_loss: 6072.03 with loss1: 970.88, loss2: 144.36 and loss3: 4956.79\n",
      "Epoch [55], train_loss: 6079.36 with loss1: 976.95, loss2: 145.67 and loss3: 4956.74\n",
      "Epoch [56], train_loss: 6073.16 with loss1: 972.21, loss2: 144.26 and loss3: 4956.69\n",
      "Epoch [57], train_loss: 6074.53 with loss1: 972.03, loss2: 145.86 and loss3: 4956.64\n",
      "Epoch [58], train_loss: 6067.15 with loss1: 966.11, loss2: 144.46 and loss3: 4956.59\n",
      "Epoch [59], train_loss: 6071.19 with loss1: 969.69, loss2: 144.96 and loss3: 4956.54\n",
      "Epoch [60], train_loss: 6062.39 with loss1: 962.26, loss2: 143.63 and loss3: 4956.49\n",
      "Epoch [61], train_loss: 6071.52 with loss1: 968.95, loss2: 146.13 and loss3: 4956.44\n",
      "Epoch [62], train_loss: 6059.91 with loss1: 959.72, loss2: 143.80 and loss3: 4956.39\n",
      "Epoch [63], train_loss: 6069.07 with loss1: 967.57, loss2: 145.15 and loss3: 4956.34\n",
      "Epoch [64], train_loss: 6059.69 with loss1: 960.01, loss2: 143.39 and loss3: 4956.29\n",
      "Epoch [65], train_loss: 6066.76 with loss1: 965.04, loss2: 145.47 and loss3: 4956.24\n",
      "Epoch [66], train_loss: 6057.80 with loss1: 958.29, loss2: 143.31 and loss3: 4956.19\n",
      "Epoch [67], train_loss: 6069.62 with loss1: 968.74, loss2: 144.75 and loss3: 4956.14\n",
      "Epoch [68], train_loss: 6057.78 with loss1: 958.32, loss2: 143.37 and loss3: 4956.09\n",
      "Epoch [69], train_loss: 6062.70 with loss1: 962.09, loss2: 144.57 and loss3: 4956.04\n",
      "Epoch [70], train_loss: 6055.98 with loss1: 956.08, loss2: 143.91 and loss3: 4955.99\n",
      "Epoch [71], train_loss: 6058.65 with loss1: 959.14, loss2: 143.57 and loss3: 4955.94\n",
      "Epoch [72], train_loss: 6052.62 with loss1: 953.37, loss2: 143.36 and loss3: 4955.89\n",
      "Epoch [73], train_loss: 6057.84 with loss1: 957.91, loss2: 144.09 and loss3: 4955.84\n",
      "Epoch [74], train_loss: 6054.36 with loss1: 954.80, loss2: 143.77 and loss3: 4955.79\n",
      "Epoch [75], train_loss: 6059.23 with loss1: 959.51, loss2: 143.98 and loss3: 4955.74\n",
      "Epoch [76], train_loss: 6054.09 with loss1: 955.66, loss2: 142.74 and loss3: 4955.69\n",
      "Epoch [77], train_loss: 6063.30 with loss1: 963.95, loss2: 143.71 and loss3: 4955.64\n",
      "Epoch [78], train_loss: 6053.18 with loss1: 955.82, loss2: 141.77 and loss3: 4955.59\n",
      "Epoch [79], train_loss: 6064.55 with loss1: 965.25, loss2: 143.76 and loss3: 4955.54\n",
      "Epoch [80], train_loss: 6055.26 with loss1: 957.50, loss2: 142.27 and loss3: 4955.49\n",
      "Epoch [81], train_loss: 6065.56 with loss1: 966.27, loss2: 143.86 and loss3: 4955.44\n",
      "Epoch [82], train_loss: 6057.05 with loss1: 959.56, loss2: 142.10 and loss3: 4955.39\n",
      "Epoch [83], train_loss: 6067.83 with loss1: 969.57, loss2: 142.92 and loss3: 4955.34\n",
      "Epoch [84], train_loss: 6057.79 with loss1: 960.75, loss2: 141.76 and loss3: 4955.29\n",
      "Epoch [85], train_loss: 6064.38 with loss1: 965.91, loss2: 143.24 and loss3: 4955.24\n",
      "Epoch [86], train_loss: 6055.42 with loss1: 959.20, loss2: 141.04 and loss3: 4955.19\n",
      "Epoch [87], train_loss: 6065.85 with loss1: 967.84, loss2: 142.88 and loss3: 4955.14\n",
      "Epoch [88], train_loss: 6056.51 with loss1: 960.45, loss2: 140.98 and loss3: 4955.09\n",
      "Epoch [89], train_loss: 6064.29 with loss1: 966.48, loss2: 142.77 and loss3: 4955.04\n",
      "Epoch [90], train_loss: 6056.71 with loss1: 960.33, loss2: 141.39 and loss3: 4954.99\n",
      "Epoch [91], train_loss: 6066.79 with loss1: 968.96, loss2: 142.89 and loss3: 4954.93\n",
      "Epoch [92], train_loss: 6055.72 with loss1: 959.40, loss2: 141.43 and loss3: 4954.88\n",
      "Epoch [93], train_loss: 6061.25 with loss1: 964.08, loss2: 142.34 and loss3: 4954.83\n",
      "Epoch [94], train_loss: 6049.23 with loss1: 953.66, loss2: 140.79 and loss3: 4954.78\n",
      "Epoch [95], train_loss: 6056.13 with loss1: 958.96, loss2: 142.44 and loss3: 4954.73\n",
      "Epoch [96], train_loss: 6044.92 with loss1: 949.39, loss2: 140.85 and loss3: 4954.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [97], train_loss: 6050.76 with loss1: 954.84, loss2: 141.28 and loss3: 4954.63\n",
      "Epoch [98], train_loss: 6042.99 with loss1: 947.78, loss2: 140.63 and loss3: 4954.58\n",
      "Epoch [99], train_loss: 6047.50 with loss1: 951.33, loss2: 141.64 and loss3: 4954.53\n",
      "Epoch [100], train_loss: 6036.67 with loss1: 940.73, loss2: 141.46 and loss3: 4954.48\n",
      "Epoch [101], train_loss: 6037.46 with loss1: 941.57, loss2: 141.46 and loss3: 4954.43\n",
      "Epoch [102], train_loss: 6026.15 with loss1: 931.83, loss2: 139.93 and loss3: 4954.38\n",
      "Epoch [103], train_loss: 6034.87 with loss1: 938.69, loss2: 141.85 and loss3: 4954.33\n",
      "Epoch [104], train_loss: 6021.02 with loss1: 926.37, loss2: 140.37 and loss3: 4954.28\n",
      "Epoch [105], train_loss: 6024.73 with loss1: 929.57, loss2: 140.92 and loss3: 4954.23\n",
      "Epoch [106], train_loss: 6017.21 with loss1: 923.50, loss2: 139.54 and loss3: 4954.18\n",
      "Epoch [107], train_loss: 6018.61 with loss1: 923.48, loss2: 140.99 and loss3: 4954.13\n",
      "Epoch [108], train_loss: 6013.68 with loss1: 919.65, loss2: 139.95 and loss3: 4954.08\n",
      "Epoch [109], train_loss: 6015.45 with loss1: 920.83, loss2: 140.59 and loss3: 4954.03\n",
      "Epoch [110], train_loss: 6009.88 with loss1: 916.96, loss2: 138.94 and loss3: 4953.98\n",
      "Epoch [111], train_loss: 6012.78 with loss1: 918.69, loss2: 140.16 and loss3: 4953.93\n",
      "Epoch [112], train_loss: 6008.52 with loss1: 914.93, loss2: 139.71 and loss3: 4953.88\n",
      "Epoch [113], train_loss: 6015.80 with loss1: 920.87, loss2: 141.10 and loss3: 4953.83\n",
      "Epoch [114], train_loss: 6007.29 with loss1: 914.48, loss2: 139.04 and loss3: 4953.78\n",
      "Epoch [115], train_loss: 6017.67 with loss1: 923.35, loss2: 140.59 and loss3: 4953.73\n",
      "Epoch [116], train_loss: 6010.34 with loss1: 917.18, loss2: 139.48 and loss3: 4953.68\n",
      "Epoch [117], train_loss: 6016.62 with loss1: 922.45, loss2: 140.54 and loss3: 4953.63\n",
      "Epoch [118], train_loss: 6013.42 with loss1: 920.18, loss2: 139.66 and loss3: 4953.58\n",
      "Epoch [119], train_loss: 6023.00 with loss1: 929.25, loss2: 140.22 and loss3: 4953.53\n",
      "Epoch [120], train_loss: 6018.78 with loss1: 926.16, loss2: 139.14 and loss3: 4953.48\n",
      "Epoch [121], train_loss: 6028.13 with loss1: 934.79, loss2: 139.90 and loss3: 4953.43\n",
      "Epoch [122], train_loss: 6020.79 with loss1: 927.90, loss2: 139.51 and loss3: 4953.38\n",
      "Epoch [123], train_loss: 6032.64 with loss1: 939.64, loss2: 139.68 and loss3: 4953.33\n",
      "Epoch [124], train_loss: 6027.52 with loss1: 935.44, loss2: 138.81 and loss3: 4953.28\n",
      "Epoch [125], train_loss: 6032.65 with loss1: 940.47, loss2: 138.95 and loss3: 4953.23\n",
      "Epoch [126], train_loss: 6030.67 with loss1: 939.29, loss2: 138.20 and loss3: 4953.18\n",
      "Epoch [127], train_loss: 6041.82 with loss1: 949.15, loss2: 139.54 and loss3: 4953.13\n",
      "Epoch [128], train_loss: 6036.47 with loss1: 943.92, loss2: 139.47 and loss3: 4953.08\n",
      "Epoch [129], train_loss: 6047.69 with loss1: 955.34, loss2: 139.32 and loss3: 4953.03\n",
      "Epoch [130], train_loss: 6039.55 with loss1: 948.87, loss2: 137.71 and loss3: 4952.98\n",
      "Epoch [131], train_loss: 6052.61 with loss1: 960.42, loss2: 139.27 and loss3: 4952.93\n",
      "Epoch [132], train_loss: 6047.86 with loss1: 957.38, loss2: 137.60 and loss3: 4952.88\n",
      "Epoch [133], train_loss: 6059.27 with loss1: 967.18, loss2: 139.26 and loss3: 4952.83\n",
      "Epoch [134], train_loss: 6052.31 with loss1: 961.45, loss2: 138.08 and loss3: 4952.78\n",
      "Epoch [135], train_loss: 6066.03 with loss1: 973.27, loss2: 140.03 and loss3: 4952.73\n",
      "Epoch [136], train_loss: 6057.30 with loss1: 965.77, loss2: 138.85 and loss3: 4952.68\n",
      "Epoch [137], train_loss: 6069.12 with loss1: 977.57, loss2: 138.92 and loss3: 4952.63\n",
      "Epoch [138], train_loss: 6058.11 with loss1: 967.92, loss2: 137.61 and loss3: 4952.58\n",
      "Epoch [139], train_loss: 6072.03 with loss1: 980.86, loss2: 138.64 and loss3: 4952.53\n",
      "Epoch [140], train_loss: 6060.56 with loss1: 971.03, loss2: 137.06 and loss3: 4952.48\n",
      "Epoch [141], train_loss: 6072.25 with loss1: 981.70, loss2: 138.12 and loss3: 4952.43\n",
      "Epoch [142], train_loss: 6062.53 with loss1: 972.71, loss2: 137.45 and loss3: 4952.38\n",
      "Epoch [143], train_loss: 6071.58 with loss1: 980.71, loss2: 138.55 and loss3: 4952.32\n",
      "Epoch [144], train_loss: 6059.24 with loss1: 969.65, loss2: 137.31 and loss3: 4952.27\n",
      "Epoch [145], train_loss: 6068.17 with loss1: 978.12, loss2: 137.83 and loss3: 4952.22\n",
      "Epoch [146], train_loss: 6052.67 with loss1: 964.11, loss2: 136.39 and loss3: 4952.17\n",
      "Epoch [147], train_loss: 6060.78 with loss1: 970.48, loss2: 138.18 and loss3: 4952.12\n",
      "Epoch [148], train_loss: 6045.74 with loss1: 957.26, loss2: 136.41 and loss3: 4952.07\n",
      "Epoch [149], train_loss: 6052.62 with loss1: 963.53, loss2: 137.06 and loss3: 4952.02\n",
      "Epoch [150], train_loss: 6038.46 with loss1: 949.86, loss2: 136.63 and loss3: 4951.97\n",
      "Epoch [151], train_loss: 6045.20 with loss1: 955.94, loss2: 137.34 and loss3: 4951.92\n",
      "Epoch [152], train_loss: 6028.27 with loss1: 939.98, loss2: 136.42 and loss3: 4951.87\n",
      "Epoch [153], train_loss: 6033.96 with loss1: 944.61, loss2: 137.54 and loss3: 4951.82\n",
      "Epoch [154], train_loss: 6017.01 with loss1: 928.97, loss2: 136.27 and loss3: 4951.77\n",
      "Epoch [155], train_loss: 6024.30 with loss1: 935.62, loss2: 136.95 and loss3: 4951.72\n",
      "Epoch [156], train_loss: 6011.11 with loss1: 923.28, loss2: 136.16 and loss3: 4951.67\n",
      "Epoch [157], train_loss: 6014.78 with loss1: 926.19, loss2: 136.96 and loss3: 4951.62\n",
      "Epoch [158], train_loss: 5999.25 with loss1: 911.77, loss2: 135.91 and loss3: 4951.57\n",
      "Epoch [159], train_loss: 6006.28 with loss1: 917.51, loss2: 137.25 and loss3: 4951.52\n",
      "Epoch [160], train_loss: 5993.15 with loss1: 905.44, loss2: 136.24 and loss3: 4951.47\n",
      "Epoch [161], train_loss: 5999.17 with loss1: 911.52, loss2: 136.23 and loss3: 4951.42\n",
      "Epoch [162], train_loss: 5982.10 with loss1: 894.77, loss2: 135.96 and loss3: 4951.37\n",
      "Epoch [163], train_loss: 5987.50 with loss1: 899.78, loss2: 136.40 and loss3: 4951.32\n",
      "Epoch [164], train_loss: 5977.20 with loss1: 890.23, loss2: 135.70 and loss3: 4951.27\n",
      "Epoch [165], train_loss: 5978.05 with loss1: 890.56, loss2: 136.27 and loss3: 4951.22\n",
      "Epoch [166], train_loss: 5971.55 with loss1: 884.86, loss2: 135.52 and loss3: 4951.17\n",
      "Epoch [167], train_loss: 5975.59 with loss1: 888.42, loss2: 136.05 and loss3: 4951.12\n",
      "Epoch [168], train_loss: 5969.11 with loss1: 883.20, loss2: 134.83 and loss3: 4951.07\n",
      "Epoch [169], train_loss: 5972.94 with loss1: 885.89, loss2: 136.03 and loss3: 4951.02\n",
      "Epoch [170], train_loss: 5967.23 with loss1: 881.03, loss2: 135.23 and loss3: 4950.97\n",
      "Epoch [171], train_loss: 5965.63 with loss1: 879.09, loss2: 135.62 and loss3: 4950.92\n",
      "Epoch [172], train_loss: 5964.47 with loss1: 878.17, loss2: 135.43 and loss3: 4950.87\n",
      "Epoch [173], train_loss: 5967.75 with loss1: 880.89, loss2: 136.04 and loss3: 4950.82\n",
      "Epoch [174], train_loss: 5957.62 with loss1: 873.14, loss2: 133.71 and loss3: 4950.77\n",
      "Epoch [175], train_loss: 5967.27 with loss1: 881.18, loss2: 135.37 and loss3: 4950.72\n",
      "Epoch [176], train_loss: 5958.75 with loss1: 873.90, loss2: 134.18 and loss3: 4950.67\n",
      "Epoch [177], train_loss: 5966.22 with loss1: 880.82, loss2: 134.78 and loss3: 4950.62\n",
      "Epoch [178], train_loss: 5957.87 with loss1: 872.68, loss2: 134.62 and loss3: 4950.57\n",
      "Epoch [179], train_loss: 5964.86 with loss1: 879.38, loss2: 134.96 and loss3: 4950.52\n",
      "Epoch [180], train_loss: 5962.26 with loss1: 877.50, loss2: 134.30 and loss3: 4950.47\n",
      "Epoch [181], train_loss: 5965.85 with loss1: 881.18, loss2: 134.26 and loss3: 4950.42\n",
      "Epoch [182], train_loss: 5960.56 with loss1: 875.39, loss2: 134.81 and loss3: 4950.37\n",
      "Epoch [183], train_loss: 5968.32 with loss1: 883.00, loss2: 134.99 and loss3: 4950.32\n",
      "Epoch [184], train_loss: 5962.81 with loss1: 878.64, loss2: 133.90 and loss3: 4950.27\n",
      "Epoch [185], train_loss: 5970.75 with loss1: 885.84, loss2: 134.69 and loss3: 4950.22\n",
      "Epoch [186], train_loss: 5967.53 with loss1: 883.93, loss2: 133.43 and loss3: 4950.17\n",
      "Epoch [187], train_loss: 5975.69 with loss1: 890.21, loss2: 135.36 and loss3: 4950.12\n",
      "Epoch [188], train_loss: 5967.19 with loss1: 883.66, loss2: 133.46 and loss3: 4950.07\n",
      "Epoch [189], train_loss: 5980.40 with loss1: 895.54, loss2: 134.85 and loss3: 4950.02\n",
      "Epoch [190], train_loss: 5973.19 with loss1: 889.67, loss2: 133.56 and loss3: 4949.97\n",
      "Epoch [191], train_loss: 5986.23 with loss1: 901.55, loss2: 134.76 and loss3: 4949.92\n",
      "Epoch [192], train_loss: 5978.49 with loss1: 895.25, loss2: 133.37 and loss3: 4949.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [193], train_loss: 5989.80 with loss1: 905.84, loss2: 134.14 and loss3: 4949.82\n",
      "Epoch [194], train_loss: 5984.02 with loss1: 900.69, loss2: 133.56 and loss3: 4949.77\n",
      "Epoch [195], train_loss: 5992.84 with loss1: 909.64, loss2: 133.49 and loss3: 4949.72\n",
      "Epoch [196], train_loss: 5985.83 with loss1: 903.70, loss2: 132.46 and loss3: 4949.67\n",
      "Epoch [197], train_loss: 5997.55 with loss1: 913.69, loss2: 134.23 and loss3: 4949.62\n",
      "Epoch [198], train_loss: 5995.57 with loss1: 912.61, loss2: 133.39 and loss3: 4949.57\n",
      "Epoch [199], train_loss: 6000.31 with loss1: 916.66, loss2: 134.13 and loss3: 4949.52\n",
      "Epoch [200], train_loss: 5991.59 with loss1: 909.29, loss2: 132.83 and loss3: 4949.47\n",
      "Epoch [201], train_loss: 6001.13 with loss1: 917.30, loss2: 134.41 and loss3: 4949.42\n",
      "Epoch [202], train_loss: 5990.90 with loss1: 909.71, loss2: 131.83 and loss3: 4949.37\n",
      "Epoch [203], train_loss: 6000.66 with loss1: 917.74, loss2: 133.59 and loss3: 4949.32\n",
      "Epoch [204], train_loss: 5990.29 with loss1: 908.76, loss2: 132.26 and loss3: 4949.27\n",
      "Epoch [205], train_loss: 6001.76 with loss1: 919.23, loss2: 133.31 and loss3: 4949.22\n",
      "Epoch [206], train_loss: 5991.02 with loss1: 909.62, loss2: 132.23 and loss3: 4949.17\n",
      "Epoch [207], train_loss: 6000.39 with loss1: 917.65, loss2: 133.62 and loss3: 4949.12\n",
      "Epoch [208], train_loss: 5991.19 with loss1: 909.96, loss2: 132.16 and loss3: 4949.07\n",
      "Epoch [209], train_loss: 5999.35 with loss1: 917.44, loss2: 132.89 and loss3: 4949.02\n",
      "Epoch [210], train_loss: 5988.93 with loss1: 908.24, loss2: 131.73 and loss3: 4948.97\n",
      "Epoch [211], train_loss: 5994.50 with loss1: 912.90, loss2: 132.68 and loss3: 4948.92\n",
      "Epoch [212], train_loss: 5985.98 with loss1: 905.43, loss2: 131.68 and loss3: 4948.87\n",
      "Epoch [213], train_loss: 5995.25 with loss1: 913.40, loss2: 133.03 and loss3: 4948.82\n",
      "Epoch [214], train_loss: 5982.73 with loss1: 902.20, loss2: 131.77 and loss3: 4948.77\n",
      "Epoch [215], train_loss: 5987.72 with loss1: 906.33, loss2: 132.67 and loss3: 4948.72\n",
      "Epoch [216], train_loss: 5980.28 with loss1: 900.01, loss2: 131.61 and loss3: 4948.67\n",
      "Epoch [217], train_loss: 5983.56 with loss1: 902.28, loss2: 132.66 and loss3: 4948.62\n",
      "Epoch [218], train_loss: 5974.32 with loss1: 894.63, loss2: 131.12 and loss3: 4948.57\n",
      "Epoch [219], train_loss: 5979.84 with loss1: 898.70, loss2: 132.63 and loss3: 4948.52\n",
      "Epoch [220], train_loss: 5972.81 with loss1: 893.30, loss2: 131.04 and loss3: 4948.47\n",
      "Epoch [221], train_loss: 5976.62 with loss1: 896.37, loss2: 131.84 and loss3: 4948.42\n",
      "Epoch [222], train_loss: 5967.77 with loss1: 887.55, loss2: 131.85 and loss3: 4948.37\n",
      "Epoch [223], train_loss: 5968.04 with loss1: 888.04, loss2: 131.68 and loss3: 4948.31\n",
      "Epoch [224], train_loss: 5961.98 with loss1: 882.90, loss2: 130.82 and loss3: 4948.26\n",
      "Epoch [225], train_loss: 5969.32 with loss1: 888.97, loss2: 132.13 and loss3: 4948.21\n",
      "Epoch [226], train_loss: 5960.48 with loss1: 881.19, loss2: 131.13 and loss3: 4948.16\n",
      "Epoch [227], train_loss: 5965.60 with loss1: 885.43, loss2: 132.05 and loss3: 4948.11\n",
      "Epoch [228], train_loss: 5956.55 with loss1: 878.22, loss2: 130.27 and loss3: 4948.06\n",
      "Epoch [229], train_loss: 5962.44 with loss1: 882.77, loss2: 131.66 and loss3: 4948.01\n",
      "Epoch [230], train_loss: 5953.04 with loss1: 874.72, loss2: 130.36 and loss3: 4947.96\n",
      "Epoch [231], train_loss: 5952.68 with loss1: 873.56, loss2: 131.21 and loss3: 4947.91\n",
      "Epoch [232], train_loss: 5944.48 with loss1: 866.86, loss2: 129.75 and loss3: 4947.86\n",
      "Epoch [233], train_loss: 5952.59 with loss1: 873.32, loss2: 131.46 and loss3: 4947.81\n",
      "Epoch [234], train_loss: 5945.13 with loss1: 866.81, loss2: 130.56 and loss3: 4947.76\n",
      "Epoch [235], train_loss: 5947.62 with loss1: 868.99, loss2: 130.92 and loss3: 4947.71\n",
      "Epoch [236], train_loss: 5942.89 with loss1: 865.00, loss2: 130.22 and loss3: 4947.66\n",
      "Epoch [237], train_loss: 5946.10 with loss1: 868.36, loss2: 130.12 and loss3: 4947.61\n",
      "Epoch [238], train_loss: 5936.75 with loss1: 859.42, loss2: 129.77 and loss3: 4947.56\n",
      "Epoch [239], train_loss: 5944.43 with loss1: 866.05, loss2: 130.87 and loss3: 4947.51\n",
      "Epoch [240], train_loss: 5937.21 with loss1: 859.58, loss2: 130.17 and loss3: 4947.46\n",
      "Epoch [241], train_loss: 5939.71 with loss1: 862.06, loss2: 130.23 and loss3: 4947.41\n",
      "Epoch [242], train_loss: 5938.14 with loss1: 861.33, loss2: 129.45 and loss3: 4947.36\n",
      "Epoch [243], train_loss: 5946.39 with loss1: 869.02, loss2: 130.05 and loss3: 4947.31\n",
      "Epoch [244], train_loss: 5936.39 with loss1: 859.65, loss2: 129.48 and loss3: 4947.26\n",
      "Epoch [245], train_loss: 5940.96 with loss1: 863.73, loss2: 130.02 and loss3: 4947.21\n",
      "Epoch [246], train_loss: 5936.08 with loss1: 859.67, loss2: 129.25 and loss3: 4947.16\n",
      "Epoch [247], train_loss: 5938.78 with loss1: 861.41, loss2: 130.26 and loss3: 4947.11\n",
      "Epoch [248], train_loss: 5931.72 with loss1: 855.35, loss2: 129.31 and loss3: 4947.06\n",
      "Epoch [249], train_loss: 5938.65 with loss1: 861.53, loss2: 130.10 and loss3: 4947.01\n",
      "Epoch [250], train_loss: 5929.41 with loss1: 853.47, loss2: 128.98 and loss3: 4946.96\n",
      "Epoch [251], train_loss: 5937.04 with loss1: 860.66, loss2: 129.47 and loss3: 4946.91\n",
      "Epoch [252], train_loss: 5932.25 with loss1: 856.77, loss2: 128.62 and loss3: 4946.86\n",
      "Epoch [253], train_loss: 5946.22 with loss1: 868.98, loss2: 130.43 and loss3: 4946.81\n",
      "Epoch [254], train_loss: 5935.71 with loss1: 860.40, loss2: 128.55 and loss3: 4946.76\n",
      "Epoch [255], train_loss: 5939.84 with loss1: 864.05, loss2: 129.08 and loss3: 4946.71\n",
      "Epoch [256], train_loss: 5936.31 with loss1: 861.38, loss2: 128.27 and loss3: 4946.66\n",
      "Epoch [257], train_loss: 5943.73 with loss1: 868.06, loss2: 129.06 and loss3: 4946.61\n",
      "Epoch [258], train_loss: 5939.48 with loss1: 864.75, loss2: 128.18 and loss3: 4946.56\n",
      "Epoch [259], train_loss: 5943.49 with loss1: 867.86, loss2: 129.11 and loss3: 4946.51\n",
      "Epoch [260], train_loss: 5939.29 with loss1: 864.78, loss2: 128.06 and loss3: 4946.46\n",
      "Epoch [261], train_loss: 5943.64 with loss1: 868.75, loss2: 128.49 and loss3: 4946.41\n",
      "Epoch [262], train_loss: 5939.66 with loss1: 865.08, loss2: 128.22 and loss3: 4946.36\n",
      "Epoch [263], train_loss: 5945.92 with loss1: 870.52, loss2: 129.09 and loss3: 4946.31\n",
      "Epoch [264], train_loss: 5941.40 with loss1: 867.63, loss2: 127.51 and loss3: 4946.26\n",
      "Epoch [265], train_loss: 5949.65 with loss1: 874.57, loss2: 128.87 and loss3: 4946.21\n",
      "Epoch [266], train_loss: 5941.66 with loss1: 868.66, loss2: 126.84 and loss3: 4946.16\n",
      "Epoch [267], train_loss: 5952.74 with loss1: 877.64, loss2: 128.99 and loss3: 4946.11\n",
      "Epoch [268], train_loss: 5945.57 with loss1: 871.99, loss2: 127.52 and loss3: 4946.06\n",
      "Epoch [269], train_loss: 5954.31 with loss1: 879.30, loss2: 128.99 and loss3: 4946.01\n",
      "Epoch [270], train_loss: 5950.04 with loss1: 876.39, loss2: 127.69 and loss3: 4945.96\n",
      "Epoch [271], train_loss: 5956.19 with loss1: 882.00, loss2: 128.27 and loss3: 4945.91\n",
      "Epoch [272], train_loss: 5949.23 with loss1: 876.17, loss2: 127.20 and loss3: 4945.86\n",
      "Epoch [273], train_loss: 5956.36 with loss1: 882.40, loss2: 128.14 and loss3: 4945.81\n",
      "Epoch [274], train_loss: 5946.47 with loss1: 873.11, loss2: 127.61 and loss3: 4945.76\n",
      "Epoch [275], train_loss: 5955.40 with loss1: 881.73, loss2: 127.96 and loss3: 4945.71\n",
      "Epoch [276], train_loss: 5947.25 with loss1: 874.68, loss2: 126.91 and loss3: 4945.66\n",
      "Epoch [277], train_loss: 5954.19 with loss1: 880.72, loss2: 127.86 and loss3: 4945.61\n",
      "Epoch [278], train_loss: 5943.90 with loss1: 871.12, loss2: 127.22 and loss3: 4945.56\n",
      "Epoch [279], train_loss: 5951.86 with loss1: 877.84, loss2: 128.51 and loss3: 4945.51\n",
      "Epoch [280], train_loss: 5939.59 with loss1: 867.45, loss2: 126.69 and loss3: 4945.46\n",
      "Epoch [281], train_loss: 5946.41 with loss1: 872.86, loss2: 128.14 and loss3: 4945.41\n",
      "Epoch [282], train_loss: 5935.41 with loss1: 863.26, loss2: 126.79 and loss3: 4945.36\n",
      "Epoch [283], train_loss: 5940.06 with loss1: 866.95, loss2: 127.80 and loss3: 4945.31\n",
      "Epoch [284], train_loss: 5929.58 with loss1: 856.94, loss2: 127.38 and loss3: 4945.26\n",
      "Epoch [285], train_loss: 5933.80 with loss1: 860.74, loss2: 127.86 and loss3: 4945.21\n",
      "Epoch [286], train_loss: 5921.11 with loss1: 849.07, loss2: 126.88 and loss3: 4945.16\n",
      "Epoch [287], train_loss: 5922.90 with loss1: 850.26, loss2: 127.53 and loss3: 4945.11\n",
      "Epoch [288], train_loss: 5914.30 with loss1: 842.32, loss2: 126.91 and loss3: 4945.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [289], train_loss: 5915.98 with loss1: 843.43, loss2: 127.55 and loss3: 4945.01\n",
      "Epoch [290], train_loss: 5909.35 with loss1: 837.79, loss2: 126.60 and loss3: 4944.96\n",
      "Epoch [291], train_loss: 5907.49 with loss1: 835.87, loss2: 126.71 and loss3: 4944.91\n",
      "Epoch [292], train_loss: 5898.33 with loss1: 827.72, loss2: 125.75 and loss3: 4944.86\n",
      "Epoch [293], train_loss: 5904.06 with loss1: 832.54, loss2: 126.72 and loss3: 4944.81\n",
      "Epoch [294], train_loss: 5894.86 with loss1: 824.59, loss2: 125.51 and loss3: 4944.76\n",
      "Epoch [295], train_loss: 5897.09 with loss1: 824.91, loss2: 127.48 and loss3: 4944.71\n",
      "Epoch [296], train_loss: 5895.09 with loss1: 824.61, loss2: 125.82 and loss3: 4944.66\n",
      "Epoch [297], train_loss: 5894.40 with loss1: 823.43, loss2: 126.36 and loss3: 4944.61\n",
      "Epoch [298], train_loss: 5887.12 with loss1: 816.97, loss2: 125.59 and loss3: 4944.56\n",
      "Epoch [299], train_loss: 5892.51 with loss1: 820.89, loss2: 127.11 and loss3: 4944.51\n",
      "Epoch [300], train_loss: 5884.95 with loss1: 814.96, loss2: 125.53 and loss3: 4944.46\n",
      "Epoch [301], train_loss: 5888.01 with loss1: 817.64, loss2: 125.97 and loss3: 4944.41\n",
      "Epoch [302], train_loss: 5887.19 with loss1: 817.78, loss2: 125.05 and loss3: 4944.36\n",
      "Epoch [303], train_loss: 5887.99 with loss1: 817.65, loss2: 126.04 and loss3: 4944.31\n",
      "Epoch [304], train_loss: 5882.99 with loss1: 813.37, loss2: 125.36 and loss3: 4944.26\n",
      "Epoch [305], train_loss: 5887.69 with loss1: 817.23, loss2: 126.26 and loss3: 4944.21\n",
      "Epoch [306], train_loss: 5884.17 with loss1: 815.06, loss2: 124.95 and loss3: 4944.16\n",
      "Epoch [307], train_loss: 5893.48 with loss1: 823.05, loss2: 126.32 and loss3: 4944.11\n",
      "Epoch [308], train_loss: 5889.03 with loss1: 819.94, loss2: 125.04 and loss3: 4944.06\n",
      "Epoch [309], train_loss: 5893.99 with loss1: 825.03, loss2: 124.95 and loss3: 4944.01\n",
      "Epoch [310], train_loss: 5892.53 with loss1: 823.74, loss2: 124.84 and loss3: 4943.96\n",
      "Epoch [311], train_loss: 5902.69 with loss1: 833.06, loss2: 125.73 and loss3: 4943.91\n",
      "Epoch [312], train_loss: 5896.65 with loss1: 828.16, loss2: 124.64 and loss3: 4943.85\n",
      "Epoch [313], train_loss: 5906.59 with loss1: 837.60, loss2: 125.19 and loss3: 4943.80\n",
      "Epoch [314], train_loss: 5902.48 with loss1: 834.31, loss2: 124.41 and loss3: 4943.75\n",
      "Epoch [315], train_loss: 5908.74 with loss1: 839.78, loss2: 125.25 and loss3: 4943.70\n",
      "Epoch [316], train_loss: 5905.59 with loss1: 837.79, loss2: 124.14 and loss3: 4943.65\n",
      "Epoch [317], train_loss: 5916.19 with loss1: 847.58, loss2: 125.00 and loss3: 4943.60\n",
      "Epoch [318], train_loss: 5913.60 with loss1: 845.80, loss2: 124.24 and loss3: 4943.55\n",
      "Epoch [319], train_loss: 5922.00 with loss1: 852.97, loss2: 125.53 and loss3: 4943.50\n",
      "Epoch [320], train_loss: 5917.05 with loss1: 848.66, loss2: 124.94 and loss3: 4943.45\n",
      "Epoch [321], train_loss: 5929.16 with loss1: 860.51, loss2: 125.25 and loss3: 4943.40\n",
      "Epoch [322], train_loss: 5924.20 with loss1: 856.97, loss2: 123.88 and loss3: 4943.35\n",
      "Epoch [323], train_loss: 5935.14 with loss1: 866.82, loss2: 125.02 and loss3: 4943.30\n",
      "Epoch [324], train_loss: 5930.98 with loss1: 863.80, loss2: 123.92 and loss3: 4943.25\n",
      "Epoch [325], train_loss: 5944.58 with loss1: 876.48, loss2: 124.90 and loss3: 4943.20\n",
      "Epoch [326], train_loss: 5940.96 with loss1: 873.83, loss2: 123.98 and loss3: 4943.15\n",
      "Epoch [327], train_loss: 5951.43 with loss1: 883.48, loss2: 124.84 and loss3: 4943.10\n",
      "Epoch [328], train_loss: 5942.85 with loss1: 876.38, loss2: 123.42 and loss3: 4943.05\n",
      "Epoch [329], train_loss: 5955.97 with loss1: 888.07, loss2: 124.89 and loss3: 4943.00\n",
      "Epoch [330], train_loss: 5945.87 with loss1: 879.40, loss2: 123.52 and loss3: 4942.95\n",
      "Epoch [331], train_loss: 5960.12 with loss1: 892.73, loss2: 124.49 and loss3: 4942.90\n",
      "Epoch [332], train_loss: 5948.42 with loss1: 881.84, loss2: 123.73 and loss3: 4942.85\n",
      "Epoch [333], train_loss: 5957.75 with loss1: 890.03, loss2: 124.92 and loss3: 4942.80\n",
      "Epoch [334], train_loss: 5945.95 with loss1: 880.40, loss2: 122.80 and loss3: 4942.75\n",
      "Epoch [335], train_loss: 5956.84 with loss1: 889.66, loss2: 124.48 and loss3: 4942.70\n",
      "Epoch [336], train_loss: 5942.30 with loss1: 876.12, loss2: 123.53 and loss3: 4942.65\n",
      "Epoch [337], train_loss: 5954.73 with loss1: 888.00, loss2: 124.13 and loss3: 4942.60\n",
      "Epoch [338], train_loss: 5937.38 with loss1: 871.34, loss2: 123.49 and loss3: 4942.55\n",
      "Epoch [339], train_loss: 5945.42 with loss1: 878.84, loss2: 124.08 and loss3: 4942.50\n",
      "Epoch [340], train_loss: 5930.92 with loss1: 865.42, loss2: 123.04 and loss3: 4942.45\n",
      "Epoch [341], train_loss: 5936.93 with loss1: 870.70, loss2: 123.83 and loss3: 4942.40\n",
      "Epoch [342], train_loss: 5920.37 with loss1: 854.68, loss2: 123.34 and loss3: 4942.35\n",
      "Epoch [343], train_loss: 5924.43 with loss1: 858.13, loss2: 124.01 and loss3: 4942.30\n",
      "Epoch [344], train_loss: 5914.75 with loss1: 849.53, loss2: 122.97 and loss3: 4942.25\n",
      "Epoch [345], train_loss: 5913.81 with loss1: 848.04, loss2: 123.57 and loss3: 4942.20\n",
      "Epoch [346], train_loss: 5900.22 with loss1: 835.61, loss2: 122.46 and loss3: 4942.15\n",
      "Epoch [347], train_loss: 5904.49 with loss1: 839.20, loss2: 123.20 and loss3: 4942.10\n",
      "Epoch [348], train_loss: 5893.95 with loss1: 829.18, loss2: 122.72 and loss3: 4942.05\n",
      "Epoch [349], train_loss: 5895.60 with loss1: 830.38, loss2: 123.22 and loss3: 4942.00\n",
      "Epoch [350], train_loss: 5883.97 with loss1: 819.83, loss2: 122.19 and loss3: 4941.95\n",
      "Epoch [351], train_loss: 5885.65 with loss1: 820.75, loss2: 123.00 and loss3: 4941.90\n",
      "Epoch [352], train_loss: 5875.04 with loss1: 811.07, loss2: 122.12 and loss3: 4941.85\n",
      "Epoch [353], train_loss: 5878.91 with loss1: 814.28, loss2: 122.83 and loss3: 4941.80\n",
      "Epoch [354], train_loss: 5871.61 with loss1: 807.45, loss2: 122.41 and loss3: 4941.75\n",
      "Epoch [355], train_loss: 5872.37 with loss1: 808.01, loss2: 122.66 and loss3: 4941.70\n",
      "Epoch [356], train_loss: 5863.80 with loss1: 800.12, loss2: 122.03 and loss3: 4941.65\n",
      "Epoch [357], train_loss: 5866.62 with loss1: 802.62, loss2: 122.39 and loss3: 4941.60\n",
      "Epoch [358], train_loss: 5860.42 with loss1: 797.08, loss2: 121.78 and loss3: 4941.55\n",
      "Epoch [359], train_loss: 5861.83 with loss1: 798.02, loss2: 122.31 and loss3: 4941.50\n",
      "Epoch [360], train_loss: 5856.11 with loss1: 792.97, loss2: 121.69 and loss3: 4941.45\n",
      "Epoch [361], train_loss: 5856.74 with loss1: 793.28, loss2: 122.06 and loss3: 4941.40\n",
      "Epoch [362], train_loss: 5854.59 with loss1: 791.59, loss2: 121.65 and loss3: 4941.35\n",
      "Epoch [363], train_loss: 5859.74 with loss1: 795.87, loss2: 122.57 and loss3: 4941.30\n",
      "Epoch [364], train_loss: 5851.52 with loss1: 788.73, loss2: 121.54 and loss3: 4941.25\n",
      "Epoch [365], train_loss: 5854.53 with loss1: 791.53, loss2: 121.80 and loss3: 4941.20\n",
      "Epoch [366], train_loss: 5850.17 with loss1: 787.74, loss2: 121.28 and loss3: 4941.15\n",
      "Epoch [367], train_loss: 5852.72 with loss1: 789.73, loss2: 121.89 and loss3: 4941.10\n",
      "Epoch [368], train_loss: 5850.47 with loss1: 787.74, loss2: 121.68 and loss3: 4941.05\n",
      "Epoch [369], train_loss: 5853.87 with loss1: 791.02, loss2: 121.86 and loss3: 4941.00\n",
      "Epoch [370], train_loss: 5848.97 with loss1: 786.96, loss2: 121.06 and loss3: 4940.95\n",
      "Epoch [371], train_loss: 5855.06 with loss1: 793.05, loss2: 121.11 and loss3: 4940.90\n",
      "Epoch [372], train_loss: 5852.70 with loss1: 790.71, loss2: 121.15 and loss3: 4940.85\n",
      "Epoch [373], train_loss: 5858.76 with loss1: 796.55, loss2: 121.41 and loss3: 4940.80\n",
      "Epoch [374], train_loss: 5855.42 with loss1: 794.16, loss2: 120.51 and loss3: 4940.75\n",
      "Epoch [375], train_loss: 5858.82 with loss1: 797.16, loss2: 120.96 and loss3: 4940.70\n",
      "Epoch [376], train_loss: 5854.84 with loss1: 793.22, loss2: 120.97 and loss3: 4940.65\n",
      "Epoch [377], train_loss: 5860.72 with loss1: 799.07, loss2: 121.05 and loss3: 4940.60\n",
      "Epoch [378], train_loss: 5857.47 with loss1: 796.15, loss2: 120.77 and loss3: 4940.55\n",
      "Epoch [379], train_loss: 5864.04 with loss1: 802.55, loss2: 120.98 and loss3: 4940.50\n",
      "Epoch [380], train_loss: 5863.35 with loss1: 802.58, loss2: 120.33 and loss3: 4940.45\n",
      "Epoch [381], train_loss: 5870.57 with loss1: 809.09, loss2: 121.07 and loss3: 4940.40\n",
      "Epoch [382], train_loss: 5865.42 with loss1: 804.82, loss2: 120.25 and loss3: 4940.35\n",
      "Epoch [383], train_loss: 5871.61 with loss1: 810.31, loss2: 121.00 and loss3: 4940.30\n",
      "Epoch [384], train_loss: 5868.83 with loss1: 808.00, loss2: 120.59 and loss3: 4940.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [385], train_loss: 5876.09 with loss1: 815.31, loss2: 120.59 and loss3: 4940.20\n",
      "Epoch [386], train_loss: 5871.30 with loss1: 811.02, loss2: 120.14 and loss3: 4940.15\n",
      "Epoch [387], train_loss: 5880.07 with loss1: 819.31, loss2: 120.66 and loss3: 4940.10\n",
      "Epoch [388], train_loss: 5870.88 with loss1: 811.15, loss2: 119.68 and loss3: 4940.05\n",
      "Epoch [389], train_loss: 5879.66 with loss1: 818.90, loss2: 120.76 and loss3: 4940.00\n",
      "Epoch [390], train_loss: 5872.91 with loss1: 812.73, loss2: 120.23 and loss3: 4939.95\n",
      "Epoch [391], train_loss: 5878.94 with loss1: 818.49, loss2: 120.55 and loss3: 4939.90\n",
      "Epoch [392], train_loss: 5873.71 with loss1: 813.77, loss2: 120.09 and loss3: 4939.85\n",
      "Epoch [393], train_loss: 5881.57 with loss1: 821.86, loss2: 119.92 and loss3: 4939.80\n",
      "Epoch [394], train_loss: 5874.88 with loss1: 815.42, loss2: 119.71 and loss3: 4939.75\n",
      "Epoch [395], train_loss: 5879.71 with loss1: 819.80, loss2: 120.20 and loss3: 4939.70\n",
      "Epoch [396], train_loss: 5874.28 with loss1: 815.13, loss2: 119.50 and loss3: 4939.65\n",
      "Epoch [397], train_loss: 5879.79 with loss1: 819.89, loss2: 120.31 and loss3: 4939.60\n",
      "Epoch [398], train_loss: 5870.89 with loss1: 811.97, loss2: 119.37 and loss3: 4939.55\n",
      "Epoch [399], train_loss: 5877.31 with loss1: 817.73, loss2: 120.08 and loss3: 4939.50\n",
      "Epoch [400], train_loss: 5869.62 with loss1: 810.69, loss2: 119.49 and loss3: 4939.45\n",
      "Epoch [401], train_loss: 5878.11 with loss1: 818.84, loss2: 119.88 and loss3: 4939.40\n",
      "Epoch [402], train_loss: 5868.49 with loss1: 810.30, loss2: 118.85 and loss3: 4939.35\n",
      "Epoch [403], train_loss: 5875.87 with loss1: 817.13, loss2: 119.44 and loss3: 4939.30\n",
      "Epoch [404], train_loss: 5867.08 with loss1: 808.93, loss2: 118.91 and loss3: 4939.25\n",
      "Epoch [405], train_loss: 5871.48 with loss1: 812.57, loss2: 119.71 and loss3: 4939.20\n",
      "Epoch [406], train_loss: 5864.78 with loss1: 807.21, loss2: 118.43 and loss3: 4939.15\n",
      "Epoch [407], train_loss: 5871.39 with loss1: 813.15, loss2: 119.15 and loss3: 4939.10\n",
      "Epoch [408], train_loss: 5861.42 with loss1: 803.05, loss2: 119.32 and loss3: 4939.05\n",
      "Epoch [409], train_loss: 5867.87 with loss1: 809.79, loss2: 119.08 and loss3: 4939.00\n",
      "Epoch [410], train_loss: 5861.48 with loss1: 803.66, loss2: 118.88 and loss3: 4938.95\n",
      "Epoch [411], train_loss: 5863.14 with loss1: 805.65, loss2: 118.60 and loss3: 4938.90\n",
      "Epoch [412], train_loss: 5857.58 with loss1: 800.62, loss2: 118.12 and loss3: 4938.85\n",
      "Epoch [413], train_loss: 5861.96 with loss1: 803.58, loss2: 119.59 and loss3: 4938.79\n",
      "Epoch [414], train_loss: 5855.47 with loss1: 798.06, loss2: 118.67 and loss3: 4938.75\n",
      "Epoch [415], train_loss: 5861.80 with loss1: 803.94, loss2: 119.17 and loss3: 4938.69\n",
      "Epoch [416], train_loss: 5854.32 with loss1: 796.81, loss2: 118.86 and loss3: 4938.65\n",
      "Epoch [417], train_loss: 5859.51 with loss1: 802.27, loss2: 118.65 and loss3: 4938.59\n",
      "Epoch [418], train_loss: 5852.44 with loss1: 795.55, loss2: 118.35 and loss3: 4938.54\n",
      "Epoch [419], train_loss: 5854.54 with loss1: 796.93, loss2: 119.12 and loss3: 4938.49\n",
      "Epoch [420], train_loss: 5849.64 with loss1: 793.48, loss2: 117.72 and loss3: 4938.44\n",
      "Epoch [421], train_loss: 5855.58 with loss1: 798.38, loss2: 118.81 and loss3: 4938.39\n",
      "Epoch [422], train_loss: 5850.52 with loss1: 794.08, loss2: 118.10 and loss3: 4938.34\n",
      "Epoch [423], train_loss: 5855.77 with loss1: 798.41, loss2: 119.07 and loss3: 4938.29\n",
      "Epoch [424], train_loss: 5844.20 with loss1: 788.31, loss2: 117.65 and loss3: 4938.24\n",
      "Epoch [425], train_loss: 5850.72 with loss1: 794.41, loss2: 118.11 and loss3: 4938.19\n",
      "Epoch [426], train_loss: 5844.04 with loss1: 788.82, loss2: 117.08 and loss3: 4938.14\n",
      "Epoch [427], train_loss: 5848.75 with loss1: 792.36, loss2: 118.30 and loss3: 4938.09\n",
      "Epoch [428], train_loss: 5841.58 with loss1: 785.73, loss2: 117.81 and loss3: 4938.04\n",
      "Epoch [429], train_loss: 5849.89 with loss1: 793.73, loss2: 118.17 and loss3: 4937.99\n",
      "Epoch [430], train_loss: 5844.63 with loss1: 789.14, loss2: 117.55 and loss3: 4937.94\n",
      "Epoch [431], train_loss: 5846.33 with loss1: 790.59, loss2: 117.85 and loss3: 4937.89\n",
      "Epoch [432], train_loss: 5842.43 with loss1: 787.19, loss2: 117.40 and loss3: 4937.84\n",
      "Epoch [433], train_loss: 5843.40 with loss1: 787.59, loss2: 118.02 and loss3: 4937.79\n",
      "Epoch [434], train_loss: 5839.30 with loss1: 785.01, loss2: 116.55 and loss3: 4937.74\n",
      "Epoch [435], train_loss: 5844.53 with loss1: 788.96, loss2: 117.87 and loss3: 4937.69\n",
      "Epoch [436], train_loss: 5840.43 with loss1: 785.70, loss2: 117.08 and loss3: 4937.64\n",
      "Epoch [437], train_loss: 5847.55 with loss1: 791.82, loss2: 118.13 and loss3: 4937.59\n",
      "Epoch [438], train_loss: 5842.46 with loss1: 788.18, loss2: 116.74 and loss3: 4937.54\n",
      "Epoch [439], train_loss: 5846.78 with loss1: 791.87, loss2: 117.42 and loss3: 4937.49\n",
      "Epoch [440], train_loss: 5842.06 with loss1: 787.74, loss2: 116.88 and loss3: 4937.44\n",
      "Epoch [441], train_loss: 5846.24 with loss1: 791.38, loss2: 117.46 and loss3: 4937.39\n",
      "Epoch [442], train_loss: 5840.06 with loss1: 785.99, loss2: 116.72 and loss3: 4937.34\n",
      "Epoch [443], train_loss: 5845.99 with loss1: 791.59, loss2: 117.11 and loss3: 4937.29\n",
      "Epoch [444], train_loss: 5841.54 with loss1: 787.96, loss2: 116.34 and loss3: 4937.24\n",
      "Epoch [445], train_loss: 5848.26 with loss1: 794.01, loss2: 117.06 and loss3: 4937.19\n",
      "Epoch [446], train_loss: 5839.60 with loss1: 785.76, loss2: 116.70 and loss3: 4937.14\n",
      "Epoch [447], train_loss: 5848.64 with loss1: 794.24, loss2: 117.31 and loss3: 4937.09\n",
      "Epoch [448], train_loss: 5840.61 with loss1: 787.40, loss2: 116.17 and loss3: 4937.04\n",
      "Epoch [449], train_loss: 5843.99 with loss1: 790.11, loss2: 116.89 and loss3: 4936.99\n",
      "Epoch [450], train_loss: 5833.66 with loss1: 780.58, loss2: 116.14 and loss3: 4936.94\n",
      "Epoch [451], train_loss: 5840.57 with loss1: 786.54, loss2: 117.14 and loss3: 4936.89\n",
      "Epoch [452], train_loss: 5836.61 with loss1: 783.43, loss2: 116.34 and loss3: 4936.84\n",
      "Epoch [453], train_loss: 5841.22 with loss1: 787.80, loss2: 116.63 and loss3: 4936.79\n",
      "Epoch [454], train_loss: 5835.28 with loss1: 782.57, loss2: 115.97 and loss3: 4936.74\n",
      "Epoch [455], train_loss: 5843.46 with loss1: 790.31, loss2: 116.46 and loss3: 4936.69\n",
      "Epoch [456], train_loss: 5837.25 with loss1: 784.44, loss2: 116.17 and loss3: 4936.64\n",
      "Epoch [457], train_loss: 5843.08 with loss1: 790.10, loss2: 116.39 and loss3: 4936.59\n",
      "Epoch [458], train_loss: 5834.69 with loss1: 782.15, loss2: 116.00 and loss3: 4936.54\n",
      "Epoch [459], train_loss: 5838.47 with loss1: 785.52, loss2: 116.46 and loss3: 4936.49\n",
      "Epoch [460], train_loss: 5831.91 with loss1: 779.97, loss2: 115.50 and loss3: 4936.44\n",
      "Epoch [461], train_loss: 5837.13 with loss1: 784.39, loss2: 116.34 and loss3: 4936.39\n",
      "Epoch [462], train_loss: 5831.25 with loss1: 779.82, loss2: 115.09 and loss3: 4936.34\n",
      "Epoch [463], train_loss: 5837.58 with loss1: 784.58, loss2: 116.71 and loss3: 4936.29\n",
      "Epoch [464], train_loss: 5832.28 with loss1: 781.05, loss2: 115.00 and loss3: 4936.24\n",
      "Epoch [465], train_loss: 5835.97 with loss1: 782.76, loss2: 117.02 and loss3: 4936.19\n",
      "Epoch [466], train_loss: 5831.35 with loss1: 780.03, loss2: 115.18 and loss3: 4936.14\n",
      "Epoch [467], train_loss: 5836.81 with loss1: 784.33, loss2: 116.40 and loss3: 4936.09\n",
      "Epoch [468], train_loss: 5826.30 with loss1: 774.63, loss2: 115.63 and loss3: 4936.04\n",
      "Epoch [469], train_loss: 5830.49 with loss1: 778.43, loss2: 116.07 and loss3: 4935.99\n",
      "Epoch [470], train_loss: 5824.06 with loss1: 773.48, loss2: 114.65 and loss3: 4935.94\n",
      "Epoch [471], train_loss: 5827.96 with loss1: 776.51, loss2: 115.56 and loss3: 4935.89\n",
      "Epoch [472], train_loss: 5821.10 with loss1: 770.36, loss2: 114.90 and loss3: 4935.84\n",
      "Epoch [473], train_loss: 5826.18 with loss1: 774.81, loss2: 115.58 and loss3: 4935.79\n",
      "Epoch [474], train_loss: 5819.43 with loss1: 768.39, loss2: 115.30 and loss3: 4935.74\n",
      "Epoch [475], train_loss: 5822.69 with loss1: 771.58, loss2: 115.42 and loss3: 4935.69\n",
      "Epoch [476], train_loss: 5818.94 with loss1: 768.58, loss2: 114.72 and loss3: 4935.64\n",
      "Epoch [477], train_loss: 5824.27 with loss1: 773.19, loss2: 115.49 and loss3: 4935.59\n",
      "Epoch [478], train_loss: 5817.97 with loss1: 767.29, loss2: 115.14 and loss3: 4935.54\n",
      "Epoch [479], train_loss: 5822.87 with loss1: 771.94, loss2: 115.44 and loss3: 4935.49\n",
      "Epoch [480], train_loss: 5816.67 with loss1: 767.03, loss2: 114.20 and loss3: 4935.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [481], train_loss: 5822.88 with loss1: 772.17, loss2: 115.32 and loss3: 4935.39\n",
      "Epoch [482], train_loss: 5817.04 with loss1: 767.23, loss2: 114.47 and loss3: 4935.34\n",
      "Epoch [483], train_loss: 5823.31 with loss1: 772.88, loss2: 115.14 and loss3: 4935.29\n",
      "Epoch [484], train_loss: 5816.76 with loss1: 767.34, loss2: 114.18 and loss3: 4935.24\n",
      "Epoch [485], train_loss: 5828.32 with loss1: 777.99, loss2: 115.14 and loss3: 4935.19\n",
      "Epoch [486], train_loss: 5820.49 with loss1: 771.55, loss2: 113.80 and loss3: 4935.14\n",
      "Epoch [487], train_loss: 5828.06 with loss1: 778.24, loss2: 114.73 and loss3: 4935.09\n",
      "Epoch [488], train_loss: 5822.42 with loss1: 773.22, loss2: 114.16 and loss3: 4935.04\n",
      "Epoch [489], train_loss: 5832.33 with loss1: 782.62, loss2: 114.72 and loss3: 4934.99\n",
      "Epoch [490], train_loss: 5827.87 with loss1: 778.61, loss2: 114.32 and loss3: 4934.94\n",
      "Epoch [491], train_loss: 5838.11 with loss1: 788.38, loss2: 114.84 and loss3: 4934.89\n",
      "Epoch [492], train_loss: 5830.71 with loss1: 781.76, loss2: 114.11 and loss3: 4934.84\n",
      "Epoch [493], train_loss: 5840.68 with loss1: 790.99, loss2: 114.90 and loss3: 4934.79\n",
      "Epoch [494], train_loss: 5832.23 with loss1: 783.58, loss2: 113.92 and loss3: 4934.74\n",
      "Epoch [495], train_loss: 5842.09 with loss1: 792.71, loss2: 114.69 and loss3: 4934.69\n",
      "Epoch [496], train_loss: 5834.84 with loss1: 786.49, loss2: 113.72 and loss3: 4934.64\n",
      "Epoch [497], train_loss: 5839.65 with loss1: 790.75, loss2: 114.31 and loss3: 4934.59\n",
      "Epoch [498], train_loss: 5833.68 with loss1: 786.04, loss2: 113.10 and loss3: 4934.54\n",
      "Epoch [499], train_loss: 5841.70 with loss1: 792.79, loss2: 114.42 and loss3: 4934.49\n",
      "Epoch [500], train_loss: 5830.47 with loss1: 782.70, loss2: 113.34 and loss3: 4934.44\n",
      "Epoch [501], train_loss: 5836.66 with loss1: 788.23, loss2: 114.05 and loss3: 4934.39\n",
      "Epoch [502], train_loss: 5826.70 with loss1: 779.13, loss2: 113.23 and loss3: 4934.34\n",
      "Epoch [503], train_loss: 5825.55 with loss1: 777.77, loss2: 113.49 and loss3: 4934.29\n",
      "Epoch [504], train_loss: 5815.71 with loss1: 767.82, loss2: 113.65 and loss3: 4934.24\n",
      "Epoch [505], train_loss: 5817.23 with loss1: 768.93, loss2: 114.12 and loss3: 4934.19\n",
      "Epoch [506], train_loss: 5805.89 with loss1: 758.57, loss2: 113.18 and loss3: 4934.14\n",
      "Epoch [507], train_loss: 5804.65 with loss1: 756.62, loss2: 113.94 and loss3: 4934.09\n",
      "Epoch [508], train_loss: 5796.15 with loss1: 748.52, loss2: 113.59 and loss3: 4934.04\n",
      "Epoch [509], train_loss: 5794.34 with loss1: 746.53, loss2: 113.82 and loss3: 4933.99\n",
      "Epoch [510], train_loss: 5787.01 with loss1: 739.83, loss2: 113.24 and loss3: 4933.94\n",
      "Epoch [511], train_loss: 5789.48 with loss1: 741.54, loss2: 114.06 and loss3: 4933.89\n",
      "Epoch [512], train_loss: 5779.01 with loss1: 732.68, loss2: 112.49 and loss3: 4933.84\n",
      "Epoch [513], train_loss: 5777.61 with loss1: 730.89, loss2: 112.94 and loss3: 4933.79\n",
      "Epoch [514], train_loss: 5772.22 with loss1: 725.96, loss2: 112.53 and loss3: 4933.74\n",
      "Epoch [515], train_loss: 5772.99 with loss1: 725.70, loss2: 113.60 and loss3: 4933.69\n",
      "Epoch [516], train_loss: 5769.21 with loss1: 722.79, loss2: 112.78 and loss3: 4933.64\n",
      "Epoch [517], train_loss: 5770.81 with loss1: 723.71, loss2: 113.51 and loss3: 4933.59\n",
      "Epoch [518], train_loss: 5766.94 with loss1: 721.07, loss2: 112.34 and loss3: 4933.54\n",
      "Epoch [519], train_loss: 5766.01 with loss1: 719.41, loss2: 113.11 and loss3: 4933.49\n",
      "Epoch [520], train_loss: 5765.80 with loss1: 719.86, loss2: 112.50 and loss3: 4933.44\n",
      "Epoch [521], train_loss: 5767.96 with loss1: 721.55, loss2: 113.02 and loss3: 4933.39\n",
      "Epoch [522], train_loss: 5764.90 with loss1: 719.35, loss2: 112.21 and loss3: 4933.34\n",
      "Epoch [523], train_loss: 5766.82 with loss1: 720.53, loss2: 113.01 and loss3: 4933.29\n",
      "Epoch [524], train_loss: 5767.14 with loss1: 721.83, loss2: 112.08 and loss3: 4933.24\n",
      "Epoch [525], train_loss: 5772.02 with loss1: 726.01, loss2: 112.82 and loss3: 4933.19\n",
      "Epoch [526], train_loss: 5771.98 with loss1: 726.59, loss2: 112.25 and loss3: 4933.14\n",
      "Epoch [527], train_loss: 5776.37 with loss1: 730.43, loss2: 112.85 and loss3: 4933.09\n",
      "Epoch [528], train_loss: 5777.34 with loss1: 732.58, loss2: 111.73 and loss3: 4933.04\n",
      "Epoch [529], train_loss: 5783.74 with loss1: 738.11, loss2: 112.64 and loss3: 4932.99\n",
      "Epoch [530], train_loss: 5782.47 with loss1: 737.91, loss2: 111.62 and loss3: 4932.94\n",
      "Epoch [531], train_loss: 5792.06 with loss1: 747.13, loss2: 112.04 and loss3: 4932.89\n",
      "Epoch [532], train_loss: 5790.19 with loss1: 745.51, loss2: 111.84 and loss3: 4932.84\n",
      "Epoch [533], train_loss: 5798.70 with loss1: 753.64, loss2: 112.27 and loss3: 4932.79\n",
      "Epoch [534], train_loss: 5797.59 with loss1: 753.81, loss2: 111.03 and loss3: 4932.74\n",
      "Epoch [535], train_loss: 5806.51 with loss1: 761.70, loss2: 112.12 and loss3: 4932.69\n",
      "Epoch [536], train_loss: 5803.99 with loss1: 759.80, loss2: 111.56 and loss3: 4932.64\n",
      "Epoch [537], train_loss: 5817.35 with loss1: 772.76, loss2: 112.01 and loss3: 4932.59\n",
      "Epoch [538], train_loss: 5815.88 with loss1: 771.96, loss2: 111.38 and loss3: 4932.54\n",
      "Epoch [539], train_loss: 5827.02 with loss1: 782.55, loss2: 111.99 and loss3: 4932.49\n",
      "Epoch [540], train_loss: 5824.48 with loss1: 780.47, loss2: 111.57 and loss3: 4932.44\n",
      "Epoch [541], train_loss: 5837.96 with loss1: 793.61, loss2: 111.96 and loss3: 4932.39\n",
      "Epoch [542], train_loss: 5833.49 with loss1: 790.04, loss2: 111.12 and loss3: 4932.34\n",
      "Epoch [543], train_loss: 5844.85 with loss1: 800.72, loss2: 111.84 and loss3: 4932.29\n",
      "Epoch [544], train_loss: 5836.95 with loss1: 793.90, loss2: 110.82 and loss3: 4932.24\n",
      "Epoch [545], train_loss: 5850.83 with loss1: 807.03, loss2: 111.62 and loss3: 4932.19\n",
      "Epoch [546], train_loss: 5844.73 with loss1: 800.90, loss2: 111.70 and loss3: 4932.14\n",
      "Epoch [547], train_loss: 5855.52 with loss1: 811.44, loss2: 112.00 and loss3: 4932.09\n",
      "Epoch [548], train_loss: 5848.85 with loss1: 805.58, loss2: 111.23 and loss3: 4932.04\n",
      "Epoch [549], train_loss: 5856.55 with loss1: 812.95, loss2: 111.62 and loss3: 4931.99\n",
      "Epoch [550], train_loss: 5845.40 with loss1: 802.39, loss2: 111.07 and loss3: 4931.94\n",
      "Epoch [551], train_loss: 5856.49 with loss1: 812.54, loss2: 112.07 and loss3: 4931.89\n",
      "Epoch [552], train_loss: 5843.90 with loss1: 800.55, loss2: 111.51 and loss3: 4931.84\n",
      "Epoch [553], train_loss: 5853.85 with loss1: 810.47, loss2: 111.59 and loss3: 4931.79\n",
      "Epoch [554], train_loss: 5835.85 with loss1: 793.87, loss2: 110.25 and loss3: 4931.74\n",
      "Epoch [555], train_loss: 5844.70 with loss1: 801.74, loss2: 111.28 and loss3: 4931.69\n",
      "Epoch [556], train_loss: 5831.72 with loss1: 789.91, loss2: 110.17 and loss3: 4931.64\n",
      "Epoch [557], train_loss: 5835.28 with loss1: 792.39, loss2: 111.31 and loss3: 4931.58\n",
      "Epoch [558], train_loss: 5823.88 with loss1: 781.89, loss2: 110.45 and loss3: 4931.54\n",
      "Epoch [559], train_loss: 5825.98 with loss1: 783.45, loss2: 111.05 and loss3: 4931.49\n",
      "Epoch [560], train_loss: 5813.95 with loss1: 772.15, loss2: 110.36 and loss3: 4931.44\n",
      "Epoch [561], train_loss: 5818.75 with loss1: 776.47, loss2: 110.90 and loss3: 4931.39\n",
      "Epoch [562], train_loss: 5807.47 with loss1: 765.87, loss2: 110.27 and loss3: 4931.33\n",
      "Epoch [563], train_loss: 5809.12 with loss1: 767.02, loss2: 110.81 and loss3: 4931.29\n",
      "Epoch [564], train_loss: 5802.24 with loss1: 760.66, loss2: 110.34 and loss3: 4931.24\n",
      "Epoch [565], train_loss: 5801.27 with loss1: 759.44, loss2: 110.65 and loss3: 4931.19\n",
      "Epoch [566], train_loss: 5789.91 with loss1: 748.68, loss2: 110.10 and loss3: 4931.14\n",
      "Epoch [567], train_loss: 5792.34 with loss1: 750.62, loss2: 110.63 and loss3: 4931.08\n",
      "Epoch [568], train_loss: 5783.69 with loss1: 742.56, loss2: 110.09 and loss3: 4931.04\n",
      "Epoch [569], train_loss: 5786.26 with loss1: 744.70, loss2: 110.58 and loss3: 4930.98\n",
      "Epoch [570], train_loss: 5779.84 with loss1: 739.03, loss2: 109.87 and loss3: 4930.93\n",
      "Epoch [571], train_loss: 5779.02 with loss1: 738.14, loss2: 110.00 and loss3: 4930.88\n",
      "Epoch [572], train_loss: 5771.06 with loss1: 730.58, loss2: 109.64 and loss3: 4930.83\n",
      "Epoch [573], train_loss: 5776.10 with loss1: 735.13, loss2: 110.18 and loss3: 4930.79\n",
      "Epoch [574], train_loss: 5765.88 with loss1: 725.28, loss2: 109.87 and loss3: 4930.73\n",
      "Epoch [575], train_loss: 5770.29 with loss1: 729.30, loss2: 110.31 and loss3: 4930.68\n",
      "Epoch [576], train_loss: 5764.29 with loss1: 723.58, loss2: 110.08 and loss3: 4930.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [577], train_loss: 5768.09 with loss1: 727.39, loss2: 110.11 and loss3: 4930.58\n",
      "Epoch [578], train_loss: 5761.59 with loss1: 721.32, loss2: 109.73 and loss3: 4930.53\n",
      "Epoch [579], train_loss: 5768.12 with loss1: 727.56, loss2: 110.08 and loss3: 4930.48\n",
      "Epoch [580], train_loss: 5758.85 with loss1: 719.11, loss2: 109.30 and loss3: 4930.43\n",
      "Epoch [581], train_loss: 5767.12 with loss1: 726.41, loss2: 110.33 and loss3: 4930.38\n",
      "Epoch [582], train_loss: 5759.22 with loss1: 719.62, loss2: 109.26 and loss3: 4930.33\n",
      "Epoch [583], train_loss: 5761.71 with loss1: 722.01, loss2: 109.41 and loss3: 4930.28\n",
      "Epoch [584], train_loss: 5756.38 with loss1: 717.06, loss2: 109.08 and loss3: 4930.23\n",
      "Epoch [585], train_loss: 5761.29 with loss1: 721.37, loss2: 109.74 and loss3: 4930.18\n",
      "Epoch [586], train_loss: 5757.02 with loss1: 718.12, loss2: 108.77 and loss3: 4930.13\n",
      "Epoch [587], train_loss: 5760.92 with loss1: 721.24, loss2: 109.60 and loss3: 4930.08\n",
      "Epoch [588], train_loss: 5756.99 with loss1: 718.02, loss2: 108.93 and loss3: 4930.03\n",
      "Epoch [589], train_loss: 5761.94 with loss1: 722.82, loss2: 109.13 and loss3: 4929.98\n",
      "Epoch [590], train_loss: 5757.54 with loss1: 718.97, loss2: 108.63 and loss3: 4929.93\n",
      "Epoch [591], train_loss: 5762.39 with loss1: 723.00, loss2: 109.50 and loss3: 4929.88\n",
      "Epoch [592], train_loss: 5759.52 with loss1: 720.66, loss2: 109.02 and loss3: 4929.83\n",
      "Epoch [593], train_loss: 5764.62 with loss1: 725.39, loss2: 109.45 and loss3: 4929.78\n",
      "Epoch [594], train_loss: 5760.88 with loss1: 722.53, loss2: 108.61 and loss3: 4929.73\n",
      "Epoch [595], train_loss: 5764.53 with loss1: 725.74, loss2: 109.10 and loss3: 4929.68\n",
      "Epoch [596], train_loss: 5761.04 with loss1: 722.92, loss2: 108.48 and loss3: 4929.63\n",
      "Epoch [597], train_loss: 5765.09 with loss1: 726.70, loss2: 108.81 and loss3: 4929.58\n",
      "Epoch [598], train_loss: 5762.49 with loss1: 724.35, loss2: 108.61 and loss3: 4929.53\n",
      "Epoch [599], train_loss: 5769.73 with loss1: 731.19, loss2: 109.06 and loss3: 4929.48\n",
      "Epoch [600], train_loss: 5763.33 with loss1: 725.78, loss2: 108.11 and loss3: 4929.43\n",
      "Epoch [601], train_loss: 5771.64 with loss1: 733.36, loss2: 108.89 and loss3: 4929.38\n",
      "Epoch [602], train_loss: 5766.19 with loss1: 728.63, loss2: 108.23 and loss3: 4929.33\n",
      "Epoch [603], train_loss: 5771.10 with loss1: 733.09, loss2: 108.73 and loss3: 4929.28\n",
      "Epoch [604], train_loss: 5763.82 with loss1: 726.31, loss2: 108.27 and loss3: 4929.23\n",
      "Epoch [605], train_loss: 5772.40 with loss1: 734.19, loss2: 109.02 and loss3: 4929.18\n",
      "Epoch [606], train_loss: 5762.68 with loss1: 725.73, loss2: 107.82 and loss3: 4929.13\n",
      "Epoch [607], train_loss: 5770.26 with loss1: 732.43, loss2: 108.75 and loss3: 4929.08\n",
      "Epoch [608], train_loss: 5767.09 with loss1: 729.34, loss2: 108.71 and loss3: 4929.03\n",
      "Epoch [609], train_loss: 5772.70 with loss1: 734.70, loss2: 109.02 and loss3: 4928.98\n",
      "Epoch [610], train_loss: 5763.21 with loss1: 726.50, loss2: 107.77 and loss3: 4928.93\n",
      "Epoch [611], train_loss: 5771.11 with loss1: 733.59, loss2: 108.63 and loss3: 4928.88\n",
      "Epoch [612], train_loss: 5765.47 with loss1: 728.41, loss2: 108.23 and loss3: 4928.83\n",
      "Epoch [613], train_loss: 5769.88 with loss1: 732.60, loss2: 108.49 and loss3: 4928.78\n",
      "Epoch [614], train_loss: 5763.28 with loss1: 726.82, loss2: 107.73 and loss3: 4928.73\n",
      "Epoch [615], train_loss: 5768.22 with loss1: 731.66, loss2: 107.87 and loss3: 4928.68\n",
      "Epoch [616], train_loss: 5761.42 with loss1: 725.23, loss2: 107.56 and loss3: 4928.63\n",
      "Epoch [617], train_loss: 5765.70 with loss1: 729.10, loss2: 108.02 and loss3: 4928.58\n",
      "Epoch [618], train_loss: 5759.49 with loss1: 723.49, loss2: 107.47 and loss3: 4928.53\n",
      "Epoch [619], train_loss: 5763.96 with loss1: 727.12, loss2: 108.35 and loss3: 4928.48\n",
      "Epoch [620], train_loss: 5758.55 with loss1: 722.66, loss2: 107.45 and loss3: 4928.43\n",
      "Epoch [621], train_loss: 5760.23 with loss1: 724.35, loss2: 107.50 and loss3: 4928.38\n",
      "Epoch [622], train_loss: 5756.17 with loss1: 720.69, loss2: 107.14 and loss3: 4928.33\n",
      "Epoch [623], train_loss: 5760.64 with loss1: 723.98, loss2: 108.38 and loss3: 4928.28\n",
      "Epoch [624], train_loss: 5750.78 with loss1: 715.64, loss2: 106.91 and loss3: 4928.23\n",
      "Epoch [625], train_loss: 5757.14 with loss1: 721.05, loss2: 107.91 and loss3: 4928.18\n",
      "Epoch [626], train_loss: 5749.73 with loss1: 714.74, loss2: 106.86 and loss3: 4928.13\n",
      "Epoch [627], train_loss: 5756.09 with loss1: 720.47, loss2: 107.54 and loss3: 4928.08\n",
      "Epoch [628], train_loss: 5749.01 with loss1: 714.07, loss2: 106.90 and loss3: 4928.03\n",
      "Epoch [629], train_loss: 5752.81 with loss1: 717.52, loss2: 107.30 and loss3: 4927.98\n",
      "Epoch [630], train_loss: 5746.36 with loss1: 711.39, loss2: 107.03 and loss3: 4927.93\n",
      "Epoch [631], train_loss: 5752.19 with loss1: 716.63, loss2: 107.67 and loss3: 4927.88\n",
      "Epoch [632], train_loss: 5744.63 with loss1: 710.13, loss2: 106.67 and loss3: 4927.83\n",
      "Epoch [633], train_loss: 5749.90 with loss1: 715.03, loss2: 107.09 and loss3: 4927.78\n",
      "Epoch [634], train_loss: 5743.93 with loss1: 709.65, loss2: 106.55 and loss3: 4927.73\n",
      "Epoch [635], train_loss: 5751.08 with loss1: 716.35, loss2: 107.05 and loss3: 4927.68\n",
      "Epoch [636], train_loss: 5743.95 with loss1: 710.38, loss2: 105.94 and loss3: 4927.63\n",
      "Epoch [637], train_loss: 5750.68 with loss1: 715.87, loss2: 107.23 and loss3: 4927.58\n",
      "Epoch [638], train_loss: 5747.70 with loss1: 713.52, loss2: 106.65 and loss3: 4927.53\n",
      "Epoch [639], train_loss: 5749.24 with loss1: 715.14, loss2: 106.62 and loss3: 4927.48\n",
      "Epoch [640], train_loss: 5747.16 with loss1: 713.49, loss2: 106.24 and loss3: 4927.43\n",
      "Epoch [641], train_loss: 5750.59 with loss1: 716.25, loss2: 106.95 and loss3: 4927.38\n",
      "Epoch [642], train_loss: 5747.22 with loss1: 713.62, loss2: 106.26 and loss3: 4927.33\n",
      "Epoch [643], train_loss: 5754.13 with loss1: 720.12, loss2: 106.72 and loss3: 4927.28\n",
      "Epoch [644], train_loss: 5748.65 with loss1: 715.38, loss2: 106.03 and loss3: 4927.23\n",
      "Epoch [645], train_loss: 5753.81 with loss1: 719.11, loss2: 107.51 and loss3: 4927.18\n",
      "Epoch [646], train_loss: 5747.54 with loss1: 714.71, loss2: 105.69 and loss3: 4927.13\n",
      "Epoch [647], train_loss: 5754.30 with loss1: 719.81, loss2: 107.40 and loss3: 4927.08\n",
      "Epoch [648], train_loss: 5748.89 with loss1: 716.31, loss2: 105.54 and loss3: 4927.03\n",
      "Epoch [649], train_loss: 5754.73 with loss1: 721.58, loss2: 106.17 and loss3: 4926.98\n",
      "Epoch [650], train_loss: 5751.56 with loss1: 718.96, loss2: 105.67 and loss3: 4926.93\n",
      "Epoch [651], train_loss: 5758.77 with loss1: 725.50, loss2: 106.38 and loss3: 4926.88\n",
      "Epoch [652], train_loss: 5755.24 with loss1: 722.79, loss2: 105.62 and loss3: 4926.83\n",
      "Epoch [653], train_loss: 5758.05 with loss1: 724.90, loss2: 106.37 and loss3: 4926.78\n",
      "Epoch [654], train_loss: 5753.94 with loss1: 721.23, loss2: 105.98 and loss3: 4926.73\n",
      "Epoch [655], train_loss: 5760.91 with loss1: 728.03, loss2: 106.20 and loss3: 4926.68\n",
      "Epoch [656], train_loss: 5753.37 with loss1: 721.33, loss2: 105.41 and loss3: 4926.63\n",
      "Epoch [657], train_loss: 5759.94 with loss1: 727.46, loss2: 105.90 and loss3: 4926.58\n",
      "Epoch [658], train_loss: 5753.84 with loss1: 721.23, loss2: 106.08 and loss3: 4926.53\n",
      "Epoch [659], train_loss: 5759.15 with loss1: 726.49, loss2: 106.17 and loss3: 4926.48\n",
      "Epoch [660], train_loss: 5751.96 with loss1: 720.14, loss2: 105.38 and loss3: 4926.43\n",
      "Epoch [661], train_loss: 5757.70 with loss1: 725.14, loss2: 106.18 and loss3: 4926.38\n",
      "Epoch [662], train_loss: 5752.49 with loss1: 720.67, loss2: 105.48 and loss3: 4926.33\n",
      "Epoch [663], train_loss: 5755.48 with loss1: 723.09, loss2: 106.11 and loss3: 4926.28\n",
      "Epoch [664], train_loss: 5746.80 with loss1: 715.43, loss2: 105.14 and loss3: 4926.23\n",
      "Epoch [665], train_loss: 5753.69 with loss1: 721.55, loss2: 105.96 and loss3: 4926.18\n",
      "Epoch [666], train_loss: 5746.93 with loss1: 715.28, loss2: 105.52 and loss3: 4926.13\n",
      "Epoch [667], train_loss: 5752.33 with loss1: 720.92, loss2: 105.33 and loss3: 4926.08\n",
      "Epoch [668], train_loss: 5748.68 with loss1: 717.47, loss2: 105.18 and loss3: 4926.03\n",
      "Epoch [669], train_loss: 5753.65 with loss1: 721.35, loss2: 106.31 and loss3: 4925.98\n",
      "Epoch [670], train_loss: 5748.43 with loss1: 717.70, loss2: 104.80 and loss3: 4925.93\n",
      "Epoch [671], train_loss: 5752.60 with loss1: 721.28, loss2: 105.44 and loss3: 4925.88\n",
      "Epoch [672], train_loss: 5748.17 with loss1: 717.68, loss2: 104.66 and loss3: 4925.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [673], train_loss: 5752.08 with loss1: 721.03, loss2: 105.27 and loss3: 4925.78\n",
      "Epoch [674], train_loss: 5746.81 with loss1: 716.26, loss2: 104.82 and loss3: 4925.73\n",
      "Epoch [675], train_loss: 5751.16 with loss1: 720.34, loss2: 105.13 and loss3: 4925.68\n",
      "Epoch [676], train_loss: 5744.75 with loss1: 714.10, loss2: 105.01 and loss3: 4925.63\n",
      "Epoch [677], train_loss: 5748.01 with loss1: 717.34, loss2: 105.09 and loss3: 4925.58\n",
      "Epoch [678], train_loss: 5743.25 with loss1: 712.54, loss2: 105.18 and loss3: 4925.53\n",
      "Epoch [679], train_loss: 5745.23 with loss1: 714.93, loss2: 104.82 and loss3: 4925.48\n",
      "Epoch [680], train_loss: 5741.29 with loss1: 711.40, loss2: 104.46 and loss3: 4925.43\n",
      "Epoch [681], train_loss: 5745.20 with loss1: 714.57, loss2: 105.24 and loss3: 4925.38\n",
      "Epoch [682], train_loss: 5738.20 with loss1: 708.68, loss2: 104.19 and loss3: 4925.33\n",
      "Epoch [683], train_loss: 5742.57 with loss1: 712.18, loss2: 105.11 and loss3: 4925.28\n",
      "Epoch [684], train_loss: 5737.31 with loss1: 707.85, loss2: 104.23 and loss3: 4925.23\n",
      "Epoch [685], train_loss: 5743.44 with loss1: 713.19, loss2: 105.06 and loss3: 4925.18\n",
      "Epoch [686], train_loss: 5736.98 with loss1: 708.05, loss2: 103.79 and loss3: 4925.13\n",
      "Epoch [687], train_loss: 5743.48 with loss1: 713.79, loss2: 104.61 and loss3: 4925.08\n",
      "Epoch [688], train_loss: 5735.68 with loss1: 706.38, loss2: 104.26 and loss3: 4925.03\n",
      "Epoch [689], train_loss: 5741.54 with loss1: 711.67, loss2: 104.89 and loss3: 4924.98\n",
      "Epoch [690], train_loss: 5735.76 with loss1: 707.10, loss2: 103.73 and loss3: 4924.93\n",
      "Epoch [691], train_loss: 5744.13 with loss1: 713.96, loss2: 105.29 and loss3: 4924.88\n",
      "Epoch [692], train_loss: 5736.44 with loss1: 707.42, loss2: 104.19 and loss3: 4924.83\n",
      "Epoch [693], train_loss: 5743.29 with loss1: 713.96, loss2: 104.54 and loss3: 4924.78\n",
      "Epoch [694], train_loss: 5737.50 with loss1: 708.74, loss2: 104.03 and loss3: 4924.73\n",
      "Epoch [695], train_loss: 5745.18 with loss1: 716.04, loss2: 104.46 and loss3: 4924.68\n",
      "Epoch [696], train_loss: 5741.55 with loss1: 713.26, loss2: 103.66 and loss3: 4924.63\n",
      "Epoch [697], train_loss: 5748.07 with loss1: 718.94, loss2: 104.55 and loss3: 4924.58\n",
      "Epoch [698], train_loss: 5741.11 with loss1: 712.92, loss2: 103.66 and loss3: 4924.53\n",
      "Epoch [699], train_loss: 5747.80 with loss1: 718.75, loss2: 104.57 and loss3: 4924.48\n",
      "Epoch [700], train_loss: 5740.49 with loss1: 712.82, loss2: 103.24 and loss3: 4924.43\n",
      "Epoch [701], train_loss: 5746.08 with loss1: 717.46, loss2: 104.24 and loss3: 4924.38\n",
      "Epoch [702], train_loss: 5741.26 with loss1: 713.16, loss2: 103.77 and loss3: 4924.33\n",
      "Epoch [703], train_loss: 5746.98 with loss1: 718.56, loss2: 104.13 and loss3: 4924.28\n",
      "Epoch [704], train_loss: 5738.62 with loss1: 711.00, loss2: 103.38 and loss3: 4924.23\n",
      "Epoch [705], train_loss: 5742.88 with loss1: 714.56, loss2: 104.14 and loss3: 4924.18\n",
      "Epoch [706], train_loss: 5735.98 with loss1: 708.54, loss2: 103.30 and loss3: 4924.13\n",
      "Epoch [707], train_loss: 5737.42 with loss1: 709.51, loss2: 103.83 and loss3: 4924.08\n",
      "Epoch [708], train_loss: 5728.19 with loss1: 701.09, loss2: 103.07 and loss3: 4924.03\n",
      "Epoch [709], train_loss: 5731.28 with loss1: 703.54, loss2: 103.76 and loss3: 4923.98\n",
      "Epoch [710], train_loss: 5723.06 with loss1: 695.99, loss2: 103.13 and loss3: 4923.93\n",
      "Epoch [711], train_loss: 5725.07 with loss1: 697.62, loss2: 103.57 and loss3: 4923.88\n",
      "Epoch [712], train_loss: 5716.79 with loss1: 689.64, loss2: 103.31 and loss3: 4923.83\n",
      "Epoch [713], train_loss: 5718.59 with loss1: 691.06, loss2: 103.75 and loss3: 4923.78\n",
      "Epoch [714], train_loss: 5713.28 with loss1: 686.24, loss2: 103.31 and loss3: 4923.73\n",
      "Epoch [715], train_loss: 5716.17 with loss1: 688.89, loss2: 103.59 and loss3: 4923.68\n",
      "Epoch [716], train_loss: 5707.59 with loss1: 681.24, loss2: 102.72 and loss3: 4923.63\n",
      "Epoch [717], train_loss: 5707.06 with loss1: 680.40, loss2: 103.08 and loss3: 4923.58\n",
      "Epoch [718], train_loss: 5702.34 with loss1: 675.99, loss2: 102.81 and loss3: 4923.53\n",
      "Epoch [719], train_loss: 5705.86 with loss1: 678.90, loss2: 103.47 and loss3: 4923.48\n",
      "Epoch [720], train_loss: 5699.39 with loss1: 673.50, loss2: 102.46 and loss3: 4923.43\n",
      "Epoch [721], train_loss: 5700.50 with loss1: 674.23, loss2: 102.88 and loss3: 4923.38\n",
      "Epoch [722], train_loss: 5697.71 with loss1: 671.84, loss2: 102.53 and loss3: 4923.33\n",
      "Epoch [723], train_loss: 5698.31 with loss1: 671.76, loss2: 103.26 and loss3: 4923.28\n",
      "Epoch [724], train_loss: 5692.62 with loss1: 666.75, loss2: 102.63 and loss3: 4923.23\n",
      "Epoch [725], train_loss: 5695.97 with loss1: 669.92, loss2: 102.87 and loss3: 4923.18\n",
      "Epoch [726], train_loss: 5695.40 with loss1: 669.64, loss2: 102.63 and loss3: 4923.13\n",
      "Epoch [727], train_loss: 5696.20 with loss1: 670.15, loss2: 102.96 and loss3: 4923.08\n",
      "Epoch [728], train_loss: 5689.51 with loss1: 664.13, loss2: 102.35 and loss3: 4923.03\n",
      "Epoch [729], train_loss: 5693.04 with loss1: 667.03, loss2: 103.03 and loss3: 4922.98\n",
      "Epoch [730], train_loss: 5690.41 with loss1: 665.27, loss2: 102.20 and loss3: 4922.93\n",
      "Epoch [731], train_loss: 5692.14 with loss1: 666.42, loss2: 102.84 and loss3: 4922.88\n",
      "Epoch [732], train_loss: 5690.25 with loss1: 665.28, loss2: 102.13 and loss3: 4922.83\n",
      "Epoch [733], train_loss: 5693.24 with loss1: 667.65, loss2: 102.81 and loss3: 4922.78\n",
      "Epoch [734], train_loss: 5692.14 with loss1: 667.02, loss2: 102.39 and loss3: 4922.73\n",
      "Epoch [735], train_loss: 5697.04 with loss1: 671.60, loss2: 102.76 and loss3: 4922.68\n",
      "Epoch [736], train_loss: 5695.48 with loss1: 670.85, loss2: 101.99 and loss3: 4922.63\n",
      "Epoch [737], train_loss: 5701.23 with loss1: 676.02, loss2: 102.62 and loss3: 4922.58\n",
      "Epoch [738], train_loss: 5700.06 with loss1: 675.60, loss2: 101.93 and loss3: 4922.53\n",
      "Epoch [739], train_loss: 5703.57 with loss1: 678.37, loss2: 102.72 and loss3: 4922.48\n",
      "Epoch [740], train_loss: 5699.89 with loss1: 675.61, loss2: 101.85 and loss3: 4922.44\n",
      "Epoch [741], train_loss: 5707.23 with loss1: 682.19, loss2: 102.66 and loss3: 4922.38\n",
      "Epoch [742], train_loss: 5706.82 with loss1: 682.86, loss2: 101.63 and loss3: 4922.33\n",
      "Epoch [743], train_loss: 5710.48 with loss1: 685.55, loss2: 102.65 and loss3: 4922.28\n",
      "Epoch [744], train_loss: 5708.82 with loss1: 684.67, loss2: 101.91 and loss3: 4922.23\n",
      "Epoch [745], train_loss: 5716.27 with loss1: 692.19, loss2: 101.89 and loss3: 4922.19\n",
      "Epoch [746], train_loss: 5711.97 with loss1: 688.24, loss2: 101.59 and loss3: 4922.13\n",
      "Epoch [747], train_loss: 5723.80 with loss1: 699.53, loss2: 102.19 and loss3: 4922.08\n",
      "Epoch [748], train_loss: 5719.52 with loss1: 695.98, loss2: 101.51 and loss3: 4922.04\n",
      "Epoch [749], train_loss: 5729.44 with loss1: 705.86, loss2: 101.59 and loss3: 4921.98\n",
      "Epoch [750], train_loss: 5725.89 with loss1: 702.45, loss2: 101.50 and loss3: 4921.94\n",
      "Epoch [751], train_loss: 5734.11 with loss1: 709.93, loss2: 102.30 and loss3: 4921.88\n",
      "Epoch [752], train_loss: 5731.13 with loss1: 708.01, loss2: 101.28 and loss3: 4921.83\n",
      "Epoch [753], train_loss: 5743.43 with loss1: 718.94, loss2: 102.70 and loss3: 4921.79\n",
      "Epoch [754], train_loss: 5737.19 with loss1: 714.67, loss2: 100.78 and loss3: 4921.73\n",
      "Epoch [755], train_loss: 5750.49 with loss1: 727.02, loss2: 101.79 and loss3: 4921.69\n",
      "Epoch [756], train_loss: 5742.63 with loss1: 719.67, loss2: 101.33 and loss3: 4921.63\n",
      "Epoch [757], train_loss: 5752.48 with loss1: 728.80, loss2: 102.10 and loss3: 4921.58\n",
      "Epoch [758], train_loss: 5744.78 with loss1: 722.51, loss2: 100.74 and loss3: 4921.54\n",
      "Epoch [759], train_loss: 5756.73 with loss1: 733.46, loss2: 101.79 and loss3: 4921.49\n",
      "Epoch [760], train_loss: 5749.91 with loss1: 727.64, loss2: 100.83 and loss3: 4921.44\n",
      "Epoch [761], train_loss: 5759.22 with loss1: 735.62, loss2: 102.21 and loss3: 4921.39\n",
      "Epoch [762], train_loss: 5748.62 with loss1: 726.45, loss2: 100.84 and loss3: 4921.33\n",
      "Epoch [763], train_loss: 5757.61 with loss1: 734.36, loss2: 101.96 and loss3: 4921.29\n",
      "Epoch [764], train_loss: 5748.14 with loss1: 726.25, loss2: 100.65 and loss3: 4921.24\n",
      "Epoch [765], train_loss: 5755.47 with loss1: 733.06, loss2: 101.23 and loss3: 4921.19\n",
      "Epoch [766], train_loss: 5744.82 with loss1: 722.60, loss2: 101.08 and loss3: 4921.14\n",
      "Epoch [767], train_loss: 5749.98 with loss1: 727.73, loss2: 101.17 and loss3: 4921.09\n",
      "Epoch [768], train_loss: 5740.97 with loss1: 719.14, loss2: 100.79 and loss3: 4921.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [769], train_loss: 5745.99 with loss1: 723.50, loss2: 101.50 and loss3: 4920.99\n",
      "Epoch [770], train_loss: 5733.13 with loss1: 711.87, loss2: 100.33 and loss3: 4920.94\n",
      "Epoch [771], train_loss: 5740.63 with loss1: 718.63, loss2: 101.12 and loss3: 4920.89\n",
      "Epoch [772], train_loss: 5727.26 with loss1: 705.85, loss2: 100.58 and loss3: 4920.84\n",
      "Epoch [773], train_loss: 5731.60 with loss1: 709.79, loss2: 101.02 and loss3: 4920.79\n",
      "Epoch [774], train_loss: 5720.75 with loss1: 700.05, loss2: 99.96 and loss3: 4920.74\n",
      "Epoch [775], train_loss: 5725.69 with loss1: 704.00, loss2: 101.01 and loss3: 4920.69\n",
      "Epoch [776], train_loss: 5717.56 with loss1: 696.31, loss2: 100.61 and loss3: 4920.64\n",
      "Epoch [777], train_loss: 5718.38 with loss1: 696.86, loss2: 100.94 and loss3: 4920.59\n",
      "Epoch [778], train_loss: 5709.61 with loss1: 688.91, loss2: 100.17 and loss3: 4920.54\n",
      "Epoch [779], train_loss: 5712.36 with loss1: 691.01, loss2: 100.87 and loss3: 4920.49\n",
      "Epoch [780], train_loss: 5703.72 with loss1: 682.78, loss2: 100.51 and loss3: 4920.44\n",
      "Epoch [781], train_loss: 5705.96 with loss1: 684.51, loss2: 101.07 and loss3: 4920.39\n",
      "Epoch [782], train_loss: 5698.53 with loss1: 677.93, loss2: 100.27 and loss3: 4920.34\n",
      "Epoch [783], train_loss: 5700.17 with loss1: 679.53, loss2: 100.36 and loss3: 4920.29\n",
      "Epoch [784], train_loss: 5694.50 with loss1: 673.77, loss2: 100.49 and loss3: 4920.24\n",
      "Epoch [785], train_loss: 5696.46 with loss1: 675.80, loss2: 100.48 and loss3: 4920.19\n",
      "Epoch [786], train_loss: 5689.18 with loss1: 669.36, loss2: 99.68 and loss3: 4920.14\n",
      "Epoch [787], train_loss: 5692.34 with loss1: 671.98, loss2: 100.28 and loss3: 4920.09\n",
      "Epoch [788], train_loss: 5685.20 with loss1: 665.33, loss2: 99.84 and loss3: 4920.04\n",
      "Epoch [789], train_loss: 5687.53 with loss1: 667.49, loss2: 100.05 and loss3: 4919.99\n",
      "Epoch [790], train_loss: 5683.00 with loss1: 663.29, loss2: 99.78 and loss3: 4919.94\n",
      "Epoch [791], train_loss: 5685.56 with loss1: 665.24, loss2: 100.43 and loss3: 4919.89\n",
      "Epoch [792], train_loss: 5681.39 with loss1: 661.84, loss2: 99.72 and loss3: 4919.84\n",
      "Epoch [793], train_loss: 5684.89 with loss1: 665.20, loss2: 99.90 and loss3: 4919.79\n",
      "Epoch [794], train_loss: 5678.36 with loss1: 659.02, loss2: 99.60 and loss3: 4919.74\n",
      "Epoch [795], train_loss: 5683.00 with loss1: 663.16, loss2: 100.16 and loss3: 4919.69\n",
      "Epoch [796], train_loss: 5681.42 with loss1: 662.52, loss2: 99.27 and loss3: 4919.64\n",
      "Epoch [797], train_loss: 5683.97 with loss1: 664.50, loss2: 99.89 and loss3: 4919.59\n",
      "Epoch [798], train_loss: 5678.14 with loss1: 659.47, loss2: 99.13 and loss3: 4919.54\n",
      "Epoch [799], train_loss: 5684.14 with loss1: 664.85, loss2: 99.80 and loss3: 4919.49\n",
      "Epoch [800], train_loss: 5679.74 with loss1: 660.72, loss2: 99.59 and loss3: 4919.44\n",
      "Epoch [801], train_loss: 5685.11 with loss1: 666.03, loss2: 99.69 and loss3: 4919.39\n",
      "Epoch [802], train_loss: 5680.93 with loss1: 662.24, loss2: 99.35 and loss3: 4919.34\n",
      "Epoch [803], train_loss: 5687.29 with loss1: 668.60, loss2: 99.40 and loss3: 4919.29\n",
      "Epoch [804], train_loss: 5682.13 with loss1: 663.19, loss2: 99.71 and loss3: 4919.24\n",
      "Epoch [805], train_loss: 5686.59 with loss1: 667.63, loss2: 99.77 and loss3: 4919.19\n",
      "Epoch [806], train_loss: 5682.02 with loss1: 663.45, loss2: 99.44 and loss3: 4919.14\n",
      "Epoch [807], train_loss: 5688.02 with loss1: 669.53, loss2: 99.40 and loss3: 4919.09\n",
      "Epoch [808], train_loss: 5682.96 with loss1: 664.83, loss2: 99.10 and loss3: 4919.04\n",
      "Epoch [809], train_loss: 5689.04 with loss1: 670.61, loss2: 99.45 and loss3: 4918.99\n",
      "Epoch [810], train_loss: 5686.56 with loss1: 668.37, loss2: 99.25 and loss3: 4918.94\n",
      "Epoch [811], train_loss: 5690.08 with loss1: 671.42, loss2: 99.78 and loss3: 4918.89\n",
      "Epoch [812], train_loss: 5686.38 with loss1: 668.58, loss2: 98.97 and loss3: 4918.84\n",
      "Epoch [813], train_loss: 5691.88 with loss1: 673.87, loss2: 99.22 and loss3: 4918.79\n",
      "Epoch [814], train_loss: 5686.04 with loss1: 668.51, loss2: 98.79 and loss3: 4918.74\n",
      "Epoch [815], train_loss: 5694.00 with loss1: 676.02, loss2: 99.29 and loss3: 4918.69\n",
      "Epoch [816], train_loss: 5688.96 with loss1: 671.82, loss2: 98.50 and loss3: 4918.64\n",
      "Epoch [817], train_loss: 5698.01 with loss1: 680.04, loss2: 99.38 and loss3: 4918.59\n",
      "Epoch [818], train_loss: 5691.75 with loss1: 674.83, loss2: 98.38 and loss3: 4918.54\n",
      "Epoch [819], train_loss: 5698.37 with loss1: 680.67, loss2: 99.22 and loss3: 4918.49\n",
      "Epoch [820], train_loss: 5693.75 with loss1: 676.60, loss2: 98.71 and loss3: 4918.44\n",
      "Epoch [821], train_loss: 5699.12 with loss1: 681.29, loss2: 99.44 and loss3: 4918.39\n",
      "Epoch [822], train_loss: 5692.86 with loss1: 676.11, loss2: 98.42 and loss3: 4918.34\n",
      "Epoch [823], train_loss: 5694.67 with loss1: 677.76, loss2: 98.62 and loss3: 4918.29\n",
      "Epoch [824], train_loss: 5690.60 with loss1: 674.12, loss2: 98.24 and loss3: 4918.24\n",
      "Epoch [825], train_loss: 5697.17 with loss1: 679.82, loss2: 99.16 and loss3: 4918.19\n",
      "Epoch [826], train_loss: 5689.64 with loss1: 672.94, loss2: 98.57 and loss3: 4918.14\n",
      "Epoch [827], train_loss: 5695.81 with loss1: 678.57, loss2: 99.15 and loss3: 4918.09\n",
      "Epoch [828], train_loss: 5689.41 with loss1: 672.72, loss2: 98.65 and loss3: 4918.04\n",
      "Epoch [829], train_loss: 5694.82 with loss1: 678.15, loss2: 98.68 and loss3: 4917.99\n",
      "Epoch [830], train_loss: 5687.34 with loss1: 671.24, loss2: 98.17 and loss3: 4917.94\n",
      "Epoch [831], train_loss: 5692.98 with loss1: 676.42, loss2: 98.68 and loss3: 4917.89\n",
      "Epoch [832], train_loss: 5685.61 with loss1: 669.50, loss2: 98.27 and loss3: 4917.84\n",
      "Epoch [833], train_loss: 5689.85 with loss1: 673.23, loss2: 98.83 and loss3: 4917.79\n",
      "Epoch [834], train_loss: 5684.77 with loss1: 669.12, loss2: 97.91 and loss3: 4917.74\n",
      "Epoch [835], train_loss: 5689.33 with loss1: 673.07, loss2: 98.58 and loss3: 4917.69\n",
      "Epoch [836], train_loss: 5682.21 with loss1: 666.24, loss2: 98.33 and loss3: 4917.64\n",
      "Epoch [837], train_loss: 5685.27 with loss1: 669.40, loss2: 98.28 and loss3: 4917.59\n",
      "Epoch [838], train_loss: 5678.66 with loss1: 663.27, loss2: 97.85 and loss3: 4917.54\n",
      "Epoch [839], train_loss: 5684.78 with loss1: 669.21, loss2: 98.08 and loss3: 4917.49\n",
      "Epoch [840], train_loss: 5676.35 with loss1: 661.44, loss2: 97.47 and loss3: 4917.44\n",
      "Epoch [841], train_loss: 5682.26 with loss1: 666.29, loss2: 98.58 and loss3: 4917.39\n",
      "Epoch [842], train_loss: 5675.78 with loss1: 660.87, loss2: 97.57 and loss3: 4917.34\n",
      "Epoch [843], train_loss: 5680.56 with loss1: 665.04, loss2: 98.24 and loss3: 4917.29\n",
      "Epoch [844], train_loss: 5675.84 with loss1: 661.20, loss2: 97.40 and loss3: 4917.24\n",
      "Epoch [845], train_loss: 5677.75 with loss1: 662.78, loss2: 97.78 and loss3: 4917.19\n",
      "Epoch [846], train_loss: 5672.87 with loss1: 658.23, loss2: 97.50 and loss3: 4917.14\n",
      "Epoch [847], train_loss: 5677.94 with loss1: 663.01, loss2: 97.84 and loss3: 4917.09\n",
      "Epoch [848], train_loss: 5672.04 with loss1: 657.06, loss2: 97.94 and loss3: 4917.04\n",
      "Epoch [849], train_loss: 5675.60 with loss1: 660.81, loss2: 97.80 and loss3: 4916.99\n",
      "Epoch [850], train_loss: 5672.25 with loss1: 658.31, loss2: 97.00 and loss3: 4916.94\n",
      "Epoch [851], train_loss: 5675.70 with loss1: 661.03, loss2: 97.78 and loss3: 4916.89\n",
      "Epoch [852], train_loss: 5671.64 with loss1: 657.60, loss2: 97.20 and loss3: 4916.84\n",
      "Epoch [853], train_loss: 5674.10 with loss1: 659.28, loss2: 98.02 and loss3: 4916.79\n",
      "Epoch [854], train_loss: 5669.09 with loss1: 655.48, loss2: 96.87 and loss3: 4916.74\n",
      "Epoch [855], train_loss: 5678.99 with loss1: 664.16, loss2: 98.14 and loss3: 4916.69\n",
      "Epoch [856], train_loss: 5671.09 with loss1: 657.71, loss2: 96.74 and loss3: 4916.64\n",
      "Epoch [857], train_loss: 5676.72 with loss1: 662.76, loss2: 97.37 and loss3: 4916.59\n",
      "Epoch [858], train_loss: 5670.62 with loss1: 657.28, loss2: 96.80 and loss3: 4916.54\n",
      "Epoch [859], train_loss: 5671.98 with loss1: 658.49, loss2: 97.00 and loss3: 4916.49\n",
      "Epoch [860], train_loss: 5668.65 with loss1: 655.28, loss2: 96.93 and loss3: 4916.44\n",
      "Epoch [861], train_loss: 5672.67 with loss1: 658.74, loss2: 97.54 and loss3: 4916.39\n",
      "Epoch [862], train_loss: 5667.21 with loss1: 654.17, loss2: 96.71 and loss3: 4916.34\n",
      "Epoch [863], train_loss: 5671.87 with loss1: 658.48, loss2: 97.10 and loss3: 4916.29\n",
      "Epoch [864], train_loss: 5667.10 with loss1: 654.00, loss2: 96.86 and loss3: 4916.24\n",
      "Epoch [865], train_loss: 5670.27 with loss1: 656.89, loss2: 97.19 and loss3: 4916.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [866], train_loss: 5663.84 with loss1: 651.16, loss2: 96.54 and loss3: 4916.14\n",
      "Epoch [867], train_loss: 5671.35 with loss1: 658.27, loss2: 96.98 and loss3: 4916.09\n",
      "Epoch [868], train_loss: 5667.92 with loss1: 655.63, loss2: 96.25 and loss3: 4916.04\n",
      "Epoch [869], train_loss: 5671.25 with loss1: 658.28, loss2: 96.97 and loss3: 4915.99\n",
      "Epoch [870], train_loss: 5667.39 with loss1: 654.46, loss2: 96.99 and loss3: 4915.94\n",
      "Epoch [871], train_loss: 5672.19 with loss1: 659.44, loss2: 96.86 and loss3: 4915.89\n",
      "Epoch [872], train_loss: 5669.69 with loss1: 657.33, loss2: 96.52 and loss3: 4915.84\n",
      "Epoch [873], train_loss: 5672.18 with loss1: 659.69, loss2: 96.69 and loss3: 4915.79\n",
      "Epoch [874], train_loss: 5667.97 with loss1: 656.16, loss2: 96.08 and loss3: 4915.74\n",
      "Epoch [875], train_loss: 5675.72 with loss1: 662.97, loss2: 97.06 and loss3: 4915.69\n",
      "Epoch [876], train_loss: 5668.67 with loss1: 656.92, loss2: 96.11 and loss3: 4915.64\n",
      "Epoch [877], train_loss: 5672.87 with loss1: 660.72, loss2: 96.56 and loss3: 4915.59\n",
      "Epoch [878], train_loss: 5669.08 with loss1: 657.35, loss2: 96.19 and loss3: 4915.54\n",
      "Epoch [879], train_loss: 5674.89 with loss1: 662.81, loss2: 96.59 and loss3: 4915.49\n",
      "Epoch [880], train_loss: 5669.60 with loss1: 657.43, loss2: 96.73 and loss3: 4915.44\n",
      "Epoch [881], train_loss: 5675.52 with loss1: 663.16, loss2: 96.97 and loss3: 4915.39\n",
      "Epoch [882], train_loss: 5672.39 with loss1: 661.23, loss2: 95.82 and loss3: 4915.34\n",
      "Epoch [883], train_loss: 5675.15 with loss1: 663.10, loss2: 96.75 and loss3: 4915.29\n",
      "Epoch [884], train_loss: 5671.51 with loss1: 660.06, loss2: 96.20 and loss3: 4915.24\n",
      "Epoch [885], train_loss: 5675.76 with loss1: 664.38, loss2: 96.19 and loss3: 4915.19\n",
      "Epoch [886], train_loss: 5670.22 with loss1: 659.37, loss2: 95.70 and loss3: 4915.14\n",
      "Epoch [887], train_loss: 5674.77 with loss1: 663.16, loss2: 96.52 and loss3: 4915.09\n",
      "Epoch [888], train_loss: 5668.32 with loss1: 657.53, loss2: 95.74 and loss3: 4915.04\n",
      "Epoch [889], train_loss: 5672.09 with loss1: 660.62, loss2: 96.48 and loss3: 4914.99\n",
      "Epoch [890], train_loss: 5669.39 with loss1: 658.36, loss2: 96.09 and loss3: 4914.94\n",
      "Epoch [891], train_loss: 5671.90 with loss1: 660.10, loss2: 96.91 and loss3: 4914.89\n",
      "Epoch [892], train_loss: 5665.70 with loss1: 655.45, loss2: 95.40 and loss3: 4914.84\n",
      "Epoch [893], train_loss: 5668.11 with loss1: 657.05, loss2: 96.26 and loss3: 4914.79\n",
      "Epoch [894], train_loss: 5662.37 with loss1: 652.11, loss2: 95.52 and loss3: 4914.74\n",
      "Epoch [895], train_loss: 5664.37 with loss1: 653.74, loss2: 95.94 and loss3: 4914.69\n",
      "Epoch [896], train_loss: 5658.54 with loss1: 648.40, loss2: 95.50 and loss3: 4914.64\n",
      "Epoch [897], train_loss: 5662.21 with loss1: 651.44, loss2: 96.17 and loss3: 4914.59\n",
      "Epoch [898], train_loss: 5653.69 with loss1: 643.69, loss2: 95.46 and loss3: 4914.54\n",
      "Epoch [899], train_loss: 5659.05 with loss1: 648.20, loss2: 96.36 and loss3: 4914.49\n",
      "Epoch [900], train_loss: 5651.61 with loss1: 641.61, loss2: 95.56 and loss3: 4914.44\n",
      "Epoch [901], train_loss: 5652.41 with loss1: 642.18, loss2: 95.83 and loss3: 4914.39\n",
      "Epoch [902], train_loss: 5650.09 with loss1: 640.19, loss2: 95.56 and loss3: 4914.34\n",
      "Epoch [903], train_loss: 5651.10 with loss1: 640.97, loss2: 95.83 and loss3: 4914.29\n",
      "Epoch [904], train_loss: 5645.82 with loss1: 636.11, loss2: 95.47 and loss3: 4914.24\n",
      "Epoch [905], train_loss: 5650.92 with loss1: 641.18, loss2: 95.55 and loss3: 4914.19\n",
      "Epoch [906], train_loss: 5643.38 with loss1: 634.14, loss2: 95.10 and loss3: 4914.14\n",
      "Epoch [907], train_loss: 5646.04 with loss1: 636.16, loss2: 95.79 and loss3: 4914.09\n",
      "Epoch [908], train_loss: 5641.73 with loss1: 632.73, loss2: 94.96 and loss3: 4914.04\n",
      "Epoch [909], train_loss: 5645.01 with loss1: 635.23, loss2: 95.79 and loss3: 4913.99\n",
      "Epoch [910], train_loss: 5640.74 with loss1: 631.33, loss2: 95.47 and loss3: 4913.94\n",
      "Epoch [911], train_loss: 5642.66 with loss1: 633.47, loss2: 95.29 and loss3: 4913.89\n",
      "Epoch [912], train_loss: 5640.70 with loss1: 631.83, loss2: 95.03 and loss3: 4913.84\n",
      "Epoch [913], train_loss: 5642.87 with loss1: 633.83, loss2: 95.24 and loss3: 4913.79\n",
      "Epoch [914], train_loss: 5641.87 with loss1: 633.63, loss2: 94.49 and loss3: 4913.74\n",
      "Epoch [915], train_loss: 5644.00 with loss1: 634.91, loss2: 95.40 and loss3: 4913.69\n",
      "Epoch [916], train_loss: 5641.84 with loss1: 633.57, loss2: 94.63 and loss3: 4913.64\n",
      "Epoch [917], train_loss: 5645.77 with loss1: 637.46, loss2: 94.71 and loss3: 4913.59\n",
      "Epoch [918], train_loss: 5642.40 with loss1: 634.52, loss2: 94.34 and loss3: 4913.54\n",
      "Epoch [919], train_loss: 5649.82 with loss1: 641.28, loss2: 95.05 and loss3: 4913.49\n",
      "Epoch [920], train_loss: 5645.02 with loss1: 637.18, loss2: 94.40 and loss3: 4913.44\n",
      "Epoch [921], train_loss: 5650.41 with loss1: 641.91, loss2: 95.11 and loss3: 4913.39\n",
      "Epoch [922], train_loss: 5647.85 with loss1: 640.08, loss2: 94.42 and loss3: 4913.35\n",
      "Epoch [923], train_loss: 5655.59 with loss1: 646.92, loss2: 95.38 and loss3: 4913.29\n",
      "Epoch [924], train_loss: 5652.70 with loss1: 645.22, loss2: 94.24 and loss3: 4913.25\n",
      "Epoch [925], train_loss: 5656.92 with loss1: 648.98, loss2: 94.74 and loss3: 4913.20\n",
      "Epoch [926], train_loss: 5656.54 with loss1: 649.34, loss2: 94.06 and loss3: 4913.15\n",
      "Epoch [927], train_loss: 5663.82 with loss1: 656.03, loss2: 94.69 and loss3: 4913.10\n",
      "Epoch [928], train_loss: 5660.00 with loss1: 652.26, loss2: 94.69 and loss3: 4913.05\n",
      "Epoch [929], train_loss: 5668.96 with loss1: 661.38, loss2: 94.59 and loss3: 4913.00\n",
      "Epoch [930], train_loss: 5666.73 with loss1: 659.90, loss2: 93.88 and loss3: 4912.95\n",
      "Epoch [931], train_loss: 5676.81 with loss1: 669.13, loss2: 94.78 and loss3: 4912.90\n",
      "Epoch [932], train_loss: 5675.41 with loss1: 668.27, loss2: 94.29 and loss3: 4912.85\n",
      "Epoch [933], train_loss: 5682.42 with loss1: 674.88, loss2: 94.74 and loss3: 4912.80\n",
      "Epoch [934], train_loss: 5676.35 with loss1: 669.38, loss2: 94.22 and loss3: 4912.75\n",
      "Epoch [935], train_loss: 5685.77 with loss1: 678.45, loss2: 94.63 and loss3: 4912.70\n",
      "Epoch [936], train_loss: 5680.46 with loss1: 673.67, loss2: 94.15 and loss3: 4912.65\n",
      "Epoch [937], train_loss: 5690.57 with loss1: 683.30, loss2: 94.67 and loss3: 4912.60\n",
      "Epoch [938], train_loss: 5682.92 with loss1: 676.32, loss2: 94.06 and loss3: 4912.55\n",
      "Epoch [939], train_loss: 5691.57 with loss1: 684.79, loss2: 94.28 and loss3: 4912.50\n",
      "Epoch [940], train_loss: 5684.64 with loss1: 678.61, loss2: 93.59 and loss3: 4912.45\n",
      "Epoch [941], train_loss: 5693.23 with loss1: 686.42, loss2: 94.42 and loss3: 4912.40\n",
      "Epoch [942], train_loss: 5685.01 with loss1: 678.90, loss2: 93.77 and loss3: 4912.35\n",
      "Epoch [943], train_loss: 5692.60 with loss1: 685.80, loss2: 94.51 and loss3: 4912.30\n",
      "Epoch [944], train_loss: 5681.71 with loss1: 675.62, loss2: 93.84 and loss3: 4912.25\n",
      "Epoch [945], train_loss: 5688.77 with loss1: 682.11, loss2: 94.46 and loss3: 4912.20\n",
      "Epoch [946], train_loss: 5680.61 with loss1: 675.08, loss2: 93.38 and loss3: 4912.15\n",
      "Epoch [947], train_loss: 5684.81 with loss1: 678.46, loss2: 94.26 and loss3: 4912.10\n",
      "Epoch [948], train_loss: 5675.98 with loss1: 670.29, loss2: 93.64 and loss3: 4912.05\n",
      "Epoch [949], train_loss: 5679.76 with loss1: 673.43, loss2: 94.33 and loss3: 4912.00\n",
      "Epoch [950], train_loss: 5670.00 with loss1: 664.52, loss2: 93.52 and loss3: 4911.95\n",
      "Epoch [951], train_loss: 5673.65 with loss1: 667.41, loss2: 94.34 and loss3: 4911.90\n",
      "Epoch [952], train_loss: 5661.71 with loss1: 656.38, loss2: 93.48 and loss3: 4911.85\n",
      "Epoch [953], train_loss: 5666.46 with loss1: 660.92, loss2: 93.75 and loss3: 4911.80\n",
      "Epoch [954], train_loss: 5654.40 with loss1: 649.22, loss2: 93.43 and loss3: 4911.75\n",
      "Epoch [955], train_loss: 5655.64 with loss1: 650.42, loss2: 93.52 and loss3: 4911.70\n",
      "Epoch [956], train_loss: 5649.18 with loss1: 643.95, loss2: 93.58 and loss3: 4911.65\n",
      "Epoch [957], train_loss: 5649.21 with loss1: 643.75, loss2: 93.87 and loss3: 4911.60\n",
      "Epoch [958], train_loss: 5642.46 with loss1: 637.61, loss2: 93.30 and loss3: 4911.55\n",
      "Epoch [959], train_loss: 5645.34 with loss1: 639.94, loss2: 93.90 and loss3: 4911.50\n",
      "Epoch [960], train_loss: 5637.18 with loss1: 632.33, loss2: 93.40 and loss3: 4911.45\n",
      "Epoch [961], train_loss: 5639.90 with loss1: 634.97, loss2: 93.53 and loss3: 4911.40\n",
      "Epoch [962], train_loss: 5634.77 with loss1: 630.38, loss2: 93.04 and loss3: 4911.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [963], train_loss: 5636.49 with loss1: 631.36, loss2: 93.84 and loss3: 4911.30\n",
      "Epoch [964], train_loss: 5629.85 with loss1: 625.42, loss2: 93.19 and loss3: 4911.25\n",
      "Epoch [965], train_loss: 5634.17 with loss1: 629.69, loss2: 93.27 and loss3: 4911.20\n",
      "Epoch [966], train_loss: 5626.81 with loss1: 622.60, loss2: 93.06 and loss3: 4911.15\n",
      "Epoch [967], train_loss: 5629.38 with loss1: 625.11, loss2: 93.17 and loss3: 4911.10\n",
      "Epoch [968], train_loss: 5624.86 with loss1: 620.46, loss2: 93.35 and loss3: 4911.05\n",
      "Epoch [969], train_loss: 5626.55 with loss1: 622.33, loss2: 93.22 and loss3: 4911.00\n",
      "Epoch [970], train_loss: 5622.35 with loss1: 618.50, loss2: 92.91 and loss3: 4910.95\n",
      "Epoch [971], train_loss: 5624.76 with loss1: 620.78, loss2: 93.08 and loss3: 4910.90\n",
      "Epoch [972], train_loss: 5620.68 with loss1: 617.18, loss2: 92.65 and loss3: 4910.85\n",
      "Epoch [973], train_loss: 5621.56 with loss1: 617.80, loss2: 92.96 and loss3: 4910.80\n",
      "Epoch [974], train_loss: 5620.43 with loss1: 617.44, loss2: 92.24 and loss3: 4910.75\n",
      "Epoch [975], train_loss: 5622.82 with loss1: 619.08, loss2: 93.05 and loss3: 4910.70\n",
      "Epoch [976], train_loss: 5619.66 with loss1: 616.08, loss2: 92.93 and loss3: 4910.65\n",
      "Epoch [977], train_loss: 5623.30 with loss1: 619.86, loss2: 92.84 and loss3: 4910.60\n",
      "Epoch [978], train_loss: 5620.82 with loss1: 617.38, loss2: 92.89 and loss3: 4910.55\n",
      "Epoch [979], train_loss: 5622.79 with loss1: 618.92, loss2: 93.37 and loss3: 4910.50\n",
      "Epoch [980], train_loss: 5618.86 with loss1: 616.11, loss2: 92.30 and loss3: 4910.45\n",
      "Epoch [981], train_loss: 5623.81 with loss1: 620.89, loss2: 92.52 and loss3: 4910.40\n",
      "Epoch [982], train_loss: 5621.76 with loss1: 618.93, loss2: 92.48 and loss3: 4910.35\n",
      "Epoch [983], train_loss: 5626.10 with loss1: 623.38, loss2: 92.42 and loss3: 4910.30\n",
      "Epoch [984], train_loss: 5620.75 with loss1: 618.15, loss2: 92.35 and loss3: 4910.25\n",
      "Epoch [985], train_loss: 5627.22 with loss1: 624.63, loss2: 92.39 and loss3: 4910.20\n",
      "Epoch [986], train_loss: 5622.74 with loss1: 620.64, loss2: 91.94 and loss3: 4910.15\n",
      "Epoch [987], train_loss: 5627.77 with loss1: 625.15, loss2: 92.52 and loss3: 4910.10\n",
      "Epoch [988], train_loss: 5624.14 with loss1: 622.38, loss2: 91.70 and loss3: 4910.05\n",
      "Epoch [989], train_loss: 5630.22 with loss1: 627.43, loss2: 92.79 and loss3: 4910.00\n",
      "Epoch [990], train_loss: 5626.21 with loss1: 624.14, loss2: 92.11 and loss3: 4909.95\n",
      "Epoch [991], train_loss: 5631.85 with loss1: 629.85, loss2: 92.10 and loss3: 4909.90\n",
      "Epoch [992], train_loss: 5629.46 with loss1: 627.28, loss2: 92.33 and loss3: 4909.85\n",
      "Epoch [993], train_loss: 5635.45 with loss1: 633.48, loss2: 92.17 and loss3: 4909.80\n",
      "Epoch [994], train_loss: 5630.02 with loss1: 628.29, loss2: 91.98 and loss3: 4909.75\n",
      "Epoch [995], train_loss: 5636.39 with loss1: 634.19, loss2: 92.50 and loss3: 4909.70\n",
      "Epoch [996], train_loss: 5633.16 with loss1: 631.49, loss2: 92.02 and loss3: 4909.65\n",
      "Epoch [997], train_loss: 5639.82 with loss1: 637.82, loss2: 92.40 and loss3: 4909.60\n",
      "Epoch [998], train_loss: 5635.37 with loss1: 634.30, loss2: 91.52 and loss3: 4909.55\n",
      "Epoch [999], train_loss: 5640.22 with loss1: 638.74, loss2: 91.98 and loss3: 4909.50\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# model version 3.5\n",
    "# with loss2 and loss3\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history = fit(epochs=1000, lr=5e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fb134b22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 5635.77 with loss1: 634.73, loss2: 91.59 and loss3: 4909.45\n",
      "Epoch [1], train_loss: 5641.42 with loss1: 639.91, loss2: 92.11 and loss3: 4909.40\n",
      "Epoch [2], train_loss: 5635.41 with loss1: 634.45, loss2: 91.60 and loss3: 4909.35\n",
      "Epoch [3], train_loss: 5641.43 with loss1: 640.45, loss2: 91.68 and loss3: 4909.30\n",
      "Epoch [4], train_loss: 5636.75 with loss1: 636.14, loss2: 91.35 and loss3: 4909.25\n",
      "Epoch [5], train_loss: 5641.24 with loss1: 639.87, loss2: 92.17 and loss3: 4909.20\n",
      "Epoch [6], train_loss: 5635.51 with loss1: 635.02, loss2: 91.33 and loss3: 4909.15\n",
      "Epoch [7], train_loss: 5640.09 with loss1: 639.14, loss2: 91.85 and loss3: 4909.10\n",
      "Epoch [8], train_loss: 5635.19 with loss1: 634.29, loss2: 91.85 and loss3: 4909.05\n",
      "Epoch [9], train_loss: 5639.52 with loss1: 638.82, loss2: 91.70 and loss3: 4909.00\n",
      "Epoch [10], train_loss: 5636.67 with loss1: 635.63, loss2: 92.09 and loss3: 4908.95\n",
      "Epoch [11], train_loss: 5641.40 with loss1: 640.32, loss2: 92.17 and loss3: 4908.90\n",
      "Epoch [12], train_loss: 5631.68 with loss1: 631.64, loss2: 91.18 and loss3: 4908.85\n",
      "Epoch [13], train_loss: 5638.42 with loss1: 637.98, loss2: 91.64 and loss3: 4908.80\n",
      "Epoch [14], train_loss: 5631.35 with loss1: 631.57, loss2: 91.03 and loss3: 4908.75\n",
      "Epoch [15], train_loss: 5638.22 with loss1: 637.64, loss2: 91.88 and loss3: 4908.70\n",
      "Epoch [16], train_loss: 5629.86 with loss1: 629.90, loss2: 91.31 and loss3: 4908.65\n",
      "Epoch [17], train_loss: 5633.91 with loss1: 633.71, loss2: 91.59 and loss3: 4908.60\n",
      "Epoch [18], train_loss: 5626.59 with loss1: 626.52, loss2: 91.51 and loss3: 4908.56\n",
      "Epoch [19], train_loss: 5629.60 with loss1: 629.68, loss2: 91.42 and loss3: 4908.50\n",
      "Epoch [20], train_loss: 5624.72 with loss1: 625.30, loss2: 90.97 and loss3: 4908.46\n",
      "Epoch [21], train_loss: 5628.05 with loss1: 628.03, loss2: 91.62 and loss3: 4908.41\n",
      "Epoch [22], train_loss: 5620.52 with loss1: 621.23, loss2: 90.94 and loss3: 4908.36\n",
      "Epoch [23], train_loss: 5623.13 with loss1: 623.54, loss2: 91.28 and loss3: 4908.31\n",
      "Epoch [24], train_loss: 5617.50 with loss1: 618.40, loss2: 90.84 and loss3: 4908.26\n",
      "Epoch [25], train_loss: 5621.18 with loss1: 621.70, loss2: 91.27 and loss3: 4908.21\n",
      "Epoch [26], train_loss: 5614.07 with loss1: 615.10, loss2: 90.81 and loss3: 4908.16\n",
      "Epoch [27], train_loss: 5616.61 with loss1: 617.71, loss2: 90.79 and loss3: 4908.11\n",
      "Epoch [28], train_loss: 5613.10 with loss1: 614.13, loss2: 90.91 and loss3: 4908.06\n",
      "Epoch [29], train_loss: 5615.83 with loss1: 616.61, loss2: 91.21 and loss3: 4908.01\n",
      "Epoch [30], train_loss: 5611.74 with loss1: 612.88, loss2: 90.90 and loss3: 4907.96\n",
      "Epoch [31], train_loss: 5614.41 with loss1: 615.49, loss2: 91.01 and loss3: 4907.91\n",
      "Epoch [32], train_loss: 5608.33 with loss1: 610.17, loss2: 90.30 and loss3: 4907.86\n",
      "Epoch [33], train_loss: 5612.17 with loss1: 613.15, loss2: 91.21 and loss3: 4907.81\n",
      "Epoch [34], train_loss: 5609.10 with loss1: 610.82, loss2: 90.52 and loss3: 4907.76\n",
      "Epoch [35], train_loss: 5612.14 with loss1: 613.62, loss2: 90.82 and loss3: 4907.71\n",
      "Epoch [36], train_loss: 5607.77 with loss1: 609.68, loss2: 90.43 and loss3: 4907.66\n",
      "Epoch [37], train_loss: 5611.53 with loss1: 612.47, loss2: 91.45 and loss3: 4907.61\n",
      "Epoch [38], train_loss: 5607.23 with loss1: 609.31, loss2: 90.36 and loss3: 4907.56\n",
      "Epoch [39], train_loss: 5611.24 with loss1: 613.06, loss2: 90.67 and loss3: 4907.51\n",
      "Epoch [40], train_loss: 5606.56 with loss1: 608.90, loss2: 90.21 and loss3: 4907.46\n",
      "Epoch [41], train_loss: 5607.72 with loss1: 609.70, loss2: 90.62 and loss3: 4907.41\n",
      "Epoch [42], train_loss: 5605.32 with loss1: 607.91, loss2: 90.06 and loss3: 4907.36\n",
      "Epoch [43], train_loss: 5608.91 with loss1: 611.16, loss2: 90.44 and loss3: 4907.31\n",
      "Epoch [44], train_loss: 5605.50 with loss1: 608.23, loss2: 90.01 and loss3: 4907.26\n",
      "Epoch [45], train_loss: 5608.56 with loss1: 611.08, loss2: 90.27 and loss3: 4907.21\n",
      "Epoch [46], train_loss: 5603.30 with loss1: 606.13, loss2: 90.02 and loss3: 4907.16\n",
      "Epoch [47], train_loss: 5607.43 with loss1: 609.43, loss2: 90.89 and loss3: 4907.11\n",
      "Epoch [48], train_loss: 5603.83 with loss1: 606.99, loss2: 89.78 and loss3: 4907.06\n",
      "Epoch [49], train_loss: 5608.65 with loss1: 611.11, loss2: 90.53 and loss3: 4907.01\n",
      "Epoch [50], train_loss: 5604.70 with loss1: 608.03, loss2: 89.72 and loss3: 4906.96\n",
      "Epoch [51], train_loss: 5612.27 with loss1: 614.99, loss2: 90.37 and loss3: 4906.91\n",
      "Epoch [52], train_loss: 5607.70 with loss1: 610.84, loss2: 90.00 and loss3: 4906.86\n",
      "Epoch [53], train_loss: 5613.53 with loss1: 616.69, loss2: 90.03 and loss3: 4906.81\n",
      "Epoch [54], train_loss: 5605.22 with loss1: 608.81, loss2: 89.66 and loss3: 4906.76\n",
      "Epoch [55], train_loss: 5611.93 with loss1: 615.20, loss2: 90.02 and loss3: 4906.71\n",
      "Epoch [56], train_loss: 5607.52 with loss1: 611.36, loss2: 89.50 and loss3: 4906.66\n",
      "Epoch [57], train_loss: 5612.44 with loss1: 615.89, loss2: 89.94 and loss3: 4906.61\n",
      "Epoch [58], train_loss: 5609.85 with loss1: 613.70, loss2: 89.60 and loss3: 4906.56\n",
      "Epoch [59], train_loss: 5615.86 with loss1: 619.21, loss2: 90.14 and loss3: 4906.51\n",
      "Epoch [60], train_loss: 5615.07 with loss1: 618.99, loss2: 89.62 and loss3: 4906.46\n",
      "Epoch [61], train_loss: 5616.15 with loss1: 619.98, loss2: 89.76 and loss3: 4906.41\n",
      "Epoch [62], train_loss: 5614.83 with loss1: 619.33, loss2: 89.13 and loss3: 4906.36\n",
      "Epoch [63], train_loss: 5620.16 with loss1: 623.63, loss2: 90.21 and loss3: 4906.31\n",
      "Epoch [64], train_loss: 5614.28 with loss1: 619.05, loss2: 88.97 and loss3: 4906.26\n",
      "Epoch [65], train_loss: 5622.71 with loss1: 626.62, loss2: 89.89 and loss3: 4906.21\n",
      "Epoch [66], train_loss: 5618.99 with loss1: 623.33, loss2: 89.50 and loss3: 4906.16\n",
      "Epoch [67], train_loss: 5625.85 with loss1: 630.14, loss2: 89.61 and loss3: 4906.11\n",
      "Epoch [68], train_loss: 5621.34 with loss1: 625.85, loss2: 89.43 and loss3: 4906.06\n",
      "Epoch [69], train_loss: 5627.33 with loss1: 631.42, loss2: 89.89 and loss3: 4906.01\n",
      "Epoch [70], train_loss: 5624.79 with loss1: 629.54, loss2: 89.28 and loss3: 4905.96\n",
      "Epoch [71], train_loss: 5632.41 with loss1: 637.15, loss2: 89.35 and loss3: 4905.91\n",
      "Epoch [72], train_loss: 5626.91 with loss1: 631.82, loss2: 89.23 and loss3: 4905.86\n",
      "Epoch [73], train_loss: 5632.25 with loss1: 636.55, loss2: 89.89 and loss3: 4905.81\n",
      "Epoch [74], train_loss: 5627.43 with loss1: 632.34, loss2: 89.33 and loss3: 4905.76\n",
      "Epoch [75], train_loss: 5631.68 with loss1: 636.62, loss2: 89.36 and loss3: 4905.71\n",
      "Epoch [76], train_loss: 5629.24 with loss1: 634.57, loss2: 89.01 and loss3: 4905.66\n",
      "Epoch [77], train_loss: 5633.84 with loss1: 638.22, loss2: 90.01 and loss3: 4905.61\n",
      "Epoch [78], train_loss: 5626.40 with loss1: 631.93, loss2: 88.91 and loss3: 4905.56\n",
      "Epoch [79], train_loss: 5631.89 with loss1: 636.69, loss2: 89.69 and loss3: 4905.51\n",
      "Epoch [80], train_loss: 5624.10 with loss1: 629.55, loss2: 89.09 and loss3: 4905.46\n",
      "Epoch [81], train_loss: 5628.60 with loss1: 633.51, loss2: 89.67 and loss3: 4905.41\n",
      "Epoch [82], train_loss: 5620.97 with loss1: 626.97, loss2: 88.64 and loss3: 4905.36\n",
      "Epoch [83], train_loss: 5625.61 with loss1: 631.18, loss2: 89.11 and loss3: 4905.31\n",
      "Epoch [84], train_loss: 5617.44 with loss1: 623.01, loss2: 89.17 and loss3: 4905.26\n",
      "Epoch [85], train_loss: 5621.35 with loss1: 626.86, loss2: 89.27 and loss3: 4905.21\n",
      "Epoch [86], train_loss: 5614.98 with loss1: 620.79, loss2: 89.03 and loss3: 4905.16\n",
      "Epoch [87], train_loss: 5617.79 with loss1: 623.42, loss2: 89.26 and loss3: 4905.11\n",
      "Epoch [88], train_loss: 5610.32 with loss1: 616.45, loss2: 88.81 and loss3: 4905.06\n",
      "Epoch [89], train_loss: 5614.63 with loss1: 620.18, loss2: 89.44 and loss3: 4905.01\n",
      "Epoch [90], train_loss: 5605.91 with loss1: 612.41, loss2: 88.53 and loss3: 4904.96\n",
      "Epoch [91], train_loss: 5609.99 with loss1: 615.94, loss2: 89.14 and loss3: 4904.91\n",
      "Epoch [92], train_loss: 5602.62 with loss1: 609.18, loss2: 88.58 and loss3: 4904.86\n",
      "Epoch [93], train_loss: 5604.27 with loss1: 610.43, loss2: 89.02 and loss3: 4904.81\n",
      "Epoch [94], train_loss: 5599.19 with loss1: 606.03, loss2: 88.40 and loss3: 4904.76\n",
      "Epoch [95], train_loss: 5600.42 with loss1: 607.11, loss2: 88.60 and loss3: 4904.71\n",
      "Epoch [96], train_loss: 5594.84 with loss1: 601.82, loss2: 88.36 and loss3: 4904.67\n",
      "Epoch [97], train_loss: 5597.67 with loss1: 604.46, loss2: 88.60 and loss3: 4904.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98], train_loss: 5591.13 with loss1: 598.39, loss2: 88.17 and loss3: 4904.57\n",
      "Epoch [99], train_loss: 5595.55 with loss1: 602.28, loss2: 88.76 and loss3: 4904.52\n",
      "Epoch [100], train_loss: 5589.03 with loss1: 596.16, loss2: 88.41 and loss3: 4904.47\n",
      "Epoch [101], train_loss: 5588.17 with loss1: 595.02, loss2: 88.72 and loss3: 4904.42\n",
      "Epoch [102], train_loss: 5584.30 with loss1: 591.65, loss2: 88.29 and loss3: 4904.37\n",
      "Epoch [103], train_loss: 5585.36 with loss1: 592.50, loss2: 88.54 and loss3: 4904.32\n",
      "Epoch [104], train_loss: 5582.50 with loss1: 590.20, loss2: 88.03 and loss3: 4904.27\n",
      "Epoch [105], train_loss: 5584.10 with loss1: 591.54, loss2: 88.34 and loss3: 4904.22\n",
      "Epoch [106], train_loss: 5579.04 with loss1: 586.77, loss2: 88.10 and loss3: 4904.17\n",
      "Epoch [107], train_loss: 5581.43 with loss1: 589.17, loss2: 88.15 and loss3: 4904.12\n",
      "Epoch [108], train_loss: 5582.25 with loss1: 589.62, loss2: 88.56 and loss3: 4904.07\n",
      "Epoch [109], train_loss: 5581.29 with loss1: 588.68, loss2: 88.59 and loss3: 4904.02\n",
      "Epoch [110], train_loss: 5577.48 with loss1: 585.76, loss2: 87.76 and loss3: 4903.97\n",
      "Epoch [111], train_loss: 5581.43 with loss1: 588.28, loss2: 89.23 and loss3: 4903.92\n",
      "Epoch [112], train_loss: 5574.39 with loss1: 582.88, loss2: 87.64 and loss3: 4903.87\n",
      "Epoch [113], train_loss: 5576.84 with loss1: 584.80, loss2: 88.22 and loss3: 4903.82\n",
      "Epoch [114], train_loss: 5575.69 with loss1: 584.19, loss2: 87.73 and loss3: 4903.77\n",
      "Epoch [115], train_loss: 5580.43 with loss1: 588.63, loss2: 88.09 and loss3: 4903.72\n",
      "Epoch [116], train_loss: 5575.55 with loss1: 584.18, loss2: 87.70 and loss3: 4903.67\n",
      "Epoch [117], train_loss: 5581.54 with loss1: 589.81, loss2: 88.11 and loss3: 4903.62\n",
      "Epoch [118], train_loss: 5575.09 with loss1: 583.94, loss2: 87.58 and loss3: 4903.57\n",
      "Epoch [119], train_loss: 5580.36 with loss1: 588.17, loss2: 88.67 and loss3: 4903.52\n",
      "Epoch [120], train_loss: 5577.29 with loss1: 586.34, loss2: 87.48 and loss3: 4903.47\n",
      "Epoch [121], train_loss: 5580.23 with loss1: 588.95, loss2: 87.86 and loss3: 4903.42\n",
      "Epoch [122], train_loss: 5578.27 with loss1: 587.38, loss2: 87.52 and loss3: 4903.37\n",
      "Epoch [123], train_loss: 5580.71 with loss1: 589.13, loss2: 88.26 and loss3: 4903.32\n",
      "Epoch [124], train_loss: 5577.62 with loss1: 586.86, loss2: 87.49 and loss3: 4903.27\n",
      "Epoch [125], train_loss: 5581.78 with loss1: 590.44, loss2: 88.13 and loss3: 4903.22\n",
      "Epoch [126], train_loss: 5579.67 with loss1: 589.09, loss2: 87.41 and loss3: 4903.17\n",
      "Epoch [127], train_loss: 5584.37 with loss1: 593.87, loss2: 87.38 and loss3: 4903.12\n",
      "Epoch [128], train_loss: 5580.70 with loss1: 590.24, loss2: 87.39 and loss3: 4903.07\n",
      "Epoch [129], train_loss: 5583.51 with loss1: 592.95, loss2: 87.54 and loss3: 4903.02\n",
      "Epoch [130], train_loss: 5582.58 with loss1: 592.27, loss2: 87.34 and loss3: 4902.97\n",
      "Epoch [131], train_loss: 5586.23 with loss1: 595.82, loss2: 87.50 and loss3: 4902.92\n",
      "Epoch [132], train_loss: 5586.48 with loss1: 596.68, loss2: 86.93 and loss3: 4902.87\n",
      "Epoch [133], train_loss: 5591.30 with loss1: 600.98, loss2: 87.50 and loss3: 4902.82\n",
      "Epoch [134], train_loss: 5585.34 with loss1: 595.74, loss2: 86.83 and loss3: 4902.77\n",
      "Epoch [135], train_loss: 5593.51 with loss1: 602.98, loss2: 87.81 and loss3: 4902.72\n",
      "Epoch [136], train_loss: 5588.96 with loss1: 599.42, loss2: 86.87 and loss3: 4902.67\n",
      "Epoch [137], train_loss: 5593.25 with loss1: 603.55, loss2: 87.08 and loss3: 4902.62\n",
      "Epoch [138], train_loss: 5590.88 with loss1: 601.22, loss2: 87.09 and loss3: 4902.57\n",
      "Epoch [139], train_loss: 5595.61 with loss1: 606.03, loss2: 87.05 and loss3: 4902.52\n",
      "Epoch [140], train_loss: 5591.76 with loss1: 602.40, loss2: 86.89 and loss3: 4902.47\n",
      "Epoch [141], train_loss: 5598.14 with loss1: 608.20, loss2: 87.51 and loss3: 4902.42\n",
      "Epoch [142], train_loss: 5596.50 with loss1: 607.48, loss2: 86.65 and loss3: 4902.37\n",
      "Epoch [143], train_loss: 5598.23 with loss1: 608.89, loss2: 87.02 and loss3: 4902.32\n",
      "Epoch [144], train_loss: 5595.08 with loss1: 606.54, loss2: 86.26 and loss3: 4902.27\n",
      "Epoch [145], train_loss: 5600.42 with loss1: 610.99, loss2: 87.21 and loss3: 4902.22\n",
      "Epoch [146], train_loss: 5595.46 with loss1: 606.21, loss2: 87.08 and loss3: 4902.17\n",
      "Epoch [147], train_loss: 5600.88 with loss1: 611.69, loss2: 87.07 and loss3: 4902.12\n",
      "Epoch [148], train_loss: 5594.15 with loss1: 605.79, loss2: 86.28 and loss3: 4902.07\n",
      "Epoch [149], train_loss: 5600.73 with loss1: 611.86, loss2: 86.85 and loss3: 4902.02\n",
      "Epoch [150], train_loss: 5598.36 with loss1: 609.88, loss2: 86.50 and loss3: 4901.97\n",
      "Epoch [151], train_loss: 5603.34 with loss1: 613.77, loss2: 87.65 and loss3: 4901.92\n",
      "Epoch [152], train_loss: 5598.68 with loss1: 610.18, loss2: 86.62 and loss3: 4901.87\n",
      "Epoch [153], train_loss: 5603.13 with loss1: 614.08, loss2: 87.23 and loss3: 4901.82\n",
      "Epoch [154], train_loss: 5595.91 with loss1: 607.64, loss2: 86.50 and loss3: 4901.77\n",
      "Epoch [155], train_loss: 5601.64 with loss1: 612.80, loss2: 87.11 and loss3: 4901.72\n",
      "Epoch [156], train_loss: 5596.80 with loss1: 609.19, loss2: 85.94 and loss3: 4901.67\n",
      "Epoch [157], train_loss: 5601.10 with loss1: 612.51, loss2: 86.97 and loss3: 4901.62\n",
      "Epoch [158], train_loss: 5597.74 with loss1: 610.01, loss2: 86.16 and loss3: 4901.57\n",
      "Epoch [159], train_loss: 5602.05 with loss1: 613.31, loss2: 87.21 and loss3: 4901.52\n",
      "Epoch [160], train_loss: 5597.20 with loss1: 609.89, loss2: 85.83 and loss3: 4901.47\n",
      "Epoch [161], train_loss: 5601.53 with loss1: 613.59, loss2: 86.52 and loss3: 4901.42\n",
      "Epoch [162], train_loss: 5596.51 with loss1: 608.99, loss2: 86.15 and loss3: 4901.37\n",
      "Epoch [163], train_loss: 5600.73 with loss1: 612.85, loss2: 86.55 and loss3: 4901.32\n",
      "Epoch [164], train_loss: 5595.27 with loss1: 607.87, loss2: 86.13 and loss3: 4901.27\n",
      "Epoch [165], train_loss: 5601.29 with loss1: 613.46, loss2: 86.60 and loss3: 4901.22\n",
      "Epoch [166], train_loss: 5596.61 with loss1: 609.08, loss2: 86.36 and loss3: 4901.17\n",
      "Epoch [167], train_loss: 5599.73 with loss1: 612.09, loss2: 86.52 and loss3: 4901.12\n",
      "Epoch [168], train_loss: 5594.57 with loss1: 607.12, loss2: 86.37 and loss3: 4901.08\n",
      "Epoch [169], train_loss: 5595.51 with loss1: 608.40, loss2: 86.09 and loss3: 4901.03\n",
      "Epoch [170], train_loss: 5590.74 with loss1: 604.02, loss2: 85.75 and loss3: 4900.98\n",
      "Epoch [171], train_loss: 5594.27 with loss1: 606.81, loss2: 86.53 and loss3: 4900.93\n",
      "Epoch [172], train_loss: 5587.40 with loss1: 600.74, loss2: 85.78 and loss3: 4900.88\n",
      "Epoch [173], train_loss: 5592.67 with loss1: 605.65, loss2: 86.19 and loss3: 4900.83\n",
      "Epoch [174], train_loss: 5589.08 with loss1: 602.45, loss2: 85.86 and loss3: 4900.78\n",
      "Epoch [175], train_loss: 5591.93 with loss1: 604.94, loss2: 86.26 and loss3: 4900.73\n",
      "Epoch [176], train_loss: 5585.51 with loss1: 599.09, loss2: 85.74 and loss3: 4900.68\n",
      "Epoch [177], train_loss: 5590.38 with loss1: 603.54, loss2: 86.21 and loss3: 4900.63\n",
      "Epoch [178], train_loss: 5583.69 with loss1: 597.54, loss2: 85.57 and loss3: 4900.58\n",
      "Epoch [179], train_loss: 5587.02 with loss1: 600.57, loss2: 85.92 and loss3: 4900.53\n",
      "Epoch [180], train_loss: 5581.77 with loss1: 595.68, loss2: 85.61 and loss3: 4900.48\n",
      "Epoch [181], train_loss: 5586.22 with loss1: 599.54, loss2: 86.25 and loss3: 4900.43\n",
      "Epoch [182], train_loss: 5578.62 with loss1: 592.76, loss2: 85.48 and loss3: 4900.38\n",
      "Epoch [183], train_loss: 5580.43 with loss1: 594.11, loss2: 85.99 and loss3: 4900.33\n",
      "Epoch [184], train_loss: 5576.47 with loss1: 590.89, loss2: 85.30 and loss3: 4900.28\n",
      "Epoch [185], train_loss: 5578.34 with loss1: 592.29, loss2: 85.83 and loss3: 4900.23\n",
      "Epoch [186], train_loss: 5574.82 with loss1: 588.78, loss2: 85.86 and loss3: 4900.18\n",
      "Epoch [187], train_loss: 5576.72 with loss1: 590.86, loss2: 85.73 and loss3: 4900.13\n",
      "Epoch [188], train_loss: 5571.85 with loss1: 585.92, loss2: 85.85 and loss3: 4900.08\n",
      "Epoch [189], train_loss: 5573.03 with loss1: 587.17, loss2: 85.82 and loss3: 4900.03\n",
      "Epoch [190], train_loss: 5570.07 with loss1: 584.52, loss2: 85.57 and loss3: 4899.98\n",
      "Epoch [191], train_loss: 5571.86 with loss1: 585.90, loss2: 86.03 and loss3: 4899.93\n",
      "Epoch [192], train_loss: 5567.74 with loss1: 582.61, loss2: 85.25 and loss3: 4899.88\n",
      "Epoch [193], train_loss: 5569.26 with loss1: 583.82, loss2: 85.61 and loss3: 4899.83\n",
      "Epoch [194], train_loss: 5565.89 with loss1: 580.91, loss2: 85.20 and loss3: 4899.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [195], train_loss: 5569.03 with loss1: 584.11, loss2: 85.19 and loss3: 4899.73\n",
      "Epoch [196], train_loss: 5566.33 with loss1: 581.40, loss2: 85.25 and loss3: 4899.68\n",
      "Epoch [197], train_loss: 5570.85 with loss1: 585.90, loss2: 85.32 and loss3: 4899.63\n",
      "Epoch [198], train_loss: 5564.93 with loss1: 580.45, loss2: 84.91 and loss3: 4899.58\n",
      "Epoch [199], train_loss: 5568.72 with loss1: 583.66, loss2: 85.54 and loss3: 4899.53\n",
      "Epoch [200], train_loss: 5565.62 with loss1: 581.02, loss2: 85.12 and loss3: 4899.48\n",
      "Epoch [201], train_loss: 5568.63 with loss1: 583.69, loss2: 85.51 and loss3: 4899.43\n",
      "Epoch [202], train_loss: 5566.25 with loss1: 581.82, loss2: 85.05 and loss3: 4899.38\n",
      "Epoch [203], train_loss: 5569.74 with loss1: 584.93, loss2: 85.47 and loss3: 4899.33\n",
      "Epoch [204], train_loss: 5566.92 with loss1: 582.60, loss2: 85.04 and loss3: 4899.28\n",
      "Epoch [205], train_loss: 5571.20 with loss1: 586.59, loss2: 85.39 and loss3: 4899.23\n",
      "Epoch [206], train_loss: 5568.31 with loss1: 584.55, loss2: 84.58 and loss3: 4899.18\n",
      "Epoch [207], train_loss: 5574.11 with loss1: 589.58, loss2: 85.40 and loss3: 4899.13\n",
      "Epoch [208], train_loss: 5570.87 with loss1: 587.11, loss2: 84.68 and loss3: 4899.08\n",
      "Epoch [209], train_loss: 5575.26 with loss1: 591.20, loss2: 85.03 and loss3: 4899.03\n",
      "Epoch [210], train_loss: 5573.39 with loss1: 589.46, loss2: 84.96 and loss3: 4898.98\n",
      "Epoch [211], train_loss: 5578.38 with loss1: 594.34, loss2: 85.11 and loss3: 4898.93\n",
      "Epoch [212], train_loss: 5576.98 with loss1: 592.77, loss2: 85.33 and loss3: 4898.88\n",
      "Epoch [213], train_loss: 5581.02 with loss1: 596.87, loss2: 85.31 and loss3: 4898.83\n",
      "Epoch [214], train_loss: 5577.06 with loss1: 593.82, loss2: 84.46 and loss3: 4898.78\n",
      "Epoch [215], train_loss: 5581.89 with loss1: 597.85, loss2: 85.30 and loss3: 4898.73\n",
      "Epoch [216], train_loss: 5580.17 with loss1: 596.74, loss2: 84.76 and loss3: 4898.68\n",
      "Epoch [217], train_loss: 5585.03 with loss1: 601.31, loss2: 85.10 and loss3: 4898.63\n",
      "Epoch [218], train_loss: 5579.95 with loss1: 596.87, loss2: 84.49 and loss3: 4898.58\n",
      "Epoch [219], train_loss: 5583.73 with loss1: 600.36, loss2: 84.84 and loss3: 4898.53\n",
      "Epoch [220], train_loss: 5577.91 with loss1: 595.10, loss2: 84.33 and loss3: 4898.48\n",
      "Epoch [221], train_loss: 5583.07 with loss1: 599.84, loss2: 84.79 and loss3: 4898.43\n",
      "Epoch [222], train_loss: 5578.86 with loss1: 595.66, loss2: 84.81 and loss3: 4898.38\n",
      "Epoch [223], train_loss: 5582.91 with loss1: 599.89, loss2: 84.69 and loss3: 4898.33\n",
      "Epoch [224], train_loss: 5577.56 with loss1: 594.40, loss2: 84.88 and loss3: 4898.28\n",
      "Epoch [225], train_loss: 5581.81 with loss1: 598.51, loss2: 85.06 and loss3: 4898.23\n",
      "Epoch [226], train_loss: 5572.60 with loss1: 589.94, loss2: 84.48 and loss3: 4898.18\n",
      "Epoch [227], train_loss: 5576.28 with loss1: 593.62, loss2: 84.53 and loss3: 4898.13\n",
      "Epoch [228], train_loss: 5573.04 with loss1: 590.49, loss2: 84.47 and loss3: 4898.08\n",
      "Epoch [229], train_loss: 5572.75 with loss1: 589.98, loss2: 84.73 and loss3: 4898.03\n",
      "Epoch [230], train_loss: 5566.43 with loss1: 583.94, loss2: 84.50 and loss3: 4897.98\n",
      "Epoch [231], train_loss: 5568.06 with loss1: 585.76, loss2: 84.36 and loss3: 4897.93\n",
      "Epoch [232], train_loss: 5563.71 with loss1: 581.60, loss2: 84.23 and loss3: 4897.88\n",
      "Epoch [233], train_loss: 5564.44 with loss1: 582.04, loss2: 84.56 and loss3: 4897.83\n",
      "Epoch [234], train_loss: 5559.51 with loss1: 577.60, loss2: 84.13 and loss3: 4897.78\n",
      "Epoch [235], train_loss: 5563.47 with loss1: 581.50, loss2: 84.24 and loss3: 4897.73\n",
      "Epoch [236], train_loss: 5558.33 with loss1: 576.81, loss2: 83.83 and loss3: 4897.68\n",
      "Epoch [237], train_loss: 5560.18 with loss1: 578.27, loss2: 84.27 and loss3: 4897.63\n",
      "Epoch [238], train_loss: 5555.23 with loss1: 572.97, loss2: 84.67 and loss3: 4897.59\n",
      "Epoch [239], train_loss: 5558.58 with loss1: 576.87, loss2: 84.17 and loss3: 4897.54\n",
      "Epoch [240], train_loss: 5553.91 with loss1: 572.16, loss2: 84.26 and loss3: 4897.49\n",
      "Epoch [241], train_loss: 5557.47 with loss1: 575.57, loss2: 84.47 and loss3: 4897.44\n",
      "Epoch [242], train_loss: 5552.04 with loss1: 570.49, loss2: 84.17 and loss3: 4897.39\n",
      "Epoch [243], train_loss: 5555.47 with loss1: 573.96, loss2: 84.18 and loss3: 4897.34\n",
      "Epoch [244], train_loss: 5550.93 with loss1: 569.88, loss2: 83.76 and loss3: 4897.29\n",
      "Epoch [245], train_loss: 5556.80 with loss1: 575.34, loss2: 84.22 and loss3: 4897.24\n",
      "Epoch [246], train_loss: 5549.44 with loss1: 568.78, loss2: 83.47 and loss3: 4897.19\n",
      "Epoch [247], train_loss: 5557.17 with loss1: 576.04, loss2: 84.00 and loss3: 4897.14\n",
      "Epoch [248], train_loss: 5552.85 with loss1: 571.82, loss2: 83.94 and loss3: 4897.09\n",
      "Epoch [249], train_loss: 5554.77 with loss1: 573.72, loss2: 84.01 and loss3: 4897.04\n",
      "Epoch [250], train_loss: 5550.44 with loss1: 570.08, loss2: 83.37 and loss3: 4896.99\n",
      "Epoch [251], train_loss: 5554.36 with loss1: 573.54, loss2: 83.89 and loss3: 4896.94\n",
      "Epoch [252], train_loss: 5549.56 with loss1: 569.27, loss2: 83.40 and loss3: 4896.89\n",
      "Epoch [253], train_loss: 5553.63 with loss1: 572.99, loss2: 83.80 and loss3: 4896.84\n",
      "Epoch [254], train_loss: 5550.51 with loss1: 569.89, loss2: 83.83 and loss3: 4896.79\n",
      "Epoch [255], train_loss: 5551.50 with loss1: 570.94, loss2: 83.82 and loss3: 4896.74\n",
      "Epoch [256], train_loss: 5549.98 with loss1: 569.93, loss2: 83.36 and loss3: 4896.69\n",
      "Epoch [257], train_loss: 5555.04 with loss1: 574.57, loss2: 83.83 and loss3: 4896.64\n",
      "Epoch [258], train_loss: 5550.34 with loss1: 570.43, loss2: 83.32 and loss3: 4896.59\n",
      "Epoch [259], train_loss: 5552.91 with loss1: 572.65, loss2: 83.72 and loss3: 4896.54\n",
      "Epoch [260], train_loss: 5548.43 with loss1: 568.71, loss2: 83.23 and loss3: 4896.49\n",
      "Epoch [261], train_loss: 5551.94 with loss1: 572.04, loss2: 83.46 and loss3: 4896.44\n",
      "Epoch [262], train_loss: 5550.65 with loss1: 571.07, loss2: 83.19 and loss3: 4896.39\n",
      "Epoch [263], train_loss: 5552.22 with loss1: 572.47, loss2: 83.41 and loss3: 4896.34\n",
      "Epoch [264], train_loss: 5549.01 with loss1: 569.35, loss2: 83.36 and loss3: 4896.29\n",
      "Epoch [265], train_loss: 5551.24 with loss1: 571.84, loss2: 83.16 and loss3: 4896.24\n",
      "Epoch [266], train_loss: 5547.72 with loss1: 568.45, loss2: 83.07 and loss3: 4896.19\n",
      "Epoch [267], train_loss: 5550.49 with loss1: 571.03, loss2: 83.32 and loss3: 4896.14\n",
      "Epoch [268], train_loss: 5547.38 with loss1: 568.54, loss2: 82.76 and loss3: 4896.09\n",
      "Epoch [269], train_loss: 5551.12 with loss1: 571.67, loss2: 83.41 and loss3: 4896.04\n",
      "Epoch [270], train_loss: 5549.40 with loss1: 570.53, loss2: 82.88 and loss3: 4895.99\n",
      "Epoch [271], train_loss: 5550.24 with loss1: 571.19, loss2: 83.11 and loss3: 4895.94\n",
      "Epoch [272], train_loss: 5547.37 with loss1: 568.52, loss2: 82.96 and loss3: 4895.89\n",
      "Epoch [273], train_loss: 5549.31 with loss1: 570.37, loss2: 83.10 and loss3: 4895.84\n",
      "Epoch [274], train_loss: 5546.04 with loss1: 567.52, loss2: 82.73 and loss3: 4895.79\n",
      "Epoch [275], train_loss: 5550.17 with loss1: 570.89, loss2: 83.54 and loss3: 4895.74\n",
      "Epoch [276], train_loss: 5545.36 with loss1: 567.21, loss2: 82.45 and loss3: 4895.69\n",
      "Epoch [277], train_loss: 5549.77 with loss1: 571.02, loss2: 83.11 and loss3: 4895.64\n",
      "Epoch [278], train_loss: 5549.04 with loss1: 570.50, loss2: 82.94 and loss3: 4895.59\n",
      "Epoch [279], train_loss: 5552.10 with loss1: 573.45, loss2: 83.11 and loss3: 4895.54\n",
      "Epoch [280], train_loss: 5545.19 with loss1: 567.15, loss2: 82.55 and loss3: 4895.49\n",
      "Epoch [281], train_loss: 5551.40 with loss1: 572.92, loss2: 83.04 and loss3: 4895.44\n",
      "Epoch [282], train_loss: 5546.26 with loss1: 568.11, loss2: 82.76 and loss3: 4895.39\n",
      "Epoch [283], train_loss: 5549.38 with loss1: 571.05, loss2: 82.98 and loss3: 4895.34\n",
      "Epoch [284], train_loss: 5546.11 with loss1: 567.98, loss2: 82.84 and loss3: 4895.29\n",
      "Epoch [285], train_loss: 5549.03 with loss1: 570.82, loss2: 82.97 and loss3: 4895.24\n",
      "Epoch [286], train_loss: 5545.25 with loss1: 567.41, loss2: 82.65 and loss3: 4895.19\n",
      "Epoch [287], train_loss: 5549.81 with loss1: 571.51, loss2: 83.16 and loss3: 4895.14\n",
      "Epoch [288], train_loss: 5545.59 with loss1: 567.73, loss2: 82.77 and loss3: 4895.09\n",
      "Epoch [289], train_loss: 5548.94 with loss1: 571.28, loss2: 82.62 and loss3: 4895.04\n",
      "Epoch [290], train_loss: 5545.14 with loss1: 567.65, loss2: 82.49 and loss3: 4894.99\n",
      "Epoch [291], train_loss: 5548.67 with loss1: 570.99, loss2: 82.73 and loss3: 4894.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [292], train_loss: 5545.81 with loss1: 568.59, loss2: 82.33 and loss3: 4894.89\n",
      "Epoch [293], train_loss: 5551.25 with loss1: 573.51, loss2: 82.90 and loss3: 4894.84\n",
      "Epoch [294], train_loss: 5545.92 with loss1: 568.68, loss2: 82.45 and loss3: 4894.79\n",
      "Epoch [295], train_loss: 5550.51 with loss1: 573.10, loss2: 82.66 and loss3: 4894.75\n",
      "Epoch [296], train_loss: 5545.23 with loss1: 568.51, loss2: 82.02 and loss3: 4894.70\n",
      "Epoch [297], train_loss: 5549.55 with loss1: 572.31, loss2: 82.60 and loss3: 4894.65\n",
      "Epoch [298], train_loss: 5546.61 with loss1: 569.77, loss2: 82.25 and loss3: 4894.60\n",
      "Epoch [299], train_loss: 5549.56 with loss1: 572.23, loss2: 82.79 and loss3: 4894.55\n",
      "Epoch [300], train_loss: 5545.04 with loss1: 568.47, loss2: 82.08 and loss3: 4894.50\n",
      "Epoch [301], train_loss: 5549.41 with loss1: 572.62, loss2: 82.34 and loss3: 4894.45\n",
      "Epoch [302], train_loss: 5544.93 with loss1: 568.52, loss2: 82.01 and loss3: 4894.40\n",
      "Epoch [303], train_loss: 5546.71 with loss1: 570.03, loss2: 82.33 and loss3: 4894.35\n",
      "Epoch [304], train_loss: 5544.22 with loss1: 568.21, loss2: 81.71 and loss3: 4894.30\n",
      "Epoch [305], train_loss: 5547.80 with loss1: 571.42, loss2: 82.14 and loss3: 4894.25\n",
      "Epoch [306], train_loss: 5543.15 with loss1: 567.35, loss2: 81.61 and loss3: 4894.20\n",
      "Epoch [307], train_loss: 5545.01 with loss1: 568.96, loss2: 81.91 and loss3: 4894.15\n",
      "Epoch [308], train_loss: 5541.82 with loss1: 565.90, loss2: 81.83 and loss3: 4894.10\n",
      "Epoch [309], train_loss: 5547.04 with loss1: 570.87, loss2: 82.12 and loss3: 4894.05\n",
      "Epoch [310], train_loss: 5545.27 with loss1: 569.54, loss2: 81.73 and loss3: 4894.00\n",
      "Epoch [311], train_loss: 5548.21 with loss1: 572.26, loss2: 82.00 and loss3: 4893.95\n",
      "Epoch [312], train_loss: 5545.79 with loss1: 569.91, loss2: 81.99 and loss3: 4893.90\n",
      "Epoch [313], train_loss: 5550.89 with loss1: 575.03, loss2: 82.01 and loss3: 4893.85\n",
      "Epoch [314], train_loss: 5545.35 with loss1: 569.86, loss2: 81.69 and loss3: 4893.80\n",
      "Epoch [315], train_loss: 5550.09 with loss1: 574.31, loss2: 82.03 and loss3: 4893.75\n",
      "Epoch [316], train_loss: 5545.32 with loss1: 569.64, loss2: 81.98 and loss3: 4893.70\n",
      "Epoch [317], train_loss: 5550.20 with loss1: 574.41, loss2: 82.14 and loss3: 4893.65\n",
      "Epoch [318], train_loss: 5545.51 with loss1: 570.31, loss2: 81.60 and loss3: 4893.60\n",
      "Epoch [319], train_loss: 5550.72 with loss1: 575.15, loss2: 82.02 and loss3: 4893.55\n",
      "Epoch [320], train_loss: 5544.79 with loss1: 569.91, loss2: 81.38 and loss3: 4893.50\n",
      "Epoch [321], train_loss: 5550.99 with loss1: 575.70, loss2: 81.84 and loss3: 4893.45\n",
      "Epoch [322], train_loss: 5545.44 with loss1: 570.62, loss2: 81.42 and loss3: 4893.40\n",
      "Epoch [323], train_loss: 5548.46 with loss1: 573.38, loss2: 81.74 and loss3: 4893.35\n",
      "Epoch [324], train_loss: 5544.42 with loss1: 570.04, loss2: 81.08 and loss3: 4893.30\n",
      "Epoch [325], train_loss: 5550.76 with loss1: 575.48, loss2: 82.02 and loss3: 4893.25\n",
      "Epoch [326], train_loss: 5543.90 with loss1: 569.67, loss2: 81.03 and loss3: 4893.20\n",
      "Epoch [327], train_loss: 5547.89 with loss1: 573.05, loss2: 81.68 and loss3: 4893.15\n",
      "Epoch [328], train_loss: 5544.36 with loss1: 570.00, loss2: 81.26 and loss3: 4893.10\n",
      "Epoch [329], train_loss: 5549.40 with loss1: 574.78, loss2: 81.57 and loss3: 4893.05\n",
      "Epoch [330], train_loss: 5544.20 with loss1: 570.09, loss2: 81.11 and loss3: 4893.00\n",
      "Epoch [331], train_loss: 5550.82 with loss1: 576.14, loss2: 81.73 and loss3: 4892.95\n",
      "Epoch [332], train_loss: 5543.62 with loss1: 569.77, loss2: 80.95 and loss3: 4892.90\n",
      "Epoch [333], train_loss: 5549.05 with loss1: 574.67, loss2: 81.53 and loss3: 4892.85\n",
      "Epoch [334], train_loss: 5544.04 with loss1: 569.83, loss2: 81.41 and loss3: 4892.80\n",
      "Epoch [335], train_loss: 5547.80 with loss1: 573.68, loss2: 81.37 and loss3: 4892.75\n",
      "Epoch [336], train_loss: 5541.83 with loss1: 568.23, loss2: 80.89 and loss3: 4892.70\n",
      "Epoch [337], train_loss: 5546.18 with loss1: 572.20, loss2: 81.33 and loss3: 4892.65\n",
      "Epoch [338], train_loss: 5542.07 with loss1: 568.41, loss2: 81.05 and loss3: 4892.60\n",
      "Epoch [339], train_loss: 5545.95 with loss1: 571.47, loss2: 81.93 and loss3: 4892.55\n",
      "Epoch [340], train_loss: 5540.98 with loss1: 567.09, loss2: 81.38 and loss3: 4892.50\n",
      "Epoch [341], train_loss: 5543.03 with loss1: 569.38, loss2: 81.20 and loss3: 4892.45\n",
      "Epoch [342], train_loss: 5539.78 with loss1: 566.42, loss2: 80.96 and loss3: 4892.40\n",
      "Epoch [343], train_loss: 5541.45 with loss1: 567.62, loss2: 81.48 and loss3: 4892.35\n",
      "Epoch [344], train_loss: 5536.31 with loss1: 563.25, loss2: 80.76 and loss3: 4892.30\n",
      "Epoch [345], train_loss: 5542.19 with loss1: 568.55, loss2: 81.39 and loss3: 4892.25\n",
      "Epoch [346], train_loss: 5535.54 with loss1: 562.93, loss2: 80.41 and loss3: 4892.20\n",
      "Epoch [347], train_loss: 5539.32 with loss1: 566.30, loss2: 80.87 and loss3: 4892.15\n",
      "Epoch [348], train_loss: 5536.00 with loss1: 562.94, loss2: 80.96 and loss3: 4892.10\n",
      "Epoch [349], train_loss: 5538.39 with loss1: 565.11, loss2: 81.23 and loss3: 4892.05\n",
      "Epoch [350], train_loss: 5534.71 with loss1: 562.06, loss2: 80.64 and loss3: 4892.00\n",
      "Epoch [351], train_loss: 5535.91 with loss1: 562.81, loss2: 81.14 and loss3: 4891.95\n",
      "Epoch [352], train_loss: 5534.16 with loss1: 561.65, loss2: 80.60 and loss3: 4891.91\n",
      "Epoch [353], train_loss: 5535.26 with loss1: 562.46, loss2: 80.95 and loss3: 4891.86\n",
      "Epoch [354], train_loss: 5531.67 with loss1: 559.25, loss2: 80.62 and loss3: 4891.81\n",
      "Epoch [355], train_loss: 5535.76 with loss1: 563.18, loss2: 80.82 and loss3: 4891.76\n",
      "Epoch [356], train_loss: 5531.12 with loss1: 558.71, loss2: 80.70 and loss3: 4891.71\n",
      "Epoch [357], train_loss: 5533.02 with loss1: 560.50, loss2: 80.87 and loss3: 4891.66\n",
      "Epoch [358], train_loss: 5530.20 with loss1: 557.93, loss2: 80.66 and loss3: 4891.61\n",
      "Epoch [359], train_loss: 5531.50 with loss1: 559.10, loss2: 80.84 and loss3: 4891.56\n",
      "Epoch [360], train_loss: 5528.62 with loss1: 556.70, loss2: 80.42 and loss3: 4891.51\n",
      "Epoch [361], train_loss: 5529.41 with loss1: 556.94, loss2: 81.02 and loss3: 4891.46\n",
      "Epoch [362], train_loss: 5526.38 with loss1: 554.60, loss2: 80.38 and loss3: 4891.41\n",
      "Epoch [363], train_loss: 5529.11 with loss1: 557.22, loss2: 80.53 and loss3: 4891.36\n",
      "Epoch [364], train_loss: 5525.18 with loss1: 553.68, loss2: 80.19 and loss3: 4891.31\n",
      "Epoch [365], train_loss: 5529.30 with loss1: 557.67, loss2: 80.37 and loss3: 4891.26\n",
      "Epoch [366], train_loss: 5525.74 with loss1: 554.28, loss2: 80.25 and loss3: 4891.21\n",
      "Epoch [367], train_loss: 5527.72 with loss1: 555.95, loss2: 80.61 and loss3: 4891.16\n",
      "Epoch [368], train_loss: 5525.30 with loss1: 553.95, loss2: 80.24 and loss3: 4891.11\n",
      "Epoch [369], train_loss: 5527.87 with loss1: 556.58, loss2: 80.23 and loss3: 4891.06\n",
      "Epoch [370], train_loss: 5524.55 with loss1: 553.42, loss2: 80.12 and loss3: 4891.01\n",
      "Epoch [371], train_loss: 5527.97 with loss1: 556.47, loss2: 80.54 and loss3: 4890.96\n",
      "Epoch [372], train_loss: 5523.96 with loss1: 552.99, loss2: 80.06 and loss3: 4890.91\n",
      "Epoch [373], train_loss: 5526.59 with loss1: 555.17, loss2: 80.56 and loss3: 4890.86\n",
      "Epoch [374], train_loss: 5522.28 with loss1: 551.56, loss2: 79.92 and loss3: 4890.81\n",
      "Epoch [375], train_loss: 5526.31 with loss1: 555.13, loss2: 80.42 and loss3: 4890.76\n",
      "Epoch [376], train_loss: 5522.55 with loss1: 551.99, loss2: 79.86 and loss3: 4890.71\n",
      "Epoch [377], train_loss: 5525.11 with loss1: 553.77, loss2: 80.68 and loss3: 4890.66\n",
      "Epoch [378], train_loss: 5525.19 with loss1: 554.35, loss2: 80.23 and loss3: 4890.61\n",
      "Epoch [379], train_loss: 5527.36 with loss1: 556.37, loss2: 80.43 and loss3: 4890.56\n",
      "Epoch [380], train_loss: 5524.54 with loss1: 553.66, loss2: 80.37 and loss3: 4890.51\n",
      "Epoch [381], train_loss: 5527.28 with loss1: 556.43, loss2: 80.38 and loss3: 4890.46\n",
      "Epoch [382], train_loss: 5524.22 with loss1: 553.87, loss2: 79.93 and loss3: 4890.41\n",
      "Epoch [383], train_loss: 5526.77 with loss1: 556.00, loss2: 80.41 and loss3: 4890.36\n",
      "Epoch [384], train_loss: 5523.74 with loss1: 553.16, loss2: 80.27 and loss3: 4890.31\n",
      "Epoch [385], train_loss: 5528.90 with loss1: 558.30, loss2: 80.34 and loss3: 4890.26\n",
      "Epoch [386], train_loss: 5524.22 with loss1: 554.18, loss2: 79.84 and loss3: 4890.21\n",
      "Epoch [387], train_loss: 5528.38 with loss1: 558.28, loss2: 79.93 and loss3: 4890.16\n",
      "Epoch [388], train_loss: 5524.74 with loss1: 554.59, loss2: 80.04 and loss3: 4890.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [389], train_loss: 5529.11 with loss1: 559.12, loss2: 79.92 and loss3: 4890.06\n",
      "Epoch [390], train_loss: 5524.23 with loss1: 554.75, loss2: 79.46 and loss3: 4890.01\n",
      "Epoch [391], train_loss: 5529.38 with loss1: 559.57, loss2: 79.85 and loss3: 4889.96\n",
      "Epoch [392], train_loss: 5526.39 with loss1: 556.85, loss2: 79.64 and loss3: 4889.91\n",
      "Epoch [393], train_loss: 5530.75 with loss1: 561.18, loss2: 79.71 and loss3: 4889.86\n",
      "Epoch [394], train_loss: 5528.85 with loss1: 559.62, loss2: 79.42 and loss3: 4889.81\n",
      "Epoch [395], train_loss: 5533.01 with loss1: 563.57, loss2: 79.67 and loss3: 4889.76\n",
      "Epoch [396], train_loss: 5528.98 with loss1: 559.72, loss2: 79.55 and loss3: 4889.71\n",
      "Epoch [397], train_loss: 5533.94 with loss1: 564.30, loss2: 79.97 and loss3: 4889.66\n",
      "Epoch [398], train_loss: 5529.18 with loss1: 560.11, loss2: 79.45 and loss3: 4889.61\n",
      "Epoch [399], train_loss: 5534.04 with loss1: 564.64, loss2: 79.83 and loss3: 4889.56\n",
      "Epoch [400], train_loss: 5530.23 with loss1: 561.13, loss2: 79.58 and loss3: 4889.51\n",
      "Epoch [401], train_loss: 5535.40 with loss1: 566.31, loss2: 79.63 and loss3: 4889.46\n",
      "Epoch [402], train_loss: 5531.58 with loss1: 562.90, loss2: 79.26 and loss3: 4889.42\n",
      "Epoch [403], train_loss: 5536.97 with loss1: 568.11, loss2: 79.49 and loss3: 4889.37\n",
      "Epoch [404], train_loss: 5532.16 with loss1: 563.46, loss2: 79.38 and loss3: 4889.32\n",
      "Epoch [405], train_loss: 5538.69 with loss1: 569.66, loss2: 79.77 and loss3: 4889.27\n",
      "Epoch [406], train_loss: 5534.13 with loss1: 565.52, loss2: 79.39 and loss3: 4889.22\n",
      "Epoch [407], train_loss: 5538.62 with loss1: 569.97, loss2: 79.49 and loss3: 4889.17\n",
      "Epoch [408], train_loss: 5534.10 with loss1: 565.85, loss2: 79.13 and loss3: 4889.12\n",
      "Epoch [409], train_loss: 5538.60 with loss1: 569.81, loss2: 79.72 and loss3: 4889.07\n",
      "Epoch [410], train_loss: 5532.47 with loss1: 564.37, loss2: 79.09 and loss3: 4889.02\n",
      "Epoch [411], train_loss: 5538.17 with loss1: 569.72, loss2: 79.48 and loss3: 4888.97\n",
      "Epoch [412], train_loss: 5532.43 with loss1: 564.37, loss2: 79.14 and loss3: 4888.92\n",
      "Epoch [413], train_loss: 5536.86 with loss1: 568.22, loss2: 79.77 and loss3: 4888.87\n",
      "Epoch [414], train_loss: 5530.59 with loss1: 562.82, loss2: 78.96 and loss3: 4888.82\n",
      "Epoch [415], train_loss: 5535.11 with loss1: 567.11, loss2: 79.24 and loss3: 4888.77\n",
      "Epoch [416], train_loss: 5529.64 with loss1: 561.95, loss2: 78.97 and loss3: 4888.72\n",
      "Epoch [417], train_loss: 5534.26 with loss1: 566.41, loss2: 79.19 and loss3: 4888.67\n",
      "Epoch [418], train_loss: 5526.65 with loss1: 559.11, loss2: 78.91 and loss3: 4888.62\n",
      "Epoch [419], train_loss: 5530.79 with loss1: 563.00, loss2: 79.22 and loss3: 4888.57\n",
      "Epoch [420], train_loss: 5525.36 with loss1: 557.86, loss2: 78.98 and loss3: 4888.52\n",
      "Epoch [421], train_loss: 5529.51 with loss1: 561.56, loss2: 79.48 and loss3: 4888.47\n",
      "Epoch [422], train_loss: 5523.09 with loss1: 556.03, loss2: 78.64 and loss3: 4888.42\n",
      "Epoch [423], train_loss: 5527.27 with loss1: 559.48, loss2: 79.42 and loss3: 4888.37\n",
      "Epoch [424], train_loss: 5521.61 with loss1: 554.07, loss2: 79.21 and loss3: 4888.32\n",
      "Epoch [425], train_loss: 5524.13 with loss1: 557.02, loss2: 78.84 and loss3: 4888.27\n",
      "Epoch [426], train_loss: 5518.48 with loss1: 551.42, loss2: 78.84 and loss3: 4888.22\n",
      "Epoch [427], train_loss: 5523.59 with loss1: 556.46, loss2: 78.96 and loss3: 4888.17\n",
      "Epoch [428], train_loss: 5516.71 with loss1: 549.82, loss2: 78.77 and loss3: 4888.12\n",
      "Epoch [429], train_loss: 5519.71 with loss1: 552.75, loss2: 78.89 and loss3: 4888.07\n",
      "Epoch [430], train_loss: 5514.22 with loss1: 547.66, loss2: 78.54 and loss3: 4888.02\n",
      "Epoch [431], train_loss: 5519.69 with loss1: 552.68, loss2: 79.04 and loss3: 4887.97\n",
      "Epoch [432], train_loss: 5514.66 with loss1: 547.81, loss2: 78.93 and loss3: 4887.92\n",
      "Epoch [433], train_loss: 5516.63 with loss1: 549.62, loss2: 79.14 and loss3: 4887.87\n",
      "Epoch [434], train_loss: 5512.92 with loss1: 546.40, loss2: 78.70 and loss3: 4887.82\n",
      "Epoch [435], train_loss: 5516.58 with loss1: 550.09, loss2: 78.72 and loss3: 4887.77\n",
      "Epoch [436], train_loss: 5509.96 with loss1: 543.72, loss2: 78.52 and loss3: 4887.72\n",
      "Epoch [437], train_loss: 5516.39 with loss1: 550.08, loss2: 78.64 and loss3: 4887.67\n",
      "Epoch [438], train_loss: 5514.22 with loss1: 548.39, loss2: 78.21 and loss3: 4887.62\n",
      "Epoch [439], train_loss: 5513.82 with loss1: 547.82, loss2: 78.43 and loss3: 4887.57\n",
      "Epoch [440], train_loss: 5509.52 with loss1: 543.53, loss2: 78.47 and loss3: 4887.52\n",
      "Epoch [441], train_loss: 5514.14 with loss1: 547.90, loss2: 78.77 and loss3: 4887.47\n",
      "Epoch [442], train_loss: 5509.30 with loss1: 543.48, loss2: 78.40 and loss3: 4887.42\n",
      "Epoch [443], train_loss: 5512.10 with loss1: 546.25, loss2: 78.48 and loss3: 4887.37\n",
      "Epoch [444], train_loss: 5508.36 with loss1: 542.47, loss2: 78.56 and loss3: 4887.32\n",
      "Epoch [445], train_loss: 5512.14 with loss1: 546.41, loss2: 78.45 and loss3: 4887.27\n",
      "Epoch [446], train_loss: 5507.54 with loss1: 542.10, loss2: 78.21 and loss3: 4887.22\n",
      "Epoch [447], train_loss: 5510.32 with loss1: 544.57, loss2: 78.57 and loss3: 4887.17\n",
      "Epoch [448], train_loss: 5506.01 with loss1: 540.75, loss2: 78.13 and loss3: 4887.12\n",
      "Epoch [449], train_loss: 5508.33 with loss1: 542.41, loss2: 78.85 and loss3: 4887.08\n",
      "Epoch [450], train_loss: 5505.01 with loss1: 539.83, loss2: 78.15 and loss3: 4887.02\n",
      "Epoch [451], train_loss: 5507.09 with loss1: 541.77, loss2: 78.34 and loss3: 4886.97\n",
      "Epoch [452], train_loss: 5503.77 with loss1: 538.60, loss2: 78.24 and loss3: 4886.93\n",
      "Epoch [453], train_loss: 5505.52 with loss1: 540.48, loss2: 78.17 and loss3: 4886.88\n",
      "Epoch [454], train_loss: 5503.19 with loss1: 538.33, loss2: 78.03 and loss3: 4886.83\n",
      "Epoch [455], train_loss: 5505.68 with loss1: 540.48, loss2: 78.43 and loss3: 4886.78\n",
      "Epoch [456], train_loss: 5501.78 with loss1: 536.80, loss2: 78.25 and loss3: 4886.73\n",
      "Epoch [457], train_loss: 5505.55 with loss1: 540.63, loss2: 78.25 and loss3: 4886.68\n",
      "Epoch [458], train_loss: 5499.95 with loss1: 535.59, loss2: 77.74 and loss3: 4886.63\n",
      "Epoch [459], train_loss: 5504.67 with loss1: 539.56, loss2: 78.54 and loss3: 4886.58\n",
      "Epoch [460], train_loss: 5499.44 with loss1: 534.82, loss2: 78.10 and loss3: 4886.53\n",
      "Epoch [461], train_loss: 5500.49 with loss1: 535.98, loss2: 78.04 and loss3: 4886.48\n",
      "Epoch [462], train_loss: 5498.18 with loss1: 534.08, loss2: 77.67 and loss3: 4886.43\n",
      "Epoch [463], train_loss: 5500.36 with loss1: 535.83, loss2: 78.15 and loss3: 4886.38\n",
      "Epoch [464], train_loss: 5497.74 with loss1: 533.48, loss2: 77.93 and loss3: 4886.33\n",
      "Epoch [465], train_loss: 5499.22 with loss1: 535.02, loss2: 77.92 and loss3: 4886.28\n",
      "Epoch [466], train_loss: 5496.42 with loss1: 532.53, loss2: 77.66 and loss3: 4886.23\n",
      "Epoch [467], train_loss: 5501.09 with loss1: 536.67, loss2: 78.24 and loss3: 4886.18\n",
      "Epoch [468], train_loss: 5496.08 with loss1: 532.27, loss2: 77.68 and loss3: 4886.13\n",
      "Epoch [469], train_loss: 5498.22 with loss1: 534.22, loss2: 77.91 and loss3: 4886.08\n",
      "Epoch [470], train_loss: 5494.92 with loss1: 531.34, loss2: 77.55 and loss3: 4886.03\n",
      "Epoch [471], train_loss: 5498.70 with loss1: 534.67, loss2: 78.05 and loss3: 4885.98\n",
      "Epoch [472], train_loss: 5494.88 with loss1: 531.41, loss2: 77.55 and loss3: 4885.93\n",
      "Epoch [473], train_loss: 5499.50 with loss1: 535.79, loss2: 77.84 and loss3: 4885.88\n",
      "Epoch [474], train_loss: 5497.54 with loss1: 534.34, loss2: 77.37 and loss3: 4885.83\n",
      "Epoch [475], train_loss: 5499.16 with loss1: 535.63, loss2: 77.74 and loss3: 4885.78\n",
      "Epoch [476], train_loss: 5496.15 with loss1: 532.32, loss2: 78.10 and loss3: 4885.73\n",
      "Epoch [477], train_loss: 5498.79 with loss1: 535.56, loss2: 77.54 and loss3: 4885.68\n",
      "Epoch [478], train_loss: 5494.79 with loss1: 531.97, loss2: 77.19 and loss3: 4885.63\n",
      "Epoch [479], train_loss: 5499.20 with loss1: 535.90, loss2: 77.72 and loss3: 4885.58\n",
      "Epoch [480], train_loss: 5494.27 with loss1: 531.46, loss2: 77.28 and loss3: 4885.53\n",
      "Epoch [481], train_loss: 5498.81 with loss1: 535.80, loss2: 77.53 and loss3: 4885.48\n",
      "Epoch [482], train_loss: 5496.79 with loss1: 534.27, loss2: 77.09 and loss3: 4885.43\n",
      "Epoch [483], train_loss: 5499.49 with loss1: 536.47, loss2: 77.64 and loss3: 4885.38\n",
      "Epoch [484], train_loss: 5496.18 with loss1: 533.83, loss2: 77.01 and loss3: 4885.33\n",
      "Epoch [485], train_loss: 5500.22 with loss1: 537.61, loss2: 77.33 and loss3: 4885.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [486], train_loss: 5497.87 with loss1: 535.25, loss2: 77.39 and loss3: 4885.23\n",
      "Epoch [487], train_loss: 5501.96 with loss1: 539.29, loss2: 77.49 and loss3: 4885.18\n",
      "Epoch [488], train_loss: 5498.55 with loss1: 536.43, loss2: 76.99 and loss3: 4885.13\n",
      "Epoch [489], train_loss: 5502.23 with loss1: 539.90, loss2: 77.24 and loss3: 4885.08\n",
      "Epoch [490], train_loss: 5501.15 with loss1: 539.18, loss2: 76.94 and loss3: 4885.03\n",
      "Epoch [491], train_loss: 5505.50 with loss1: 543.06, loss2: 77.45 and loss3: 4884.98\n",
      "Epoch [492], train_loss: 5502.02 with loss1: 540.29, loss2: 76.80 and loss3: 4884.93\n",
      "Epoch [493], train_loss: 5506.36 with loss1: 544.05, loss2: 77.43 and loss3: 4884.88\n",
      "Epoch [494], train_loss: 5504.32 with loss1: 542.46, loss2: 77.02 and loss3: 4884.83\n",
      "Epoch [495], train_loss: 5509.57 with loss1: 547.43, loss2: 77.36 and loss3: 4884.78\n",
      "Epoch [496], train_loss: 5505.98 with loss1: 544.05, loss2: 77.19 and loss3: 4884.74\n",
      "Epoch [497], train_loss: 5511.03 with loss1: 549.12, loss2: 77.22 and loss3: 4884.69\n",
      "Epoch [498], train_loss: 5507.03 with loss1: 545.94, loss2: 76.46 and loss3: 4884.64\n",
      "Epoch [499], train_loss: 5513.12 with loss1: 551.42, loss2: 77.11 and loss3: 4884.59\n",
      "Epoch [500], train_loss: 5509.65 with loss1: 548.59, loss2: 76.53 and loss3: 4884.54\n",
      "Epoch [501], train_loss: 5516.42 with loss1: 554.91, loss2: 77.02 and loss3: 4884.49\n",
      "Epoch [502], train_loss: 5511.11 with loss1: 550.06, loss2: 76.61 and loss3: 4884.44\n",
      "Epoch [503], train_loss: 5518.08 with loss1: 556.56, loss2: 77.13 and loss3: 4884.39\n",
      "Epoch [504], train_loss: 5512.91 with loss1: 552.01, loss2: 76.56 and loss3: 4884.34\n",
      "Epoch [505], train_loss: 5518.27 with loss1: 557.17, loss2: 76.81 and loss3: 4884.29\n",
      "Epoch [506], train_loss: 5512.27 with loss1: 551.37, loss2: 76.66 and loss3: 4884.24\n",
      "Epoch [507], train_loss: 5517.28 with loss1: 556.02, loss2: 77.07 and loss3: 4884.19\n",
      "Epoch [508], train_loss: 5514.35 with loss1: 553.71, loss2: 76.51 and loss3: 4884.14\n",
      "Epoch [509], train_loss: 5518.57 with loss1: 557.33, loss2: 77.15 and loss3: 4884.09\n",
      "Epoch [510], train_loss: 5513.68 with loss1: 553.51, loss2: 76.13 and loss3: 4884.04\n",
      "Epoch [511], train_loss: 5517.76 with loss1: 556.81, loss2: 76.96 and loss3: 4883.99\n",
      "Epoch [512], train_loss: 5511.84 with loss1: 551.43, loss2: 76.48 and loss3: 4883.94\n",
      "Epoch [513], train_loss: 5517.95 with loss1: 556.84, loss2: 77.22 and loss3: 4883.89\n",
      "Epoch [514], train_loss: 5513.63 with loss1: 553.19, loss2: 76.60 and loss3: 4883.84\n",
      "Epoch [515], train_loss: 5517.95 with loss1: 557.40, loss2: 76.77 and loss3: 4883.79\n",
      "Epoch [516], train_loss: 5512.31 with loss1: 551.88, loss2: 76.69 and loss3: 4883.74\n",
      "Epoch [517], train_loss: 5516.08 with loss1: 555.36, loss2: 77.03 and loss3: 4883.69\n",
      "Epoch [518], train_loss: 5511.49 with loss1: 551.48, loss2: 76.37 and loss3: 4883.64\n",
      "Epoch [519], train_loss: 5513.26 with loss1: 553.10, loss2: 76.57 and loss3: 4883.59\n",
      "Epoch [520], train_loss: 5507.71 with loss1: 547.97, loss2: 76.20 and loss3: 4883.54\n",
      "Epoch [521], train_loss: 5512.49 with loss1: 552.22, loss2: 76.77 and loss3: 4883.49\n",
      "Epoch [522], train_loss: 5506.87 with loss1: 546.80, loss2: 76.63 and loss3: 4883.44\n",
      "Epoch [523], train_loss: 5508.09 with loss1: 548.14, loss2: 76.55 and loss3: 4883.39\n",
      "Epoch [524], train_loss: 5503.45 with loss1: 543.87, loss2: 76.24 and loss3: 4883.34\n",
      "Epoch [525], train_loss: 5505.18 with loss1: 545.53, loss2: 76.36 and loss3: 4883.29\n",
      "Epoch [526], train_loss: 5500.55 with loss1: 541.42, loss2: 75.89 and loss3: 4883.24\n",
      "Epoch [527], train_loss: 5503.16 with loss1: 543.47, loss2: 76.50 and loss3: 4883.19\n",
      "Epoch [528], train_loss: 5499.14 with loss1: 539.57, loss2: 76.42 and loss3: 4883.14\n",
      "Epoch [529], train_loss: 5501.78 with loss1: 542.26, loss2: 76.42 and loss3: 4883.09\n",
      "Epoch [530], train_loss: 5497.30 with loss1: 538.07, loss2: 76.19 and loss3: 4883.04\n",
      "Epoch [531], train_loss: 5500.88 with loss1: 541.52, loss2: 76.36 and loss3: 4882.99\n",
      "Epoch [532], train_loss: 5495.18 with loss1: 536.14, loss2: 76.09 and loss3: 4882.94\n",
      "Epoch [533], train_loss: 5497.07 with loss1: 537.77, loss2: 76.41 and loss3: 4882.89\n",
      "Epoch [534], train_loss: 5492.65 with loss1: 533.66, loss2: 76.14 and loss3: 4882.84\n",
      "Epoch [535], train_loss: 5495.75 with loss1: 536.76, loss2: 76.19 and loss3: 4882.79\n",
      "Epoch [536], train_loss: 5492.26 with loss1: 533.68, loss2: 75.84 and loss3: 4882.74\n",
      "Epoch [537], train_loss: 5492.44 with loss1: 533.67, loss2: 76.08 and loss3: 4882.69\n",
      "Epoch [538], train_loss: 5489.76 with loss1: 531.38, loss2: 75.73 and loss3: 4882.65\n",
      "Epoch [539], train_loss: 5490.94 with loss1: 532.13, loss2: 76.22 and loss3: 4882.59\n",
      "Epoch [540], train_loss: 5487.06 with loss1: 528.29, loss2: 76.23 and loss3: 4882.54\n",
      "Epoch [541], train_loss: 5490.03 with loss1: 531.06, loss2: 76.48 and loss3: 4882.50\n",
      "Epoch [542], train_loss: 5486.71 with loss1: 528.39, loss2: 75.87 and loss3: 4882.45\n",
      "Epoch [543], train_loss: 5488.48 with loss1: 529.90, loss2: 76.18 and loss3: 4882.40\n",
      "Epoch [544], train_loss: 5486.75 with loss1: 528.62, loss2: 75.78 and loss3: 4882.35\n",
      "Epoch [545], train_loss: 5486.16 with loss1: 527.92, loss2: 75.95 and loss3: 4882.30\n",
      "Epoch [546], train_loss: 5485.53 with loss1: 527.56, loss2: 75.73 and loss3: 4882.25\n",
      "Epoch [547], train_loss: 5487.23 with loss1: 529.06, loss2: 75.97 and loss3: 4882.20\n",
      "Epoch [548], train_loss: 5483.52 with loss1: 525.80, loss2: 75.57 and loss3: 4882.15\n",
      "Epoch [549], train_loss: 5484.93 with loss1: 526.89, loss2: 75.94 and loss3: 4882.10\n",
      "Epoch [550], train_loss: 5481.95 with loss1: 524.24, loss2: 75.66 and loss3: 4882.05\n",
      "Epoch [551], train_loss: 5485.69 with loss1: 527.52, loss2: 76.18 and loss3: 4882.00\n",
      "Epoch [552], train_loss: 5484.15 with loss1: 526.70, loss2: 75.50 and loss3: 4881.95\n",
      "Epoch [553], train_loss: 5486.42 with loss1: 528.69, loss2: 75.83 and loss3: 4881.90\n",
      "Epoch [554], train_loss: 5484.60 with loss1: 527.35, loss2: 75.40 and loss3: 4881.85\n",
      "Epoch [555], train_loss: 5485.83 with loss1: 528.33, loss2: 75.70 and loss3: 4881.80\n",
      "Epoch [556], train_loss: 5483.00 with loss1: 525.78, loss2: 75.47 and loss3: 4881.75\n",
      "Epoch [557], train_loss: 5487.46 with loss1: 529.81, loss2: 75.95 and loss3: 4881.70\n",
      "Epoch [558], train_loss: 5483.64 with loss1: 526.59, loss2: 75.40 and loss3: 4881.65\n",
      "Epoch [559], train_loss: 5486.19 with loss1: 529.21, loss2: 75.38 and loss3: 4881.60\n",
      "Epoch [560], train_loss: 5482.27 with loss1: 525.52, loss2: 75.20 and loss3: 4881.55\n",
      "Epoch [561], train_loss: 5487.38 with loss1: 529.99, loss2: 75.89 and loss3: 4881.50\n",
      "Epoch [562], train_loss: 5483.18 with loss1: 526.43, loss2: 75.30 and loss3: 4881.45\n",
      "Epoch [563], train_loss: 5483.62 with loss1: 526.65, loss2: 75.57 and loss3: 4881.40\n",
      "Epoch [564], train_loss: 5480.96 with loss1: 524.28, loss2: 75.32 and loss3: 4881.35\n",
      "Epoch [565], train_loss: 5484.17 with loss1: 527.19, loss2: 75.67 and loss3: 4881.30\n",
      "Epoch [566], train_loss: 5482.57 with loss1: 525.58, loss2: 75.73 and loss3: 4881.25\n",
      "Epoch [567], train_loss: 5484.24 with loss1: 527.58, loss2: 75.46 and loss3: 4881.20\n",
      "Epoch [568], train_loss: 5481.65 with loss1: 525.32, loss2: 75.18 and loss3: 4881.15\n",
      "Epoch [569], train_loss: 5484.47 with loss1: 528.09, loss2: 75.27 and loss3: 4881.10\n",
      "Epoch [570], train_loss: 5480.59 with loss1: 524.33, loss2: 75.21 and loss3: 4881.05\n",
      "Epoch [571], train_loss: 5484.09 with loss1: 527.80, loss2: 75.29 and loss3: 4881.00\n",
      "Epoch [572], train_loss: 5479.52 with loss1: 523.52, loss2: 75.04 and loss3: 4880.95\n",
      "Epoch [573], train_loss: 5482.36 with loss1: 526.06, loss2: 75.39 and loss3: 4880.90\n",
      "Epoch [574], train_loss: 5479.31 with loss1: 523.57, loss2: 74.88 and loss3: 4880.85\n",
      "Epoch [575], train_loss: 5480.92 with loss1: 524.97, loss2: 75.15 and loss3: 4880.80\n",
      "Epoch [576], train_loss: 5478.14 with loss1: 522.40, loss2: 74.98 and loss3: 4880.75\n",
      "Epoch [577], train_loss: 5482.89 with loss1: 527.13, loss2: 75.05 and loss3: 4880.70\n",
      "Epoch [578], train_loss: 5479.87 with loss1: 524.33, loss2: 74.89 and loss3: 4880.65\n",
      "Epoch [579], train_loss: 5482.43 with loss1: 526.66, loss2: 75.17 and loss3: 4880.60\n",
      "Epoch [580], train_loss: 5480.15 with loss1: 524.55, loss2: 75.04 and loss3: 4880.55\n",
      "Epoch [581], train_loss: 5483.65 with loss1: 528.20, loss2: 74.95 and loss3: 4880.50\n",
      "Epoch [582], train_loss: 5480.04 with loss1: 524.52, loss2: 75.07 and loss3: 4880.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [583], train_loss: 5481.85 with loss1: 525.93, loss2: 75.51 and loss3: 4880.41\n",
      "Epoch [584], train_loss: 5479.55 with loss1: 524.25, loss2: 74.94 and loss3: 4880.36\n",
      "Epoch [585], train_loss: 5481.69 with loss1: 526.36, loss2: 75.02 and loss3: 4880.31\n",
      "Epoch [586], train_loss: 5479.13 with loss1: 523.76, loss2: 75.11 and loss3: 4880.26\n",
      "Epoch [587], train_loss: 5482.23 with loss1: 526.89, loss2: 75.13 and loss3: 4880.21\n",
      "Epoch [588], train_loss: 5478.82 with loss1: 523.99, loss2: 74.67 and loss3: 4880.16\n",
      "Epoch [589], train_loss: 5479.75 with loss1: 524.55, loss2: 75.09 and loss3: 4880.11\n",
      "Epoch [590], train_loss: 5477.31 with loss1: 522.59, loss2: 74.66 and loss3: 4880.06\n",
      "Epoch [591], train_loss: 5482.13 with loss1: 527.06, loss2: 75.07 and loss3: 4880.01\n",
      "Epoch [592], train_loss: 5475.58 with loss1: 521.18, loss2: 74.44 and loss3: 4879.96\n",
      "Epoch [593], train_loss: 5478.88 with loss1: 524.05, loss2: 74.92 and loss3: 4879.91\n",
      "Epoch [594], train_loss: 5475.85 with loss1: 521.67, loss2: 74.32 and loss3: 4879.86\n",
      "Epoch [595], train_loss: 5479.28 with loss1: 524.71, loss2: 74.76 and loss3: 4879.81\n",
      "Epoch [596], train_loss: 5475.78 with loss1: 521.34, loss2: 74.68 and loss3: 4879.76\n",
      "Epoch [597], train_loss: 5477.79 with loss1: 523.42, loss2: 74.66 and loss3: 4879.71\n",
      "Epoch [598], train_loss: 5475.17 with loss1: 521.46, loss2: 74.05 and loss3: 4879.66\n",
      "Epoch [599], train_loss: 5480.04 with loss1: 525.87, loss2: 74.57 and loss3: 4879.61\n",
      "Epoch [600], train_loss: 5478.14 with loss1: 523.76, loss2: 74.82 and loss3: 4879.56\n",
      "Epoch [601], train_loss: 5478.34 with loss1: 524.39, loss2: 74.45 and loss3: 4879.51\n",
      "Epoch [602], train_loss: 5477.19 with loss1: 523.35, loss2: 74.39 and loss3: 4879.46\n",
      "Epoch [603], train_loss: 5480.17 with loss1: 525.80, loss2: 74.96 and loss3: 4879.41\n",
      "Epoch [604], train_loss: 5477.19 with loss1: 523.51, loss2: 74.32 and loss3: 4879.36\n",
      "Epoch [605], train_loss: 5480.71 with loss1: 526.72, loss2: 74.68 and loss3: 4879.31\n",
      "Epoch [606], train_loss: 5479.07 with loss1: 525.44, loss2: 74.37 and loss3: 4879.26\n",
      "Epoch [607], train_loss: 5481.03 with loss1: 527.17, loss2: 74.65 and loss3: 4879.21\n",
      "Epoch [608], train_loss: 5478.98 with loss1: 525.61, loss2: 74.21 and loss3: 4879.16\n",
      "Epoch [609], train_loss: 5482.62 with loss1: 528.98, loss2: 74.52 and loss3: 4879.11\n",
      "Epoch [610], train_loss: 5477.09 with loss1: 524.00, loss2: 74.02 and loss3: 4879.06\n",
      "Epoch [611], train_loss: 5479.71 with loss1: 526.35, loss2: 74.35 and loss3: 4879.01\n",
      "Epoch [612], train_loss: 5476.57 with loss1: 523.66, loss2: 73.95 and loss3: 4878.96\n",
      "Epoch [613], train_loss: 5480.05 with loss1: 526.70, loss2: 74.43 and loss3: 4878.91\n",
      "Epoch [614], train_loss: 5478.39 with loss1: 525.38, loss2: 74.14 and loss3: 4878.86\n",
      "Epoch [615], train_loss: 5483.14 with loss1: 529.76, loss2: 74.56 and loss3: 4878.81\n",
      "Epoch [616], train_loss: 5480.19 with loss1: 527.13, loss2: 74.29 and loss3: 4878.76\n",
      "Epoch [617], train_loss: 5484.41 with loss1: 531.27, loss2: 74.42 and loss3: 4878.71\n",
      "Epoch [618], train_loss: 5480.41 with loss1: 527.27, loss2: 74.48 and loss3: 4878.66\n",
      "Epoch [619], train_loss: 5484.04 with loss1: 531.27, loss2: 74.15 and loss3: 4878.62\n",
      "Epoch [620], train_loss: 5480.64 with loss1: 527.97, loss2: 74.10 and loss3: 4878.57\n",
      "Epoch [621], train_loss: 5485.18 with loss1: 532.14, loss2: 74.53 and loss3: 4878.52\n",
      "Epoch [622], train_loss: 5479.12 with loss1: 527.06, loss2: 73.60 and loss3: 4878.47\n",
      "Epoch [623], train_loss: 5485.03 with loss1: 532.27, loss2: 74.34 and loss3: 4878.42\n",
      "Epoch [624], train_loss: 5480.93 with loss1: 528.38, loss2: 74.18 and loss3: 4878.37\n",
      "Epoch [625], train_loss: 5484.45 with loss1: 531.79, loss2: 74.34 and loss3: 4878.32\n",
      "Epoch [626], train_loss: 5479.64 with loss1: 527.92, loss2: 73.46 and loss3: 4878.27\n",
      "Epoch [627], train_loss: 5483.56 with loss1: 531.31, loss2: 74.03 and loss3: 4878.22\n",
      "Epoch [628], train_loss: 5480.72 with loss1: 528.80, loss2: 73.74 and loss3: 4878.17\n",
      "Epoch [629], train_loss: 5485.45 with loss1: 533.25, loss2: 74.08 and loss3: 4878.12\n",
      "Epoch [630], train_loss: 5480.27 with loss1: 528.77, loss2: 73.44 and loss3: 4878.07\n",
      "Epoch [631], train_loss: 5485.96 with loss1: 533.85, loss2: 74.09 and loss3: 4878.02\n",
      "Epoch [632], train_loss: 5481.07 with loss1: 529.59, loss2: 73.51 and loss3: 4877.97\n",
      "Epoch [633], train_loss: 5485.17 with loss1: 532.81, loss2: 74.44 and loss3: 4877.92\n",
      "Epoch [634], train_loss: 5480.06 with loss1: 528.60, loss2: 73.59 and loss3: 4877.87\n",
      "Epoch [635], train_loss: 5484.23 with loss1: 532.63, loss2: 73.78 and loss3: 4877.82\n",
      "Epoch [636], train_loss: 5479.40 with loss1: 528.15, loss2: 73.49 and loss3: 4877.77\n",
      "Epoch [637], train_loss: 5483.12 with loss1: 531.65, loss2: 73.75 and loss3: 4877.72\n",
      "Epoch [638], train_loss: 5478.28 with loss1: 527.31, loss2: 73.30 and loss3: 4877.67\n",
      "Epoch [639], train_loss: 5484.00 with loss1: 532.44, loss2: 73.94 and loss3: 4877.62\n",
      "Epoch [640], train_loss: 5480.48 with loss1: 529.58, loss2: 73.33 and loss3: 4877.57\n",
      "Epoch [641], train_loss: 5483.69 with loss1: 532.24, loss2: 73.93 and loss3: 4877.52\n",
      "Epoch [642], train_loss: 5479.51 with loss1: 528.41, loss2: 73.63 and loss3: 4877.47\n",
      "Epoch [643], train_loss: 5482.10 with loss1: 530.91, loss2: 73.77 and loss3: 4877.42\n",
      "Epoch [644], train_loss: 5478.12 with loss1: 527.32, loss2: 73.43 and loss3: 4877.37\n",
      "Epoch [645], train_loss: 5479.99 with loss1: 529.11, loss2: 73.56 and loss3: 4877.32\n",
      "Epoch [646], train_loss: 5475.24 with loss1: 524.78, loss2: 73.18 and loss3: 4877.27\n",
      "Epoch [647], train_loss: 5480.67 with loss1: 529.37, loss2: 74.08 and loss3: 4877.22\n",
      "Epoch [648], train_loss: 5474.18 with loss1: 523.66, loss2: 73.35 and loss3: 4877.17\n",
      "Epoch [649], train_loss: 5480.01 with loss1: 529.01, loss2: 73.88 and loss3: 4877.12\n",
      "Epoch [650], train_loss: 5473.94 with loss1: 523.79, loss2: 73.07 and loss3: 4877.07\n",
      "Epoch [651], train_loss: 5475.59 with loss1: 525.15, loss2: 73.41 and loss3: 4877.02\n",
      "Epoch [652], train_loss: 5471.57 with loss1: 521.08, loss2: 73.52 and loss3: 4876.97\n",
      "Epoch [653], train_loss: 5474.42 with loss1: 524.08, loss2: 73.42 and loss3: 4876.92\n",
      "Epoch [654], train_loss: 5468.05 with loss1: 518.09, loss2: 73.08 and loss3: 4876.88\n",
      "Epoch [655], train_loss: 5471.82 with loss1: 521.37, loss2: 73.63 and loss3: 4876.83\n",
      "Epoch [656], train_loss: 5466.87 with loss1: 517.07, loss2: 73.02 and loss3: 4876.78\n",
      "Epoch [657], train_loss: 5469.89 with loss1: 519.44, loss2: 73.73 and loss3: 4876.73\n",
      "Epoch [658], train_loss: 5464.63 with loss1: 514.91, loss2: 73.04 and loss3: 4876.68\n",
      "Epoch [659], train_loss: 5468.34 with loss1: 518.44, loss2: 73.27 and loss3: 4876.63\n",
      "Epoch [660], train_loss: 5464.06 with loss1: 514.49, loss2: 73.00 and loss3: 4876.58\n",
      "Epoch [661], train_loss: 5464.80 with loss1: 514.79, loss2: 73.49 and loss3: 4876.53\n",
      "Epoch [662], train_loss: 5461.02 with loss1: 511.37, loss2: 73.18 and loss3: 4876.48\n",
      "Epoch [663], train_loss: 5462.75 with loss1: 513.08, loss2: 73.25 and loss3: 4876.43\n",
      "Epoch [664], train_loss: 5456.75 with loss1: 507.50, loss2: 72.87 and loss3: 4876.38\n",
      "Epoch [665], train_loss: 5460.02 with loss1: 510.61, loss2: 73.08 and loss3: 4876.33\n",
      "Epoch [666], train_loss: 5455.98 with loss1: 506.67, loss2: 73.03 and loss3: 4876.28\n",
      "Epoch [667], train_loss: 5457.20 with loss1: 507.71, loss2: 73.26 and loss3: 4876.23\n",
      "Epoch [668], train_loss: 5454.62 with loss1: 505.43, loss2: 73.01 and loss3: 4876.18\n",
      "Epoch [669], train_loss: 5455.46 with loss1: 506.42, loss2: 72.92 and loss3: 4876.13\n",
      "Epoch [670], train_loss: 5451.22 with loss1: 502.30, loss2: 72.84 and loss3: 4876.08\n",
      "Epoch [671], train_loss: 5454.53 with loss1: 505.28, loss2: 73.22 and loss3: 4876.03\n",
      "Epoch [672], train_loss: 5450.91 with loss1: 502.06, loss2: 72.87 and loss3: 4875.98\n",
      "Epoch [673], train_loss: 5452.05 with loss1: 503.21, loss2: 72.90 and loss3: 4875.93\n",
      "Epoch [674], train_loss: 5450.29 with loss1: 501.64, loss2: 72.77 and loss3: 4875.88\n",
      "Epoch [675], train_loss: 5451.71 with loss1: 503.05, loss2: 72.82 and loss3: 4875.83\n",
      "Epoch [676], train_loss: 5447.87 with loss1: 499.36, loss2: 72.72 and loss3: 4875.78\n",
      "Epoch [677], train_loss: 5450.80 with loss1: 502.31, loss2: 72.76 and loss3: 4875.73\n",
      "Epoch [678], train_loss: 5447.99 with loss1: 499.76, loss2: 72.55 and loss3: 4875.68\n",
      "Epoch [679], train_loss: 5451.06 with loss1: 502.61, loss2: 72.82 and loss3: 4875.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [680], train_loss: 5449.10 with loss1: 500.94, loss2: 72.58 and loss3: 4875.58\n",
      "Epoch [681], train_loss: 5452.54 with loss1: 504.42, loss2: 72.59 and loss3: 4875.53\n",
      "Epoch [682], train_loss: 5451.17 with loss1: 503.30, loss2: 72.39 and loss3: 4875.48\n",
      "Epoch [683], train_loss: 5454.14 with loss1: 505.67, loss2: 73.04 and loss3: 4875.43\n",
      "Epoch [684], train_loss: 5450.47 with loss1: 502.62, loss2: 72.47 and loss3: 4875.38\n",
      "Epoch [685], train_loss: 5454.89 with loss1: 506.41, loss2: 73.14 and loss3: 4875.33\n",
      "Epoch [686], train_loss: 5452.56 with loss1: 504.71, loss2: 72.57 and loss3: 4875.28\n",
      "Epoch [687], train_loss: 5455.89 with loss1: 508.26, loss2: 72.39 and loss3: 4875.23\n",
      "Epoch [688], train_loss: 5454.44 with loss1: 506.73, loss2: 72.53 and loss3: 4875.18\n",
      "Epoch [689], train_loss: 5457.59 with loss1: 509.70, loss2: 72.76 and loss3: 4875.13\n",
      "Epoch [690], train_loss: 5454.78 with loss1: 506.78, loss2: 72.91 and loss3: 4875.08\n",
      "Epoch [691], train_loss: 5459.12 with loss1: 511.25, loss2: 72.83 and loss3: 4875.04\n",
      "Epoch [692], train_loss: 5457.11 with loss1: 509.76, loss2: 72.36 and loss3: 4874.99\n",
      "Epoch [693], train_loss: 5460.30 with loss1: 513.05, loss2: 72.31 and loss3: 4874.94\n",
      "Epoch [694], train_loss: 5458.55 with loss1: 511.62, loss2: 72.04 and loss3: 4874.89\n",
      "Epoch [695], train_loss: 5461.80 with loss1: 514.48, loss2: 72.49 and loss3: 4874.84\n",
      "Epoch [696], train_loss: 5459.45 with loss1: 512.14, loss2: 72.52 and loss3: 4874.79\n",
      "Epoch [697], train_loss: 5463.22 with loss1: 516.06, loss2: 72.42 and loss3: 4874.74\n",
      "Epoch [698], train_loss: 5459.00 with loss1: 512.28, loss2: 72.03 and loss3: 4874.69\n",
      "Epoch [699], train_loss: 5464.58 with loss1: 517.72, loss2: 72.22 and loss3: 4874.64\n",
      "Epoch [700], train_loss: 5459.67 with loss1: 513.05, loss2: 72.03 and loss3: 4874.59\n",
      "Epoch [701], train_loss: 5464.92 with loss1: 518.05, loss2: 72.33 and loss3: 4874.54\n",
      "Epoch [702], train_loss: 5462.23 with loss1: 515.59, loss2: 72.15 and loss3: 4874.49\n",
      "Epoch [703], train_loss: 5465.65 with loss1: 519.01, loss2: 72.19 and loss3: 4874.44\n",
      "Epoch [704], train_loss: 5462.09 with loss1: 515.84, loss2: 71.86 and loss3: 4874.39\n",
      "Epoch [705], train_loss: 5466.83 with loss1: 520.33, loss2: 72.17 and loss3: 4874.34\n",
      "Epoch [706], train_loss: 5463.53 with loss1: 517.27, loss2: 71.98 and loss3: 4874.29\n",
      "Epoch [707], train_loss: 5466.35 with loss1: 519.93, loss2: 72.18 and loss3: 4874.24\n",
      "Epoch [708], train_loss: 5463.38 with loss1: 517.37, loss2: 71.82 and loss3: 4874.19\n",
      "Epoch [709], train_loss: 5467.81 with loss1: 521.34, loss2: 72.33 and loss3: 4874.14\n",
      "Epoch [710], train_loss: 5462.34 with loss1: 516.64, loss2: 71.62 and loss3: 4874.09\n",
      "Epoch [711], train_loss: 5465.50 with loss1: 519.55, loss2: 71.91 and loss3: 4874.04\n",
      "Epoch [712], train_loss: 5461.33 with loss1: 515.53, loss2: 71.81 and loss3: 4873.99\n",
      "Epoch [713], train_loss: 5465.73 with loss1: 519.57, loss2: 72.22 and loss3: 4873.94\n",
      "Epoch [714], train_loss: 5462.39 with loss1: 516.84, loss2: 71.66 and loss3: 4873.89\n",
      "Epoch [715], train_loss: 5464.27 with loss1: 518.42, loss2: 72.01 and loss3: 4873.84\n",
      "Epoch [716], train_loss: 5457.99 with loss1: 512.53, loss2: 71.67 and loss3: 4873.79\n",
      "Epoch [717], train_loss: 5461.87 with loss1: 515.87, loss2: 72.26 and loss3: 4873.74\n",
      "Epoch [718], train_loss: 5458.08 with loss1: 512.52, loss2: 71.86 and loss3: 4873.69\n",
      "Epoch [719], train_loss: 5460.45 with loss1: 514.84, loss2: 71.97 and loss3: 4873.64\n",
      "Epoch [720], train_loss: 5455.14 with loss1: 509.81, loss2: 71.74 and loss3: 4873.59\n",
      "Epoch [721], train_loss: 5459.17 with loss1: 513.77, loss2: 71.86 and loss3: 4873.54\n",
      "Epoch [722], train_loss: 5455.52 with loss1: 510.13, loss2: 71.90 and loss3: 4873.49\n",
      "Epoch [723], train_loss: 5459.49 with loss1: 514.21, loss2: 71.84 and loss3: 4873.44\n",
      "Epoch [724], train_loss: 5453.76 with loss1: 508.58, loss2: 71.79 and loss3: 4873.39\n",
      "Epoch [725], train_loss: 5455.96 with loss1: 510.82, loss2: 71.80 and loss3: 4873.34\n",
      "Epoch [726], train_loss: 5452.70 with loss1: 507.83, loss2: 71.58 and loss3: 4873.29\n",
      "Epoch [727], train_loss: 5454.31 with loss1: 509.43, loss2: 71.64 and loss3: 4873.25\n",
      "Epoch [728], train_loss: 5449.70 with loss1: 504.86, loss2: 71.65 and loss3: 4873.20\n",
      "Epoch [729], train_loss: 5452.64 with loss1: 507.70, loss2: 71.79 and loss3: 4873.15\n",
      "Epoch [730], train_loss: 5449.45 with loss1: 504.60, loss2: 71.75 and loss3: 4873.10\n",
      "Epoch [731], train_loss: 5452.15 with loss1: 507.28, loss2: 71.83 and loss3: 4873.05\n",
      "Epoch [732], train_loss: 5448.75 with loss1: 504.38, loss2: 71.38 and loss3: 4873.00\n",
      "Epoch [733], train_loss: 5451.65 with loss1: 507.15, loss2: 71.56 and loss3: 4872.95\n",
      "Epoch [734], train_loss: 5447.84 with loss1: 503.51, loss2: 71.44 and loss3: 4872.90\n",
      "Epoch [735], train_loss: 5450.91 with loss1: 506.52, loss2: 71.54 and loss3: 4872.85\n",
      "Epoch [736], train_loss: 5447.23 with loss1: 503.07, loss2: 71.36 and loss3: 4872.80\n",
      "Epoch [737], train_loss: 5449.80 with loss1: 505.58, loss2: 71.47 and loss3: 4872.75\n",
      "Epoch [738], train_loss: 5446.28 with loss1: 502.48, loss2: 71.10 and loss3: 4872.70\n",
      "Epoch [739], train_loss: 5450.02 with loss1: 506.04, loss2: 71.33 and loss3: 4872.65\n",
      "Epoch [740], train_loss: 5447.37 with loss1: 503.85, loss2: 70.92 and loss3: 4872.60\n",
      "Epoch [741], train_loss: 5453.58 with loss1: 509.79, loss2: 71.24 and loss3: 4872.55\n",
      "Epoch [742], train_loss: 5448.14 with loss1: 504.56, loss2: 71.08 and loss3: 4872.50\n",
      "Epoch [743], train_loss: 5452.18 with loss1: 508.26, loss2: 71.48 and loss3: 4872.45\n",
      "Epoch [744], train_loss: 5448.91 with loss1: 505.36, loss2: 71.15 and loss3: 4872.40\n",
      "Epoch [745], train_loss: 5451.50 with loss1: 507.70, loss2: 71.45 and loss3: 4872.35\n",
      "Epoch [746], train_loss: 5448.18 with loss1: 504.77, loss2: 71.12 and loss3: 4872.30\n",
      "Epoch [747], train_loss: 5451.95 with loss1: 508.23, loss2: 71.47 and loss3: 4872.25\n",
      "Epoch [748], train_loss: 5449.30 with loss1: 505.73, loss2: 71.37 and loss3: 4872.20\n",
      "Epoch [749], train_loss: 5453.44 with loss1: 509.63, loss2: 71.66 and loss3: 4872.15\n",
      "Epoch [750], train_loss: 5448.87 with loss1: 505.64, loss2: 71.13 and loss3: 4872.10\n",
      "Epoch [751], train_loss: 5453.23 with loss1: 510.18, loss2: 70.99 and loss3: 4872.05\n",
      "Epoch [752], train_loss: 5450.61 with loss1: 507.54, loss2: 71.07 and loss3: 4872.00\n",
      "Epoch [753], train_loss: 5455.95 with loss1: 512.11, loss2: 71.89 and loss3: 4871.95\n",
      "Epoch [754], train_loss: 5451.32 with loss1: 508.48, loss2: 70.93 and loss3: 4871.90\n",
      "Epoch [755], train_loss: 5456.54 with loss1: 513.39, loss2: 71.30 and loss3: 4871.85\n",
      "Epoch [756], train_loss: 5451.44 with loss1: 508.80, loss2: 70.83 and loss3: 4871.80\n",
      "Epoch [757], train_loss: 5457.51 with loss1: 514.29, loss2: 71.46 and loss3: 4871.75\n",
      "Epoch [758], train_loss: 5452.38 with loss1: 509.92, loss2: 70.75 and loss3: 4871.70\n",
      "Epoch [759], train_loss: 5458.07 with loss1: 515.27, loss2: 71.15 and loss3: 4871.65\n",
      "Epoch [760], train_loss: 5455.60 with loss1: 513.51, loss2: 70.49 and loss3: 4871.60\n",
      "Epoch [761], train_loss: 5457.94 with loss1: 515.18, loss2: 71.20 and loss3: 4871.56\n",
      "Epoch [762], train_loss: 5453.75 with loss1: 511.54, loss2: 70.70 and loss3: 4871.51\n",
      "Epoch [763], train_loss: 5457.99 with loss1: 515.24, loss2: 71.29 and loss3: 4871.46\n",
      "Epoch [764], train_loss: 5457.27 with loss1: 515.16, loss2: 70.70 and loss3: 4871.41\n",
      "Epoch [765], train_loss: 5458.42 with loss1: 516.17, loss2: 70.89 and loss3: 4871.36\n",
      "Epoch [766], train_loss: 5455.58 with loss1: 513.28, loss2: 71.00 and loss3: 4871.31\n",
      "Epoch [767], train_loss: 5460.03 with loss1: 517.74, loss2: 71.03 and loss3: 4871.26\n",
      "Epoch [768], train_loss: 5457.29 with loss1: 515.44, loss2: 70.64 and loss3: 4871.21\n",
      "Epoch [769], train_loss: 5462.44 with loss1: 520.06, loss2: 71.22 and loss3: 4871.16\n",
      "Epoch [770], train_loss: 5456.68 with loss1: 515.20, loss2: 70.37 and loss3: 4871.11\n",
      "Epoch [771], train_loss: 5461.33 with loss1: 519.49, loss2: 70.79 and loss3: 4871.06\n",
      "Epoch [772], train_loss: 5456.11 with loss1: 514.65, loss2: 70.46 and loss3: 4871.01\n",
      "Epoch [773], train_loss: 5460.25 with loss1: 518.32, loss2: 70.97 and loss3: 4870.96\n",
      "Epoch [774], train_loss: 5456.26 with loss1: 514.69, loss2: 70.65 and loss3: 4870.91\n",
      "Epoch [775], train_loss: 5459.80 with loss1: 518.23, loss2: 70.71 and loss3: 4870.86\n",
      "Epoch [776], train_loss: 5455.86 with loss1: 514.68, loss2: 70.38 and loss3: 4870.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [777], train_loss: 5460.17 with loss1: 518.63, loss2: 70.78 and loss3: 4870.76\n",
      "Epoch [778], train_loss: 5457.42 with loss1: 516.30, loss2: 70.41 and loss3: 4870.71\n",
      "Epoch [779], train_loss: 5459.71 with loss1: 517.83, loss2: 71.22 and loss3: 4870.66\n",
      "Epoch [780], train_loss: 5454.68 with loss1: 513.75, loss2: 70.32 and loss3: 4870.61\n",
      "Epoch [781], train_loss: 5459.13 with loss1: 517.47, loss2: 71.10 and loss3: 4870.56\n",
      "Epoch [782], train_loss: 5454.07 with loss1: 513.27, loss2: 70.29 and loss3: 4870.51\n",
      "Epoch [783], train_loss: 5456.83 with loss1: 515.41, loss2: 70.96 and loss3: 4870.46\n",
      "Epoch [784], train_loss: 5453.60 with loss1: 512.90, loss2: 70.29 and loss3: 4870.41\n",
      "Epoch [785], train_loss: 5454.49 with loss1: 513.55, loss2: 70.58 and loss3: 4870.36\n",
      "Epoch [786], train_loss: 5452.48 with loss1: 512.02, loss2: 70.15 and loss3: 4870.31\n",
      "Epoch [787], train_loss: 5454.32 with loss1: 513.52, loss2: 70.54 and loss3: 4870.26\n",
      "Epoch [788], train_loss: 5448.36 with loss1: 508.27, loss2: 69.87 and loss3: 4870.21\n",
      "Epoch [789], train_loss: 5454.28 with loss1: 513.77, loss2: 70.34 and loss3: 4870.16\n",
      "Epoch [790], train_loss: 5447.69 with loss1: 507.36, loss2: 70.21 and loss3: 4870.11\n",
      "Epoch [791], train_loss: 5449.92 with loss1: 509.60, loss2: 70.26 and loss3: 4870.06\n",
      "Epoch [792], train_loss: 5444.62 with loss1: 504.61, loss2: 70.00 and loss3: 4870.01\n",
      "Epoch [793], train_loss: 5447.34 with loss1: 507.12, loss2: 70.25 and loss3: 4869.96\n",
      "Epoch [794], train_loss: 5443.83 with loss1: 503.67, loss2: 70.24 and loss3: 4869.92\n",
      "Epoch [795], train_loss: 5444.25 with loss1: 503.93, loss2: 70.46 and loss3: 4869.87\n",
      "Epoch [796], train_loss: 5438.37 with loss1: 498.51, loss2: 70.04 and loss3: 4869.82\n",
      "Epoch [797], train_loss: 5440.91 with loss1: 500.53, loss2: 70.62 and loss3: 4869.77\n",
      "Epoch [798], train_loss: 5437.48 with loss1: 497.90, loss2: 69.86 and loss3: 4869.72\n",
      "Epoch [799], train_loss: 5439.41 with loss1: 499.70, loss2: 70.05 and loss3: 4869.67\n",
      "Epoch [800], train_loss: 5435.15 with loss1: 495.72, loss2: 69.81 and loss3: 4869.62\n",
      "Epoch [801], train_loss: 5438.60 with loss1: 498.46, loss2: 70.56 and loss3: 4869.57\n",
      "Epoch [802], train_loss: 5435.69 with loss1: 496.40, loss2: 69.77 and loss3: 4869.52\n",
      "Epoch [803], train_loss: 5438.12 with loss1: 498.42, loss2: 70.24 and loss3: 4869.47\n",
      "Epoch [804], train_loss: 5434.00 with loss1: 494.73, loss2: 69.85 and loss3: 4869.42\n",
      "Epoch [805], train_loss: 5435.57 with loss1: 495.79, loss2: 70.41 and loss3: 4869.37\n",
      "Epoch [806], train_loss: 5432.74 with loss1: 493.61, loss2: 69.81 and loss3: 4869.32\n",
      "Epoch [807], train_loss: 5435.96 with loss1: 496.74, loss2: 69.95 and loss3: 4869.27\n",
      "Epoch [808], train_loss: 5431.50 with loss1: 492.53, loss2: 69.76 and loss3: 4869.22\n",
      "Epoch [809], train_loss: 5433.39 with loss1: 494.30, loss2: 69.92 and loss3: 4869.17\n",
      "Epoch [810], train_loss: 5432.80 with loss1: 494.02, loss2: 69.66 and loss3: 4869.12\n",
      "Epoch [811], train_loss: 5433.38 with loss1: 494.65, loss2: 69.66 and loss3: 4869.07\n",
      "Epoch [812], train_loss: 5430.66 with loss1: 492.25, loss2: 69.39 and loss3: 4869.02\n",
      "Epoch [813], train_loss: 5434.94 with loss1: 495.98, loss2: 69.99 and loss3: 4868.97\n",
      "Epoch [814], train_loss: 5431.07 with loss1: 492.87, loss2: 69.28 and loss3: 4868.92\n",
      "Epoch [815], train_loss: 5433.30 with loss1: 494.56, loss2: 69.87 and loss3: 4868.87\n",
      "Epoch [816], train_loss: 5433.51 with loss1: 494.62, loss2: 70.06 and loss3: 4868.82\n",
      "Epoch [817], train_loss: 5436.22 with loss1: 497.56, loss2: 69.89 and loss3: 4868.77\n",
      "Epoch [818], train_loss: 5433.88 with loss1: 495.66, loss2: 69.50 and loss3: 4868.72\n",
      "Epoch [819], train_loss: 5435.63 with loss1: 497.29, loss2: 69.67 and loss3: 4868.67\n",
      "Epoch [820], train_loss: 5434.17 with loss1: 495.69, loss2: 69.86 and loss3: 4868.62\n",
      "Epoch [821], train_loss: 5435.46 with loss1: 497.01, loss2: 69.88 and loss3: 4868.57\n",
      "Epoch [822], train_loss: 5433.79 with loss1: 495.64, loss2: 69.62 and loss3: 4868.52\n",
      "Epoch [823], train_loss: 5436.70 with loss1: 498.65, loss2: 69.58 and loss3: 4868.47\n",
      "Epoch [824], train_loss: 5434.68 with loss1: 496.75, loss2: 69.50 and loss3: 4868.42\n",
      "Epoch [825], train_loss: 5436.38 with loss1: 498.32, loss2: 69.69 and loss3: 4868.38\n",
      "Epoch [826], train_loss: 5434.67 with loss1: 497.09, loss2: 69.26 and loss3: 4868.33\n",
      "Epoch [827], train_loss: 5436.54 with loss1: 498.73, loss2: 69.54 and loss3: 4868.28\n",
      "Epoch [828], train_loss: 5435.27 with loss1: 497.68, loss2: 69.36 and loss3: 4868.23\n",
      "Epoch [829], train_loss: 5438.52 with loss1: 500.79, loss2: 69.55 and loss3: 4868.18\n",
      "Epoch [830], train_loss: 5435.17 with loss1: 497.69, loss2: 69.35 and loss3: 4868.13\n",
      "Epoch [831], train_loss: 5438.14 with loss1: 500.58, loss2: 69.48 and loss3: 4868.08\n",
      "Epoch [832], train_loss: 5434.04 with loss1: 496.64, loss2: 69.37 and loss3: 4868.03\n",
      "Epoch [833], train_loss: 5437.56 with loss1: 500.05, loss2: 69.54 and loss3: 4867.98\n",
      "Epoch [834], train_loss: 5434.85 with loss1: 497.69, loss2: 69.23 and loss3: 4867.93\n",
      "Epoch [835], train_loss: 5437.56 with loss1: 500.09, loss2: 69.60 and loss3: 4867.88\n",
      "Epoch [836], train_loss: 5433.46 with loss1: 496.66, loss2: 68.97 and loss3: 4867.83\n",
      "Epoch [837], train_loss: 5437.94 with loss1: 500.68, loss2: 69.49 and loss3: 4867.78\n",
      "Epoch [838], train_loss: 5434.69 with loss1: 497.63, loss2: 69.33 and loss3: 4867.73\n",
      "Epoch [839], train_loss: 5438.00 with loss1: 500.79, loss2: 69.53 and loss3: 4867.68\n",
      "Epoch [840], train_loss: 5434.32 with loss1: 497.62, loss2: 69.07 and loss3: 4867.63\n",
      "Epoch [841], train_loss: 5438.99 with loss1: 501.87, loss2: 69.54 and loss3: 4867.58\n",
      "Epoch [842], train_loss: 5433.54 with loss1: 497.03, loss2: 68.98 and loss3: 4867.53\n",
      "Epoch [843], train_loss: 5438.24 with loss1: 501.51, loss2: 69.25 and loss3: 4867.48\n",
      "Epoch [844], train_loss: 5432.55 with loss1: 496.26, loss2: 68.86 and loss3: 4867.43\n",
      "Epoch [845], train_loss: 5437.28 with loss1: 500.71, loss2: 69.19 and loss3: 4867.38\n",
      "Epoch [846], train_loss: 5433.41 with loss1: 497.13, loss2: 68.95 and loss3: 4867.33\n",
      "Epoch [847], train_loss: 5439.11 with loss1: 502.27, loss2: 69.55 and loss3: 4867.28\n",
      "Epoch [848], train_loss: 5433.53 with loss1: 497.46, loss2: 68.84 and loss3: 4867.23\n",
      "Epoch [849], train_loss: 5435.03 with loss1: 498.82, loss2: 69.03 and loss3: 4867.18\n",
      "Epoch [850], train_loss: 5431.66 with loss1: 495.88, loss2: 68.64 and loss3: 4867.13\n",
      "Epoch [851], train_loss: 5435.73 with loss1: 499.38, loss2: 69.26 and loss3: 4867.08\n",
      "Epoch [852], train_loss: 5430.57 with loss1: 494.74, loss2: 68.80 and loss3: 4867.03\n",
      "Epoch [853], train_loss: 5433.98 with loss1: 497.75, loss2: 69.25 and loss3: 4866.98\n",
      "Epoch [854], train_loss: 5431.80 with loss1: 496.10, loss2: 68.77 and loss3: 4866.93\n",
      "Epoch [855], train_loss: 5434.84 with loss1: 499.12, loss2: 68.84 and loss3: 4866.88\n",
      "Epoch [856], train_loss: 5429.78 with loss1: 494.26, loss2: 68.69 and loss3: 4866.83\n",
      "Epoch [857], train_loss: 5432.09 with loss1: 496.47, loss2: 68.84 and loss3: 4866.79\n",
      "Epoch [858], train_loss: 5428.71 with loss1: 493.37, loss2: 68.61 and loss3: 4866.74\n",
      "Epoch [859], train_loss: 5431.76 with loss1: 495.86, loss2: 69.22 and loss3: 4866.69\n",
      "Epoch [860], train_loss: 5428.72 with loss1: 493.47, loss2: 68.61 and loss3: 4866.64\n",
      "Epoch [861], train_loss: 5432.03 with loss1: 496.10, loss2: 69.35 and loss3: 4866.59\n",
      "Epoch [862], train_loss: 5427.59 with loss1: 492.34, loss2: 68.71 and loss3: 4866.54\n",
      "Epoch [863], train_loss: 5429.79 with loss1: 494.11, loss2: 69.20 and loss3: 4866.49\n",
      "Epoch [864], train_loss: 5426.54 with loss1: 491.60, loss2: 68.50 and loss3: 4866.44\n",
      "Epoch [865], train_loss: 5430.69 with loss1: 495.35, loss2: 68.95 and loss3: 4866.39\n",
      "Epoch [866], train_loss: 5426.64 with loss1: 491.56, loss2: 68.75 and loss3: 4866.34\n",
      "Epoch [867], train_loss: 5430.25 with loss1: 495.18, loss2: 68.79 and loss3: 4866.29\n",
      "Epoch [868], train_loss: 5425.81 with loss1: 491.34, loss2: 68.23 and loss3: 4866.24\n",
      "Epoch [869], train_loss: 5427.67 with loss1: 492.80, loss2: 68.68 and loss3: 4866.19\n",
      "Epoch [870], train_loss: 5425.89 with loss1: 490.95, loss2: 68.81 and loss3: 4866.14\n",
      "Epoch [871], train_loss: 5426.95 with loss1: 492.04, loss2: 68.82 and loss3: 4866.09\n",
      "Epoch [872], train_loss: 5423.62 with loss1: 489.18, loss2: 68.40 and loss3: 4866.04\n",
      "Epoch [873], train_loss: 5427.38 with loss1: 492.70, loss2: 68.69 and loss3: 4865.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [874], train_loss: 5422.44 with loss1: 488.28, loss2: 68.23 and loss3: 4865.94\n",
      "Epoch [875], train_loss: 5425.21 with loss1: 490.67, loss2: 68.65 and loss3: 4865.89\n",
      "Epoch [876], train_loss: 5421.93 with loss1: 487.83, loss2: 68.26 and loss3: 4865.84\n",
      "Epoch [877], train_loss: 5426.27 with loss1: 491.77, loss2: 68.71 and loss3: 4865.79\n",
      "Epoch [878], train_loss: 5422.71 with loss1: 488.32, loss2: 68.64 and loss3: 4865.74\n",
      "Epoch [879], train_loss: 5424.21 with loss1: 490.02, loss2: 68.50 and loss3: 4865.69\n",
      "Epoch [880], train_loss: 5421.47 with loss1: 487.56, loss2: 68.26 and loss3: 4865.64\n",
      "Epoch [881], train_loss: 5425.07 with loss1: 490.75, loss2: 68.72 and loss3: 4865.59\n",
      "Epoch [882], train_loss: 5421.88 with loss1: 488.00, loss2: 68.33 and loss3: 4865.54\n",
      "Epoch [883], train_loss: 5425.66 with loss1: 491.73, loss2: 68.43 and loss3: 4865.49\n",
      "Epoch [884], train_loss: 5422.17 with loss1: 488.30, loss2: 68.42 and loss3: 4865.44\n",
      "Epoch [885], train_loss: 5424.12 with loss1: 490.50, loss2: 68.22 and loss3: 4865.39\n",
      "Epoch [886], train_loss: 5421.92 with loss1: 488.50, loss2: 68.07 and loss3: 4865.34\n",
      "Epoch [887], train_loss: 5424.44 with loss1: 490.62, loss2: 68.52 and loss3: 4865.29\n",
      "Epoch [888], train_loss: 5421.27 with loss1: 487.94, loss2: 68.08 and loss3: 4865.25\n",
      "Epoch [889], train_loss: 5426.22 with loss1: 492.63, loss2: 68.39 and loss3: 4865.20\n",
      "Epoch [890], train_loss: 5422.27 with loss1: 489.21, loss2: 67.91 and loss3: 4865.15\n",
      "Epoch [891], train_loss: 5426.54 with loss1: 492.75, loss2: 68.69 and loss3: 4865.10\n",
      "Epoch [892], train_loss: 5423.46 with loss1: 490.65, loss2: 67.77 and loss3: 4865.05\n",
      "Epoch [893], train_loss: 5427.11 with loss1: 493.69, loss2: 68.42 and loss3: 4865.00\n",
      "Epoch [894], train_loss: 5424.07 with loss1: 491.27, loss2: 67.86 and loss3: 4864.95\n",
      "Epoch [895], train_loss: 5426.03 with loss1: 493.21, loss2: 67.92 and loss3: 4864.90\n",
      "Epoch [896], train_loss: 5423.54 with loss1: 490.65, loss2: 68.04 and loss3: 4864.85\n",
      "Epoch [897], train_loss: 5426.55 with loss1: 493.47, loss2: 68.28 and loss3: 4864.80\n",
      "Epoch [898], train_loss: 5423.77 with loss1: 490.99, loss2: 68.03 and loss3: 4864.75\n",
      "Epoch [899], train_loss: 5426.61 with loss1: 493.92, loss2: 67.99 and loss3: 4864.70\n",
      "Epoch [900], train_loss: 5424.90 with loss1: 492.40, loss2: 67.85 and loss3: 4864.65\n",
      "Epoch [901], train_loss: 5428.62 with loss1: 495.98, loss2: 68.03 and loss3: 4864.60\n",
      "Epoch [902], train_loss: 5424.21 with loss1: 491.59, loss2: 68.07 and loss3: 4864.55\n",
      "Epoch [903], train_loss: 5427.09 with loss1: 494.53, loss2: 68.06 and loss3: 4864.50\n",
      "Epoch [904], train_loss: 5423.25 with loss1: 490.39, loss2: 68.41 and loss3: 4864.45\n",
      "Epoch [905], train_loss: 5427.66 with loss1: 495.11, loss2: 68.15 and loss3: 4864.40\n",
      "Epoch [906], train_loss: 5426.02 with loss1: 493.87, loss2: 67.80 and loss3: 4864.35\n",
      "Epoch [907], train_loss: 5427.73 with loss1: 495.23, loss2: 68.19 and loss3: 4864.30\n",
      "Epoch [908], train_loss: 5423.11 with loss1: 491.29, loss2: 67.57 and loss3: 4864.25\n",
      "Epoch [909], train_loss: 5426.84 with loss1: 494.74, loss2: 67.89 and loss3: 4864.20\n",
      "Epoch [910], train_loss: 5422.76 with loss1: 491.07, loss2: 67.53 and loss3: 4864.15\n",
      "Epoch [911], train_loss: 5428.37 with loss1: 496.09, loss2: 68.18 and loss3: 4864.10\n",
      "Epoch [912], train_loss: 5423.51 with loss1: 491.70, loss2: 67.76 and loss3: 4864.05\n",
      "Epoch [913], train_loss: 5427.40 with loss1: 495.66, loss2: 67.74 and loss3: 4864.00\n",
      "Epoch [914], train_loss: 5422.99 with loss1: 491.40, loss2: 67.64 and loss3: 4863.95\n",
      "Epoch [915], train_loss: 5427.98 with loss1: 496.30, loss2: 67.77 and loss3: 4863.90\n",
      "Epoch [916], train_loss: 5425.00 with loss1: 493.24, loss2: 67.90 and loss3: 4863.85\n",
      "Epoch [917], train_loss: 5428.48 with loss1: 496.91, loss2: 67.77 and loss3: 4863.81\n",
      "Epoch [918], train_loss: 5424.27 with loss1: 493.09, loss2: 67.43 and loss3: 4863.76\n",
      "Epoch [919], train_loss: 5428.28 with loss1: 496.74, loss2: 67.83 and loss3: 4863.71\n",
      "Epoch [920], train_loss: 5425.29 with loss1: 493.94, loss2: 67.70 and loss3: 4863.66\n",
      "Epoch [921], train_loss: 5427.95 with loss1: 496.61, loss2: 67.74 and loss3: 4863.61\n",
      "Epoch [922], train_loss: 5426.14 with loss1: 495.11, loss2: 67.48 and loss3: 4863.56\n",
      "Epoch [923], train_loss: 5429.62 with loss1: 498.18, loss2: 67.93 and loss3: 4863.51\n",
      "Epoch [924], train_loss: 5425.92 with loss1: 494.99, loss2: 67.47 and loss3: 4863.46\n",
      "Epoch [925], train_loss: 5429.34 with loss1: 498.45, loss2: 67.48 and loss3: 4863.41\n",
      "Epoch [926], train_loss: 5425.60 with loss1: 494.86, loss2: 67.39 and loss3: 4863.36\n",
      "Epoch [927], train_loss: 5429.82 with loss1: 498.98, loss2: 67.54 and loss3: 4863.31\n",
      "Epoch [928], train_loss: 5427.59 with loss1: 497.05, loss2: 67.27 and loss3: 4863.26\n",
      "Epoch [929], train_loss: 5430.73 with loss1: 500.08, loss2: 67.44 and loss3: 4863.21\n",
      "Epoch [930], train_loss: 5426.25 with loss1: 495.39, loss2: 67.69 and loss3: 4863.16\n",
      "Epoch [931], train_loss: 5430.54 with loss1: 499.76, loss2: 67.68 and loss3: 4863.11\n",
      "Epoch [932], train_loss: 5426.80 with loss1: 496.21, loss2: 67.54 and loss3: 4863.06\n",
      "Epoch [933], train_loss: 5430.09 with loss1: 499.42, loss2: 67.67 and loss3: 4863.01\n",
      "Epoch [934], train_loss: 5425.73 with loss1: 495.48, loss2: 67.29 and loss3: 4862.96\n",
      "Epoch [935], train_loss: 5429.86 with loss1: 499.59, loss2: 67.36 and loss3: 4862.91\n",
      "Epoch [936], train_loss: 5426.07 with loss1: 495.82, loss2: 67.39 and loss3: 4862.86\n",
      "Epoch [937], train_loss: 5429.65 with loss1: 499.26, loss2: 67.58 and loss3: 4862.81\n",
      "Epoch [938], train_loss: 5424.79 with loss1: 494.81, loss2: 67.22 and loss3: 4862.76\n",
      "Epoch [939], train_loss: 5428.50 with loss1: 498.30, loss2: 67.50 and loss3: 4862.71\n",
      "Epoch [940], train_loss: 5423.81 with loss1: 493.66, loss2: 67.49 and loss3: 4862.66\n",
      "Epoch [941], train_loss: 5424.74 with loss1: 494.83, loss2: 67.30 and loss3: 4862.61\n",
      "Epoch [942], train_loss: 5420.27 with loss1: 490.84, loss2: 66.87 and loss3: 4862.56\n",
      "Epoch [943], train_loss: 5422.85 with loss1: 493.25, loss2: 67.09 and loss3: 4862.51\n",
      "Epoch [944], train_loss: 5419.24 with loss1: 489.73, loss2: 67.04 and loss3: 4862.46\n",
      "Epoch [945], train_loss: 5419.93 with loss1: 490.19, loss2: 67.33 and loss3: 4862.42\n",
      "Epoch [946], train_loss: 5416.59 with loss1: 487.52, loss2: 66.70 and loss3: 4862.37\n",
      "Epoch [947], train_loss: 5418.11 with loss1: 488.22, loss2: 67.57 and loss3: 4862.32\n",
      "Epoch [948], train_loss: 5414.26 with loss1: 484.84, loss2: 67.15 and loss3: 4862.27\n",
      "Epoch [949], train_loss: 5414.63 with loss1: 485.36, loss2: 67.05 and loss3: 4862.22\n",
      "Epoch [950], train_loss: 5411.27 with loss1: 481.70, loss2: 67.41 and loss3: 4862.17\n",
      "Epoch [951], train_loss: 5413.95 with loss1: 484.14, loss2: 67.69 and loss3: 4862.12\n",
      "Epoch [952], train_loss: 5406.56 with loss1: 477.65, loss2: 66.84 and loss3: 4862.07\n",
      "Epoch [953], train_loss: 5408.47 with loss1: 479.54, loss2: 66.92 and loss3: 4862.02\n",
      "Epoch [954], train_loss: 5403.92 with loss1: 475.08, loss2: 66.86 and loss3: 4861.97\n",
      "Epoch [955], train_loss: 5407.68 with loss1: 478.84, loss2: 66.92 and loss3: 4861.92\n",
      "Epoch [956], train_loss: 5402.16 with loss1: 473.43, loss2: 66.86 and loss3: 4861.87\n",
      "Epoch [957], train_loss: 5401.72 with loss1: 472.99, loss2: 66.91 and loss3: 4861.82\n",
      "Epoch [958], train_loss: 5401.45 with loss1: 472.90, loss2: 66.78 and loss3: 4861.77\n",
      "Epoch [959], train_loss: 5399.97 with loss1: 471.39, loss2: 66.87 and loss3: 4861.72\n",
      "Epoch [960], train_loss: 5396.32 with loss1: 467.71, loss2: 66.94 and loss3: 4861.67\n",
      "Epoch [961], train_loss: 5398.60 with loss1: 470.10, loss2: 66.88 and loss3: 4861.62\n",
      "Epoch [962], train_loss: 5394.24 with loss1: 465.74, loss2: 66.93 and loss3: 4861.57\n",
      "Epoch [963], train_loss: 5396.02 with loss1: 467.65, loss2: 66.85 and loss3: 4861.52\n",
      "Epoch [964], train_loss: 5393.07 with loss1: 464.66, loss2: 66.93 and loss3: 4861.47\n",
      "Epoch [965], train_loss: 5394.06 with loss1: 465.58, loss2: 67.06 and loss3: 4861.42\n",
      "Epoch [966], train_loss: 5391.44 with loss1: 463.41, loss2: 66.66 and loss3: 4861.37\n",
      "Epoch [967], train_loss: 5392.75 with loss1: 464.72, loss2: 66.71 and loss3: 4861.32\n",
      "Epoch [968], train_loss: 5390.49 with loss1: 462.79, loss2: 66.43 and loss3: 4861.27\n",
      "Epoch [969], train_loss: 5393.34 with loss1: 465.49, loss2: 66.63 and loss3: 4861.22\n",
      "Epoch [970], train_loss: 5389.25 with loss1: 461.74, loss2: 66.34 and loss3: 4861.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [971], train_loss: 5393.18 with loss1: 465.35, loss2: 66.71 and loss3: 4861.12\n",
      "Epoch [972], train_loss: 5390.48 with loss1: 462.95, loss2: 66.45 and loss3: 4861.07\n",
      "Epoch [973], train_loss: 5391.08 with loss1: 463.14, loss2: 66.92 and loss3: 4861.02\n",
      "Epoch [974], train_loss: 5390.75 with loss1: 463.30, loss2: 66.48 and loss3: 4860.98\n",
      "Epoch [975], train_loss: 5393.23 with loss1: 465.77, loss2: 66.54 and loss3: 4860.93\n",
      "Epoch [976], train_loss: 5390.26 with loss1: 462.87, loss2: 66.51 and loss3: 4860.88\n",
      "Epoch [977], train_loss: 5393.78 with loss1: 466.41, loss2: 66.55 and loss3: 4860.83\n",
      "Epoch [978], train_loss: 5391.35 with loss1: 464.08, loss2: 66.49 and loss3: 4860.78\n",
      "Epoch [979], train_loss: 5392.17 with loss1: 465.06, loss2: 66.38 and loss3: 4860.73\n",
      "Epoch [980], train_loss: 5392.15 with loss1: 465.00, loss2: 66.48 and loss3: 4860.68\n",
      "Epoch [981], train_loss: 5393.85 with loss1: 466.79, loss2: 66.43 and loss3: 4860.63\n",
      "Epoch [982], train_loss: 5394.00 with loss1: 467.35, loss2: 66.07 and loss3: 4860.58\n",
      "Epoch [983], train_loss: 5394.22 with loss1: 467.21, loss2: 66.48 and loss3: 4860.53\n",
      "Epoch [984], train_loss: 5392.28 with loss1: 465.58, loss2: 66.22 and loss3: 4860.48\n",
      "Epoch [985], train_loss: 5396.09 with loss1: 469.44, loss2: 66.22 and loss3: 4860.43\n",
      "Epoch [986], train_loss: 5395.32 with loss1: 468.80, loss2: 66.14 and loss3: 4860.38\n",
      "Epoch [987], train_loss: 5398.23 with loss1: 471.49, loss2: 66.41 and loss3: 4860.33\n",
      "Epoch [988], train_loss: 5396.63 with loss1: 470.32, loss2: 66.03 and loss3: 4860.28\n",
      "Epoch [989], train_loss: 5401.07 with loss1: 474.54, loss2: 66.30 and loss3: 4860.23\n",
      "Epoch [990], train_loss: 5397.59 with loss1: 471.21, loss2: 66.20 and loss3: 4860.18\n",
      "Epoch [991], train_loss: 5402.92 with loss1: 475.71, loss2: 67.07 and loss3: 4860.13\n",
      "Epoch [992], train_loss: 5400.35 with loss1: 474.27, loss2: 65.99 and loss3: 4860.08\n",
      "Epoch [993], train_loss: 5404.88 with loss1: 478.65, loss2: 66.19 and loss3: 4860.03\n",
      "Epoch [994], train_loss: 5403.82 with loss1: 477.73, loss2: 66.11 and loss3: 4859.98\n",
      "Epoch [995], train_loss: 5406.62 with loss1: 480.55, loss2: 66.14 and loss3: 4859.93\n",
      "Epoch [996], train_loss: 5405.80 with loss1: 479.87, loss2: 66.05 and loss3: 4859.88\n",
      "Epoch [997], train_loss: 5410.82 with loss1: 484.69, loss2: 66.30 and loss3: 4859.83\n",
      "Epoch [998], train_loss: 5409.33 with loss1: 483.40, loss2: 66.15 and loss3: 4859.78\n",
      "Epoch [999], train_loss: 5414.21 with loss1: 488.05, loss2: 66.43 and loss3: 4859.73\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1.\n",
    "# model version 3.5\n",
    "# with loss2 and loss3\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history = fit(epochs=1000, lr=5e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f95107ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 5411.40 with loss1: 485.95, loss2: 65.76 and loss3: 4859.68\n",
      "Epoch [1], train_loss: 5416.79 with loss1: 490.91, loss2: 66.25 and loss3: 4859.63\n",
      "Epoch [2], train_loss: 5415.05 with loss1: 489.62, loss2: 65.85 and loss3: 4859.58\n",
      "Epoch [3], train_loss: 5420.06 with loss1: 494.33, loss2: 66.20 and loss3: 4859.54\n",
      "Epoch [4], train_loss: 5417.21 with loss1: 491.77, loss2: 65.95 and loss3: 4859.49\n",
      "Epoch [5], train_loss: 5423.61 with loss1: 498.05, loss2: 66.12 and loss3: 4859.44\n",
      "Epoch [6], train_loss: 5419.52 with loss1: 494.48, loss2: 65.66 and loss3: 4859.39\n",
      "Epoch [7], train_loss: 5426.50 with loss1: 501.19, loss2: 65.97 and loss3: 4859.34\n",
      "Epoch [8], train_loss: 5424.80 with loss1: 499.85, loss2: 65.65 and loss3: 4859.29\n",
      "Epoch [9], train_loss: 5429.48 with loss1: 504.23, loss2: 66.01 and loss3: 4859.24\n",
      "Epoch [10], train_loss: 5426.93 with loss1: 501.78, loss2: 65.96 and loss3: 4859.19\n",
      "Epoch [11], train_loss: 5433.04 with loss1: 507.77, loss2: 66.13 and loss3: 4859.14\n",
      "Epoch [12], train_loss: 5427.35 with loss1: 502.67, loss2: 65.59 and loss3: 4859.09\n",
      "Epoch [13], train_loss: 5432.81 with loss1: 507.75, loss2: 66.03 and loss3: 4859.04\n",
      "Epoch [14], train_loss: 5429.88 with loss1: 505.03, loss2: 65.86 and loss3: 4858.99\n",
      "Epoch [15], train_loss: 5435.76 with loss1: 510.65, loss2: 66.17 and loss3: 4858.94\n",
      "Epoch [16], train_loss: 5430.95 with loss1: 506.43, loss2: 65.63 and loss3: 4858.89\n",
      "Epoch [17], train_loss: 5434.87 with loss1: 509.82, loss2: 66.21 and loss3: 4858.84\n",
      "Epoch [18], train_loss: 5429.69 with loss1: 505.40, loss2: 65.50 and loss3: 4858.79\n",
      "Epoch [19], train_loss: 5435.48 with loss1: 510.75, loss2: 65.99 and loss3: 4858.74\n",
      "Epoch [20], train_loss: 5429.95 with loss1: 505.76, loss2: 65.50 and loss3: 4858.69\n",
      "Epoch [21], train_loss: 5433.65 with loss1: 509.07, loss2: 65.94 and loss3: 4858.64\n",
      "Epoch [22], train_loss: 5428.83 with loss1: 504.67, loss2: 65.57 and loss3: 4858.59\n",
      "Epoch [23], train_loss: 5433.97 with loss1: 509.68, loss2: 65.74 and loss3: 4858.54\n",
      "Epoch [24], train_loss: 5427.60 with loss1: 503.72, loss2: 65.38 and loss3: 4858.49\n",
      "Epoch [25], train_loss: 5431.12 with loss1: 506.94, loss2: 65.74 and loss3: 4858.44\n",
      "Epoch [26], train_loss: 5422.81 with loss1: 499.15, loss2: 65.27 and loss3: 4858.39\n",
      "Epoch [27], train_loss: 5426.00 with loss1: 501.82, loss2: 65.84 and loss3: 4858.34\n",
      "Epoch [28], train_loss: 5419.32 with loss1: 495.82, loss2: 65.20 and loss3: 4858.29\n",
      "Epoch [29], train_loss: 5423.41 with loss1: 499.15, loss2: 66.02 and loss3: 4858.25\n",
      "Epoch [30], train_loss: 5416.67 with loss1: 493.20, loss2: 65.28 and loss3: 4858.20\n",
      "Epoch [31], train_loss: 5419.20 with loss1: 495.25, loss2: 65.80 and loss3: 4858.15\n",
      "Epoch [32], train_loss: 5413.30 with loss1: 489.85, loss2: 65.36 and loss3: 4858.10\n",
      "Epoch [33], train_loss: 5415.48 with loss1: 491.72, loss2: 65.72 and loss3: 4858.05\n",
      "Epoch [34], train_loss: 5409.96 with loss1: 486.56, loss2: 65.41 and loss3: 4858.00\n",
      "Epoch [35], train_loss: 5412.30 with loss1: 488.55, loss2: 65.80 and loss3: 4857.95\n",
      "Epoch [36], train_loss: 5406.38 with loss1: 483.26, loss2: 65.22 and loss3: 4857.90\n",
      "Epoch [37], train_loss: 5409.32 with loss1: 485.90, loss2: 65.57 and loss3: 4857.85\n",
      "Epoch [38], train_loss: 5405.88 with loss1: 482.96, loss2: 65.12 and loss3: 4857.80\n",
      "Epoch [39], train_loss: 5405.69 with loss1: 482.01, loss2: 65.94 and loss3: 4857.75\n",
      "Epoch [40], train_loss: 5401.27 with loss1: 478.31, loss2: 65.26 and loss3: 4857.70\n",
      "Epoch [41], train_loss: 5401.74 with loss1: 478.67, loss2: 65.43 and loss3: 4857.65\n",
      "Epoch [42], train_loss: 5397.03 with loss1: 474.41, loss2: 65.03 and loss3: 4857.60\n",
      "Epoch [43], train_loss: 5398.34 with loss1: 475.38, loss2: 65.41 and loss3: 4857.55\n",
      "Epoch [44], train_loss: 5395.63 with loss1: 472.61, loss2: 65.51 and loss3: 4857.50\n",
      "Epoch [45], train_loss: 5397.77 with loss1: 474.91, loss2: 65.42 and loss3: 4857.45\n",
      "Epoch [46], train_loss: 5392.57 with loss1: 469.91, loss2: 65.26 and loss3: 4857.40\n",
      "Epoch [47], train_loss: 5395.39 with loss1: 472.45, loss2: 65.59 and loss3: 4857.35\n",
      "Epoch [48], train_loss: 5393.10 with loss1: 470.63, loss2: 65.17 and loss3: 4857.30\n",
      "Epoch [49], train_loss: 5392.18 with loss1: 469.65, loss2: 65.28 and loss3: 4857.25\n",
      "Epoch [50], train_loss: 5389.83 with loss1: 467.58, loss2: 65.05 and loss3: 4857.20\n",
      "Epoch [51], train_loss: 5391.30 with loss1: 468.76, loss2: 65.39 and loss3: 4857.15\n",
      "Epoch [52], train_loss: 5388.89 with loss1: 466.88, loss2: 64.90 and loss3: 4857.10\n",
      "Epoch [53], train_loss: 5391.26 with loss1: 469.04, loss2: 65.17 and loss3: 4857.05\n",
      "Epoch [54], train_loss: 5388.24 with loss1: 466.19, loss2: 65.04 and loss3: 4857.00\n",
      "Epoch [55], train_loss: 5391.52 with loss1: 469.19, loss2: 65.37 and loss3: 4856.96\n",
      "Epoch [56], train_loss: 5389.20 with loss1: 467.27, loss2: 65.02 and loss3: 4856.91\n",
      "Epoch [57], train_loss: 5391.64 with loss1: 469.63, loss2: 65.15 and loss3: 4856.86\n",
      "Epoch [58], train_loss: 5389.62 with loss1: 467.72, loss2: 65.09 and loss3: 4856.81\n",
      "Epoch [59], train_loss: 5393.17 with loss1: 471.23, loss2: 65.19 and loss3: 4856.76\n",
      "Epoch [60], train_loss: 5387.77 with loss1: 465.94, loss2: 65.13 and loss3: 4856.71\n",
      "Epoch [61], train_loss: 5390.88 with loss1: 469.31, loss2: 64.91 and loss3: 4856.66\n",
      "Epoch [62], train_loss: 5387.06 with loss1: 465.79, loss2: 64.66 and loss3: 4856.61\n",
      "Epoch [63], train_loss: 5390.04 with loss1: 468.33, loss2: 65.15 and loss3: 4856.56\n",
      "Epoch [64], train_loss: 5387.76 with loss1: 466.47, loss2: 64.78 and loss3: 4856.51\n",
      "Epoch [65], train_loss: 5389.78 with loss1: 468.19, loss2: 65.13 and loss3: 4856.46\n",
      "Epoch [66], train_loss: 5388.58 with loss1: 467.44, loss2: 64.73 and loss3: 4856.41\n",
      "Epoch [67], train_loss: 5389.03 with loss1: 467.85, loss2: 64.81 and loss3: 4856.36\n",
      "Epoch [68], train_loss: 5388.67 with loss1: 467.79, loss2: 64.56 and loss3: 4856.31\n",
      "Epoch [69], train_loss: 5392.74 with loss1: 471.54, loss2: 64.95 and loss3: 4856.26\n",
      "Epoch [70], train_loss: 5391.55 with loss1: 470.45, loss2: 64.90 and loss3: 4856.21\n",
      "Epoch [71], train_loss: 5393.62 with loss1: 472.57, loss2: 64.88 and loss3: 4856.16\n",
      "Epoch [72], train_loss: 5390.31 with loss1: 469.57, loss2: 64.63 and loss3: 4856.11\n",
      "Epoch [73], train_loss: 5395.22 with loss1: 474.34, loss2: 64.82 and loss3: 4856.06\n",
      "Epoch [74], train_loss: 5390.99 with loss1: 470.46, loss2: 64.51 and loss3: 4856.01\n",
      "Epoch [75], train_loss: 5395.03 with loss1: 474.23, loss2: 64.84 and loss3: 4855.96\n",
      "Epoch [76], train_loss: 5392.95 with loss1: 472.56, loss2: 64.47 and loss3: 4855.91\n",
      "Epoch [77], train_loss: 5395.42 with loss1: 474.46, loss2: 65.09 and loss3: 4855.86\n",
      "Epoch [78], train_loss: 5392.19 with loss1: 471.76, loss2: 64.61 and loss3: 4855.81\n",
      "Epoch [79], train_loss: 5397.06 with loss1: 476.23, loss2: 65.07 and loss3: 4855.76\n",
      "Epoch [80], train_loss: 5393.80 with loss1: 473.58, loss2: 64.51 and loss3: 4855.71\n",
      "Epoch [81], train_loss: 5395.56 with loss1: 475.25, loss2: 64.65 and loss3: 4855.67\n",
      "Epoch [82], train_loss: 5392.31 with loss1: 472.29, loss2: 64.40 and loss3: 4855.62\n",
      "Epoch [83], train_loss: 5395.52 with loss1: 475.24, loss2: 64.72 and loss3: 4855.57\n",
      "Epoch [84], train_loss: 5392.15 with loss1: 472.33, loss2: 64.30 and loss3: 4855.52\n",
      "Epoch [85], train_loss: 5396.92 with loss1: 476.75, loss2: 64.70 and loss3: 4855.47\n",
      "Epoch [86], train_loss: 5392.32 with loss1: 472.65, loss2: 64.25 and loss3: 4855.42\n",
      "Epoch [87], train_loss: 5395.46 with loss1: 475.36, loss2: 64.73 and loss3: 4855.37\n",
      "Epoch [88], train_loss: 5391.79 with loss1: 472.30, loss2: 64.17 and loss3: 4855.32\n",
      "Epoch [89], train_loss: 5395.19 with loss1: 475.15, loss2: 64.77 and loss3: 4855.27\n",
      "Epoch [90], train_loss: 5391.00 with loss1: 471.28, loss2: 64.50 and loss3: 4855.22\n",
      "Epoch [91], train_loss: 5393.73 with loss1: 473.90, loss2: 64.65 and loss3: 4855.17\n",
      "Epoch [92], train_loss: 5390.61 with loss1: 471.22, loss2: 64.27 and loss3: 4855.12\n",
      "Epoch [93], train_loss: 5392.88 with loss1: 473.40, loss2: 64.42 and loss3: 4855.07\n",
      "Epoch [94], train_loss: 5389.47 with loss1: 470.37, loss2: 64.07 and loss3: 4855.02\n",
      "Epoch [95], train_loss: 5393.41 with loss1: 473.90, loss2: 64.54 and loss3: 4854.97\n",
      "Epoch [96], train_loss: 5391.21 with loss1: 471.87, loss2: 64.42 and loss3: 4854.92\n",
      "Epoch [97], train_loss: 5393.06 with loss1: 473.87, loss2: 64.31 and loss3: 4854.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98], train_loss: 5390.46 with loss1: 471.38, loss2: 64.25 and loss3: 4854.82\n",
      "Epoch [99], train_loss: 5393.86 with loss1: 474.79, loss2: 64.29 and loss3: 4854.77\n",
      "Epoch [100], train_loss: 5390.63 with loss1: 471.68, loss2: 64.23 and loss3: 4854.72\n",
      "Epoch [101], train_loss: 5394.00 with loss1: 474.81, loss2: 64.51 and loss3: 4854.67\n",
      "Epoch [102], train_loss: 5388.90 with loss1: 470.26, loss2: 64.02 and loss3: 4854.62\n",
      "Epoch [103], train_loss: 5391.62 with loss1: 472.73, loss2: 64.32 and loss3: 4854.58\n",
      "Epoch [104], train_loss: 5388.16 with loss1: 469.54, loss2: 64.09 and loss3: 4854.53\n",
      "Epoch [105], train_loss: 5392.13 with loss1: 473.52, loss2: 64.13 and loss3: 4854.48\n",
      "Epoch [106], train_loss: 5388.12 with loss1: 469.42, loss2: 64.28 and loss3: 4854.43\n",
      "Epoch [107], train_loss: 5391.75 with loss1: 473.09, loss2: 64.29 and loss3: 4854.38\n",
      "Epoch [108], train_loss: 5389.43 with loss1: 470.94, loss2: 64.17 and loss3: 4854.33\n",
      "Epoch [109], train_loss: 5392.11 with loss1: 473.47, loss2: 64.36 and loss3: 4854.28\n",
      "Epoch [110], train_loss: 5388.31 with loss1: 470.01, loss2: 64.07 and loss3: 4854.23\n",
      "Epoch [111], train_loss: 5391.58 with loss1: 473.00, loss2: 64.40 and loss3: 4854.18\n",
      "Epoch [112], train_loss: 5386.63 with loss1: 468.63, loss2: 63.87 and loss3: 4854.13\n",
      "Epoch [113], train_loss: 5388.56 with loss1: 470.23, loss2: 64.25 and loss3: 4854.08\n",
      "Epoch [114], train_loss: 5386.06 with loss1: 468.07, loss2: 63.96 and loss3: 4854.03\n",
      "Epoch [115], train_loss: 5386.64 with loss1: 468.34, loss2: 64.33 and loss3: 4853.98\n",
      "Epoch [116], train_loss: 5384.90 with loss1: 467.00, loss2: 63.97 and loss3: 4853.93\n",
      "Epoch [117], train_loss: 5388.61 with loss1: 470.44, loss2: 64.29 and loss3: 4853.88\n",
      "Epoch [118], train_loss: 5385.30 with loss1: 467.71, loss2: 63.76 and loss3: 4853.83\n",
      "Epoch [119], train_loss: 5388.11 with loss1: 470.38, loss2: 63.95 and loss3: 4853.78\n",
      "Epoch [120], train_loss: 5386.17 with loss1: 468.51, loss2: 63.93 and loss3: 4853.73\n",
      "Epoch [121], train_loss: 5388.48 with loss1: 470.80, loss2: 63.99 and loss3: 4853.68\n",
      "Epoch [122], train_loss: 5386.52 with loss1: 469.29, loss2: 63.60 and loss3: 4853.63\n",
      "Epoch [123], train_loss: 5390.30 with loss1: 472.37, loss2: 64.34 and loss3: 4853.58\n",
      "Epoch [124], train_loss: 5386.93 with loss1: 469.60, loss2: 63.79 and loss3: 4853.53\n",
      "Epoch [125], train_loss: 5388.34 with loss1: 470.88, loss2: 63.97 and loss3: 4853.48\n",
      "Epoch [126], train_loss: 5384.54 with loss1: 467.36, loss2: 63.74 and loss3: 4853.44\n",
      "Epoch [127], train_loss: 5388.25 with loss1: 471.06, loss2: 63.81 and loss3: 4853.39\n",
      "Epoch [128], train_loss: 5384.88 with loss1: 467.92, loss2: 63.62 and loss3: 4853.34\n",
      "Epoch [129], train_loss: 5387.91 with loss1: 470.59, loss2: 64.03 and loss3: 4853.29\n",
      "Epoch [130], train_loss: 5383.92 with loss1: 467.20, loss2: 63.48 and loss3: 4853.24\n",
      "Epoch [131], train_loss: 5387.64 with loss1: 470.55, loss2: 63.91 and loss3: 4853.19\n",
      "Epoch [132], train_loss: 5384.31 with loss1: 467.48, loss2: 63.69 and loss3: 4853.14\n",
      "Epoch [133], train_loss: 5387.14 with loss1: 470.34, loss2: 63.71 and loss3: 4853.09\n",
      "Epoch [134], train_loss: 5385.62 with loss1: 468.94, loss2: 63.64 and loss3: 4853.04\n",
      "Epoch [135], train_loss: 5387.00 with loss1: 470.34, loss2: 63.67 and loss3: 4852.99\n",
      "Epoch [136], train_loss: 5384.89 with loss1: 468.39, loss2: 63.56 and loss3: 4852.94\n",
      "Epoch [137], train_loss: 5388.89 with loss1: 471.92, loss2: 64.07 and loss3: 4852.89\n",
      "Epoch [138], train_loss: 5384.46 with loss1: 468.02, loss2: 63.60 and loss3: 4852.84\n",
      "Epoch [139], train_loss: 5388.04 with loss1: 471.57, loss2: 63.68 and loss3: 4852.79\n",
      "Epoch [140], train_loss: 5385.75 with loss1: 469.54, loss2: 63.47 and loss3: 4852.74\n",
      "Epoch [141], train_loss: 5388.70 with loss1: 472.27, loss2: 63.73 and loss3: 4852.69\n",
      "Epoch [142], train_loss: 5384.37 with loss1: 468.25, loss2: 63.48 and loss3: 4852.64\n",
      "Epoch [143], train_loss: 5390.06 with loss1: 473.69, loss2: 63.78 and loss3: 4852.59\n",
      "Epoch [144], train_loss: 5384.77 with loss1: 468.52, loss2: 63.71 and loss3: 4852.54\n",
      "Epoch [145], train_loss: 5388.79 with loss1: 472.57, loss2: 63.72 and loss3: 4852.49\n",
      "Epoch [146], train_loss: 5385.07 with loss1: 469.32, loss2: 63.31 and loss3: 4852.44\n",
      "Epoch [147], train_loss: 5387.66 with loss1: 471.80, loss2: 63.47 and loss3: 4852.39\n",
      "Epoch [148], train_loss: 5384.02 with loss1: 468.36, loss2: 63.31 and loss3: 4852.34\n",
      "Epoch [149], train_loss: 5387.66 with loss1: 471.89, loss2: 63.47 and loss3: 4852.29\n",
      "Epoch [150], train_loss: 5385.67 with loss1: 470.19, loss2: 63.23 and loss3: 4852.25\n",
      "Epoch [151], train_loss: 5388.35 with loss1: 472.49, loss2: 63.66 and loss3: 4852.20\n",
      "Epoch [152], train_loss: 5385.20 with loss1: 469.60, loss2: 63.45 and loss3: 4852.15\n",
      "Epoch [153], train_loss: 5388.15 with loss1: 472.70, loss2: 63.35 and loss3: 4852.10\n",
      "Epoch [154], train_loss: 5383.64 with loss1: 468.27, loss2: 63.32 and loss3: 4852.05\n",
      "Epoch [155], train_loss: 5388.18 with loss1: 472.86, loss2: 63.32 and loss3: 4852.00\n",
      "Epoch [156], train_loss: 5386.03 with loss1: 470.76, loss2: 63.32 and loss3: 4851.95\n",
      "Epoch [157], train_loss: 5388.83 with loss1: 473.34, loss2: 63.59 and loss3: 4851.90\n",
      "Epoch [158], train_loss: 5385.44 with loss1: 470.27, loss2: 63.31 and loss3: 4851.85\n",
      "Epoch [159], train_loss: 5389.61 with loss1: 474.59, loss2: 63.22 and loss3: 4851.80\n",
      "Epoch [160], train_loss: 5385.07 with loss1: 470.28, loss2: 63.04 and loss3: 4851.75\n",
      "Epoch [161], train_loss: 5389.11 with loss1: 473.75, loss2: 63.66 and loss3: 4851.70\n",
      "Epoch [162], train_loss: 5384.53 with loss1: 469.43, loss2: 63.44 and loss3: 4851.65\n",
      "Epoch [163], train_loss: 5389.38 with loss1: 474.18, loss2: 63.60 and loss3: 4851.60\n",
      "Epoch [164], train_loss: 5384.71 with loss1: 470.15, loss2: 63.00 and loss3: 4851.55\n",
      "Epoch [165], train_loss: 5386.39 with loss1: 471.52, loss2: 63.37 and loss3: 4851.50\n",
      "Epoch [166], train_loss: 5382.90 with loss1: 468.40, loss2: 63.05 and loss3: 4851.45\n",
      "Epoch [167], train_loss: 5384.73 with loss1: 470.09, loss2: 63.24 and loss3: 4851.40\n",
      "Epoch [168], train_loss: 5381.68 with loss1: 467.35, loss2: 62.97 and loss3: 4851.35\n",
      "Epoch [169], train_loss: 5384.17 with loss1: 469.91, loss2: 62.96 and loss3: 4851.30\n",
      "Epoch [170], train_loss: 5379.25 with loss1: 465.08, loss2: 62.92 and loss3: 4851.25\n",
      "Epoch [171], train_loss: 5382.68 with loss1: 468.23, loss2: 63.25 and loss3: 4851.21\n",
      "Epoch [172], train_loss: 5379.76 with loss1: 465.44, loss2: 63.17 and loss3: 4851.16\n",
      "Epoch [173], train_loss: 5380.60 with loss1: 466.54, loss2: 62.96 and loss3: 4851.11\n",
      "Epoch [174], train_loss: 5377.29 with loss1: 463.38, loss2: 62.86 and loss3: 4851.06\n",
      "Epoch [175], train_loss: 5379.58 with loss1: 465.38, loss2: 63.19 and loss3: 4851.01\n",
      "Epoch [176], train_loss: 5376.14 with loss1: 462.23, loss2: 62.94 and loss3: 4850.96\n",
      "Epoch [177], train_loss: 5379.50 with loss1: 465.38, loss2: 63.21 and loss3: 4850.91\n",
      "Epoch [178], train_loss: 5376.56 with loss1: 462.88, loss2: 62.83 and loss3: 4850.86\n",
      "Epoch [179], train_loss: 5376.74 with loss1: 462.58, loss2: 63.36 and loss3: 4850.81\n",
      "Epoch [180], train_loss: 5373.68 with loss1: 459.99, loss2: 62.93 and loss3: 4850.76\n",
      "Epoch [181], train_loss: 5376.10 with loss1: 462.21, loss2: 63.17 and loss3: 4850.71\n",
      "Epoch [182], train_loss: 5371.81 with loss1: 458.60, loss2: 62.54 and loss3: 4850.66\n",
      "Epoch [183], train_loss: 5374.54 with loss1: 461.01, loss2: 62.93 and loss3: 4850.61\n",
      "Epoch [184], train_loss: 5371.95 with loss1: 458.22, loss2: 63.17 and loss3: 4850.56\n",
      "Epoch [185], train_loss: 5373.44 with loss1: 460.04, loss2: 62.88 and loss3: 4850.51\n",
      "Epoch [186], train_loss: 5371.02 with loss1: 457.92, loss2: 62.64 and loss3: 4850.46\n",
      "Epoch [187], train_loss: 5372.15 with loss1: 458.93, loss2: 62.81 and loss3: 4850.41\n",
      "Epoch [188], train_loss: 5369.00 with loss1: 456.19, loss2: 62.45 and loss3: 4850.36\n",
      "Epoch [189], train_loss: 5370.71 with loss1: 457.48, loss2: 62.92 and loss3: 4850.31\n",
      "Epoch [190], train_loss: 5368.64 with loss1: 455.74, loss2: 62.64 and loss3: 4850.26\n",
      "Epoch [191], train_loss: 5369.73 with loss1: 456.80, loss2: 62.72 and loss3: 4850.21\n",
      "Epoch [192], train_loss: 5366.05 with loss1: 453.38, loss2: 62.51 and loss3: 4850.16\n",
      "Epoch [193], train_loss: 5368.32 with loss1: 455.54, loss2: 62.66 and loss3: 4850.12\n",
      "Epoch [194], train_loss: 5366.45 with loss1: 453.95, loss2: 62.43 and loss3: 4850.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [195], train_loss: 5366.66 with loss1: 453.88, loss2: 62.77 and loss3: 4850.02\n",
      "Epoch [196], train_loss: 5364.12 with loss1: 451.68, loss2: 62.48 and loss3: 4849.97\n",
      "Epoch [197], train_loss: 5366.58 with loss1: 454.02, loss2: 62.64 and loss3: 4849.92\n",
      "Epoch [198], train_loss: 5363.65 with loss1: 451.38, loss2: 62.40 and loss3: 4849.87\n",
      "Epoch [199], train_loss: 5365.23 with loss1: 452.54, loss2: 62.88 and loss3: 4849.82\n",
      "Epoch [200], train_loss: 5362.14 with loss1: 450.11, loss2: 62.26 and loss3: 4849.77\n",
      "Epoch [201], train_loss: 5365.74 with loss1: 452.87, loss2: 63.15 and loss3: 4849.72\n",
      "Epoch [202], train_loss: 5362.43 with loss1: 450.32, loss2: 62.45 and loss3: 4849.67\n",
      "Epoch [203], train_loss: 5364.94 with loss1: 452.31, loss2: 63.02 and loss3: 4849.62\n",
      "Epoch [204], train_loss: 5362.02 with loss1: 449.86, loss2: 62.59 and loss3: 4849.57\n",
      "Epoch [205], train_loss: 5364.73 with loss1: 452.62, loss2: 62.59 and loss3: 4849.52\n",
      "Epoch [206], train_loss: 5362.21 with loss1: 450.52, loss2: 62.22 and loss3: 4849.47\n",
      "Epoch [207], train_loss: 5363.83 with loss1: 452.02, loss2: 62.39 and loss3: 4849.42\n",
      "Epoch [208], train_loss: 5364.19 with loss1: 452.42, loss2: 62.40 and loss3: 4849.37\n",
      "Epoch [209], train_loss: 5365.48 with loss1: 453.62, loss2: 62.54 and loss3: 4849.32\n",
      "Epoch [210], train_loss: 5362.73 with loss1: 451.43, loss2: 62.03 and loss3: 4849.27\n",
      "Epoch [211], train_loss: 5366.95 with loss1: 455.04, loss2: 62.68 and loss3: 4849.22\n",
      "Epoch [212], train_loss: 5363.26 with loss1: 451.65, loss2: 62.43 and loss3: 4849.17\n",
      "Epoch [213], train_loss: 5364.23 with loss1: 452.63, loss2: 62.47 and loss3: 4849.12\n",
      "Epoch [214], train_loss: 5363.05 with loss1: 452.02, loss2: 61.96 and loss3: 4849.07\n",
      "Epoch [215], train_loss: 5367.27 with loss1: 456.00, loss2: 62.24 and loss3: 4849.02\n",
      "Epoch [216], train_loss: 5364.56 with loss1: 453.06, loss2: 62.52 and loss3: 4848.98\n",
      "Epoch [217], train_loss: 5367.12 with loss1: 455.84, loss2: 62.35 and loss3: 4848.93\n",
      "Epoch [218], train_loss: 5366.43 with loss1: 455.37, loss2: 62.19 and loss3: 4848.88\n",
      "Epoch [219], train_loss: 5367.95 with loss1: 456.87, loss2: 62.25 and loss3: 4848.83\n",
      "Epoch [220], train_loss: 5367.27 with loss1: 456.05, loss2: 62.44 and loss3: 4848.78\n",
      "Epoch [221], train_loss: 5369.87 with loss1: 458.71, loss2: 62.43 and loss3: 4848.73\n",
      "Epoch [222], train_loss: 5367.48 with loss1: 456.55, loss2: 62.25 and loss3: 4848.68\n",
      "Epoch [223], train_loss: 5370.40 with loss1: 459.71, loss2: 62.07 and loss3: 4848.63\n",
      "Epoch [224], train_loss: 5370.20 with loss1: 459.38, loss2: 62.24 and loss3: 4848.58\n",
      "Epoch [225], train_loss: 5373.27 with loss1: 462.25, loss2: 62.49 and loss3: 4848.53\n",
      "Epoch [226], train_loss: 5371.52 with loss1: 461.09, loss2: 61.95 and loss3: 4848.48\n",
      "Epoch [227], train_loss: 5374.33 with loss1: 463.49, loss2: 62.40 and loss3: 4848.43\n",
      "Epoch [228], train_loss: 5372.27 with loss1: 461.98, loss2: 61.90 and loss3: 4848.38\n",
      "Epoch [229], train_loss: 5374.81 with loss1: 464.28, loss2: 62.20 and loss3: 4848.33\n",
      "Epoch [230], train_loss: 5373.53 with loss1: 463.28, loss2: 61.97 and loss3: 4848.28\n",
      "Epoch [231], train_loss: 5377.51 with loss1: 467.26, loss2: 62.01 and loss3: 4848.23\n",
      "Epoch [232], train_loss: 5373.71 with loss1: 463.75, loss2: 61.77 and loss3: 4848.18\n",
      "Epoch [233], train_loss: 5379.28 with loss1: 469.06, loss2: 62.09 and loss3: 4848.13\n",
      "Epoch [234], train_loss: 5375.88 with loss1: 466.13, loss2: 61.67 and loss3: 4848.08\n",
      "Epoch [235], train_loss: 5379.94 with loss1: 469.93, loss2: 61.98 and loss3: 4848.03\n",
      "Epoch [236], train_loss: 5377.77 with loss1: 467.79, loss2: 62.00 and loss3: 4847.98\n",
      "Epoch [237], train_loss: 5381.97 with loss1: 471.67, loss2: 62.36 and loss3: 4847.93\n",
      "Epoch [238], train_loss: 5378.19 with loss1: 468.75, loss2: 61.55 and loss3: 4847.88\n",
      "Epoch [239], train_loss: 5384.20 with loss1: 474.54, loss2: 61.83 and loss3: 4847.84\n",
      "Epoch [240], train_loss: 5381.13 with loss1: 471.63, loss2: 61.72 and loss3: 4847.79\n",
      "Epoch [241], train_loss: 5385.66 with loss1: 475.91, loss2: 62.01 and loss3: 4847.74\n",
      "Epoch [242], train_loss: 5381.81 with loss1: 472.55, loss2: 61.57 and loss3: 4847.69\n",
      "Epoch [243], train_loss: 5386.03 with loss1: 476.51, loss2: 61.88 and loss3: 4847.64\n",
      "Epoch [244], train_loss: 5383.39 with loss1: 473.65, loss2: 62.16 and loss3: 4847.59\n",
      "Epoch [245], train_loss: 5389.05 with loss1: 479.41, loss2: 62.10 and loss3: 4847.54\n",
      "Epoch [246], train_loss: 5383.81 with loss1: 474.84, loss2: 61.48 and loss3: 4847.49\n",
      "Epoch [247], train_loss: 5389.51 with loss1: 480.12, loss2: 61.95 and loss3: 4847.44\n",
      "Epoch [248], train_loss: 5382.91 with loss1: 473.98, loss2: 61.54 and loss3: 4847.39\n",
      "Epoch [249], train_loss: 5386.09 with loss1: 476.92, loss2: 61.83 and loss3: 4847.34\n",
      "Epoch [250], train_loss: 5383.59 with loss1: 474.83, loss2: 61.47 and loss3: 4847.29\n",
      "Epoch [251], train_loss: 5385.50 with loss1: 476.19, loss2: 62.08 and loss3: 4847.24\n",
      "Epoch [252], train_loss: 5382.01 with loss1: 473.25, loss2: 61.58 and loss3: 4847.19\n",
      "Epoch [253], train_loss: 5385.61 with loss1: 476.32, loss2: 62.15 and loss3: 4847.14\n",
      "Epoch [254], train_loss: 5380.70 with loss1: 472.09, loss2: 61.52 and loss3: 4847.09\n",
      "Epoch [255], train_loss: 5382.88 with loss1: 474.16, loss2: 61.68 and loss3: 4847.04\n",
      "Epoch [256], train_loss: 5378.11 with loss1: 469.43, loss2: 61.69 and loss3: 4846.99\n",
      "Epoch [257], train_loss: 5378.58 with loss1: 469.89, loss2: 61.75 and loss3: 4846.94\n",
      "Epoch [258], train_loss: 5375.70 with loss1: 467.29, loss2: 61.52 and loss3: 4846.89\n",
      "Epoch [259], train_loss: 5379.48 with loss1: 471.08, loss2: 61.56 and loss3: 4846.84\n",
      "Epoch [260], train_loss: 5373.28 with loss1: 465.31, loss2: 61.17 and loss3: 4846.79\n",
      "Epoch [261], train_loss: 5375.04 with loss1: 466.69, loss2: 61.61 and loss3: 4846.75\n",
      "Epoch [262], train_loss: 5371.84 with loss1: 463.60, loss2: 61.54 and loss3: 4846.70\n",
      "Epoch [263], train_loss: 5372.73 with loss1: 464.39, loss2: 61.69 and loss3: 4846.65\n",
      "Epoch [264], train_loss: 5368.76 with loss1: 460.77, loss2: 61.39 and loss3: 4846.60\n",
      "Epoch [265], train_loss: 5372.52 with loss1: 464.09, loss2: 61.89 and loss3: 4846.55\n",
      "Epoch [266], train_loss: 5367.19 with loss1: 459.04, loss2: 61.65 and loss3: 4846.50\n",
      "Epoch [267], train_loss: 5369.40 with loss1: 461.61, loss2: 61.34 and loss3: 4846.45\n",
      "Epoch [268], train_loss: 5364.74 with loss1: 457.13, loss2: 61.21 and loss3: 4846.40\n",
      "Epoch [269], train_loss: 5366.66 with loss1: 458.81, loss2: 61.50 and loss3: 4846.35\n",
      "Epoch [270], train_loss: 5364.47 with loss1: 456.77, loss2: 61.39 and loss3: 4846.30\n",
      "Epoch [271], train_loss: 5364.65 with loss1: 457.00, loss2: 61.40 and loss3: 4846.25\n",
      "Epoch [272], train_loss: 5361.95 with loss1: 454.56, loss2: 61.18 and loss3: 4846.20\n",
      "Epoch [273], train_loss: 5364.09 with loss1: 456.56, loss2: 61.39 and loss3: 4846.15\n",
      "Epoch [274], train_loss: 5359.78 with loss1: 452.42, loss2: 61.26 and loss3: 4846.10\n",
      "Epoch [275], train_loss: 5362.18 with loss1: 454.88, loss2: 61.25 and loss3: 4846.05\n",
      "Epoch [276], train_loss: 5362.77 with loss1: 455.70, loss2: 61.07 and loss3: 4846.00\n",
      "Epoch [277], train_loss: 5363.85 with loss1: 456.15, loss2: 61.75 and loss3: 4845.95\n",
      "Epoch [278], train_loss: 5360.21 with loss1: 453.04, loss2: 61.27 and loss3: 4845.90\n",
      "Epoch [279], train_loss: 5362.75 with loss1: 455.57, loss2: 61.33 and loss3: 4845.85\n",
      "Epoch [280], train_loss: 5359.59 with loss1: 452.77, loss2: 61.01 and loss3: 4845.80\n",
      "Epoch [281], train_loss: 5363.01 with loss1: 456.20, loss2: 61.06 and loss3: 4845.75\n",
      "Epoch [282], train_loss: 5360.59 with loss1: 453.74, loss2: 61.15 and loss3: 4845.71\n",
      "Epoch [283], train_loss: 5361.44 with loss1: 454.55, loss2: 61.24 and loss3: 4845.66\n",
      "Epoch [284], train_loss: 5358.98 with loss1: 452.51, loss2: 60.86 and loss3: 4845.61\n",
      "Epoch [285], train_loss: 5361.70 with loss1: 454.99, loss2: 61.15 and loss3: 4845.56\n",
      "Epoch [286], train_loss: 5359.39 with loss1: 453.03, loss2: 60.86 and loss3: 4845.51\n",
      "Epoch [287], train_loss: 5360.38 with loss1: 453.64, loss2: 61.28 and loss3: 4845.46\n",
      "Epoch [288], train_loss: 5359.87 with loss1: 453.46, loss2: 61.00 and loss3: 4845.41\n",
      "Epoch [289], train_loss: 5362.00 with loss1: 455.53, loss2: 61.12 and loss3: 4845.36\n",
      "Epoch [290], train_loss: 5359.79 with loss1: 453.50, loss2: 60.99 and loss3: 4845.31\n",
      "Epoch [291], train_loss: 5362.11 with loss1: 455.49, loss2: 61.36 and loss3: 4845.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [292], train_loss: 5360.39 with loss1: 454.43, loss2: 60.75 and loss3: 4845.21\n",
      "Epoch [293], train_loss: 5363.37 with loss1: 457.29, loss2: 60.92 and loss3: 4845.16\n",
      "Epoch [294], train_loss: 5361.82 with loss1: 455.84, loss2: 60.87 and loss3: 4845.11\n",
      "Epoch [295], train_loss: 5365.69 with loss1: 459.51, loss2: 61.12 and loss3: 4845.06\n",
      "Epoch [296], train_loss: 5363.45 with loss1: 457.40, loss2: 61.04 and loss3: 4845.01\n",
      "Epoch [297], train_loss: 5364.97 with loss1: 459.18, loss2: 60.82 and loss3: 4844.96\n",
      "Epoch [298], train_loss: 5365.88 with loss1: 460.18, loss2: 60.79 and loss3: 4844.91\n",
      "Epoch [299], train_loss: 5367.69 with loss1: 461.64, loss2: 61.18 and loss3: 4844.86\n",
      "Epoch [300], train_loss: 5365.30 with loss1: 459.44, loss2: 61.05 and loss3: 4844.81\n",
      "Epoch [301], train_loss: 5369.23 with loss1: 463.26, loss2: 61.20 and loss3: 4844.76\n",
      "Epoch [302], train_loss: 5365.53 with loss1: 460.29, loss2: 60.53 and loss3: 4844.71\n",
      "Epoch [303], train_loss: 5370.65 with loss1: 465.01, loss2: 60.98 and loss3: 4844.66\n",
      "Epoch [304], train_loss: 5367.23 with loss1: 461.83, loss2: 60.79 and loss3: 4844.62\n",
      "Epoch [305], train_loss: 5370.80 with loss1: 464.98, loss2: 61.25 and loss3: 4844.57\n",
      "Epoch [306], train_loss: 5368.58 with loss1: 463.32, loss2: 60.74 and loss3: 4844.52\n",
      "Epoch [307], train_loss: 5372.35 with loss1: 467.17, loss2: 60.72 and loss3: 4844.47\n",
      "Epoch [308], train_loss: 5368.80 with loss1: 463.73, loss2: 60.66 and loss3: 4844.42\n",
      "Epoch [309], train_loss: 5373.70 with loss1: 468.45, loss2: 60.88 and loss3: 4844.37\n",
      "Epoch [310], train_loss: 5371.06 with loss1: 466.08, loss2: 60.66 and loss3: 4844.32\n",
      "Epoch [311], train_loss: 5373.65 with loss1: 468.31, loss2: 61.07 and loss3: 4844.27\n",
      "Epoch [312], train_loss: 5372.09 with loss1: 467.26, loss2: 60.61 and loss3: 4844.22\n",
      "Epoch [313], train_loss: 5373.74 with loss1: 468.58, loss2: 60.99 and loss3: 4844.17\n",
      "Epoch [314], train_loss: 5368.54 with loss1: 463.99, loss2: 60.43 and loss3: 4844.12\n",
      "Epoch [315], train_loss: 5373.44 with loss1: 468.25, loss2: 61.12 and loss3: 4844.07\n",
      "Epoch [316], train_loss: 5369.80 with loss1: 465.17, loss2: 60.61 and loss3: 4844.02\n",
      "Epoch [317], train_loss: 5372.90 with loss1: 468.06, loss2: 60.87 and loss3: 4843.97\n",
      "Epoch [318], train_loss: 5369.65 with loss1: 465.00, loss2: 60.73 and loss3: 4843.92\n",
      "Epoch [319], train_loss: 5372.11 with loss1: 467.59, loss2: 60.65 and loss3: 4843.87\n",
      "Epoch [320], train_loss: 5367.55 with loss1: 463.19, loss2: 60.54 and loss3: 4843.82\n",
      "Epoch [321], train_loss: 5371.08 with loss1: 466.56, loss2: 60.74 and loss3: 4843.77\n",
      "Epoch [322], train_loss: 5363.33 with loss1: 458.95, loss2: 60.65 and loss3: 4843.72\n",
      "Epoch [323], train_loss: 5366.08 with loss1: 461.83, loss2: 60.58 and loss3: 4843.67\n",
      "Epoch [324], train_loss: 5361.47 with loss1: 457.37, loss2: 60.48 and loss3: 4843.62\n",
      "Epoch [325], train_loss: 5364.27 with loss1: 459.94, loss2: 60.76 and loss3: 4843.57\n",
      "Epoch [326], train_loss: 5359.94 with loss1: 456.21, loss2: 60.20 and loss3: 4843.53\n",
      "Epoch [327], train_loss: 5362.28 with loss1: 458.02, loss2: 60.78 and loss3: 4843.48\n",
      "Epoch [328], train_loss: 5358.35 with loss1: 454.20, loss2: 60.73 and loss3: 4843.43\n",
      "Epoch [329], train_loss: 5358.82 with loss1: 454.71, loss2: 60.74 and loss3: 4843.38\n",
      "Epoch [330], train_loss: 5354.28 with loss1: 450.74, loss2: 60.21 and loss3: 4843.33\n",
      "Epoch [331], train_loss: 5358.08 with loss1: 453.72, loss2: 61.08 and loss3: 4843.28\n",
      "Epoch [332], train_loss: 5353.61 with loss1: 449.99, loss2: 60.40 and loss3: 4843.23\n",
      "Epoch [333], train_loss: 5354.47 with loss1: 450.67, loss2: 60.63 and loss3: 4843.18\n",
      "Epoch [334], train_loss: 5351.91 with loss1: 448.31, loss2: 60.47 and loss3: 4843.13\n",
      "Epoch [335], train_loss: 5353.06 with loss1: 449.53, loss2: 60.45 and loss3: 4843.08\n",
      "Epoch [336], train_loss: 5348.74 with loss1: 445.37, loss2: 60.35 and loss3: 4843.03\n",
      "Epoch [337], train_loss: 5350.20 with loss1: 447.06, loss2: 60.15 and loss3: 4842.98\n",
      "Epoch [338], train_loss: 5347.96 with loss1: 444.89, loss2: 60.13 and loss3: 4842.93\n",
      "Epoch [339], train_loss: 5349.99 with loss1: 446.74, loss2: 60.37 and loss3: 4842.88\n",
      "Epoch [340], train_loss: 5346.63 with loss1: 443.78, loss2: 60.01 and loss3: 4842.83\n",
      "Epoch [341], train_loss: 5349.44 with loss1: 446.09, loss2: 60.57 and loss3: 4842.78\n",
      "Epoch [342], train_loss: 5346.52 with loss1: 443.73, loss2: 60.07 and loss3: 4842.73\n",
      "Epoch [343], train_loss: 5347.98 with loss1: 445.07, loss2: 60.23 and loss3: 4842.68\n",
      "Epoch [344], train_loss: 5346.83 with loss1: 443.88, loss2: 60.32 and loss3: 4842.63\n",
      "Epoch [345], train_loss: 5347.90 with loss1: 445.00, loss2: 60.31 and loss3: 4842.58\n",
      "Epoch [346], train_loss: 5345.62 with loss1: 443.04, loss2: 60.04 and loss3: 4842.53\n",
      "Epoch [347], train_loss: 5347.10 with loss1: 444.53, loss2: 60.09 and loss3: 4842.48\n",
      "Epoch [348], train_loss: 5344.65 with loss1: 442.35, loss2: 59.86 and loss3: 4842.44\n",
      "Epoch [349], train_loss: 5348.20 with loss1: 445.71, loss2: 60.10 and loss3: 4842.39\n",
      "Epoch [350], train_loss: 5345.01 with loss1: 442.67, loss2: 60.00 and loss3: 4842.34\n",
      "Epoch [351], train_loss: 5348.14 with loss1: 445.79, loss2: 60.06 and loss3: 4842.29\n",
      "Epoch [352], train_loss: 5345.82 with loss1: 443.55, loss2: 60.03 and loss3: 4842.24\n",
      "Epoch [353], train_loss: 5347.33 with loss1: 444.94, loss2: 60.21 and loss3: 4842.19\n",
      "Epoch [354], train_loss: 5344.45 with loss1: 442.26, loss2: 60.05 and loss3: 4842.14\n",
      "Epoch [355], train_loss: 5347.36 with loss1: 445.23, loss2: 60.04 and loss3: 4842.09\n",
      "Epoch [356], train_loss: 5345.98 with loss1: 443.85, loss2: 60.09 and loss3: 4842.04\n",
      "Epoch [357], train_loss: 5348.54 with loss1: 446.41, loss2: 60.14 and loss3: 4841.99\n",
      "Epoch [358], train_loss: 5347.94 with loss1: 445.38, loss2: 60.62 and loss3: 4841.94\n",
      "Epoch [359], train_loss: 5349.22 with loss1: 447.40, loss2: 59.93 and loss3: 4841.89\n",
      "Epoch [360], train_loss: 5346.87 with loss1: 445.09, loss2: 59.94 and loss3: 4841.84\n",
      "Epoch [361], train_loss: 5350.28 with loss1: 448.47, loss2: 60.02 and loss3: 4841.79\n",
      "Epoch [362], train_loss: 5347.25 with loss1: 445.61, loss2: 59.89 and loss3: 4841.74\n",
      "Epoch [363], train_loss: 5350.32 with loss1: 448.54, loss2: 60.08 and loss3: 4841.69\n",
      "Epoch [364], train_loss: 5347.64 with loss1: 446.05, loss2: 59.94 and loss3: 4841.64\n",
      "Epoch [365], train_loss: 5350.50 with loss1: 449.17, loss2: 59.74 and loss3: 4841.59\n",
      "Epoch [366], train_loss: 5349.32 with loss1: 447.90, loss2: 59.88 and loss3: 4841.54\n",
      "Epoch [367], train_loss: 5353.53 with loss1: 451.64, loss2: 60.39 and loss3: 4841.49\n",
      "Epoch [368], train_loss: 5348.82 with loss1: 447.64, loss2: 59.74 and loss3: 4841.45\n",
      "Epoch [369], train_loss: 5350.97 with loss1: 449.59, loss2: 59.99 and loss3: 4841.40\n",
      "Epoch [370], train_loss: 5350.02 with loss1: 448.71, loss2: 59.97 and loss3: 4841.35\n",
      "Epoch [371], train_loss: 5352.15 with loss1: 450.98, loss2: 59.87 and loss3: 4841.30\n",
      "Epoch [372], train_loss: 5348.48 with loss1: 447.67, loss2: 59.56 and loss3: 4841.25\n",
      "Epoch [373], train_loss: 5351.74 with loss1: 450.86, loss2: 59.68 and loss3: 4841.20\n",
      "Epoch [374], train_loss: 5348.94 with loss1: 447.50, loss2: 60.29 and loss3: 4841.15\n",
      "Epoch [375], train_loss: 5351.02 with loss1: 449.89, loss2: 60.03 and loss3: 4841.10\n",
      "Epoch [376], train_loss: 5349.15 with loss1: 448.57, loss2: 59.52 and loss3: 4841.05\n",
      "Epoch [377], train_loss: 5350.55 with loss1: 449.80, loss2: 59.75 and loss3: 4841.00\n",
      "Epoch [378], train_loss: 5348.97 with loss1: 448.48, loss2: 59.54 and loss3: 4840.95\n",
      "Epoch [379], train_loss: 5350.36 with loss1: 449.64, loss2: 59.82 and loss3: 4840.90\n",
      "Epoch [380], train_loss: 5348.90 with loss1: 448.65, loss2: 59.40 and loss3: 4840.85\n",
      "Epoch [381], train_loss: 5351.32 with loss1: 450.71, loss2: 59.82 and loss3: 4840.80\n",
      "Epoch [382], train_loss: 5348.40 with loss1: 448.18, loss2: 59.46 and loss3: 4840.75\n",
      "Epoch [383], train_loss: 5350.83 with loss1: 450.63, loss2: 59.50 and loss3: 4840.70\n",
      "Epoch [384], train_loss: 5347.42 with loss1: 447.04, loss2: 59.73 and loss3: 4840.65\n",
      "Epoch [385], train_loss: 5349.63 with loss1: 449.39, loss2: 59.63 and loss3: 4840.60\n",
      "Epoch [386], train_loss: 5346.24 with loss1: 446.10, loss2: 59.59 and loss3: 4840.55\n",
      "Epoch [387], train_loss: 5349.34 with loss1: 449.12, loss2: 59.72 and loss3: 4840.50\n",
      "Epoch [388], train_loss: 5345.33 with loss1: 445.56, loss2: 59.32 and loss3: 4840.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [389], train_loss: 5348.66 with loss1: 448.74, loss2: 59.52 and loss3: 4840.41\n",
      "Epoch [390], train_loss: 5346.57 with loss1: 446.79, loss2: 59.43 and loss3: 4840.36\n",
      "Epoch [391], train_loss: 5349.21 with loss1: 449.43, loss2: 59.47 and loss3: 4840.31\n",
      "Epoch [392], train_loss: 5345.62 with loss1: 445.93, loss2: 59.43 and loss3: 4840.26\n",
      "Epoch [393], train_loss: 5348.58 with loss1: 448.76, loss2: 59.62 and loss3: 4840.21\n",
      "Epoch [394], train_loss: 5344.58 with loss1: 445.02, loss2: 59.40 and loss3: 4840.16\n",
      "Epoch [395], train_loss: 5347.68 with loss1: 448.01, loss2: 59.56 and loss3: 4840.11\n",
      "Epoch [396], train_loss: 5343.91 with loss1: 444.47, loss2: 59.38 and loss3: 4840.06\n",
      "Epoch [397], train_loss: 5347.43 with loss1: 448.06, loss2: 59.37 and loss3: 4840.01\n",
      "Epoch [398], train_loss: 5344.23 with loss1: 444.83, loss2: 59.44 and loss3: 4839.96\n",
      "Epoch [399], train_loss: 5347.41 with loss1: 448.12, loss2: 59.38 and loss3: 4839.91\n",
      "Epoch [400], train_loss: 5343.93 with loss1: 444.67, loss2: 59.40 and loss3: 4839.86\n",
      "Epoch [401], train_loss: 5346.17 with loss1: 446.74, loss2: 59.62 and loss3: 4839.81\n",
      "Epoch [402], train_loss: 5342.51 with loss1: 443.68, loss2: 59.06 and loss3: 4839.76\n",
      "Epoch [403], train_loss: 5345.01 with loss1: 445.90, loss2: 59.39 and loss3: 4839.71\n",
      "Epoch [404], train_loss: 5342.83 with loss1: 443.63, loss2: 59.54 and loss3: 4839.66\n",
      "Epoch [405], train_loss: 5344.61 with loss1: 445.72, loss2: 59.28 and loss3: 4839.61\n",
      "Epoch [406], train_loss: 5341.82 with loss1: 442.98, loss2: 59.27 and loss3: 4839.56\n",
      "Epoch [407], train_loss: 5343.41 with loss1: 444.36, loss2: 59.54 and loss3: 4839.51\n",
      "Epoch [408], train_loss: 5340.86 with loss1: 442.45, loss2: 58.94 and loss3: 4839.46\n",
      "Epoch [409], train_loss: 5343.29 with loss1: 444.70, loss2: 59.18 and loss3: 4839.42\n",
      "Epoch [410], train_loss: 5341.11 with loss1: 442.65, loss2: 59.10 and loss3: 4839.37\n",
      "Epoch [411], train_loss: 5342.36 with loss1: 443.70, loss2: 59.34 and loss3: 4839.32\n",
      "Epoch [412], train_loss: 5340.45 with loss1: 442.32, loss2: 58.86 and loss3: 4839.27\n",
      "Epoch [413], train_loss: 5341.58 with loss1: 443.25, loss2: 59.12 and loss3: 4839.22\n",
      "Epoch [414], train_loss: 5338.78 with loss1: 440.63, loss2: 58.98 and loss3: 4839.17\n",
      "Epoch [415], train_loss: 5341.95 with loss1: 443.41, loss2: 59.43 and loss3: 4839.12\n",
      "Epoch [416], train_loss: 5339.26 with loss1: 441.14, loss2: 59.05 and loss3: 4839.07\n",
      "Epoch [417], train_loss: 5341.32 with loss1: 443.21, loss2: 59.09 and loss3: 4839.02\n",
      "Epoch [418], train_loss: 5339.74 with loss1: 441.93, loss2: 58.84 and loss3: 4838.97\n",
      "Epoch [419], train_loss: 5341.75 with loss1: 443.42, loss2: 59.41 and loss3: 4838.92\n",
      "Epoch [420], train_loss: 5339.70 with loss1: 441.81, loss2: 59.02 and loss3: 4838.87\n",
      "Epoch [421], train_loss: 5341.47 with loss1: 443.47, loss2: 59.18 and loss3: 4838.82\n",
      "Epoch [422], train_loss: 5338.12 with loss1: 440.48, loss2: 58.87 and loss3: 4838.77\n",
      "Epoch [423], train_loss: 5342.35 with loss1: 444.72, loss2: 58.91 and loss3: 4838.72\n",
      "Epoch [424], train_loss: 5339.45 with loss1: 441.87, loss2: 58.91 and loss3: 4838.67\n",
      "Epoch [425], train_loss: 5341.23 with loss1: 443.71, loss2: 58.90 and loss3: 4838.62\n",
      "Epoch [426], train_loss: 5340.23 with loss1: 442.90, loss2: 58.75 and loss3: 4838.57\n",
      "Epoch [427], train_loss: 5342.08 with loss1: 444.31, loss2: 59.25 and loss3: 4838.52\n",
      "Epoch [428], train_loss: 5341.59 with loss1: 444.43, loss2: 58.69 and loss3: 4838.47\n",
      "Epoch [429], train_loss: 5343.35 with loss1: 446.09, loss2: 58.84 and loss3: 4838.42\n",
      "Epoch [430], train_loss: 5340.60 with loss1: 443.44, loss2: 58.79 and loss3: 4838.37\n",
      "Epoch [431], train_loss: 5343.34 with loss1: 446.12, loss2: 58.89 and loss3: 4838.33\n",
      "Epoch [432], train_loss: 5341.18 with loss1: 444.39, loss2: 58.52 and loss3: 4838.28\n",
      "Epoch [433], train_loss: 5345.29 with loss1: 448.17, loss2: 58.90 and loss3: 4838.23\n",
      "Epoch [434], train_loss: 5343.50 with loss1: 446.73, loss2: 58.59 and loss3: 4838.18\n",
      "Epoch [435], train_loss: 5345.80 with loss1: 448.90, loss2: 58.77 and loss3: 4838.13\n",
      "Epoch [436], train_loss: 5343.98 with loss1: 447.01, loss2: 58.90 and loss3: 4838.08\n",
      "Epoch [437], train_loss: 5348.60 with loss1: 451.41, loss2: 59.16 and loss3: 4838.03\n",
      "Epoch [438], train_loss: 5345.05 with loss1: 448.71, loss2: 58.36 and loss3: 4837.98\n",
      "Epoch [439], train_loss: 5349.92 with loss1: 453.11, loss2: 58.88 and loss3: 4837.93\n",
      "Epoch [440], train_loss: 5346.61 with loss1: 450.31, loss2: 58.42 and loss3: 4837.88\n",
      "Epoch [441], train_loss: 5351.40 with loss1: 454.87, loss2: 58.70 and loss3: 4837.83\n",
      "Epoch [442], train_loss: 5348.32 with loss1: 451.97, loss2: 58.56 and loss3: 4837.78\n",
      "Epoch [443], train_loss: 5352.14 with loss1: 455.33, loss2: 59.08 and loss3: 4837.73\n",
      "Epoch [444], train_loss: 5349.35 with loss1: 453.12, loss2: 58.54 and loss3: 4837.68\n",
      "Epoch [445], train_loss: 5353.15 with loss1: 456.87, loss2: 58.65 and loss3: 4837.63\n",
      "Epoch [446], train_loss: 5350.77 with loss1: 454.67, loss2: 58.52 and loss3: 4837.58\n",
      "Epoch [447], train_loss: 5354.40 with loss1: 458.12, loss2: 58.75 and loss3: 4837.53\n",
      "Epoch [448], train_loss: 5351.36 with loss1: 455.42, loss2: 58.46 and loss3: 4837.48\n",
      "Epoch [449], train_loss: 5355.84 with loss1: 459.67, loss2: 58.74 and loss3: 4837.43\n",
      "Epoch [450], train_loss: 5352.70 with loss1: 456.75, loss2: 58.57 and loss3: 4837.38\n",
      "Epoch [451], train_loss: 5356.46 with loss1: 460.28, loss2: 58.85 and loss3: 4837.33\n",
      "Epoch [452], train_loss: 5354.20 with loss1: 458.51, loss2: 58.41 and loss3: 4837.29\n",
      "Epoch [453], train_loss: 5356.33 with loss1: 460.53, loss2: 58.56 and loss3: 4837.24\n",
      "Epoch [454], train_loss: 5353.01 with loss1: 457.66, loss2: 58.17 and loss3: 4837.19\n",
      "Epoch [455], train_loss: 5355.72 with loss1: 460.09, loss2: 58.50 and loss3: 4837.14\n",
      "Epoch [456], train_loss: 5350.85 with loss1: 455.66, loss2: 58.09 and loss3: 4837.09\n",
      "Epoch [457], train_loss: 5354.20 with loss1: 458.36, loss2: 58.80 and loss3: 4837.04\n",
      "Epoch [458], train_loss: 5350.37 with loss1: 454.99, loss2: 58.39 and loss3: 4836.99\n",
      "Epoch [459], train_loss: 5351.91 with loss1: 456.44, loss2: 58.53 and loss3: 4836.94\n",
      "Epoch [460], train_loss: 5348.80 with loss1: 453.53, loss2: 58.38 and loss3: 4836.89\n",
      "Epoch [461], train_loss: 5351.26 with loss1: 455.71, loss2: 58.70 and loss3: 4836.84\n",
      "Epoch [462], train_loss: 5346.88 with loss1: 451.48, loss2: 58.61 and loss3: 4836.79\n",
      "Epoch [463], train_loss: 5349.60 with loss1: 454.32, loss2: 58.53 and loss3: 4836.74\n",
      "Epoch [464], train_loss: 5346.04 with loss1: 451.22, loss2: 58.13 and loss3: 4836.69\n",
      "Epoch [465], train_loss: 5348.56 with loss1: 453.26, loss2: 58.66 and loss3: 4836.64\n",
      "Epoch [466], train_loss: 5343.70 with loss1: 448.82, loss2: 58.29 and loss3: 4836.59\n",
      "Epoch [467], train_loss: 5346.11 with loss1: 450.92, loss2: 58.65 and loss3: 4836.54\n",
      "Epoch [468], train_loss: 5342.74 with loss1: 448.06, loss2: 58.19 and loss3: 4836.49\n",
      "Epoch [469], train_loss: 5343.01 with loss1: 447.86, loss2: 58.71 and loss3: 4836.44\n",
      "Epoch [470], train_loss: 5340.22 with loss1: 445.68, loss2: 58.14 and loss3: 4836.39\n",
      "Epoch [471], train_loss: 5341.71 with loss1: 446.94, loss2: 58.42 and loss3: 4836.34\n",
      "Epoch [472], train_loss: 5339.09 with loss1: 444.54, loss2: 58.25 and loss3: 4836.30\n",
      "Epoch [473], train_loss: 5339.22 with loss1: 444.58, loss2: 58.39 and loss3: 4836.25\n",
      "Epoch [474], train_loss: 5336.41 with loss1: 442.07, loss2: 58.14 and loss3: 4836.20\n",
      "Epoch [475], train_loss: 5336.92 with loss1: 442.69, loss2: 58.08 and loss3: 4836.15\n",
      "Epoch [476], train_loss: 5334.20 with loss1: 439.96, loss2: 58.14 and loss3: 4836.10\n",
      "Epoch [477], train_loss: 5335.21 with loss1: 440.86, loss2: 58.30 and loss3: 4836.05\n",
      "Epoch [478], train_loss: 5332.24 with loss1: 438.23, loss2: 58.01 and loss3: 4836.00\n",
      "Epoch [479], train_loss: 5333.96 with loss1: 439.78, loss2: 58.24 and loss3: 4835.95\n",
      "Epoch [480], train_loss: 5332.65 with loss1: 438.56, loss2: 58.19 and loss3: 4835.90\n",
      "Epoch [481], train_loss: 5333.23 with loss1: 439.14, loss2: 58.24 and loss3: 4835.85\n",
      "Epoch [482], train_loss: 5330.79 with loss1: 436.90, loss2: 58.09 and loss3: 4835.80\n",
      "Epoch [483], train_loss: 5331.64 with loss1: 437.63, loss2: 58.26 and loss3: 4835.75\n",
      "Epoch [484], train_loss: 5330.18 with loss1: 436.50, loss2: 57.98 and loss3: 4835.70\n",
      "Epoch [485], train_loss: 5331.56 with loss1: 437.78, loss2: 58.12 and loss3: 4835.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [486], train_loss: 5330.62 with loss1: 436.85, loss2: 58.18 and loss3: 4835.60\n",
      "Epoch [487], train_loss: 5330.19 with loss1: 436.57, loss2: 58.07 and loss3: 4835.55\n",
      "Epoch [488], train_loss: 5328.53 with loss1: 435.25, loss2: 57.78 and loss3: 4835.50\n",
      "Epoch [489], train_loss: 5329.95 with loss1: 436.51, loss2: 57.99 and loss3: 4835.45\n",
      "Epoch [490], train_loss: 5326.59 with loss1: 433.47, loss2: 57.71 and loss3: 4835.40\n",
      "Epoch [491], train_loss: 5329.44 with loss1: 435.81, loss2: 58.27 and loss3: 4835.35\n",
      "Epoch [492], train_loss: 5326.75 with loss1: 433.59, loss2: 57.86 and loss3: 4835.31\n",
      "Epoch [493], train_loss: 5327.34 with loss1: 433.92, loss2: 58.17 and loss3: 4835.26\n",
      "Epoch [494], train_loss: 5324.78 with loss1: 431.78, loss2: 57.80 and loss3: 4835.21\n",
      "Epoch [495], train_loss: 5327.47 with loss1: 434.29, loss2: 58.03 and loss3: 4835.16\n",
      "Epoch [496], train_loss: 5327.37 with loss1: 434.45, loss2: 57.81 and loss3: 4835.11\n",
      "Epoch [497], train_loss: 5327.34 with loss1: 434.36, loss2: 57.92 and loss3: 4835.06\n",
      "Epoch [498], train_loss: 5324.77 with loss1: 431.94, loss2: 57.82 and loss3: 4835.01\n",
      "Epoch [499], train_loss: 5326.24 with loss1: 433.36, loss2: 57.91 and loss3: 4834.96\n",
      "Epoch [500], train_loss: 5324.87 with loss1: 432.48, loss2: 57.48 and loss3: 4834.91\n",
      "Epoch [501], train_loss: 5325.69 with loss1: 433.13, loss2: 57.70 and loss3: 4834.86\n",
      "Epoch [502], train_loss: 5324.29 with loss1: 431.82, loss2: 57.66 and loss3: 4834.81\n",
      "Epoch [503], train_loss: 5325.83 with loss1: 433.06, loss2: 58.01 and loss3: 4834.76\n",
      "Epoch [504], train_loss: 5325.99 with loss1: 433.54, loss2: 57.74 and loss3: 4834.71\n",
      "Epoch [505], train_loss: 5327.02 with loss1: 434.66, loss2: 57.69 and loss3: 4834.66\n",
      "Epoch [506], train_loss: 5324.70 with loss1: 432.39, loss2: 57.70 and loss3: 4834.61\n",
      "Epoch [507], train_loss: 5326.15 with loss1: 433.52, loss2: 58.07 and loss3: 4834.56\n",
      "Epoch [508], train_loss: 5326.13 with loss1: 433.76, loss2: 57.86 and loss3: 4834.51\n",
      "Epoch [509], train_loss: 5326.29 with loss1: 433.82, loss2: 58.01 and loss3: 4834.46\n",
      "Epoch [510], train_loss: 5324.12 with loss1: 432.03, loss2: 57.67 and loss3: 4834.41\n",
      "Epoch [511], train_loss: 5326.54 with loss1: 434.50, loss2: 57.67 and loss3: 4834.37\n",
      "Epoch [512], train_loss: 5326.28 with loss1: 434.34, loss2: 57.63 and loss3: 4834.32\n",
      "Epoch [513], train_loss: 5326.75 with loss1: 434.75, loss2: 57.73 and loss3: 4834.27\n",
      "Epoch [514], train_loss: 5326.49 with loss1: 434.75, loss2: 57.52 and loss3: 4834.22\n",
      "Epoch [515], train_loss: 5326.95 with loss1: 435.05, loss2: 57.74 and loss3: 4834.17\n",
      "Epoch [516], train_loss: 5324.49 with loss1: 432.91, loss2: 57.47 and loss3: 4834.12\n",
      "Epoch [517], train_loss: 5327.53 with loss1: 435.63, loss2: 57.83 and loss3: 4834.07\n",
      "Epoch [518], train_loss: 5325.75 with loss1: 434.32, loss2: 57.41 and loss3: 4834.02\n",
      "Epoch [519], train_loss: 5327.77 with loss1: 436.19, loss2: 57.61 and loss3: 4833.97\n",
      "Epoch [520], train_loss: 5325.92 with loss1: 434.64, loss2: 57.36 and loss3: 4833.92\n",
      "Epoch [521], train_loss: 5328.15 with loss1: 436.72, loss2: 57.56 and loss3: 4833.87\n",
      "Epoch [522], train_loss: 5328.46 with loss1: 437.31, loss2: 57.33 and loss3: 4833.82\n",
      "Epoch [523], train_loss: 5330.30 with loss1: 439.04, loss2: 57.49 and loss3: 4833.77\n",
      "Epoch [524], train_loss: 5327.83 with loss1: 436.67, loss2: 57.44 and loss3: 4833.72\n",
      "Epoch [525], train_loss: 5331.55 with loss1: 440.30, loss2: 57.57 and loss3: 4833.67\n",
      "Epoch [526], train_loss: 5328.62 with loss1: 437.52, loss2: 57.47 and loss3: 4833.62\n",
      "Epoch [527], train_loss: 5330.89 with loss1: 439.81, loss2: 57.51 and loss3: 4833.57\n",
      "Epoch [528], train_loss: 5329.30 with loss1: 438.65, loss2: 57.13 and loss3: 4833.52\n",
      "Epoch [529], train_loss: 5332.70 with loss1: 441.66, loss2: 57.56 and loss3: 4833.47\n",
      "Epoch [530], train_loss: 5330.56 with loss1: 439.59, loss2: 57.54 and loss3: 4833.42\n",
      "Epoch [531], train_loss: 5332.34 with loss1: 441.57, loss2: 57.40 and loss3: 4833.38\n",
      "Epoch [532], train_loss: 5330.33 with loss1: 439.76, loss2: 57.25 and loss3: 4833.33\n",
      "Epoch [533], train_loss: 5332.27 with loss1: 441.65, loss2: 57.35 and loss3: 4833.28\n",
      "Epoch [534], train_loss: 5330.78 with loss1: 440.20, loss2: 57.36 and loss3: 4833.23\n",
      "Epoch [535], train_loss: 5333.92 with loss1: 443.34, loss2: 57.40 and loss3: 4833.18\n",
      "Epoch [536], train_loss: 5332.56 with loss1: 442.34, loss2: 57.09 and loss3: 4833.13\n",
      "Epoch [537], train_loss: 5336.45 with loss1: 445.93, loss2: 57.44 and loss3: 4833.08\n",
      "Epoch [538], train_loss: 5332.85 with loss1: 442.77, loss2: 57.05 and loss3: 4833.03\n",
      "Epoch [539], train_loss: 5335.70 with loss1: 445.27, loss2: 57.45 and loss3: 4832.98\n",
      "Epoch [540], train_loss: 5333.44 with loss1: 443.21, loss2: 57.31 and loss3: 4832.93\n",
      "Epoch [541], train_loss: 5336.43 with loss1: 446.22, loss2: 57.34 and loss3: 4832.88\n",
      "Epoch [542], train_loss: 5331.85 with loss1: 442.09, loss2: 56.93 and loss3: 4832.83\n",
      "Epoch [543], train_loss: 5335.72 with loss1: 445.65, loss2: 57.29 and loss3: 4832.78\n",
      "Epoch [544], train_loss: 5333.87 with loss1: 443.94, loss2: 57.20 and loss3: 4832.73\n",
      "Epoch [545], train_loss: 5335.60 with loss1: 445.31, loss2: 57.60 and loss3: 4832.68\n",
      "Epoch [546], train_loss: 5333.20 with loss1: 443.47, loss2: 57.10 and loss3: 4832.63\n",
      "Epoch [547], train_loss: 5334.74 with loss1: 444.31, loss2: 57.85 and loss3: 4832.58\n",
      "Epoch [548], train_loss: 5333.16 with loss1: 443.57, loss2: 57.06 and loss3: 4832.53\n",
      "Epoch [549], train_loss: 5335.08 with loss1: 445.46, loss2: 57.13 and loss3: 4832.48\n",
      "Epoch [550], train_loss: 5332.92 with loss1: 443.70, loss2: 56.78 and loss3: 4832.43\n",
      "Epoch [551], train_loss: 5335.46 with loss1: 445.70, loss2: 57.38 and loss3: 4832.39\n",
      "Epoch [552], train_loss: 5331.13 with loss1: 441.86, loss2: 56.93 and loss3: 4832.34\n",
      "Epoch [553], train_loss: 5333.37 with loss1: 443.98, loss2: 57.10 and loss3: 4832.29\n",
      "Epoch [554], train_loss: 5330.44 with loss1: 441.52, loss2: 56.68 and loss3: 4832.24\n",
      "Epoch [555], train_loss: 5334.86 with loss1: 445.56, loss2: 57.11 and loss3: 4832.19\n",
      "Epoch [556], train_loss: 5333.08 with loss1: 444.15, loss2: 56.79 and loss3: 4832.14\n",
      "Epoch [557], train_loss: 5335.47 with loss1: 446.25, loss2: 57.13 and loss3: 4832.09\n",
      "Epoch [558], train_loss: 5331.94 with loss1: 442.72, loss2: 57.18 and loss3: 4832.04\n",
      "Epoch [559], train_loss: 5334.85 with loss1: 445.58, loss2: 57.28 and loss3: 4831.99\n",
      "Epoch [560], train_loss: 5331.36 with loss1: 442.52, loss2: 56.90 and loss3: 4831.94\n",
      "Epoch [561], train_loss: 5334.18 with loss1: 445.11, loss2: 57.18 and loss3: 4831.89\n",
      "Epoch [562], train_loss: 5330.51 with loss1: 441.91, loss2: 56.76 and loss3: 4831.84\n",
      "Epoch [563], train_loss: 5333.07 with loss1: 444.20, loss2: 57.09 and loss3: 4831.79\n",
      "Epoch [564], train_loss: 5330.25 with loss1: 441.90, loss2: 56.61 and loss3: 4831.74\n",
      "Epoch [565], train_loss: 5333.81 with loss1: 445.12, loss2: 57.00 and loss3: 4831.69\n",
      "Epoch [566], train_loss: 5330.73 with loss1: 441.97, loss2: 57.11 and loss3: 4831.64\n",
      "Epoch [567], train_loss: 5332.68 with loss1: 444.15, loss2: 56.94 and loss3: 4831.59\n",
      "Epoch [568], train_loss: 5329.02 with loss1: 440.71, loss2: 56.77 and loss3: 4831.54\n",
      "Epoch [569], train_loss: 5331.46 with loss1: 443.04, loss2: 56.93 and loss3: 4831.49\n",
      "Epoch [570], train_loss: 5328.97 with loss1: 440.84, loss2: 56.68 and loss3: 4831.45\n",
      "Epoch [571], train_loss: 5330.85 with loss1: 442.63, loss2: 56.82 and loss3: 4831.40\n",
      "Epoch [572], train_loss: 5327.66 with loss1: 439.49, loss2: 56.82 and loss3: 4831.35\n",
      "Epoch [573], train_loss: 5331.41 with loss1: 442.98, loss2: 57.13 and loss3: 4831.30\n",
      "Epoch [574], train_loss: 5326.70 with loss1: 438.70, loss2: 56.76 and loss3: 4831.25\n",
      "Epoch [575], train_loss: 5329.60 with loss1: 441.47, loss2: 56.93 and loss3: 4831.20\n",
      "Epoch [576], train_loss: 5326.79 with loss1: 439.18, loss2: 56.46 and loss3: 4831.15\n",
      "Epoch [577], train_loss: 5328.17 with loss1: 440.43, loss2: 56.65 and loss3: 4831.10\n",
      "Epoch [578], train_loss: 5325.90 with loss1: 437.92, loss2: 56.93 and loss3: 4831.05\n",
      "Epoch [579], train_loss: 5328.70 with loss1: 440.08, loss2: 57.61 and loss3: 4831.00\n",
      "Epoch [580], train_loss: 5323.66 with loss1: 435.87, loss2: 56.83 and loss3: 4830.95\n",
      "Epoch [581], train_loss: 5324.49 with loss1: 436.75, loss2: 56.84 and loss3: 4830.90\n",
      "Epoch [582], train_loss: 5321.96 with loss1: 434.33, loss2: 56.77 and loss3: 4830.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [583], train_loss: 5322.99 with loss1: 435.50, loss2: 56.69 and loss3: 4830.80\n",
      "Epoch [584], train_loss: 5320.80 with loss1: 433.61, loss2: 56.44 and loss3: 4830.75\n",
      "Epoch [585], train_loss: 5322.69 with loss1: 434.64, loss2: 57.34 and loss3: 4830.70\n",
      "Epoch [586], train_loss: 5317.50 with loss1: 430.49, loss2: 56.36 and loss3: 4830.65\n",
      "Epoch [587], train_loss: 5321.27 with loss1: 433.98, loss2: 56.69 and loss3: 4830.60\n",
      "Epoch [588], train_loss: 5317.53 with loss1: 430.23, loss2: 56.75 and loss3: 4830.55\n",
      "Epoch [589], train_loss: 5319.98 with loss1: 432.85, loss2: 56.63 and loss3: 4830.50\n",
      "Epoch [590], train_loss: 5316.31 with loss1: 429.29, loss2: 56.56 and loss3: 4830.46\n",
      "Epoch [591], train_loss: 5317.62 with loss1: 430.64, loss2: 56.57 and loss3: 4830.41\n",
      "Epoch [592], train_loss: 5316.04 with loss1: 429.08, loss2: 56.61 and loss3: 4830.36\n",
      "Epoch [593], train_loss: 5318.13 with loss1: 431.39, loss2: 56.43 and loss3: 4830.31\n",
      "Epoch [594], train_loss: 5315.27 with loss1: 428.63, loss2: 56.38 and loss3: 4830.26\n",
      "Epoch [595], train_loss: 5316.24 with loss1: 429.53, loss2: 56.50 and loss3: 4830.21\n",
      "Epoch [596], train_loss: 5314.11 with loss1: 427.59, loss2: 56.37 and loss3: 4830.16\n",
      "Epoch [597], train_loss: 5316.07 with loss1: 429.48, loss2: 56.48 and loss3: 4830.11\n",
      "Epoch [598], train_loss: 5313.35 with loss1: 426.99, loss2: 56.30 and loss3: 4830.06\n",
      "Epoch [599], train_loss: 5316.10 with loss1: 429.82, loss2: 56.27 and loss3: 4830.01\n",
      "Epoch [600], train_loss: 5313.35 with loss1: 427.17, loss2: 56.22 and loss3: 4829.96\n",
      "Epoch [601], train_loss: 5314.86 with loss1: 428.47, loss2: 56.48 and loss3: 4829.91\n",
      "Epoch [602], train_loss: 5312.91 with loss1: 426.85, loss2: 56.20 and loss3: 4829.86\n",
      "Epoch [603], train_loss: 5314.84 with loss1: 428.48, loss2: 56.55 and loss3: 4829.81\n",
      "Epoch [604], train_loss: 5311.89 with loss1: 425.95, loss2: 56.18 and loss3: 4829.76\n",
      "Epoch [605], train_loss: 5313.74 with loss1: 427.50, loss2: 56.53 and loss3: 4829.71\n",
      "Epoch [606], train_loss: 5310.88 with loss1: 425.04, loss2: 56.18 and loss3: 4829.66\n",
      "Epoch [607], train_loss: 5312.35 with loss1: 426.29, loss2: 56.45 and loss3: 4829.61\n",
      "Epoch [608], train_loss: 5309.40 with loss1: 423.72, loss2: 56.12 and loss3: 4829.56\n",
      "Epoch [609], train_loss: 5312.52 with loss1: 426.78, loss2: 56.23 and loss3: 4829.52\n",
      "Epoch [610], train_loss: 5311.19 with loss1: 425.52, loss2: 56.20 and loss3: 4829.47\n",
      "Epoch [611], train_loss: 5313.58 with loss1: 427.67, loss2: 56.49 and loss3: 4829.42\n",
      "Epoch [612], train_loss: 5312.34 with loss1: 426.80, loss2: 56.17 and loss3: 4829.37\n",
      "Epoch [613], train_loss: 5311.90 with loss1: 426.40, loss2: 56.18 and loss3: 4829.32\n",
      "Epoch [614], train_loss: 5310.19 with loss1: 424.73, loss2: 56.19 and loss3: 4829.27\n",
      "Epoch [615], train_loss: 5311.93 with loss1: 426.68, loss2: 56.03 and loss3: 4829.22\n",
      "Epoch [616], train_loss: 5310.34 with loss1: 425.01, loss2: 56.16 and loss3: 4829.17\n",
      "Epoch [617], train_loss: 5311.90 with loss1: 426.42, loss2: 56.36 and loss3: 4829.12\n",
      "Epoch [618], train_loss: 5310.34 with loss1: 424.96, loss2: 56.32 and loss3: 4829.07\n",
      "Epoch [619], train_loss: 5311.85 with loss1: 426.02, loss2: 56.81 and loss3: 4829.02\n",
      "Epoch [620], train_loss: 5310.11 with loss1: 425.22, loss2: 55.92 and loss3: 4828.97\n",
      "Epoch [621], train_loss: 5312.67 with loss1: 427.41, loss2: 56.35 and loss3: 4828.92\n",
      "Epoch [622], train_loss: 5309.99 with loss1: 425.18, loss2: 55.94 and loss3: 4828.87\n",
      "Epoch [623], train_loss: 5313.13 with loss1: 428.29, loss2: 56.01 and loss3: 4828.82\n",
      "Epoch [624], train_loss: 5311.07 with loss1: 425.91, loss2: 56.39 and loss3: 4828.77\n",
      "Epoch [625], train_loss: 5313.04 with loss1: 428.27, loss2: 56.05 and loss3: 4828.72\n",
      "Epoch [626], train_loss: 5311.40 with loss1: 426.75, loss2: 55.97 and loss3: 4828.67\n",
      "Epoch [627], train_loss: 5314.31 with loss1: 429.67, loss2: 56.02 and loss3: 4828.62\n",
      "Epoch [628], train_loss: 5312.88 with loss1: 428.10, loss2: 56.21 and loss3: 4828.58\n",
      "Epoch [629], train_loss: 5316.19 with loss1: 431.51, loss2: 56.15 and loss3: 4828.53\n",
      "Epoch [630], train_loss: 5312.97 with loss1: 428.65, loss2: 55.84 and loss3: 4828.48\n",
      "Epoch [631], train_loss: 5315.96 with loss1: 431.53, loss2: 56.00 and loss3: 4828.43\n",
      "Epoch [632], train_loss: 5313.86 with loss1: 429.56, loss2: 55.92 and loss3: 4828.38\n",
      "Epoch [633], train_loss: 5316.36 with loss1: 431.82, loss2: 56.21 and loss3: 4828.33\n",
      "Epoch [634], train_loss: 5313.17 with loss1: 429.05, loss2: 55.84 and loss3: 4828.28\n",
      "Epoch [635], train_loss: 5316.21 with loss1: 431.96, loss2: 56.02 and loss3: 4828.23\n",
      "Epoch [636], train_loss: 5314.15 with loss1: 430.06, loss2: 55.91 and loss3: 4828.18\n",
      "Epoch [637], train_loss: 5317.37 with loss1: 433.49, loss2: 55.76 and loss3: 4828.13\n",
      "Epoch [638], train_loss: 5314.06 with loss1: 430.28, loss2: 55.71 and loss3: 4828.08\n",
      "Epoch [639], train_loss: 5317.61 with loss1: 433.69, loss2: 55.90 and loss3: 4828.03\n",
      "Epoch [640], train_loss: 5313.26 with loss1: 429.46, loss2: 55.81 and loss3: 4827.98\n",
      "Epoch [641], train_loss: 5316.99 with loss1: 432.58, loss2: 56.48 and loss3: 4827.93\n",
      "Epoch [642], train_loss: 5313.27 with loss1: 429.63, loss2: 55.76 and loss3: 4827.88\n",
      "Epoch [643], train_loss: 5316.63 with loss1: 432.85, loss2: 55.95 and loss3: 4827.83\n",
      "Epoch [644], train_loss: 5313.26 with loss1: 429.73, loss2: 55.75 and loss3: 4827.78\n",
      "Epoch [645], train_loss: 5316.42 with loss1: 433.02, loss2: 55.67 and loss3: 4827.73\n",
      "Epoch [646], train_loss: 5313.21 with loss1: 429.77, loss2: 55.76 and loss3: 4827.68\n",
      "Epoch [647], train_loss: 5315.47 with loss1: 432.16, loss2: 55.68 and loss3: 4827.64\n",
      "Epoch [648], train_loss: 5312.12 with loss1: 428.81, loss2: 55.72 and loss3: 4827.59\n",
      "Epoch [649], train_loss: 5315.24 with loss1: 431.62, loss2: 56.08 and loss3: 4827.54\n",
      "Epoch [650], train_loss: 5311.27 with loss1: 428.28, loss2: 55.51 and loss3: 4827.49\n",
      "Epoch [651], train_loss: 5313.51 with loss1: 430.11, loss2: 55.96 and loss3: 4827.44\n",
      "Epoch [652], train_loss: 5311.38 with loss1: 428.47, loss2: 55.52 and loss3: 4827.39\n",
      "Epoch [653], train_loss: 5313.51 with loss1: 430.51, loss2: 55.66 and loss3: 4827.34\n",
      "Epoch [654], train_loss: 5310.18 with loss1: 427.34, loss2: 55.55 and loss3: 4827.29\n",
      "Epoch [655], train_loss: 5313.47 with loss1: 430.22, loss2: 56.01 and loss3: 4827.24\n",
      "Epoch [656], train_loss: 5309.64 with loss1: 426.78, loss2: 55.67 and loss3: 4827.19\n",
      "Epoch [657], train_loss: 5311.95 with loss1: 429.19, loss2: 55.62 and loss3: 4827.14\n",
      "Epoch [658], train_loss: 5308.92 with loss1: 426.40, loss2: 55.43 and loss3: 4827.09\n",
      "Epoch [659], train_loss: 5311.75 with loss1: 429.08, loss2: 55.64 and loss3: 4827.04\n",
      "Epoch [660], train_loss: 5308.84 with loss1: 426.26, loss2: 55.59 and loss3: 4826.99\n",
      "Epoch [661], train_loss: 5310.51 with loss1: 427.92, loss2: 55.64 and loss3: 4826.94\n",
      "Epoch [662], train_loss: 5308.06 with loss1: 425.75, loss2: 55.41 and loss3: 4826.89\n",
      "Epoch [663], train_loss: 5311.00 with loss1: 428.55, loss2: 55.61 and loss3: 4826.84\n",
      "Epoch [664], train_loss: 5308.01 with loss1: 425.76, loss2: 55.45 and loss3: 4826.79\n",
      "Epoch [665], train_loss: 5310.68 with loss1: 428.31, loss2: 55.62 and loss3: 4826.75\n",
      "Epoch [666], train_loss: 5308.46 with loss1: 425.86, loss2: 55.91 and loss3: 4826.70\n",
      "Epoch [667], train_loss: 5310.55 with loss1: 428.30, loss2: 55.60 and loss3: 4826.65\n",
      "Epoch [668], train_loss: 5307.99 with loss1: 425.94, loss2: 55.46 and loss3: 4826.60\n",
      "Epoch [669], train_loss: 5310.71 with loss1: 428.51, loss2: 55.65 and loss3: 4826.55\n",
      "Epoch [670], train_loss: 5308.61 with loss1: 426.64, loss2: 55.47 and loss3: 4826.50\n",
      "Epoch [671], train_loss: 5310.82 with loss1: 428.92, loss2: 55.45 and loss3: 4826.45\n",
      "Epoch [672], train_loss: 5310.41 with loss1: 428.63, loss2: 55.38 and loss3: 4826.40\n",
      "Epoch [673], train_loss: 5312.30 with loss1: 430.15, loss2: 55.80 and loss3: 4826.35\n",
      "Epoch [674], train_loss: 5309.03 with loss1: 427.55, loss2: 55.19 and loss3: 4826.30\n",
      "Epoch [675], train_loss: 5313.81 with loss1: 431.96, loss2: 55.60 and loss3: 4826.25\n",
      "Epoch [676], train_loss: 5312.26 with loss1: 430.70, loss2: 55.37 and loss3: 4826.20\n",
      "Epoch [677], train_loss: 5313.23 with loss1: 431.58, loss2: 55.49 and loss3: 4826.15\n",
      "Epoch [678], train_loss: 5311.68 with loss1: 430.20, loss2: 55.37 and loss3: 4826.10\n",
      "Epoch [679], train_loss: 5314.54 with loss1: 433.10, loss2: 55.38 and loss3: 4826.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [680], train_loss: 5312.08 with loss1: 430.85, loss2: 55.23 and loss3: 4826.00\n",
      "Epoch [681], train_loss: 5317.39 with loss1: 436.03, loss2: 55.40 and loss3: 4825.95\n",
      "Epoch [682], train_loss: 5313.95 with loss1: 432.90, loss2: 55.14 and loss3: 4825.90\n",
      "Epoch [683], train_loss: 5318.06 with loss1: 436.89, loss2: 55.31 and loss3: 4825.85\n",
      "Epoch [684], train_loss: 5315.84 with loss1: 435.00, loss2: 55.03 and loss3: 4825.81\n",
      "Epoch [685], train_loss: 5319.20 with loss1: 438.06, loss2: 55.38 and loss3: 4825.76\n",
      "Epoch [686], train_loss: 5316.01 with loss1: 435.35, loss2: 54.96 and loss3: 4825.71\n",
      "Epoch [687], train_loss: 5320.76 with loss1: 439.66, loss2: 55.44 and loss3: 4825.66\n",
      "Epoch [688], train_loss: 5317.63 with loss1: 437.03, loss2: 54.99 and loss3: 4825.61\n",
      "Epoch [689], train_loss: 5321.45 with loss1: 440.47, loss2: 55.42 and loss3: 4825.56\n",
      "Epoch [690], train_loss: 5317.59 with loss1: 437.07, loss2: 55.01 and loss3: 4825.51\n",
      "Epoch [691], train_loss: 5321.18 with loss1: 440.22, loss2: 55.50 and loss3: 4825.46\n",
      "Epoch [692], train_loss: 5317.59 with loss1: 437.21, loss2: 54.97 and loss3: 4825.41\n",
      "Epoch [693], train_loss: 5320.70 with loss1: 440.02, loss2: 55.32 and loss3: 4825.36\n",
      "Epoch [694], train_loss: 5318.92 with loss1: 438.73, loss2: 54.87 and loss3: 4825.31\n",
      "Epoch [695], train_loss: 5320.63 with loss1: 440.03, loss2: 55.34 and loss3: 4825.26\n",
      "Epoch [696], train_loss: 5317.00 with loss1: 436.83, loss2: 54.96 and loss3: 4825.21\n",
      "Epoch [697], train_loss: 5320.20 with loss1: 439.73, loss2: 55.31 and loss3: 4825.16\n",
      "Epoch [698], train_loss: 5317.04 with loss1: 436.86, loss2: 55.06 and loss3: 4825.11\n",
      "Epoch [699], train_loss: 5321.16 with loss1: 440.58, loss2: 55.52 and loss3: 4825.06\n",
      "Epoch [700], train_loss: 5317.34 with loss1: 437.30, loss2: 55.02 and loss3: 4825.01\n",
      "Epoch [701], train_loss: 5319.03 with loss1: 438.92, loss2: 55.15 and loss3: 4824.96\n",
      "Epoch [702], train_loss: 5315.87 with loss1: 435.92, loss2: 55.03 and loss3: 4824.92\n",
      "Epoch [703], train_loss: 5318.77 with loss1: 438.91, loss2: 55.00 and loss3: 4824.87\n",
      "Epoch [704], train_loss: 5314.39 with loss1: 434.68, loss2: 54.90 and loss3: 4824.82\n",
      "Epoch [705], train_loss: 5318.41 with loss1: 438.26, loss2: 55.39 and loss3: 4824.77\n",
      "Epoch [706], train_loss: 5314.50 with loss1: 434.77, loss2: 55.02 and loss3: 4824.72\n",
      "Epoch [707], train_loss: 5316.60 with loss1: 436.65, loss2: 55.29 and loss3: 4824.67\n",
      "Epoch [708], train_loss: 5311.80 with loss1: 432.49, loss2: 54.69 and loss3: 4824.62\n",
      "Epoch [709], train_loss: 5314.58 with loss1: 434.83, loss2: 55.19 and loss3: 4824.57\n",
      "Epoch [710], train_loss: 5310.24 with loss1: 430.63, loss2: 55.09 and loss3: 4824.52\n",
      "Epoch [711], train_loss: 5313.42 with loss1: 433.69, loss2: 55.26 and loss3: 4824.47\n",
      "Epoch [712], train_loss: 5311.74 with loss1: 432.32, loss2: 54.99 and loss3: 4824.42\n",
      "Epoch [713], train_loss: 5311.27 with loss1: 431.91, loss2: 54.98 and loss3: 4824.37\n",
      "Epoch [714], train_loss: 5307.78 with loss1: 428.53, loss2: 54.92 and loss3: 4824.32\n",
      "Epoch [715], train_loss: 5309.48 with loss1: 430.18, loss2: 55.03 and loss3: 4824.27\n",
      "Epoch [716], train_loss: 5307.01 with loss1: 428.07, loss2: 54.72 and loss3: 4824.22\n",
      "Epoch [717], train_loss: 5308.57 with loss1: 429.39, loss2: 55.01 and loss3: 4824.17\n",
      "Epoch [718], train_loss: 5305.90 with loss1: 427.04, loss2: 54.74 and loss3: 4824.12\n",
      "Epoch [719], train_loss: 5307.33 with loss1: 428.19, loss2: 55.06 and loss3: 4824.08\n",
      "Epoch [720], train_loss: 5304.18 with loss1: 425.35, loss2: 54.80 and loss3: 4824.03\n",
      "Epoch [721], train_loss: 5305.11 with loss1: 426.40, loss2: 54.72 and loss3: 4823.98\n",
      "Epoch [722], train_loss: 5301.99 with loss1: 423.38, loss2: 54.69 and loss3: 4823.93\n",
      "Epoch [723], train_loss: 5305.09 with loss1: 426.27, loss2: 54.94 and loss3: 4823.88\n",
      "Epoch [724], train_loss: 5301.24 with loss1: 422.80, loss2: 54.61 and loss3: 4823.83\n",
      "Epoch [725], train_loss: 5304.00 with loss1: 425.33, loss2: 54.89 and loss3: 4823.78\n",
      "Epoch [726], train_loss: 5300.58 with loss1: 421.92, loss2: 54.93 and loss3: 4823.73\n",
      "Epoch [727], train_loss: 5301.83 with loss1: 423.22, loss2: 54.93 and loss3: 4823.68\n",
      "Epoch [728], train_loss: 5299.42 with loss1: 421.13, loss2: 54.65 and loss3: 4823.63\n",
      "Epoch [729], train_loss: 5300.76 with loss1: 422.25, loss2: 54.94 and loss3: 4823.58\n",
      "Epoch [730], train_loss: 5297.54 with loss1: 419.50, loss2: 54.51 and loss3: 4823.53\n",
      "Epoch [731], train_loss: 5300.07 with loss1: 421.87, loss2: 54.72 and loss3: 4823.48\n",
      "Epoch [732], train_loss: 5296.95 with loss1: 419.05, loss2: 54.47 and loss3: 4823.43\n",
      "Epoch [733], train_loss: 5299.76 with loss1: 421.69, loss2: 54.69 and loss3: 4823.38\n",
      "Epoch [734], train_loss: 5297.01 with loss1: 419.25, loss2: 54.42 and loss3: 4823.33\n",
      "Epoch [735], train_loss: 5299.36 with loss1: 421.38, loss2: 54.70 and loss3: 4823.28\n",
      "Epoch [736], train_loss: 5298.85 with loss1: 420.96, loss2: 54.65 and loss3: 4823.24\n",
      "Epoch [737], train_loss: 5300.56 with loss1: 422.57, loss2: 54.80 and loss3: 4823.19\n",
      "Epoch [738], train_loss: 5297.60 with loss1: 420.05, loss2: 54.41 and loss3: 4823.14\n",
      "Epoch [739], train_loss: 5299.57 with loss1: 421.81, loss2: 54.67 and loss3: 4823.09\n",
      "Epoch [740], train_loss: 5297.50 with loss1: 419.89, loss2: 54.56 and loss3: 4823.04\n",
      "Epoch [741], train_loss: 5298.92 with loss1: 421.35, loss2: 54.58 and loss3: 4822.99\n",
      "Epoch [742], train_loss: 5298.15 with loss1: 420.42, loss2: 54.78 and loss3: 4822.94\n",
      "Epoch [743], train_loss: 5299.49 with loss1: 422.14, loss2: 54.45 and loss3: 4822.89\n",
      "Epoch [744], train_loss: 5296.45 with loss1: 419.27, loss2: 54.34 and loss3: 4822.84\n",
      "Epoch [745], train_loss: 5298.57 with loss1: 421.19, loss2: 54.59 and loss3: 4822.79\n",
      "Epoch [746], train_loss: 5295.83 with loss1: 418.85, loss2: 54.24 and loss3: 4822.74\n",
      "Epoch [747], train_loss: 5298.72 with loss1: 421.57, loss2: 54.45 and loss3: 4822.69\n",
      "Epoch [748], train_loss: 5295.81 with loss1: 418.83, loss2: 54.34 and loss3: 4822.64\n",
      "Epoch [749], train_loss: 5298.51 with loss1: 421.20, loss2: 54.72 and loss3: 4822.59\n",
      "Epoch [750], train_loss: 5295.38 with loss1: 418.66, loss2: 54.18 and loss3: 4822.54\n",
      "Epoch [751], train_loss: 5299.20 with loss1: 422.36, loss2: 54.35 and loss3: 4822.49\n",
      "Epoch [752], train_loss: 5295.75 with loss1: 419.12, loss2: 54.18 and loss3: 4822.44\n",
      "Epoch [753], train_loss: 5298.98 with loss1: 421.89, loss2: 54.70 and loss3: 4822.40\n",
      "Epoch [754], train_loss: 5296.42 with loss1: 419.34, loss2: 54.74 and loss3: 4822.35\n",
      "Epoch [755], train_loss: 5297.89 with loss1: 421.27, loss2: 54.33 and loss3: 4822.30\n",
      "Epoch [756], train_loss: 5295.35 with loss1: 418.86, loss2: 54.25 and loss3: 4822.25\n",
      "Epoch [757], train_loss: 5298.81 with loss1: 422.15, loss2: 54.47 and loss3: 4822.20\n",
      "Epoch [758], train_loss: 5296.88 with loss1: 420.51, loss2: 54.22 and loss3: 4822.15\n",
      "Epoch [759], train_loss: 5299.27 with loss1: 422.52, loss2: 54.66 and loss3: 4822.10\n",
      "Epoch [760], train_loss: 5297.75 with loss1: 421.36, loss2: 54.35 and loss3: 4822.05\n",
      "Epoch [761], train_loss: 5298.07 with loss1: 421.74, loss2: 54.33 and loss3: 4822.00\n",
      "Epoch [762], train_loss: 5296.33 with loss1: 420.14, loss2: 54.23 and loss3: 4821.95\n",
      "Epoch [763], train_loss: 5299.17 with loss1: 422.76, loss2: 54.51 and loss3: 4821.90\n",
      "Epoch [764], train_loss: 5296.30 with loss1: 420.19, loss2: 54.26 and loss3: 4821.85\n",
      "Epoch [765], train_loss: 5299.80 with loss1: 423.76, loss2: 54.24 and loss3: 4821.80\n",
      "Epoch [766], train_loss: 5297.73 with loss1: 422.04, loss2: 53.93 and loss3: 4821.75\n",
      "Epoch [767], train_loss: 5300.06 with loss1: 424.02, loss2: 54.34 and loss3: 4821.70\n",
      "Epoch [768], train_loss: 5297.11 with loss1: 421.33, loss2: 54.12 and loss3: 4821.65\n",
      "Epoch [769], train_loss: 5300.31 with loss1: 424.28, loss2: 54.42 and loss3: 4821.60\n",
      "Epoch [770], train_loss: 5297.51 with loss1: 421.95, loss2: 54.01 and loss3: 4821.55\n",
      "Epoch [771], train_loss: 5300.35 with loss1: 424.54, loss2: 54.30 and loss3: 4821.51\n",
      "Epoch [772], train_loss: 5297.91 with loss1: 422.52, loss2: 53.93 and loss3: 4821.46\n",
      "Epoch [773], train_loss: 5300.66 with loss1: 424.99, loss2: 54.26 and loss3: 4821.41\n",
      "Epoch [774], train_loss: 5297.97 with loss1: 422.63, loss2: 53.98 and loss3: 4821.36\n",
      "Epoch [775], train_loss: 5301.88 with loss1: 426.21, loss2: 54.37 and loss3: 4821.31\n",
      "Epoch [776], train_loss: 5299.44 with loss1: 424.30, loss2: 53.89 and loss3: 4821.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [777], train_loss: 5301.08 with loss1: 425.73, loss2: 54.13 and loss3: 4821.21\n",
      "Epoch [778], train_loss: 5299.00 with loss1: 424.02, loss2: 53.81 and loss3: 4821.16\n",
      "Epoch [779], train_loss: 5301.75 with loss1: 426.48, loss2: 54.16 and loss3: 4821.11\n",
      "Epoch [780], train_loss: 5300.55 with loss1: 425.18, loss2: 54.31 and loss3: 4821.06\n",
      "Epoch [781], train_loss: 5302.64 with loss1: 427.46, loss2: 54.17 and loss3: 4821.01\n",
      "Epoch [782], train_loss: 5299.75 with loss1: 425.00, loss2: 53.79 and loss3: 4820.96\n",
      "Epoch [783], train_loss: 5303.94 with loss1: 429.02, loss2: 54.01 and loss3: 4820.91\n",
      "Epoch [784], train_loss: 5299.18 with loss1: 424.72, loss2: 53.60 and loss3: 4820.86\n",
      "Epoch [785], train_loss: 5303.51 with loss1: 428.70, loss2: 54.00 and loss3: 4820.81\n",
      "Epoch [786], train_loss: 5301.45 with loss1: 426.90, loss2: 53.78 and loss3: 4820.76\n",
      "Epoch [787], train_loss: 5303.99 with loss1: 429.31, loss2: 53.97 and loss3: 4820.71\n",
      "Epoch [788], train_loss: 5300.52 with loss1: 426.16, loss2: 53.69 and loss3: 4820.67\n",
      "Epoch [789], train_loss: 5302.77 with loss1: 428.17, loss2: 53.98 and loss3: 4820.62\n",
      "Epoch [790], train_loss: 5301.02 with loss1: 426.59, loss2: 53.86 and loss3: 4820.57\n",
      "Epoch [791], train_loss: 5303.22 with loss1: 428.60, loss2: 54.11 and loss3: 4820.52\n",
      "Epoch [792], train_loss: 5300.81 with loss1: 426.63, loss2: 53.71 and loss3: 4820.47\n",
      "Epoch [793], train_loss: 5303.76 with loss1: 429.53, loss2: 53.81 and loss3: 4820.42\n",
      "Epoch [794], train_loss: 5300.39 with loss1: 426.48, loss2: 53.54 and loss3: 4820.37\n",
      "Epoch [795], train_loss: 5303.12 with loss1: 428.60, loss2: 54.20 and loss3: 4820.32\n",
      "Epoch [796], train_loss: 5301.17 with loss1: 426.97, loss2: 53.92 and loss3: 4820.27\n",
      "Epoch [797], train_loss: 5303.74 with loss1: 429.68, loss2: 53.84 and loss3: 4820.22\n",
      "Epoch [798], train_loss: 5299.96 with loss1: 426.24, loss2: 53.55 and loss3: 4820.17\n",
      "Epoch [799], train_loss: 5303.15 with loss1: 429.25, loss2: 53.78 and loss3: 4820.12\n",
      "Epoch [800], train_loss: 5300.25 with loss1: 426.72, loss2: 53.46 and loss3: 4820.07\n",
      "Epoch [801], train_loss: 5303.64 with loss1: 429.34, loss2: 54.28 and loss3: 4820.02\n",
      "Epoch [802], train_loss: 5300.14 with loss1: 426.63, loss2: 53.54 and loss3: 4819.97\n",
      "Epoch [803], train_loss: 5302.13 with loss1: 428.32, loss2: 53.88 and loss3: 4819.92\n",
      "Epoch [804], train_loss: 5298.06 with loss1: 424.59, loss2: 53.59 and loss3: 4819.88\n",
      "Epoch [805], train_loss: 5301.58 with loss1: 427.99, loss2: 53.76 and loss3: 4819.83\n",
      "Epoch [806], train_loss: 5298.50 with loss1: 424.98, loss2: 53.74 and loss3: 4819.78\n",
      "Epoch [807], train_loss: 5301.37 with loss1: 427.81, loss2: 53.83 and loss3: 4819.73\n",
      "Epoch [808], train_loss: 5297.72 with loss1: 424.28, loss2: 53.76 and loss3: 4819.68\n",
      "Epoch [809], train_loss: 5299.59 with loss1: 426.34, loss2: 53.62 and loss3: 4819.63\n",
      "Epoch [810], train_loss: 5297.52 with loss1: 424.43, loss2: 53.51 and loss3: 4819.58\n",
      "Epoch [811], train_loss: 5298.26 with loss1: 425.11, loss2: 53.61 and loss3: 4819.53\n",
      "Epoch [812], train_loss: 5295.87 with loss1: 422.84, loss2: 53.55 and loss3: 4819.48\n",
      "Epoch [813], train_loss: 5299.15 with loss1: 425.95, loss2: 53.77 and loss3: 4819.43\n",
      "Epoch [814], train_loss: 5294.68 with loss1: 421.95, loss2: 53.35 and loss3: 4819.38\n",
      "Epoch [815], train_loss: 5296.15 with loss1: 423.07, loss2: 53.75 and loss3: 4819.33\n",
      "Epoch [816], train_loss: 5293.35 with loss1: 420.42, loss2: 53.64 and loss3: 4819.28\n",
      "Epoch [817], train_loss: 5295.19 with loss1: 422.30, loss2: 53.66 and loss3: 4819.23\n",
      "Epoch [818], train_loss: 5291.79 with loss1: 419.14, loss2: 53.47 and loss3: 4819.18\n",
      "Epoch [819], train_loss: 5293.24 with loss1: 420.66, loss2: 53.45 and loss3: 4819.13\n",
      "Epoch [820], train_loss: 5291.18 with loss1: 418.79, loss2: 53.31 and loss3: 4819.08\n",
      "Epoch [821], train_loss: 5293.31 with loss1: 420.70, loss2: 53.58 and loss3: 4819.04\n",
      "Epoch [822], train_loss: 5291.29 with loss1: 418.97, loss2: 53.33 and loss3: 4818.99\n",
      "Epoch [823], train_loss: 5292.09 with loss1: 419.69, loss2: 53.47 and loss3: 4818.94\n",
      "Epoch [824], train_loss: 5289.44 with loss1: 417.12, loss2: 53.44 and loss3: 4818.89\n",
      "Epoch [825], train_loss: 5290.02 with loss1: 417.78, loss2: 53.40 and loss3: 4818.84\n",
      "Epoch [826], train_loss: 5288.85 with loss1: 416.50, loss2: 53.56 and loss3: 4818.79\n",
      "Epoch [827], train_loss: 5289.66 with loss1: 417.45, loss2: 53.48 and loss3: 4818.74\n",
      "Epoch [828], train_loss: 5286.15 with loss1: 414.12, loss2: 53.34 and loss3: 4818.69\n",
      "Epoch [829], train_loss: 5287.71 with loss1: 415.73, loss2: 53.34 and loss3: 4818.64\n",
      "Epoch [830], train_loss: 5285.16 with loss1: 413.13, loss2: 53.45 and loss3: 4818.59\n",
      "Epoch [831], train_loss: 5287.53 with loss1: 415.62, loss2: 53.37 and loss3: 4818.54\n",
      "Epoch [832], train_loss: 5284.36 with loss1: 412.72, loss2: 53.15 and loss3: 4818.49\n",
      "Epoch [833], train_loss: 5284.38 with loss1: 412.55, loss2: 53.39 and loss3: 4818.44\n",
      "Epoch [834], train_loss: 5282.10 with loss1: 410.52, loss2: 53.19 and loss3: 4818.39\n",
      "Epoch [835], train_loss: 5284.27 with loss1: 412.49, loss2: 53.44 and loss3: 4818.34\n",
      "Epoch [836], train_loss: 5280.73 with loss1: 409.09, loss2: 53.35 and loss3: 4818.29\n",
      "Epoch [837], train_loss: 5282.10 with loss1: 410.42, loss2: 53.43 and loss3: 4818.25\n",
      "Epoch [838], train_loss: 5279.63 with loss1: 408.24, loss2: 53.20 and loss3: 4818.20\n",
      "Epoch [839], train_loss: 5280.63 with loss1: 409.06, loss2: 53.43 and loss3: 4818.15\n",
      "Epoch [840], train_loss: 5278.74 with loss1: 407.19, loss2: 53.45 and loss3: 4818.10\n",
      "Epoch [841], train_loss: 5280.65 with loss1: 409.50, loss2: 53.11 and loss3: 4818.05\n",
      "Epoch [842], train_loss: 5277.50 with loss1: 406.47, loss2: 53.03 and loss3: 4818.00\n",
      "Epoch [843], train_loss: 5279.08 with loss1: 407.93, loss2: 53.21 and loss3: 4817.95\n",
      "Epoch [844], train_loss: 5278.67 with loss1: 407.41, loss2: 53.37 and loss3: 4817.90\n",
      "Epoch [845], train_loss: 5279.86 with loss1: 408.78, loss2: 53.23 and loss3: 4817.85\n",
      "Epoch [846], train_loss: 5276.89 with loss1: 405.98, loss2: 53.12 and loss3: 4817.80\n",
      "Epoch [847], train_loss: 5279.78 with loss1: 408.86, loss2: 53.17 and loss3: 4817.75\n",
      "Epoch [848], train_loss: 5277.26 with loss1: 406.52, loss2: 53.03 and loss3: 4817.70\n",
      "Epoch [849], train_loss: 5279.80 with loss1: 409.02, loss2: 53.13 and loss3: 4817.65\n",
      "Epoch [850], train_loss: 5278.80 with loss1: 408.17, loss2: 53.03 and loss3: 4817.60\n",
      "Epoch [851], train_loss: 5279.49 with loss1: 408.66, loss2: 53.28 and loss3: 4817.55\n",
      "Epoch [852], train_loss: 5278.15 with loss1: 407.65, loss2: 52.99 and loss3: 4817.50\n",
      "Epoch [853], train_loss: 5279.00 with loss1: 408.44, loss2: 53.10 and loss3: 4817.45\n",
      "Epoch [854], train_loss: 5277.85 with loss1: 407.56, loss2: 52.89 and loss3: 4817.41\n",
      "Epoch [855], train_loss: 5280.13 with loss1: 409.50, loss2: 53.28 and loss3: 4817.36\n",
      "Epoch [856], train_loss: 5278.39 with loss1: 407.99, loss2: 53.10 and loss3: 4817.31\n",
      "Epoch [857], train_loss: 5281.48 with loss1: 410.99, loss2: 53.24 and loss3: 4817.26\n",
      "Epoch [858], train_loss: 5279.36 with loss1: 409.06, loss2: 53.09 and loss3: 4817.21\n",
      "Epoch [859], train_loss: 5280.85 with loss1: 410.68, loss2: 53.01 and loss3: 4817.16\n",
      "Epoch [860], train_loss: 5280.46 with loss1: 410.58, loss2: 52.77 and loss3: 4817.11\n",
      "Epoch [861], train_loss: 5282.13 with loss1: 412.13, loss2: 52.94 and loss3: 4817.06\n",
      "Epoch [862], train_loss: 5280.02 with loss1: 410.09, loss2: 52.91 and loss3: 4817.01\n",
      "Epoch [863], train_loss: 5283.84 with loss1: 414.02, loss2: 52.86 and loss3: 4816.96\n",
      "Epoch [864], train_loss: 5281.61 with loss1: 411.92, loss2: 52.78 and loss3: 4816.91\n",
      "Epoch [865], train_loss: 5283.32 with loss1: 413.21, loss2: 53.25 and loss3: 4816.86\n",
      "Epoch [866], train_loss: 5281.19 with loss1: 411.70, loss2: 52.68 and loss3: 4816.81\n",
      "Epoch [867], train_loss: 5283.27 with loss1: 413.47, loss2: 53.04 and loss3: 4816.76\n",
      "Epoch [868], train_loss: 5282.14 with loss1: 412.53, loss2: 52.90 and loss3: 4816.71\n",
      "Epoch [869], train_loss: 5284.28 with loss1: 414.69, loss2: 52.92 and loss3: 4816.66\n",
      "Epoch [870], train_loss: 5282.81 with loss1: 413.23, loss2: 52.96 and loss3: 4816.62\n",
      "Epoch [871], train_loss: 5286.76 with loss1: 417.25, loss2: 52.94 and loss3: 4816.57\n",
      "Epoch [872], train_loss: 5283.55 with loss1: 414.50, loss2: 52.53 and loss3: 4816.52\n",
      "Epoch [873], train_loss: 5288.18 with loss1: 418.86, loss2: 52.85 and loss3: 4816.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [874], train_loss: 5285.16 with loss1: 415.74, loss2: 53.00 and loss3: 4816.42\n",
      "Epoch [875], train_loss: 5287.10 with loss1: 417.97, loss2: 52.77 and loss3: 4816.37\n",
      "Epoch [876], train_loss: 5287.08 with loss1: 418.00, loss2: 52.76 and loss3: 4816.32\n",
      "Epoch [877], train_loss: 5288.74 with loss1: 419.41, loss2: 53.07 and loss3: 4816.27\n",
      "Epoch [878], train_loss: 5286.90 with loss1: 418.07, loss2: 52.61 and loss3: 4816.22\n",
      "Epoch [879], train_loss: 5289.81 with loss1: 420.69, loss2: 52.94 and loss3: 4816.17\n",
      "Epoch [880], train_loss: 5286.93 with loss1: 417.98, loss2: 52.83 and loss3: 4816.12\n",
      "Epoch [881], train_loss: 5290.94 with loss1: 421.94, loss2: 52.93 and loss3: 4816.07\n",
      "Epoch [882], train_loss: 5288.99 with loss1: 420.30, loss2: 52.66 and loss3: 4816.02\n",
      "Epoch [883], train_loss: 5289.92 with loss1: 421.27, loss2: 52.69 and loss3: 4815.97\n",
      "Epoch [884], train_loss: 5287.54 with loss1: 419.23, loss2: 52.39 and loss3: 4815.92\n",
      "Epoch [885], train_loss: 5292.22 with loss1: 423.09, loss2: 53.26 and loss3: 4815.87\n",
      "Epoch [886], train_loss: 5288.08 with loss1: 419.61, loss2: 52.64 and loss3: 4815.82\n",
      "Epoch [887], train_loss: 5291.98 with loss1: 423.46, loss2: 52.74 and loss3: 4815.78\n",
      "Epoch [888], train_loss: 5289.72 with loss1: 421.40, loss2: 52.60 and loss3: 4815.73\n",
      "Epoch [889], train_loss: 5292.22 with loss1: 423.79, loss2: 52.76 and loss3: 4815.68\n",
      "Epoch [890], train_loss: 5289.72 with loss1: 421.33, loss2: 52.76 and loss3: 4815.63\n",
      "Epoch [891], train_loss: 5291.14 with loss1: 422.66, loss2: 52.90 and loss3: 4815.58\n",
      "Epoch [892], train_loss: 5288.61 with loss1: 420.73, loss2: 52.35 and loss3: 4815.53\n",
      "Epoch [893], train_loss: 5290.34 with loss1: 422.27, loss2: 52.59 and loss3: 4815.48\n",
      "Epoch [894], train_loss: 5288.09 with loss1: 420.11, loss2: 52.56 and loss3: 4815.43\n",
      "Epoch [895], train_loss: 5289.07 with loss1: 421.04, loss2: 52.66 and loss3: 4815.38\n",
      "Epoch [896], train_loss: 5287.56 with loss1: 419.82, loss2: 52.41 and loss3: 4815.33\n",
      "Epoch [897], train_loss: 5287.85 with loss1: 420.05, loss2: 52.52 and loss3: 4815.28\n",
      "Epoch [898], train_loss: 5285.44 with loss1: 417.92, loss2: 52.29 and loss3: 4815.23\n",
      "Epoch [899], train_loss: 5287.44 with loss1: 419.58, loss2: 52.67 and loss3: 4815.18\n",
      "Epoch [900], train_loss: 5285.74 with loss1: 418.24, loss2: 52.37 and loss3: 4815.13\n",
      "Epoch [901], train_loss: 5286.14 with loss1: 418.51, loss2: 52.54 and loss3: 4815.08\n",
      "Epoch [902], train_loss: 5283.39 with loss1: 415.79, loss2: 52.56 and loss3: 4815.03\n",
      "Epoch [903], train_loss: 5285.67 with loss1: 418.21, loss2: 52.48 and loss3: 4814.99\n",
      "Epoch [904], train_loss: 5284.35 with loss1: 417.12, loss2: 52.30 and loss3: 4814.94\n",
      "Epoch [905], train_loss: 5284.48 with loss1: 417.26, loss2: 52.34 and loss3: 4814.89\n",
      "Epoch [906], train_loss: 5281.84 with loss1: 414.67, loss2: 52.34 and loss3: 4814.84\n",
      "Epoch [907], train_loss: 5283.62 with loss1: 416.15, loss2: 52.68 and loss3: 4814.79\n",
      "Epoch [908], train_loss: 5281.23 with loss1: 414.13, loss2: 52.36 and loss3: 4814.74\n",
      "Epoch [909], train_loss: 5283.30 with loss1: 415.92, loss2: 52.69 and loss3: 4814.69\n",
      "Epoch [910], train_loss: 5281.60 with loss1: 414.28, loss2: 52.68 and loss3: 4814.64\n",
      "Epoch [911], train_loss: 5285.06 with loss1: 418.01, loss2: 52.46 and loss3: 4814.59\n",
      "Epoch [912], train_loss: 5281.75 with loss1: 414.86, loss2: 52.36 and loss3: 4814.54\n",
      "Epoch [913], train_loss: 5286.25 with loss1: 419.29, loss2: 52.47 and loss3: 4814.49\n",
      "Epoch [914], train_loss: 5282.62 with loss1: 415.71, loss2: 52.46 and loss3: 4814.44\n",
      "Epoch [915], train_loss: 5284.38 with loss1: 417.38, loss2: 52.61 and loss3: 4814.39\n",
      "Epoch [916], train_loss: 5280.79 with loss1: 414.41, loss2: 52.04 and loss3: 4814.34\n",
      "Epoch [917], train_loss: 5284.27 with loss1: 417.62, loss2: 52.35 and loss3: 4814.29\n",
      "Epoch [918], train_loss: 5279.80 with loss1: 413.38, loss2: 52.17 and loss3: 4814.24\n",
      "Epoch [919], train_loss: 5283.56 with loss1: 416.84, loss2: 52.53 and loss3: 4814.20\n",
      "Epoch [920], train_loss: 5282.94 with loss1: 416.59, loss2: 52.21 and loss3: 4814.15\n",
      "Epoch [921], train_loss: 5284.45 with loss1: 418.00, loss2: 52.35 and loss3: 4814.10\n",
      "Epoch [922], train_loss: 5282.14 with loss1: 415.79, loss2: 52.30 and loss3: 4814.05\n",
      "Epoch [923], train_loss: 5284.37 with loss1: 418.09, loss2: 52.29 and loss3: 4814.00\n",
      "Epoch [924], train_loss: 5282.91 with loss1: 416.39, loss2: 52.57 and loss3: 4813.95\n",
      "Epoch [925], train_loss: 5284.69 with loss1: 418.57, loss2: 52.23 and loss3: 4813.90\n",
      "Epoch [926], train_loss: 5282.95 with loss1: 417.04, loss2: 52.06 and loss3: 4813.85\n",
      "Epoch [927], train_loss: 5285.00 with loss1: 418.59, loss2: 52.61 and loss3: 4813.80\n",
      "Epoch [928], train_loss: 5281.66 with loss1: 415.78, loss2: 52.13 and loss3: 4813.75\n",
      "Epoch [929], train_loss: 5285.84 with loss1: 419.77, loss2: 52.36 and loss3: 4813.70\n",
      "Epoch [930], train_loss: 5282.33 with loss1: 416.81, loss2: 51.87 and loss3: 4813.65\n",
      "Epoch [931], train_loss: 5284.75 with loss1: 418.77, loss2: 52.38 and loss3: 4813.60\n",
      "Epoch [932], train_loss: 5282.99 with loss1: 417.69, loss2: 51.75 and loss3: 4813.55\n",
      "Epoch [933], train_loss: 5284.14 with loss1: 418.52, loss2: 52.12 and loss3: 4813.50\n",
      "Epoch [934], train_loss: 5282.40 with loss1: 416.92, loss2: 52.02 and loss3: 4813.46\n",
      "Epoch [935], train_loss: 5285.44 with loss1: 419.91, loss2: 52.12 and loss3: 4813.41\n",
      "Epoch [936], train_loss: 5281.61 with loss1: 416.20, loss2: 52.05 and loss3: 4813.36\n",
      "Epoch [937], train_loss: 5284.03 with loss1: 418.45, loss2: 52.27 and loss3: 4813.31\n",
      "Epoch [938], train_loss: 5281.04 with loss1: 415.90, loss2: 51.88 and loss3: 4813.26\n",
      "Epoch [939], train_loss: 5283.52 with loss1: 418.22, loss2: 52.10 and loss3: 4813.21\n",
      "Epoch [940], train_loss: 5280.03 with loss1: 415.18, loss2: 51.69 and loss3: 4813.16\n",
      "Epoch [941], train_loss: 5282.64 with loss1: 417.15, loss2: 52.38 and loss3: 4813.11\n",
      "Epoch [942], train_loss: 5278.90 with loss1: 414.04, loss2: 51.80 and loss3: 4813.06\n",
      "Epoch [943], train_loss: 5281.47 with loss1: 416.45, loss2: 52.01 and loss3: 4813.01\n",
      "Epoch [944], train_loss: 5278.53 with loss1: 413.73, loss2: 51.85 and loss3: 4812.96\n",
      "Epoch [945], train_loss: 5281.86 with loss1: 416.80, loss2: 52.15 and loss3: 4812.91\n",
      "Epoch [946], train_loss: 5279.40 with loss1: 414.79, loss2: 51.75 and loss3: 4812.86\n",
      "Epoch [947], train_loss: 5280.42 with loss1: 415.59, loss2: 52.01 and loss3: 4812.81\n",
      "Epoch [948], train_loss: 5277.50 with loss1: 412.92, loss2: 51.81 and loss3: 4812.76\n",
      "Epoch [949], train_loss: 5279.89 with loss1: 415.14, loss2: 52.03 and loss3: 4812.71\n",
      "Epoch [950], train_loss: 5277.26 with loss1: 412.62, loss2: 51.98 and loss3: 4812.66\n",
      "Epoch [951], train_loss: 5279.20 with loss1: 414.53, loss2: 52.06 and loss3: 4812.62\n",
      "Epoch [952], train_loss: 5275.57 with loss1: 411.13, loss2: 51.87 and loss3: 4812.57\n",
      "Epoch [953], train_loss: 5276.00 with loss1: 411.69, loss2: 51.80 and loss3: 4812.52\n",
      "Epoch [954], train_loss: 5275.34 with loss1: 411.00, loss2: 51.87 and loss3: 4812.47\n",
      "Epoch [955], train_loss: 5276.66 with loss1: 412.14, loss2: 52.10 and loss3: 4812.42\n",
      "Epoch [956], train_loss: 5274.67 with loss1: 410.60, loss2: 51.70 and loss3: 4812.37\n",
      "Epoch [957], train_loss: 5276.09 with loss1: 411.89, loss2: 51.88 and loss3: 4812.32\n",
      "Epoch [958], train_loss: 5273.82 with loss1: 409.67, loss2: 51.88 and loss3: 4812.27\n",
      "Epoch [959], train_loss: 5275.99 with loss1: 411.62, loss2: 52.15 and loss3: 4812.22\n",
      "Epoch [960], train_loss: 5272.98 with loss1: 409.13, loss2: 51.68 and loss3: 4812.17\n",
      "Epoch [961], train_loss: 5274.74 with loss1: 410.68, loss2: 51.94 and loss3: 4812.12\n",
      "Epoch [962], train_loss: 5271.73 with loss1: 408.12, loss2: 51.55 and loss3: 4812.07\n",
      "Epoch [963], train_loss: 5275.11 with loss1: 411.09, loss2: 52.00 and loss3: 4812.02\n",
      "Epoch [964], train_loss: 5271.55 with loss1: 408.04, loss2: 51.54 and loss3: 4811.97\n",
      "Epoch [965], train_loss: 5274.08 with loss1: 410.15, loss2: 52.01 and loss3: 4811.92\n",
      "Epoch [966], train_loss: 5271.49 with loss1: 408.05, loss2: 51.56 and loss3: 4811.88\n",
      "Epoch [967], train_loss: 5273.36 with loss1: 409.84, loss2: 51.70 and loss3: 4811.83\n",
      "Epoch [968], train_loss: 5272.52 with loss1: 409.21, loss2: 51.53 and loss3: 4811.78\n",
      "Epoch [969], train_loss: 5275.17 with loss1: 411.66, loss2: 51.78 and loss3: 4811.73\n",
      "Epoch [970], train_loss: 5272.57 with loss1: 409.36, loss2: 51.53 and loss3: 4811.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [971], train_loss: 5273.92 with loss1: 410.57, loss2: 51.71 and loss3: 4811.63\n",
      "Epoch [972], train_loss: 5272.77 with loss1: 409.50, loss2: 51.69 and loss3: 4811.58\n",
      "Epoch [973], train_loss: 5275.44 with loss1: 411.81, loss2: 52.10 and loss3: 4811.53\n",
      "Epoch [974], train_loss: 5272.84 with loss1: 409.93, loss2: 51.43 and loss3: 4811.48\n",
      "Epoch [975], train_loss: 5276.29 with loss1: 413.22, loss2: 51.64 and loss3: 4811.43\n",
      "Epoch [976], train_loss: 5274.50 with loss1: 411.51, loss2: 51.61 and loss3: 4811.38\n",
      "Epoch [977], train_loss: 5275.99 with loss1: 413.01, loss2: 51.66 and loss3: 4811.33\n",
      "Epoch [978], train_loss: 5276.29 with loss1: 413.32, loss2: 51.69 and loss3: 4811.28\n",
      "Epoch [979], train_loss: 5275.80 with loss1: 412.87, loss2: 51.70 and loss3: 4811.23\n",
      "Epoch [980], train_loss: 5274.83 with loss1: 411.97, loss2: 51.67 and loss3: 4811.18\n",
      "Epoch [981], train_loss: 5276.58 with loss1: 413.62, loss2: 51.83 and loss3: 4811.13\n",
      "Epoch [982], train_loss: 5273.99 with loss1: 411.44, loss2: 51.47 and loss3: 4811.08\n",
      "Epoch [983], train_loss: 5277.78 with loss1: 414.81, loss2: 51.93 and loss3: 4811.04\n",
      "Epoch [984], train_loss: 5273.80 with loss1: 411.50, loss2: 51.31 and loss3: 4810.99\n",
      "Epoch [985], train_loss: 5277.16 with loss1: 414.56, loss2: 51.67 and loss3: 4810.94\n",
      "Epoch [986], train_loss: 5274.19 with loss1: 411.99, loss2: 51.32 and loss3: 4810.89\n",
      "Epoch [987], train_loss: 5275.96 with loss1: 413.49, loss2: 51.63 and loss3: 4810.84\n",
      "Epoch [988], train_loss: 5274.32 with loss1: 412.27, loss2: 51.26 and loss3: 4810.79\n",
      "Epoch [989], train_loss: 5277.03 with loss1: 414.60, loss2: 51.69 and loss3: 4810.74\n",
      "Epoch [990], train_loss: 5273.24 with loss1: 410.96, loss2: 51.59 and loss3: 4810.69\n",
      "Epoch [991], train_loss: 5275.62 with loss1: 413.41, loss2: 51.57 and loss3: 4810.64\n",
      "Epoch [992], train_loss: 5274.00 with loss1: 412.12, loss2: 51.29 and loss3: 4810.59\n",
      "Epoch [993], train_loss: 5274.73 with loss1: 412.70, loss2: 51.49 and loss3: 4810.54\n",
      "Epoch [994], train_loss: 5273.75 with loss1: 411.95, loss2: 51.31 and loss3: 4810.49\n",
      "Epoch [995], train_loss: 5275.53 with loss1: 413.52, loss2: 51.56 and loss3: 4810.44\n",
      "Epoch [996], train_loss: 5272.00 with loss1: 410.26, loss2: 51.34 and loss3: 4810.39\n",
      "Epoch [997], train_loss: 5274.02 with loss1: 412.11, loss2: 51.56 and loss3: 4810.34\n",
      "Epoch [998], train_loss: 5271.85 with loss1: 410.37, loss2: 51.18 and loss3: 4810.30\n",
      "Epoch [999], train_loss: 5274.78 with loss1: 412.92, loss2: 51.62 and loss3: 4810.25\n",
      "Epoch [1000], train_loss: 5271.90 with loss1: 410.36, loss2: 51.34 and loss3: 4810.20\n",
      "Epoch [1001], train_loss: 5275.13 with loss1: 413.23, loss2: 51.75 and loss3: 4810.15\n",
      "Epoch [1002], train_loss: 5272.27 with loss1: 410.90, loss2: 51.27 and loss3: 4810.10\n",
      "Epoch [1003], train_loss: 5275.62 with loss1: 414.25, loss2: 51.33 and loss3: 4810.05\n",
      "Epoch [1004], train_loss: 5271.95 with loss1: 410.77, loss2: 51.18 and loss3: 4810.00\n",
      "Epoch [1005], train_loss: 5274.47 with loss1: 413.10, loss2: 51.42 and loss3: 4809.95\n",
      "Epoch [1006], train_loss: 5271.93 with loss1: 410.86, loss2: 51.17 and loss3: 4809.90\n",
      "Epoch [1007], train_loss: 5273.54 with loss1: 412.18, loss2: 51.51 and loss3: 4809.85\n",
      "Epoch [1008], train_loss: 5270.78 with loss1: 410.01, loss2: 50.96 and loss3: 4809.80\n",
      "Epoch [1009], train_loss: 5273.30 with loss1: 412.30, loss2: 51.24 and loss3: 4809.75\n",
      "Epoch [1010], train_loss: 5271.38 with loss1: 410.39, loss2: 51.29 and loss3: 4809.70\n",
      "Epoch [1011], train_loss: 5272.18 with loss1: 411.23, loss2: 51.30 and loss3: 4809.65\n",
      "Epoch [1012], train_loss: 5270.06 with loss1: 409.34, loss2: 51.11 and loss3: 4809.60\n",
      "Epoch [1013], train_loss: 5271.60 with loss1: 410.80, loss2: 51.25 and loss3: 4809.55\n",
      "Epoch [1014], train_loss: 5269.76 with loss1: 409.05, loss2: 51.21 and loss3: 4809.51\n",
      "Epoch [1015], train_loss: 5271.43 with loss1: 410.40, loss2: 51.57 and loss3: 4809.46\n",
      "Epoch [1016], train_loss: 5268.63 with loss1: 408.20, loss2: 51.03 and loss3: 4809.41\n",
      "Epoch [1017], train_loss: 5270.65 with loss1: 410.00, loss2: 51.30 and loss3: 4809.36\n",
      "Epoch [1018], train_loss: 5268.59 with loss1: 408.32, loss2: 50.97 and loss3: 4809.31\n",
      "Epoch [1019], train_loss: 5268.87 with loss1: 408.38, loss2: 51.23 and loss3: 4809.26\n",
      "Epoch [1020], train_loss: 5267.89 with loss1: 407.87, loss2: 50.81 and loss3: 4809.21\n",
      "Epoch [1021], train_loss: 5269.85 with loss1: 409.44, loss2: 51.26 and loss3: 4809.16\n",
      "Epoch [1022], train_loss: 5266.55 with loss1: 406.03, loss2: 51.41 and loss3: 4809.11\n",
      "Epoch [1023], train_loss: 5268.80 with loss1: 408.31, loss2: 51.42 and loss3: 4809.06\n",
      "Epoch [1024], train_loss: 5266.45 with loss1: 406.64, loss2: 50.81 and loss3: 4809.01\n",
      "Epoch [1025], train_loss: 5267.53 with loss1: 407.40, loss2: 51.17 and loss3: 4808.96\n",
      "Epoch [1026], train_loss: 5265.74 with loss1: 405.75, loss2: 51.08 and loss3: 4808.91\n",
      "Epoch [1027], train_loss: 5266.71 with loss1: 406.67, loss2: 51.17 and loss3: 4808.86\n",
      "Epoch [1028], train_loss: 5265.65 with loss1: 405.80, loss2: 51.03 and loss3: 4808.81\n",
      "Epoch [1029], train_loss: 5266.65 with loss1: 406.74, loss2: 51.15 and loss3: 4808.77\n",
      "Epoch [1030], train_loss: 5264.76 with loss1: 405.16, loss2: 50.89 and loss3: 4808.72\n",
      "Epoch [1031], train_loss: 5265.25 with loss1: 405.45, loss2: 51.14 and loss3: 4808.67\n",
      "Epoch [1032], train_loss: 5264.36 with loss1: 404.82, loss2: 50.92 and loss3: 4808.62\n",
      "Epoch [1033], train_loss: 5266.06 with loss1: 406.36, loss2: 51.13 and loss3: 4808.57\n",
      "Epoch [1034], train_loss: 5264.08 with loss1: 404.65, loss2: 50.91 and loss3: 4808.52\n",
      "Epoch [1035], train_loss: 5266.06 with loss1: 406.29, loss2: 51.30 and loss3: 4808.47\n",
      "Epoch [1036], train_loss: 5263.93 with loss1: 404.62, loss2: 50.89 and loss3: 4808.42\n",
      "Epoch [1037], train_loss: 5265.56 with loss1: 406.15, loss2: 51.03 and loss3: 4808.37\n",
      "Epoch [1038], train_loss: 5263.39 with loss1: 404.37, loss2: 50.70 and loss3: 4808.32\n",
      "Epoch [1039], train_loss: 5264.85 with loss1: 405.54, loss2: 51.04 and loss3: 4808.27\n",
      "Epoch [1040], train_loss: 5263.65 with loss1: 404.55, loss2: 50.88 and loss3: 4808.22\n",
      "Epoch [1041], train_loss: 5265.63 with loss1: 406.55, loss2: 50.91 and loss3: 4808.17\n",
      "Epoch [1042], train_loss: 5262.32 with loss1: 403.40, loss2: 50.80 and loss3: 4808.12\n",
      "Epoch [1043], train_loss: 5265.57 with loss1: 406.66, loss2: 50.83 and loss3: 4808.07\n",
      "Epoch [1044], train_loss: 5262.35 with loss1: 403.40, loss2: 50.92 and loss3: 4808.02\n",
      "Epoch [1045], train_loss: 5264.50 with loss1: 405.49, loss2: 51.04 and loss3: 4807.98\n",
      "Epoch [1046], train_loss: 5262.46 with loss1: 403.84, loss2: 50.69 and loss3: 4807.93\n",
      "Epoch [1047], train_loss: 5266.60 with loss1: 407.86, loss2: 50.87 and loss3: 4807.88\n",
      "Epoch [1048], train_loss: 5264.26 with loss1: 405.64, loss2: 50.80 and loss3: 4807.83\n",
      "Epoch [1049], train_loss: 5265.45 with loss1: 406.75, loss2: 50.93 and loss3: 4807.78\n",
      "Epoch [1050], train_loss: 5263.35 with loss1: 404.98, loss2: 50.63 and loss3: 4807.73\n",
      "Epoch [1051], train_loss: 5265.75 with loss1: 407.24, loss2: 50.83 and loss3: 4807.68\n",
      "Epoch [1052], train_loss: 5263.59 with loss1: 405.11, loss2: 50.85 and loss3: 4807.63\n",
      "Epoch [1053], train_loss: 5267.62 with loss1: 409.30, loss2: 50.74 and loss3: 4807.58\n",
      "Epoch [1054], train_loss: 5263.85 with loss1: 405.13, loss2: 51.20 and loss3: 4807.53\n",
      "Epoch [1055], train_loss: 5266.82 with loss1: 408.63, loss2: 50.71 and loss3: 4807.48\n",
      "Epoch [1056], train_loss: 5264.61 with loss1: 406.56, loss2: 50.62 and loss3: 4807.43\n",
      "Epoch [1057], train_loss: 5267.43 with loss1: 409.42, loss2: 50.63 and loss3: 4807.38\n",
      "Epoch [1058], train_loss: 5265.35 with loss1: 407.51, loss2: 50.50 and loss3: 4807.33\n",
      "Epoch [1059], train_loss: 5268.60 with loss1: 410.51, loss2: 50.80 and loss3: 4807.29\n",
      "Epoch [1060], train_loss: 5265.68 with loss1: 407.64, loss2: 50.80 and loss3: 4807.24\n",
      "Epoch [1061], train_loss: 5266.57 with loss1: 408.88, loss2: 50.50 and loss3: 4807.19\n",
      "Epoch [1062], train_loss: 5264.62 with loss1: 406.99, loss2: 50.49 and loss3: 4807.14\n",
      "Epoch [1063], train_loss: 5267.48 with loss1: 409.72, loss2: 50.67 and loss3: 4807.09\n",
      "Epoch [1064], train_loss: 5264.50 with loss1: 407.05, loss2: 50.41 and loss3: 4807.04\n",
      "Epoch [1065], train_loss: 5267.02 with loss1: 409.48, loss2: 50.55 and loss3: 4806.99\n",
      "Epoch [1066], train_loss: 5264.14 with loss1: 406.61, loss2: 50.59 and loss3: 4806.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1067], train_loss: 5266.33 with loss1: 408.99, loss2: 50.45 and loss3: 4806.89\n",
      "Epoch [1068], train_loss: 5263.45 with loss1: 406.13, loss2: 50.48 and loss3: 4806.84\n",
      "Epoch [1069], train_loss: 5265.50 with loss1: 408.15, loss2: 50.55 and loss3: 4806.79\n",
      "Epoch [1070], train_loss: 5263.64 with loss1: 406.36, loss2: 50.54 and loss3: 4806.74\n",
      "Epoch [1071], train_loss: 5266.10 with loss1: 408.87, loss2: 50.54 and loss3: 4806.69\n",
      "Epoch [1072], train_loss: 5263.87 with loss1: 406.76, loss2: 50.47 and loss3: 4806.64\n",
      "Epoch [1073], train_loss: 5267.79 with loss1: 410.56, loss2: 50.64 and loss3: 4806.59\n",
      "Epoch [1074], train_loss: 5264.92 with loss1: 407.85, loss2: 50.53 and loss3: 4806.54\n",
      "Epoch [1075], train_loss: 5267.30 with loss1: 409.96, loss2: 50.85 and loss3: 4806.50\n",
      "Epoch [1076], train_loss: 5264.02 with loss1: 407.27, loss2: 50.31 and loss3: 4806.45\n",
      "Epoch [1077], train_loss: 5267.22 with loss1: 410.36, loss2: 50.47 and loss3: 4806.40\n",
      "Epoch [1078], train_loss: 5263.28 with loss1: 406.71, loss2: 50.22 and loss3: 4806.35\n",
      "Epoch [1079], train_loss: 5265.77 with loss1: 408.75, loss2: 50.73 and loss3: 4806.30\n",
      "Epoch [1080], train_loss: 5263.18 with loss1: 406.48, loss2: 50.46 and loss3: 4806.25\n",
      "Epoch [1081], train_loss: 5266.40 with loss1: 409.46, loss2: 50.74 and loss3: 4806.20\n",
      "Epoch [1082], train_loss: 5263.35 with loss1: 406.87, loss2: 50.33 and loss3: 4806.15\n",
      "Epoch [1083], train_loss: 5266.68 with loss1: 409.83, loss2: 50.74 and loss3: 4806.10\n",
      "Epoch [1084], train_loss: 5263.30 with loss1: 407.08, loss2: 50.17 and loss3: 4806.05\n",
      "Epoch [1085], train_loss: 5266.22 with loss1: 409.63, loss2: 50.59 and loss3: 4806.00\n",
      "Epoch [1086], train_loss: 5262.89 with loss1: 406.75, loss2: 50.19 and loss3: 4805.95\n",
      "Epoch [1087], train_loss: 5266.15 with loss1: 409.80, loss2: 50.45 and loss3: 4805.90\n",
      "Epoch [1088], train_loss: 5261.83 with loss1: 405.73, loss2: 50.25 and loss3: 4805.85\n",
      "Epoch [1089], train_loss: 5264.52 with loss1: 408.41, loss2: 50.31 and loss3: 4805.80\n",
      "Epoch [1090], train_loss: 5261.34 with loss1: 405.50, loss2: 50.09 and loss3: 4805.75\n",
      "Epoch [1091], train_loss: 5263.82 with loss1: 407.91, loss2: 50.21 and loss3: 4805.71\n",
      "Epoch [1092], train_loss: 5261.47 with loss1: 405.16, loss2: 50.65 and loss3: 4805.66\n",
      "Epoch [1093], train_loss: 5262.84 with loss1: 406.93, loss2: 50.30 and loss3: 4805.61\n",
      "Epoch [1094], train_loss: 5259.66 with loss1: 404.03, loss2: 50.07 and loss3: 4805.56\n",
      "Epoch [1095], train_loss: 5262.35 with loss1: 406.60, loss2: 50.23 and loss3: 4805.51\n",
      "Epoch [1096], train_loss: 5259.85 with loss1: 404.23, loss2: 50.16 and loss3: 4805.46\n",
      "Epoch [1097], train_loss: 5261.42 with loss1: 405.88, loss2: 50.13 and loss3: 4805.41\n",
      "Epoch [1098], train_loss: 5258.18 with loss1: 402.66, loss2: 50.16 and loss3: 4805.36\n",
      "Epoch [1099], train_loss: 5259.54 with loss1: 403.87, loss2: 50.36 and loss3: 4805.31\n",
      "Epoch [1100], train_loss: 5256.84 with loss1: 401.42, loss2: 50.17 and loss3: 4805.26\n",
      "Epoch [1101], train_loss: 5257.65 with loss1: 402.20, loss2: 50.24 and loss3: 4805.21\n",
      "Epoch [1102], train_loss: 5256.12 with loss1: 400.67, loss2: 50.29 and loss3: 4805.16\n",
      "Epoch [1103], train_loss: 5257.42 with loss1: 401.95, loss2: 50.35 and loss3: 4805.11\n",
      "Epoch [1104], train_loss: 5254.43 with loss1: 398.93, loss2: 50.44 and loss3: 4805.06\n",
      "Epoch [1105], train_loss: 5256.18 with loss1: 400.45, loss2: 50.72 and loss3: 4805.01\n",
      "Epoch [1106], train_loss: 5252.77 with loss1: 397.86, loss2: 49.94 and loss3: 4804.97\n",
      "Epoch [1107], train_loss: 5254.80 with loss1: 399.70, loss2: 50.18 and loss3: 4804.92\n",
      "Epoch [1108], train_loss: 5252.91 with loss1: 397.86, loss2: 50.19 and loss3: 4804.87\n",
      "Epoch [1109], train_loss: 5253.86 with loss1: 399.06, loss2: 49.99 and loss3: 4804.82\n",
      "Epoch [1110], train_loss: 5252.17 with loss1: 397.20, loss2: 50.21 and loss3: 4804.77\n",
      "Epoch [1111], train_loss: 5253.58 with loss1: 398.59, loss2: 50.27 and loss3: 4804.72\n",
      "Epoch [1112], train_loss: 5251.81 with loss1: 397.09, loss2: 50.05 and loss3: 4804.67\n",
      "Epoch [1113], train_loss: 5251.96 with loss1: 397.26, loss2: 50.08 and loss3: 4804.62\n",
      "Epoch [1114], train_loss: 5249.67 with loss1: 395.24, loss2: 49.85 and loss3: 4804.57\n",
      "Epoch [1115], train_loss: 5252.03 with loss1: 397.34, loss2: 50.17 and loss3: 4804.52\n",
      "Epoch [1116], train_loss: 5249.57 with loss1: 395.06, loss2: 50.04 and loss3: 4804.47\n",
      "Epoch [1117], train_loss: 5250.74 with loss1: 396.18, loss2: 50.14 and loss3: 4804.42\n",
      "Epoch [1118], train_loss: 5249.36 with loss1: 395.15, loss2: 49.83 and loss3: 4804.37\n",
      "Epoch [1119], train_loss: 5251.10 with loss1: 396.79, loss2: 49.98 and loss3: 4804.32\n",
      "Epoch [1120], train_loss: 5249.16 with loss1: 395.08, loss2: 49.80 and loss3: 4804.27\n",
      "Epoch [1121], train_loss: 5250.78 with loss1: 396.72, loss2: 49.83 and loss3: 4804.23\n",
      "Epoch [1122], train_loss: 5248.36 with loss1: 394.25, loss2: 49.94 and loss3: 4804.18\n",
      "Epoch [1123], train_loss: 5249.98 with loss1: 395.75, loss2: 50.11 and loss3: 4804.13\n",
      "Epoch [1124], train_loss: 5249.31 with loss1: 395.29, loss2: 49.94 and loss3: 4804.08\n",
      "Epoch [1125], train_loss: 5249.03 with loss1: 395.13, loss2: 49.87 and loss3: 4804.03\n",
      "Epoch [1126], train_loss: 5248.46 with loss1: 394.65, loss2: 49.84 and loss3: 4803.98\n",
      "Epoch [1127], train_loss: 5250.37 with loss1: 396.49, loss2: 49.95 and loss3: 4803.93\n",
      "Epoch [1128], train_loss: 5248.81 with loss1: 395.03, loss2: 49.89 and loss3: 4803.88\n",
      "Epoch [1129], train_loss: 5250.58 with loss1: 396.44, loss2: 50.32 and loss3: 4803.83\n",
      "Epoch [1130], train_loss: 5247.47 with loss1: 393.97, loss2: 49.71 and loss3: 4803.78\n",
      "Epoch [1131], train_loss: 5250.27 with loss1: 396.74, loss2: 49.81 and loss3: 4803.73\n",
      "Epoch [1132], train_loss: 5248.58 with loss1: 394.94, loss2: 49.96 and loss3: 4803.68\n",
      "Epoch [1133], train_loss: 5251.16 with loss1: 397.72, loss2: 49.80 and loss3: 4803.63\n",
      "Epoch [1134], train_loss: 5250.57 with loss1: 397.01, loss2: 49.98 and loss3: 4803.58\n",
      "Epoch [1135], train_loss: 5252.59 with loss1: 399.06, loss2: 50.00 and loss3: 4803.53\n",
      "Epoch [1136], train_loss: 5249.57 with loss1: 396.33, loss2: 49.75 and loss3: 4803.49\n",
      "Epoch [1137], train_loss: 5251.85 with loss1: 398.35, loss2: 50.06 and loss3: 4803.44\n",
      "Epoch [1138], train_loss: 5249.28 with loss1: 396.22, loss2: 49.67 and loss3: 4803.39\n",
      "Epoch [1139], train_loss: 5252.52 with loss1: 399.31, loss2: 49.87 and loss3: 4803.34\n",
      "Epoch [1140], train_loss: 5250.23 with loss1: 397.18, loss2: 49.77 and loss3: 4803.29\n",
      "Epoch [1141], train_loss: 5251.72 with loss1: 398.67, loss2: 49.81 and loss3: 4803.24\n",
      "Epoch [1142], train_loss: 5249.82 with loss1: 397.12, loss2: 49.51 and loss3: 4803.19\n",
      "Epoch [1143], train_loss: 5252.12 with loss1: 399.07, loss2: 49.91 and loss3: 4803.14\n",
      "Epoch [1144], train_loss: 5250.13 with loss1: 397.42, loss2: 49.62 and loss3: 4803.09\n",
      "Epoch [1145], train_loss: 5253.06 with loss1: 400.35, loss2: 49.67 and loss3: 4803.04\n",
      "Epoch [1146], train_loss: 5251.12 with loss1: 398.56, loss2: 49.56 and loss3: 4802.99\n",
      "Epoch [1147], train_loss: 5253.05 with loss1: 400.32, loss2: 49.79 and loss3: 4802.94\n",
      "Epoch [1148], train_loss: 5252.03 with loss1: 399.45, loss2: 49.68 and loss3: 4802.89\n",
      "Epoch [1149], train_loss: 5254.04 with loss1: 401.51, loss2: 49.68 and loss3: 4802.84\n",
      "Epoch [1150], train_loss: 5253.10 with loss1: 400.73, loss2: 49.58 and loss3: 4802.79\n",
      "Epoch [1151], train_loss: 5254.60 with loss1: 402.16, loss2: 49.69 and loss3: 4802.75\n",
      "Epoch [1152], train_loss: 5253.17 with loss1: 401.03, loss2: 49.44 and loss3: 4802.70\n",
      "Epoch [1153], train_loss: 5255.93 with loss1: 403.55, loss2: 49.73 and loss3: 4802.65\n",
      "Epoch [1154], train_loss: 5254.16 with loss1: 401.91, loss2: 49.65 and loss3: 4802.60\n",
      "Epoch [1155], train_loss: 5257.18 with loss1: 404.91, loss2: 49.72 and loss3: 4802.55\n",
      "Epoch [1156], train_loss: 5253.88 with loss1: 401.97, loss2: 49.42 and loss3: 4802.50\n",
      "Epoch [1157], train_loss: 5256.13 with loss1: 404.19, loss2: 49.49 and loss3: 4802.45\n",
      "Epoch [1158], train_loss: 5253.52 with loss1: 401.72, loss2: 49.40 and loss3: 4802.40\n",
      "Epoch [1159], train_loss: 5256.02 with loss1: 404.00, loss2: 49.66 and loss3: 4802.35\n",
      "Epoch [1160], train_loss: 5254.88 with loss1: 403.10, loss2: 49.48 and loss3: 4802.30\n",
      "Epoch [1161], train_loss: 5256.68 with loss1: 404.72, loss2: 49.71 and loss3: 4802.25\n",
      "Epoch [1162], train_loss: 5255.35 with loss1: 403.52, loss2: 49.63 and loss3: 4802.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1163], train_loss: 5257.77 with loss1: 405.66, loss2: 49.95 and loss3: 4802.15\n",
      "Epoch [1164], train_loss: 5254.32 with loss1: 402.94, loss2: 49.28 and loss3: 4802.10\n",
      "Epoch [1165], train_loss: 5257.40 with loss1: 405.81, loss2: 49.53 and loss3: 4802.05\n",
      "Epoch [1166], train_loss: 5254.81 with loss1: 403.37, loss2: 49.43 and loss3: 4802.01\n",
      "Epoch [1167], train_loss: 5257.71 with loss1: 406.11, loss2: 49.64 and loss3: 4801.96\n",
      "Epoch [1168], train_loss: 5253.84 with loss1: 402.52, loss2: 49.41 and loss3: 4801.91\n",
      "Epoch [1169], train_loss: 5256.56 with loss1: 405.26, loss2: 49.45 and loss3: 4801.86\n",
      "Epoch [1170], train_loss: 5253.73 with loss1: 402.74, loss2: 49.18 and loss3: 4801.81\n",
      "Epoch [1171], train_loss: 5256.90 with loss1: 405.72, loss2: 49.43 and loss3: 4801.76\n",
      "Epoch [1172], train_loss: 5253.08 with loss1: 402.07, loss2: 49.30 and loss3: 4801.71\n",
      "Epoch [1173], train_loss: 5257.39 with loss1: 406.28, loss2: 49.45 and loss3: 4801.66\n",
      "Epoch [1174], train_loss: 5254.06 with loss1: 402.95, loss2: 49.51 and loss3: 4801.61\n",
      "Epoch [1175], train_loss: 5256.87 with loss1: 405.80, loss2: 49.51 and loss3: 4801.56\n",
      "Epoch [1176], train_loss: 5254.22 with loss1: 403.53, loss2: 49.18 and loss3: 4801.51\n",
      "Epoch [1177], train_loss: 5255.54 with loss1: 404.80, loss2: 49.27 and loss3: 4801.46\n",
      "Epoch [1178], train_loss: 5253.44 with loss1: 402.78, loss2: 49.25 and loss3: 4801.41\n",
      "Epoch [1179], train_loss: 5256.54 with loss1: 405.82, loss2: 49.36 and loss3: 4801.36\n",
      "Epoch [1180], train_loss: 5253.40 with loss1: 402.85, loss2: 49.24 and loss3: 4801.31\n",
      "Epoch [1181], train_loss: 5257.03 with loss1: 406.17, loss2: 49.59 and loss3: 4801.27\n",
      "Epoch [1182], train_loss: 5253.64 with loss1: 403.33, loss2: 49.09 and loss3: 4801.22\n",
      "Epoch [1183], train_loss: 5255.92 with loss1: 405.47, loss2: 49.28 and loss3: 4801.17\n",
      "Epoch [1184], train_loss: 5253.65 with loss1: 403.11, loss2: 49.42 and loss3: 4801.12\n",
      "Epoch [1185], train_loss: 5256.93 with loss1: 406.36, loss2: 49.50 and loss3: 4801.07\n",
      "Epoch [1186], train_loss: 5252.35 with loss1: 402.19, loss2: 49.14 and loss3: 4801.02\n",
      "Epoch [1187], train_loss: 5255.66 with loss1: 405.07, loss2: 49.62 and loss3: 4800.97\n",
      "Epoch [1188], train_loss: 5252.55 with loss1: 402.52, loss2: 49.11 and loss3: 4800.92\n",
      "Epoch [1189], train_loss: 5254.46 with loss1: 404.22, loss2: 49.36 and loss3: 4800.87\n",
      "Epoch [1190], train_loss: 5251.55 with loss1: 401.67, loss2: 49.06 and loss3: 4800.82\n",
      "Epoch [1191], train_loss: 5255.68 with loss1: 405.56, loss2: 49.36 and loss3: 4800.77\n",
      "Epoch [1192], train_loss: 5251.78 with loss1: 402.00, loss2: 49.06 and loss3: 4800.72\n",
      "Epoch [1193], train_loss: 5253.86 with loss1: 403.91, loss2: 49.28 and loss3: 4800.67\n",
      "Epoch [1194], train_loss: 5250.94 with loss1: 401.13, loss2: 49.19 and loss3: 4800.62\n",
      "Epoch [1195], train_loss: 5252.93 with loss1: 403.11, loss2: 49.24 and loss3: 4800.57\n",
      "Epoch [1196], train_loss: 5251.35 with loss1: 401.52, loss2: 49.31 and loss3: 4800.53\n",
      "Epoch [1197], train_loss: 5252.46 with loss1: 402.66, loss2: 49.33 and loss3: 4800.48\n",
      "Epoch [1198], train_loss: 5250.56 with loss1: 400.86, loss2: 49.27 and loss3: 4800.43\n",
      "Epoch [1199], train_loss: 5251.82 with loss1: 402.25, loss2: 49.19 and loss3: 4800.38\n",
      "Epoch [1200], train_loss: 5249.70 with loss1: 400.38, loss2: 48.99 and loss3: 4800.33\n",
      "Epoch [1201], train_loss: 5252.46 with loss1: 402.88, loss2: 49.30 and loss3: 4800.28\n",
      "Epoch [1202], train_loss: 5248.96 with loss1: 399.71, loss2: 49.02 and loss3: 4800.23\n",
      "Epoch [1203], train_loss: 5251.03 with loss1: 401.54, loss2: 49.31 and loss3: 4800.18\n",
      "Epoch [1204], train_loss: 5249.50 with loss1: 400.39, loss2: 48.98 and loss3: 4800.13\n",
      "Epoch [1205], train_loss: 5250.12 with loss1: 400.74, loss2: 49.30 and loss3: 4800.08\n",
      "Epoch [1206], train_loss: 5249.45 with loss1: 400.37, loss2: 49.05 and loss3: 4800.03\n",
      "Epoch [1207], train_loss: 5251.69 with loss1: 402.48, loss2: 49.23 and loss3: 4799.98\n",
      "Epoch [1208], train_loss: 5249.45 with loss1: 400.64, loss2: 48.88 and loss3: 4799.93\n",
      "Epoch [1209], train_loss: 5251.63 with loss1: 402.62, loss2: 49.13 and loss3: 4799.88\n",
      "Epoch [1210], train_loss: 5247.70 with loss1: 399.05, loss2: 48.81 and loss3: 4799.83\n",
      "Epoch [1211], train_loss: 5250.74 with loss1: 401.82, loss2: 49.14 and loss3: 4799.79\n",
      "Epoch [1212], train_loss: 5248.38 with loss1: 399.86, loss2: 48.78 and loss3: 4799.74\n",
      "Epoch [1213], train_loss: 5251.22 with loss1: 402.23, loss2: 49.30 and loss3: 4799.69\n",
      "Epoch [1214], train_loss: 5247.55 with loss1: 399.10, loss2: 48.82 and loss3: 4799.64\n",
      "Epoch [1215], train_loss: 5250.25 with loss1: 401.64, loss2: 49.02 and loss3: 4799.59\n",
      "Epoch [1216], train_loss: 5246.82 with loss1: 398.32, loss2: 48.96 and loss3: 4799.54\n",
      "Epoch [1217], train_loss: 5250.42 with loss1: 402.01, loss2: 48.92 and loss3: 4799.49\n",
      "Epoch [1218], train_loss: 5248.04 with loss1: 399.71, loss2: 48.89 and loss3: 4799.44\n",
      "Epoch [1219], train_loss: 5249.84 with loss1: 401.33, loss2: 49.12 and loss3: 4799.39\n",
      "Epoch [1220], train_loss: 5247.68 with loss1: 399.47, loss2: 48.86 and loss3: 4799.34\n",
      "Epoch [1221], train_loss: 5250.18 with loss1: 401.84, loss2: 49.04 and loss3: 4799.29\n",
      "Epoch [1222], train_loss: 5249.67 with loss1: 401.68, loss2: 48.74 and loss3: 4799.24\n",
      "Epoch [1223], train_loss: 5250.94 with loss1: 402.64, loss2: 49.11 and loss3: 4799.19\n",
      "Epoch [1224], train_loss: 5249.26 with loss1: 401.25, loss2: 48.87 and loss3: 4799.14\n",
      "Epoch [1225], train_loss: 5251.44 with loss1: 403.52, loss2: 48.82 and loss3: 4799.09\n",
      "Epoch [1226], train_loss: 5249.30 with loss1: 401.40, loss2: 48.85 and loss3: 4799.05\n",
      "Epoch [1227], train_loss: 5251.18 with loss1: 403.35, loss2: 48.84 and loss3: 4799.00\n",
      "Epoch [1228], train_loss: 5247.98 with loss1: 400.30, loss2: 48.74 and loss3: 4798.95\n",
      "Epoch [1229], train_loss: 5251.07 with loss1: 403.11, loss2: 49.06 and loss3: 4798.90\n",
      "Epoch [1230], train_loss: 5248.35 with loss1: 400.98, loss2: 48.53 and loss3: 4798.85\n",
      "Epoch [1231], train_loss: 5250.26 with loss1: 402.53, loss2: 48.93 and loss3: 4798.80\n",
      "Epoch [1232], train_loss: 5248.28 with loss1: 400.96, loss2: 48.57 and loss3: 4798.75\n",
      "Epoch [1233], train_loss: 5251.44 with loss1: 403.92, loss2: 48.82 and loss3: 4798.70\n",
      "Epoch [1234], train_loss: 5247.90 with loss1: 400.63, loss2: 48.62 and loss3: 4798.65\n",
      "Epoch [1235], train_loss: 5250.95 with loss1: 403.22, loss2: 49.14 and loss3: 4798.60\n",
      "Epoch [1236], train_loss: 5247.66 with loss1: 400.51, loss2: 48.60 and loss3: 4798.55\n",
      "Epoch [1237], train_loss: 5250.07 with loss1: 402.73, loss2: 48.83 and loss3: 4798.50\n",
      "Epoch [1238], train_loss: 5248.12 with loss1: 401.15, loss2: 48.52 and loss3: 4798.45\n",
      "Epoch [1239], train_loss: 5249.84 with loss1: 402.61, loss2: 48.82 and loss3: 4798.40\n",
      "Epoch [1240], train_loss: 5247.49 with loss1: 400.70, loss2: 48.44 and loss3: 4798.36\n",
      "Epoch [1241], train_loss: 5250.15 with loss1: 403.00, loss2: 48.84 and loss3: 4798.31\n",
      "Epoch [1242], train_loss: 5245.43 with loss1: 398.74, loss2: 48.43 and loss3: 4798.26\n",
      "Epoch [1243], train_loss: 5248.54 with loss1: 401.67, loss2: 48.66 and loss3: 4798.21\n",
      "Epoch [1244], train_loss: 5246.62 with loss1: 399.61, loss2: 48.85 and loss3: 4798.16\n",
      "Epoch [1245], train_loss: 5247.60 with loss1: 400.82, loss2: 48.68 and loss3: 4798.11\n",
      "Epoch [1246], train_loss: 5245.12 with loss1: 398.65, loss2: 48.41 and loss3: 4798.06\n",
      "Epoch [1247], train_loss: 5246.38 with loss1: 399.59, loss2: 48.78 and loss3: 4798.01\n",
      "Epoch [1248], train_loss: 5244.27 with loss1: 397.75, loss2: 48.55 and loss3: 4797.96\n",
      "Epoch [1249], train_loss: 5245.21 with loss1: 398.68, loss2: 48.62 and loss3: 4797.91\n",
      "Epoch [1250], train_loss: 5243.93 with loss1: 397.35, loss2: 48.71 and loss3: 4797.86\n",
      "Epoch [1251], train_loss: 5245.34 with loss1: 398.77, loss2: 48.75 and loss3: 4797.81\n",
      "Epoch [1252], train_loss: 5242.70 with loss1: 396.60, loss2: 48.34 and loss3: 4797.76\n",
      "Epoch [1253], train_loss: 5244.16 with loss1: 397.84, loss2: 48.60 and loss3: 4797.71\n",
      "Epoch [1254], train_loss: 5241.93 with loss1: 395.92, loss2: 48.34 and loss3: 4797.67\n",
      "Epoch [1255], train_loss: 5244.57 with loss1: 398.16, loss2: 48.79 and loss3: 4797.62\n",
      "Epoch [1256], train_loss: 5241.62 with loss1: 395.56, loss2: 48.49 and loss3: 4797.57\n",
      "Epoch [1257], train_loss: 5242.80 with loss1: 396.72, loss2: 48.57 and loss3: 4797.52\n",
      "Epoch [1258], train_loss: 5239.32 with loss1: 393.72, loss2: 48.13 and loss3: 4797.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1259], train_loss: 5241.57 with loss1: 395.62, loss2: 48.53 and loss3: 4797.42\n",
      "Epoch [1260], train_loss: 5239.31 with loss1: 393.52, loss2: 48.41 and loss3: 4797.37\n",
      "Epoch [1261], train_loss: 5240.98 with loss1: 395.29, loss2: 48.38 and loss3: 4797.32\n",
      "Epoch [1262], train_loss: 5237.62 with loss1: 392.04, loss2: 48.30 and loss3: 4797.27\n",
      "Epoch [1263], train_loss: 5239.13 with loss1: 393.30, loss2: 48.61 and loss3: 4797.22\n",
      "Epoch [1264], train_loss: 5237.51 with loss1: 392.07, loss2: 48.26 and loss3: 4797.17\n",
      "Epoch [1265], train_loss: 5238.72 with loss1: 392.99, loss2: 48.61 and loss3: 4797.12\n",
      "Epoch [1266], train_loss: 5236.88 with loss1: 391.23, loss2: 48.58 and loss3: 4797.07\n",
      "Epoch [1267], train_loss: 5238.93 with loss1: 393.44, loss2: 48.46 and loss3: 4797.02\n",
      "Epoch [1268], train_loss: 5235.22 with loss1: 389.98, loss2: 48.27 and loss3: 4796.98\n",
      "Epoch [1269], train_loss: 5237.46 with loss1: 392.17, loss2: 48.36 and loss3: 4796.93\n",
      "Epoch [1270], train_loss: 5235.93 with loss1: 390.85, loss2: 48.20 and loss3: 4796.88\n",
      "Epoch [1271], train_loss: 5236.40 with loss1: 391.15, loss2: 48.42 and loss3: 4796.83\n",
      "Epoch [1272], train_loss: 5234.57 with loss1: 389.67, loss2: 48.12 and loss3: 4796.78\n",
      "Epoch [1273], train_loss: 5236.47 with loss1: 391.44, loss2: 48.30 and loss3: 4796.73\n",
      "Epoch [1274], train_loss: 5235.05 with loss1: 389.93, loss2: 48.44 and loss3: 4796.68\n",
      "Epoch [1275], train_loss: 5236.38 with loss1: 391.45, loss2: 48.31 and loss3: 4796.63\n",
      "Epoch [1276], train_loss: 5233.57 with loss1: 388.86, loss2: 48.13 and loss3: 4796.58\n",
      "Epoch [1277], train_loss: 5237.34 with loss1: 392.40, loss2: 48.41 and loss3: 4796.53\n",
      "Epoch [1278], train_loss: 5233.70 with loss1: 388.99, loss2: 48.23 and loss3: 4796.48\n",
      "Epoch [1279], train_loss: 5238.02 with loss1: 392.81, loss2: 48.78 and loss3: 4796.43\n",
      "Epoch [1280], train_loss: 5235.53 with loss1: 390.51, loss2: 48.64 and loss3: 4796.38\n",
      "Epoch [1281], train_loss: 5238.27 with loss1: 393.54, loss2: 48.39 and loss3: 4796.33\n",
      "Epoch [1282], train_loss: 5234.57 with loss1: 390.26, loss2: 48.02 and loss3: 4796.29\n",
      "Epoch [1283], train_loss: 5237.92 with loss1: 393.44, loss2: 48.24 and loss3: 4796.24\n",
      "Epoch [1284], train_loss: 5236.55 with loss1: 392.09, loss2: 48.28 and loss3: 4796.19\n",
      "Epoch [1285], train_loss: 5239.10 with loss1: 394.73, loss2: 48.23 and loss3: 4796.14\n",
      "Epoch [1286], train_loss: 5237.18 with loss1: 392.85, loss2: 48.24 and loss3: 4796.09\n",
      "Epoch [1287], train_loss: 5239.47 with loss1: 395.31, loss2: 48.12 and loss3: 4796.04\n",
      "Epoch [1288], train_loss: 5237.55 with loss1: 393.36, loss2: 48.20 and loss3: 4795.99\n",
      "Epoch [1289], train_loss: 5240.00 with loss1: 395.92, loss2: 48.14 and loss3: 4795.94\n",
      "Epoch [1290], train_loss: 5237.46 with loss1: 393.69, loss2: 47.89 and loss3: 4795.89\n",
      "Epoch [1291], train_loss: 5240.65 with loss1: 396.57, loss2: 48.23 and loss3: 4795.84\n",
      "Epoch [1292], train_loss: 5238.47 with loss1: 394.72, loss2: 47.96 and loss3: 4795.79\n",
      "Epoch [1293], train_loss: 5241.02 with loss1: 397.06, loss2: 48.22 and loss3: 4795.74\n",
      "Epoch [1294], train_loss: 5238.73 with loss1: 395.06, loss2: 47.97 and loss3: 4795.69\n",
      "Epoch [1295], train_loss: 5241.70 with loss1: 398.02, loss2: 48.04 and loss3: 4795.64\n",
      "Epoch [1296], train_loss: 5239.21 with loss1: 395.60, loss2: 48.02 and loss3: 4795.59\n",
      "Epoch [1297], train_loss: 5241.99 with loss1: 398.34, loss2: 48.11 and loss3: 4795.55\n",
      "Epoch [1298], train_loss: 5240.01 with loss1: 396.48, loss2: 48.04 and loss3: 4795.50\n",
      "Epoch [1299], train_loss: 5243.28 with loss1: 399.46, loss2: 48.37 and loss3: 4795.45\n",
      "Epoch [1300], train_loss: 5240.01 with loss1: 396.57, loss2: 48.04 and loss3: 4795.40\n",
      "Epoch [1301], train_loss: 5243.28 with loss1: 399.74, loss2: 48.19 and loss3: 4795.35\n",
      "Epoch [1302], train_loss: 5240.52 with loss1: 396.99, loss2: 48.23 and loss3: 4795.30\n",
      "Epoch [1303], train_loss: 5243.43 with loss1: 399.99, loss2: 48.19 and loss3: 4795.25\n",
      "Epoch [1304], train_loss: 5239.81 with loss1: 396.71, loss2: 47.90 and loss3: 4795.20\n",
      "Epoch [1305], train_loss: 5242.81 with loss1: 399.41, loss2: 48.24 and loss3: 4795.15\n",
      "Epoch [1306], train_loss: 5240.81 with loss1: 397.76, loss2: 47.95 and loss3: 4795.10\n",
      "Epoch [1307], train_loss: 5243.03 with loss1: 400.01, loss2: 47.97 and loss3: 4795.05\n",
      "Epoch [1308], train_loss: 5239.24 with loss1: 396.53, loss2: 47.71 and loss3: 4795.00\n",
      "Epoch [1309], train_loss: 5241.71 with loss1: 398.85, loss2: 47.90 and loss3: 4794.95\n",
      "Epoch [1310], train_loss: 5238.58 with loss1: 395.96, loss2: 47.72 and loss3: 4794.90\n",
      "Epoch [1311], train_loss: 5241.34 with loss1: 398.60, loss2: 47.89 and loss3: 4794.86\n",
      "Epoch [1312], train_loss: 5238.14 with loss1: 395.24, loss2: 48.10 and loss3: 4794.81\n",
      "Epoch [1313], train_loss: 5240.57 with loss1: 397.84, loss2: 47.98 and loss3: 4794.76\n",
      "Epoch [1314], train_loss: 5235.88 with loss1: 393.45, loss2: 47.73 and loss3: 4794.71\n",
      "Epoch [1315], train_loss: 5238.12 with loss1: 395.51, loss2: 47.96 and loss3: 4794.66\n",
      "Epoch [1316], train_loss: 5235.70 with loss1: 393.04, loss2: 48.05 and loss3: 4794.61\n",
      "Epoch [1317], train_loss: 5237.23 with loss1: 394.68, loss2: 48.00 and loss3: 4794.56\n",
      "Epoch [1318], train_loss: 5234.34 with loss1: 392.01, loss2: 47.83 and loss3: 4794.51\n",
      "Epoch [1319], train_loss: 5236.68 with loss1: 394.26, loss2: 47.96 and loss3: 4794.46\n",
      "Epoch [1320], train_loss: 5233.52 with loss1: 391.41, loss2: 47.70 and loss3: 4794.41\n",
      "Epoch [1321], train_loss: 5235.47 with loss1: 393.13, loss2: 47.97 and loss3: 4794.36\n",
      "Epoch [1322], train_loss: 5234.55 with loss1: 392.26, loss2: 47.98 and loss3: 4794.31\n",
      "Epoch [1323], train_loss: 5234.47 with loss1: 392.39, loss2: 47.82 and loss3: 4794.26\n",
      "Epoch [1324], train_loss: 5232.42 with loss1: 390.46, loss2: 47.75 and loss3: 4794.21\n",
      "Epoch [1325], train_loss: 5233.66 with loss1: 391.93, loss2: 47.56 and loss3: 4794.17\n",
      "Epoch [1326], train_loss: 5231.72 with loss1: 389.93, loss2: 47.67 and loss3: 4794.12\n",
      "Epoch [1327], train_loss: 5231.58 with loss1: 389.68, loss2: 47.83 and loss3: 4794.07\n",
      "Epoch [1328], train_loss: 5229.58 with loss1: 388.00, loss2: 47.57 and loss3: 4794.02\n",
      "Epoch [1329], train_loss: 5231.37 with loss1: 389.61, loss2: 47.79 and loss3: 4793.97\n",
      "Epoch [1330], train_loss: 5228.93 with loss1: 387.33, loss2: 47.68 and loss3: 4793.92\n",
      "Epoch [1331], train_loss: 5231.21 with loss1: 389.62, loss2: 47.71 and loss3: 4793.87\n",
      "Epoch [1332], train_loss: 5229.02 with loss1: 387.67, loss2: 47.54 and loss3: 4793.82\n",
      "Epoch [1333], train_loss: 5230.40 with loss1: 388.80, loss2: 47.83 and loss3: 4793.77\n",
      "Epoch [1334], train_loss: 5226.80 with loss1: 385.44, loss2: 47.64 and loss3: 4793.72\n",
      "Epoch [1335], train_loss: 5227.68 with loss1: 386.34, loss2: 47.67 and loss3: 4793.67\n",
      "Epoch [1336], train_loss: 5226.10 with loss1: 384.82, loss2: 47.66 and loss3: 4793.62\n",
      "Epoch [1337], train_loss: 5228.63 with loss1: 387.28, loss2: 47.77 and loss3: 4793.57\n",
      "Epoch [1338], train_loss: 5226.91 with loss1: 385.40, loss2: 47.98 and loss3: 4793.52\n",
      "Epoch [1339], train_loss: 5228.38 with loss1: 387.20, loss2: 47.71 and loss3: 4793.48\n",
      "Epoch [1340], train_loss: 5226.48 with loss1: 385.46, loss2: 47.59 and loss3: 4793.43\n",
      "Epoch [1341], train_loss: 5228.20 with loss1: 387.29, loss2: 47.53 and loss3: 4793.38\n",
      "Epoch [1342], train_loss: 5226.61 with loss1: 385.80, loss2: 47.48 and loss3: 4793.33\n",
      "Epoch [1343], train_loss: 5227.22 with loss1: 386.37, loss2: 47.57 and loss3: 4793.28\n",
      "Epoch [1344], train_loss: 5226.42 with loss1: 385.45, loss2: 47.74 and loss3: 4793.23\n",
      "Epoch [1345], train_loss: 5227.34 with loss1: 386.61, loss2: 47.55 and loss3: 4793.18\n",
      "Epoch [1346], train_loss: 5225.94 with loss1: 385.32, loss2: 47.49 and loss3: 4793.13\n",
      "Epoch [1347], train_loss: 5228.88 with loss1: 388.33, loss2: 47.47 and loss3: 4793.08\n",
      "Epoch [1348], train_loss: 5226.82 with loss1: 386.31, loss2: 47.47 and loss3: 4793.03\n",
      "Epoch [1349], train_loss: 5228.70 with loss1: 388.19, loss2: 47.53 and loss3: 4792.98\n",
      "Epoch [1350], train_loss: 5228.37 with loss1: 387.87, loss2: 47.57 and loss3: 4792.93\n",
      "Epoch [1351], train_loss: 5229.94 with loss1: 389.54, loss2: 47.52 and loss3: 4792.88\n",
      "Epoch [1352], train_loss: 5228.36 with loss1: 388.03, loss2: 47.49 and loss3: 4792.83\n",
      "Epoch [1353], train_loss: 5231.13 with loss1: 390.75, loss2: 47.60 and loss3: 4792.79\n",
      "Epoch [1354], train_loss: 5230.01 with loss1: 389.45, loss2: 47.83 and loss3: 4792.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1355], train_loss: 5232.47 with loss1: 392.37, loss2: 47.41 and loss3: 4792.69\n",
      "Epoch [1356], train_loss: 5231.20 with loss1: 391.32, loss2: 47.25 and loss3: 4792.64\n",
      "Epoch [1357], train_loss: 5235.10 with loss1: 394.77, loss2: 47.75 and loss3: 4792.59\n",
      "Epoch [1358], train_loss: 5232.66 with loss1: 392.61, loss2: 47.51 and loss3: 4792.54\n",
      "Epoch [1359], train_loss: 5236.50 with loss1: 396.17, loss2: 47.84 and loss3: 4792.49\n",
      "Epoch [1360], train_loss: 5233.54 with loss1: 393.84, loss2: 47.26 and loss3: 4792.44\n",
      "Epoch [1361], train_loss: 5237.74 with loss1: 397.84, loss2: 47.50 and loss3: 4792.39\n",
      "Epoch [1362], train_loss: 5235.34 with loss1: 395.65, loss2: 47.35 and loss3: 4792.34\n",
      "Epoch [1363], train_loss: 5238.43 with loss1: 398.82, loss2: 47.32 and loss3: 4792.29\n",
      "Epoch [1364], train_loss: 5236.33 with loss1: 396.77, loss2: 47.32 and loss3: 4792.24\n",
      "Epoch [1365], train_loss: 5240.15 with loss1: 400.54, loss2: 47.41 and loss3: 4792.19\n",
      "Epoch [1366], train_loss: 5238.52 with loss1: 398.81, loss2: 47.56 and loss3: 4792.15\n",
      "Epoch [1367], train_loss: 5241.22 with loss1: 401.68, loss2: 47.44 and loss3: 4792.10\n",
      "Epoch [1368], train_loss: 5240.47 with loss1: 401.00, loss2: 47.42 and loss3: 4792.05\n",
      "Epoch [1369], train_loss: 5243.31 with loss1: 403.98, loss2: 47.34 and loss3: 4792.00\n",
      "Epoch [1370], train_loss: 5240.82 with loss1: 401.53, loss2: 47.35 and loss3: 4791.95\n",
      "Epoch [1371], train_loss: 5245.29 with loss1: 406.15, loss2: 47.24 and loss3: 4791.90\n",
      "Epoch [1372], train_loss: 5242.84 with loss1: 403.58, loss2: 47.41 and loss3: 4791.85\n",
      "Epoch [1373], train_loss: 5247.45 with loss1: 408.26, loss2: 47.39 and loss3: 4791.80\n",
      "Epoch [1374], train_loss: 5244.69 with loss1: 405.67, loss2: 47.28 and loss3: 4791.75\n",
      "Epoch [1375], train_loss: 5247.89 with loss1: 408.79, loss2: 47.40 and loss3: 4791.70\n",
      "Epoch [1376], train_loss: 5242.90 with loss1: 404.13, loss2: 47.12 and loss3: 4791.65\n",
      "Epoch [1377], train_loss: 5245.86 with loss1: 406.91, loss2: 47.35 and loss3: 4791.60\n",
      "Epoch [1378], train_loss: 5243.23 with loss1: 404.41, loss2: 47.26 and loss3: 4791.55\n",
      "Epoch [1379], train_loss: 5245.67 with loss1: 406.64, loss2: 47.53 and loss3: 4791.50\n",
      "Epoch [1380], train_loss: 5241.52 with loss1: 403.03, loss2: 47.03 and loss3: 4791.46\n",
      "Epoch [1381], train_loss: 5245.38 with loss1: 406.75, loss2: 47.23 and loss3: 4791.41\n",
      "Epoch [1382], train_loss: 5240.11 with loss1: 401.57, loss2: 47.18 and loss3: 4791.36\n",
      "Epoch [1383], train_loss: 5242.50 with loss1: 403.78, loss2: 47.41 and loss3: 4791.31\n",
      "Epoch [1384], train_loss: 5239.43 with loss1: 400.34, loss2: 47.83 and loss3: 4791.26\n",
      "Epoch [1385], train_loss: 5241.14 with loss1: 402.59, loss2: 47.33 and loss3: 4791.21\n",
      "Epoch [1386], train_loss: 5236.44 with loss1: 398.04, loss2: 47.25 and loss3: 4791.16\n",
      "Epoch [1387], train_loss: 5237.88 with loss1: 399.27, loss2: 47.51 and loss3: 4791.11\n",
      "Epoch [1388], train_loss: 5234.20 with loss1: 396.20, loss2: 46.94 and loss3: 4791.06\n",
      "Epoch [1389], train_loss: 5234.78 with loss1: 396.48, loss2: 47.28 and loss3: 4791.01\n",
      "Epoch [1390], train_loss: 5230.51 with loss1: 392.32, loss2: 47.22 and loss3: 4790.96\n",
      "Epoch [1391], train_loss: 5231.01 with loss1: 392.88, loss2: 47.22 and loss3: 4790.91\n",
      "Epoch [1392], train_loss: 5227.37 with loss1: 389.56, loss2: 46.94 and loss3: 4790.86\n",
      "Epoch [1393], train_loss: 5228.12 with loss1: 390.18, loss2: 47.13 and loss3: 4790.81\n",
      "Epoch [1394], train_loss: 5225.32 with loss1: 387.50, loss2: 47.05 and loss3: 4790.77\n",
      "Epoch [1395], train_loss: 5225.75 with loss1: 387.89, loss2: 47.14 and loss3: 4790.72\n",
      "Epoch [1396], train_loss: 5221.80 with loss1: 384.24, loss2: 46.90 and loss3: 4790.67\n",
      "Epoch [1397], train_loss: 5223.11 with loss1: 385.17, loss2: 47.32 and loss3: 4790.62\n",
      "Epoch [1398], train_loss: 5220.48 with loss1: 382.80, loss2: 47.11 and loss3: 4790.57\n",
      "Epoch [1399], train_loss: 5222.32 with loss1: 384.66, loss2: 47.14 and loss3: 4790.52\n",
      "Epoch [1400], train_loss: 5218.66 with loss1: 381.35, loss2: 46.84 and loss3: 4790.47\n",
      "Epoch [1401], train_loss: 5220.73 with loss1: 383.06, loss2: 47.26 and loss3: 4790.42\n",
      "Epoch [1402], train_loss: 5217.77 with loss1: 380.50, loss2: 46.90 and loss3: 4790.37\n",
      "Epoch [1403], train_loss: 5217.63 with loss1: 380.36, loss2: 46.94 and loss3: 4790.32\n",
      "Epoch [1404], train_loss: 5216.59 with loss1: 379.36, loss2: 46.95 and loss3: 4790.27\n",
      "Epoch [1405], train_loss: 5217.59 with loss1: 380.43, loss2: 46.93 and loss3: 4790.22\n",
      "Epoch [1406], train_loss: 5216.02 with loss1: 378.76, loss2: 47.09 and loss3: 4790.17\n",
      "Epoch [1407], train_loss: 5218.08 with loss1: 380.87, loss2: 47.09 and loss3: 4790.12\n",
      "Epoch [1408], train_loss: 5214.70 with loss1: 377.83, loss2: 46.79 and loss3: 4790.08\n",
      "Epoch [1409], train_loss: 5215.11 with loss1: 378.02, loss2: 47.06 and loss3: 4790.03\n",
      "Epoch [1410], train_loss: 5215.57 with loss1: 378.69, loss2: 46.90 and loss3: 4789.98\n",
      "Epoch [1411], train_loss: 5216.68 with loss1: 379.77, loss2: 46.98 and loss3: 4789.93\n",
      "Epoch [1412], train_loss: 5213.73 with loss1: 377.01, loss2: 46.84 and loss3: 4789.88\n",
      "Epoch [1413], train_loss: 5215.60 with loss1: 378.50, loss2: 47.27 and loss3: 4789.83\n",
      "Epoch [1414], train_loss: 5214.22 with loss1: 377.48, loss2: 46.96 and loss3: 4789.78\n",
      "Epoch [1415], train_loss: 5214.54 with loss1: 377.92, loss2: 46.89 and loss3: 4789.73\n",
      "Epoch [1416], train_loss: 5212.86 with loss1: 376.44, loss2: 46.74 and loss3: 4789.68\n",
      "Epoch [1417], train_loss: 5215.07 with loss1: 378.60, loss2: 46.84 and loss3: 4789.63\n",
      "Epoch [1418], train_loss: 5214.10 with loss1: 377.75, loss2: 46.76 and loss3: 4789.58\n",
      "Epoch [1419], train_loss: 5215.78 with loss1: 379.21, loss2: 47.03 and loss3: 4789.53\n",
      "Epoch [1420], train_loss: 5214.21 with loss1: 378.03, loss2: 46.70 and loss3: 4789.49\n",
      "Epoch [1421], train_loss: 5216.52 with loss1: 380.15, loss2: 46.93 and loss3: 4789.44\n",
      "Epoch [1422], train_loss: 5215.25 with loss1: 379.15, loss2: 46.72 and loss3: 4789.39\n",
      "Epoch [1423], train_loss: 5217.48 with loss1: 381.41, loss2: 46.73 and loss3: 4789.34\n",
      "Epoch [1424], train_loss: 5216.80 with loss1: 380.77, loss2: 46.73 and loss3: 4789.29\n",
      "Epoch [1425], train_loss: 5219.79 with loss1: 383.81, loss2: 46.74 and loss3: 4789.24\n",
      "Epoch [1426], train_loss: 5217.03 with loss1: 381.05, loss2: 46.79 and loss3: 4789.19\n",
      "Epoch [1427], train_loss: 5219.56 with loss1: 383.48, loss2: 46.94 and loss3: 4789.14\n",
      "Epoch [1428], train_loss: 5217.99 with loss1: 382.19, loss2: 46.71 and loss3: 4789.09\n",
      "Epoch [1429], train_loss: 5219.82 with loss1: 384.13, loss2: 46.66 and loss3: 4789.04\n",
      "Epoch [1430], train_loss: 5218.80 with loss1: 383.24, loss2: 46.57 and loss3: 4788.99\n",
      "Epoch [1431], train_loss: 5221.81 with loss1: 386.06, loss2: 46.81 and loss3: 4788.94\n",
      "Epoch [1432], train_loss: 5221.71 with loss1: 386.01, loss2: 46.81 and loss3: 4788.89\n",
      "Epoch [1433], train_loss: 5223.04 with loss1: 387.51, loss2: 46.68 and loss3: 4788.84\n",
      "Epoch [1434], train_loss: 5222.71 with loss1: 387.02, loss2: 46.89 and loss3: 4788.80\n",
      "Epoch [1435], train_loss: 5224.10 with loss1: 388.68, loss2: 46.68 and loss3: 4788.75\n",
      "Epoch [1436], train_loss: 5224.19 with loss1: 388.81, loss2: 46.69 and loss3: 4788.70\n",
      "Epoch [1437], train_loss: 5225.56 with loss1: 390.31, loss2: 46.61 and loss3: 4788.65\n",
      "Epoch [1438], train_loss: 5224.89 with loss1: 389.71, loss2: 46.58 and loss3: 4788.60\n",
      "Epoch [1439], train_loss: 5227.38 with loss1: 391.96, loss2: 46.87 and loss3: 4788.55\n",
      "Epoch [1440], train_loss: 5227.30 with loss1: 392.42, loss2: 46.38 and loss3: 4788.50\n",
      "Epoch [1441], train_loss: 5229.48 with loss1: 394.28, loss2: 46.74 and loss3: 4788.45\n",
      "Epoch [1442], train_loss: 5228.12 with loss1: 392.90, loss2: 46.82 and loss3: 4788.40\n",
      "Epoch [1443], train_loss: 5230.94 with loss1: 396.01, loss2: 46.57 and loss3: 4788.35\n",
      "Epoch [1444], train_loss: 5228.06 with loss1: 393.24, loss2: 46.51 and loss3: 4788.30\n",
      "Epoch [1445], train_loss: 5231.79 with loss1: 396.69, loss2: 46.85 and loss3: 4788.25\n",
      "Epoch [1446], train_loss: 5229.96 with loss1: 395.35, loss2: 46.40 and loss3: 4788.20\n",
      "Epoch [1447], train_loss: 5232.41 with loss1: 397.66, loss2: 46.60 and loss3: 4788.15\n",
      "Epoch [1448], train_loss: 5230.69 with loss1: 396.00, loss2: 46.59 and loss3: 4788.11\n",
      "Epoch [1449], train_loss: 5234.19 with loss1: 399.46, loss2: 46.68 and loss3: 4788.06\n",
      "Epoch [1450], train_loss: 5231.34 with loss1: 396.89, loss2: 46.45 and loss3: 4788.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1451], train_loss: 5234.63 with loss1: 400.10, loss2: 46.58 and loss3: 4787.96\n",
      "Epoch [1452], train_loss: 5233.28 with loss1: 398.90, loss2: 46.48 and loss3: 4787.91\n",
      "Epoch [1453], train_loss: 5236.52 with loss1: 402.13, loss2: 46.54 and loss3: 4787.86\n",
      "Epoch [1454], train_loss: 5233.20 with loss1: 399.14, loss2: 46.24 and loss3: 4787.81\n",
      "Epoch [1455], train_loss: 5236.15 with loss1: 401.80, loss2: 46.59 and loss3: 4787.76\n",
      "Epoch [1456], train_loss: 5233.27 with loss1: 399.26, loss2: 46.30 and loss3: 4787.71\n",
      "Epoch [1457], train_loss: 5236.60 with loss1: 402.25, loss2: 46.68 and loss3: 4787.66\n",
      "Epoch [1458], train_loss: 5232.48 with loss1: 398.33, loss2: 46.54 and loss3: 4787.61\n",
      "Epoch [1459], train_loss: 5235.84 with loss1: 401.36, loss2: 46.92 and loss3: 4787.56\n",
      "Epoch [1460], train_loss: 5231.76 with loss1: 397.90, loss2: 46.34 and loss3: 4787.51\n",
      "Epoch [1461], train_loss: 5235.33 with loss1: 401.32, loss2: 46.55 and loss3: 4787.47\n",
      "Epoch [1462], train_loss: 5231.63 with loss1: 397.67, loss2: 46.55 and loss3: 4787.42\n",
      "Epoch [1463], train_loss: 5233.47 with loss1: 399.65, loss2: 46.45 and loss3: 4787.37\n",
      "Epoch [1464], train_loss: 5229.12 with loss1: 395.58, loss2: 46.22 and loss3: 4787.32\n",
      "Epoch [1465], train_loss: 5232.92 with loss1: 399.22, loss2: 46.43 and loss3: 4787.27\n",
      "Epoch [1466], train_loss: 5229.86 with loss1: 396.44, loss2: 46.20 and loss3: 4787.22\n",
      "Epoch [1467], train_loss: 5230.99 with loss1: 397.12, loss2: 46.70 and loss3: 4787.17\n",
      "Epoch [1468], train_loss: 5229.16 with loss1: 395.47, loss2: 46.57 and loss3: 4787.12\n",
      "Epoch [1469], train_loss: 5229.75 with loss1: 396.16, loss2: 46.52 and loss3: 4787.07\n",
      "Epoch [1470], train_loss: 5226.27 with loss1: 392.75, loss2: 46.50 and loss3: 4787.02\n",
      "Epoch [1471], train_loss: 5228.07 with loss1: 394.64, loss2: 46.46 and loss3: 4786.97\n",
      "Epoch [1472], train_loss: 5224.63 with loss1: 391.70, loss2: 46.01 and loss3: 4786.92\n",
      "Epoch [1473], train_loss: 5226.60 with loss1: 393.17, loss2: 46.56 and loss3: 4786.87\n",
      "Epoch [1474], train_loss: 5222.74 with loss1: 389.64, loss2: 46.27 and loss3: 4786.83\n",
      "Epoch [1475], train_loss: 5224.86 with loss1: 391.72, loss2: 46.36 and loss3: 4786.78\n",
      "Epoch [1476], train_loss: 5221.58 with loss1: 388.71, loss2: 46.15 and loss3: 4786.73\n",
      "Epoch [1477], train_loss: 5223.28 with loss1: 390.30, loss2: 46.30 and loss3: 4786.68\n",
      "Epoch [1478], train_loss: 5220.47 with loss1: 387.44, loss2: 46.40 and loss3: 4786.63\n",
      "Epoch [1479], train_loss: 5221.03 with loss1: 388.18, loss2: 46.26 and loss3: 4786.58\n",
      "Epoch [1480], train_loss: 5218.25 with loss1: 385.45, loss2: 46.27 and loss3: 4786.53\n",
      "Epoch [1481], train_loss: 5219.62 with loss1: 386.85, loss2: 46.30 and loss3: 4786.48\n",
      "Epoch [1482], train_loss: 5217.00 with loss1: 384.43, loss2: 46.14 and loss3: 4786.43\n",
      "Epoch [1483], train_loss: 5218.01 with loss1: 385.30, loss2: 46.33 and loss3: 4786.38\n",
      "Epoch [1484], train_loss: 5215.40 with loss1: 383.03, loss2: 46.04 and loss3: 4786.33\n",
      "Epoch [1485], train_loss: 5217.99 with loss1: 385.15, loss2: 46.55 and loss3: 4786.28\n",
      "Epoch [1486], train_loss: 5214.60 with loss1: 382.34, loss2: 46.03 and loss3: 4786.23\n",
      "Epoch [1487], train_loss: 5216.89 with loss1: 384.37, loss2: 46.34 and loss3: 4786.18\n",
      "Epoch [1488], train_loss: 5213.64 with loss1: 381.34, loss2: 46.16 and loss3: 4786.14\n",
      "Epoch [1489], train_loss: 5215.60 with loss1: 383.27, loss2: 46.24 and loss3: 4786.09\n",
      "Epoch [1490], train_loss: 5214.75 with loss1: 382.66, loss2: 46.05 and loss3: 4786.04\n",
      "Epoch [1491], train_loss: 5215.88 with loss1: 383.25, loss2: 46.65 and loss3: 4785.99\n",
      "Epoch [1492], train_loss: 5213.29 with loss1: 381.29, loss2: 46.06 and loss3: 4785.94\n",
      "Epoch [1493], train_loss: 5214.18 with loss1: 382.06, loss2: 46.22 and loss3: 4785.89\n",
      "Epoch [1494], train_loss: 5211.80 with loss1: 379.90, loss2: 46.05 and loss3: 4785.84\n",
      "Epoch [1495], train_loss: 5213.19 with loss1: 381.29, loss2: 46.11 and loss3: 4785.79\n",
      "Epoch [1496], train_loss: 5211.29 with loss1: 379.66, loss2: 45.89 and loss3: 4785.74\n",
      "Epoch [1497], train_loss: 5213.30 with loss1: 381.55, loss2: 46.07 and loss3: 4785.69\n",
      "Epoch [1498], train_loss: 5211.81 with loss1: 380.08, loss2: 46.09 and loss3: 4785.64\n",
      "Epoch [1499], train_loss: 5213.28 with loss1: 381.29, loss2: 46.39 and loss3: 4785.59\n",
      "Epoch [1500], train_loss: 5211.69 with loss1: 380.24, loss2: 45.91 and loss3: 4785.54\n",
      "Epoch [1501], train_loss: 5213.14 with loss1: 381.53, loss2: 46.11 and loss3: 4785.50\n",
      "Epoch [1502], train_loss: 5211.18 with loss1: 379.77, loss2: 45.96 and loss3: 4785.45\n",
      "Epoch [1503], train_loss: 5212.68 with loss1: 381.20, loss2: 46.08 and loss3: 4785.40\n",
      "Epoch [1504], train_loss: 5211.36 with loss1: 380.13, loss2: 45.88 and loss3: 4785.35\n",
      "Epoch [1505], train_loss: 5213.32 with loss1: 381.54, loss2: 46.48 and loss3: 4785.30\n",
      "Epoch [1506], train_loss: 5209.96 with loss1: 378.77, loss2: 45.94 and loss3: 4785.25\n",
      "Epoch [1507], train_loss: 5212.23 with loss1: 380.97, loss2: 46.06 and loss3: 4785.20\n",
      "Epoch [1508], train_loss: 5210.48 with loss1: 379.40, loss2: 45.93 and loss3: 4785.15\n",
      "Epoch [1509], train_loss: 5211.71 with loss1: 380.64, loss2: 45.97 and loss3: 4785.10\n",
      "Epoch [1510], train_loss: 5210.26 with loss1: 379.21, loss2: 46.00 and loss3: 4785.05\n",
      "Epoch [1511], train_loss: 5212.70 with loss1: 381.88, loss2: 45.82 and loss3: 4785.00\n",
      "Epoch [1512], train_loss: 5210.62 with loss1: 379.72, loss2: 45.95 and loss3: 4784.95\n",
      "Epoch [1513], train_loss: 5213.61 with loss1: 382.53, loss2: 46.18 and loss3: 4784.90\n",
      "Epoch [1514], train_loss: 5210.71 with loss1: 379.90, loss2: 45.96 and loss3: 4784.86\n",
      "Epoch [1515], train_loss: 5212.77 with loss1: 381.84, loss2: 46.13 and loss3: 4784.81\n",
      "Epoch [1516], train_loss: 5210.86 with loss1: 380.38, loss2: 45.73 and loss3: 4784.76\n",
      "Epoch [1517], train_loss: 5212.68 with loss1: 382.02, loss2: 45.96 and loss3: 4784.71\n",
      "Epoch [1518], train_loss: 5211.06 with loss1: 380.73, loss2: 45.68 and loss3: 4784.66\n",
      "Epoch [1519], train_loss: 5213.92 with loss1: 383.48, loss2: 45.83 and loss3: 4784.61\n",
      "Epoch [1520], train_loss: 5212.01 with loss1: 381.56, loss2: 45.89 and loss3: 4784.56\n",
      "Epoch [1521], train_loss: 5213.66 with loss1: 383.22, loss2: 45.93 and loss3: 4784.51\n",
      "Epoch [1522], train_loss: 5212.29 with loss1: 382.00, loss2: 45.83 and loss3: 4784.46\n",
      "Epoch [1523], train_loss: 5214.05 with loss1: 383.78, loss2: 45.86 and loss3: 4784.41\n",
      "Epoch [1524], train_loss: 5211.36 with loss1: 381.20, loss2: 45.80 and loss3: 4784.36\n",
      "Epoch [1525], train_loss: 5214.23 with loss1: 384.08, loss2: 45.84 and loss3: 4784.31\n",
      "Epoch [1526], train_loss: 5212.77 with loss1: 382.84, loss2: 45.66 and loss3: 4784.26\n",
      "Epoch [1527], train_loss: 5214.86 with loss1: 384.76, loss2: 45.88 and loss3: 4784.22\n",
      "Epoch [1528], train_loss: 5213.35 with loss1: 383.21, loss2: 45.98 and loss3: 4784.17\n",
      "Epoch [1529], train_loss: 5214.90 with loss1: 384.91, loss2: 45.87 and loss3: 4784.12\n",
      "Epoch [1530], train_loss: 5211.90 with loss1: 382.22, loss2: 45.61 and loss3: 4784.07\n",
      "Epoch [1531], train_loss: 5215.10 with loss1: 384.98, loss2: 46.10 and loss3: 4784.02\n",
      "Epoch [1532], train_loss: 5212.81 with loss1: 383.12, loss2: 45.72 and loss3: 4783.97\n",
      "Epoch [1533], train_loss: 5216.20 with loss1: 386.30, loss2: 45.99 and loss3: 4783.92\n",
      "Epoch [1534], train_loss: 5213.51 with loss1: 384.07, loss2: 45.57 and loss3: 4783.87\n",
      "Epoch [1535], train_loss: 5215.83 with loss1: 386.23, loss2: 45.78 and loss3: 4783.82\n",
      "Epoch [1536], train_loss: 5213.66 with loss1: 384.20, loss2: 45.68 and loss3: 4783.77\n",
      "Epoch [1537], train_loss: 5214.98 with loss1: 385.52, loss2: 45.73 and loss3: 4783.72\n",
      "Epoch [1538], train_loss: 5213.06 with loss1: 383.52, loss2: 45.86 and loss3: 4783.67\n",
      "Epoch [1539], train_loss: 5215.58 with loss1: 386.20, loss2: 45.76 and loss3: 4783.62\n",
      "Epoch [1540], train_loss: 5213.20 with loss1: 384.16, loss2: 45.46 and loss3: 4783.58\n",
      "Epoch [1541], train_loss: 5216.00 with loss1: 386.77, loss2: 45.71 and loss3: 4783.53\n",
      "Epoch [1542], train_loss: 5213.89 with loss1: 384.57, loss2: 45.84 and loss3: 4783.48\n",
      "Epoch [1543], train_loss: 5215.50 with loss1: 386.34, loss2: 45.73 and loss3: 4783.43\n",
      "Epoch [1544], train_loss: 5212.72 with loss1: 383.75, loss2: 45.58 and loss3: 4783.38\n",
      "Epoch [1545], train_loss: 5214.91 with loss1: 385.86, loss2: 45.72 and loss3: 4783.33\n",
      "Epoch [1546], train_loss: 5212.41 with loss1: 383.64, loss2: 45.49 and loss3: 4783.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1547], train_loss: 5215.42 with loss1: 386.54, loss2: 45.65 and loss3: 4783.23\n",
      "Epoch [1548], train_loss: 5212.20 with loss1: 383.57, loss2: 45.45 and loss3: 4783.18\n",
      "Epoch [1549], train_loss: 5215.22 with loss1: 386.26, loss2: 45.83 and loss3: 4783.13\n",
      "Epoch [1550], train_loss: 5212.06 with loss1: 383.45, loss2: 45.53 and loss3: 4783.08\n",
      "Epoch [1551], train_loss: 5213.04 with loss1: 384.45, loss2: 45.56 and loss3: 4783.03\n",
      "Epoch [1552], train_loss: 5211.83 with loss1: 383.19, loss2: 45.66 and loss3: 4782.98\n",
      "Epoch [1553], train_loss: 5213.67 with loss1: 385.18, loss2: 45.56 and loss3: 4782.94\n",
      "Epoch [1554], train_loss: 5210.80 with loss1: 381.99, loss2: 45.92 and loss3: 4782.89\n",
      "Epoch [1555], train_loss: 5212.83 with loss1: 384.53, loss2: 45.46 and loss3: 4782.84\n",
      "Epoch [1556], train_loss: 5209.05 with loss1: 380.97, loss2: 45.30 and loss3: 4782.79\n",
      "Epoch [1557], train_loss: 5211.93 with loss1: 383.66, loss2: 45.53 and loss3: 4782.74\n",
      "Epoch [1558], train_loss: 5209.70 with loss1: 381.65, loss2: 45.37 and loss3: 4782.69\n",
      "Epoch [1559], train_loss: 5211.95 with loss1: 383.71, loss2: 45.60 and loss3: 4782.64\n",
      "Epoch [1560], train_loss: 5208.54 with loss1: 380.69, loss2: 45.26 and loss3: 4782.59\n",
      "Epoch [1561], train_loss: 5211.22 with loss1: 383.18, loss2: 45.50 and loss3: 4782.54\n",
      "Epoch [1562], train_loss: 5207.94 with loss1: 380.23, loss2: 45.22 and loss3: 4782.49\n",
      "Epoch [1563], train_loss: 5209.82 with loss1: 381.99, loss2: 45.39 and loss3: 4782.44\n",
      "Epoch [1564], train_loss: 5207.64 with loss1: 379.81, loss2: 45.43 and loss3: 4782.39\n",
      "Epoch [1565], train_loss: 5208.57 with loss1: 380.70, loss2: 45.52 and loss3: 4782.34\n",
      "Epoch [1566], train_loss: 5206.49 with loss1: 378.82, loss2: 45.38 and loss3: 4782.30\n",
      "Epoch [1567], train_loss: 5209.59 with loss1: 381.70, loss2: 45.65 and loss3: 4782.25\n",
      "Epoch [1568], train_loss: 5206.42 with loss1: 378.94, loss2: 45.28 and loss3: 4782.20\n",
      "Epoch [1569], train_loss: 5208.73 with loss1: 381.20, loss2: 45.39 and loss3: 4782.15\n",
      "Epoch [1570], train_loss: 5205.97 with loss1: 378.51, loss2: 45.37 and loss3: 4782.10\n",
      "Epoch [1571], train_loss: 5207.42 with loss1: 380.16, loss2: 45.22 and loss3: 4782.05\n",
      "Epoch [1572], train_loss: 5204.94 with loss1: 377.52, loss2: 45.41 and loss3: 4782.00\n",
      "Epoch [1573], train_loss: 5207.02 with loss1: 379.26, loss2: 45.81 and loss3: 4781.95\n",
      "Epoch [1574], train_loss: 5204.04 with loss1: 376.84, loss2: 45.30 and loss3: 4781.90\n",
      "Epoch [1575], train_loss: 5206.18 with loss1: 378.79, loss2: 45.53 and loss3: 4781.85\n",
      "Epoch [1576], train_loss: 5203.55 with loss1: 376.45, loss2: 45.30 and loss3: 4781.80\n",
      "Epoch [1577], train_loss: 5205.09 with loss1: 377.95, loss2: 45.38 and loss3: 4781.75\n",
      "Epoch [1578], train_loss: 5203.02 with loss1: 376.09, loss2: 45.22 and loss3: 4781.70\n",
      "Epoch [1579], train_loss: 5205.06 with loss1: 378.12, loss2: 45.28 and loss3: 4781.66\n",
      "Epoch [1580], train_loss: 5204.33 with loss1: 377.50, loss2: 45.22 and loss3: 4781.61\n",
      "Epoch [1581], train_loss: 5206.20 with loss1: 379.42, loss2: 45.22 and loss3: 4781.56\n",
      "Epoch [1582], train_loss: 5203.51 with loss1: 376.63, loss2: 45.38 and loss3: 4781.51\n",
      "Epoch [1583], train_loss: 5205.76 with loss1: 378.92, loss2: 45.38 and loss3: 4781.46\n",
      "Epoch [1584], train_loss: 5203.15 with loss1: 376.61, loss2: 45.13 and loss3: 4781.41\n",
      "Epoch [1585], train_loss: 5205.61 with loss1: 378.95, loss2: 45.30 and loss3: 4781.36\n",
      "Epoch [1586], train_loss: 5203.80 with loss1: 377.19, loss2: 45.29 and loss3: 4781.31\n",
      "Epoch [1587], train_loss: 5206.09 with loss1: 379.33, loss2: 45.50 and loss3: 4781.26\n",
      "Epoch [1588], train_loss: 5203.84 with loss1: 377.50, loss2: 45.13 and loss3: 4781.21\n",
      "Epoch [1589], train_loss: 5206.77 with loss1: 380.36, loss2: 45.25 and loss3: 4781.16\n",
      "Epoch [1590], train_loss: 5203.46 with loss1: 377.12, loss2: 45.23 and loss3: 4781.11\n",
      "Epoch [1591], train_loss: 5206.45 with loss1: 380.09, loss2: 45.30 and loss3: 4781.06\n",
      "Epoch [1592], train_loss: 5203.83 with loss1: 377.65, loss2: 45.16 and loss3: 4781.02\n",
      "Epoch [1593], train_loss: 5207.05 with loss1: 380.90, loss2: 45.19 and loss3: 4780.97\n",
      "Epoch [1594], train_loss: 5205.23 with loss1: 379.25, loss2: 45.06 and loss3: 4780.92\n",
      "Epoch [1595], train_loss: 5207.26 with loss1: 381.05, loss2: 45.34 and loss3: 4780.87\n",
      "Epoch [1596], train_loss: 5205.81 with loss1: 379.64, loss2: 45.34 and loss3: 4780.82\n",
      "Epoch [1597], train_loss: 5205.91 with loss1: 379.86, loss2: 45.28 and loss3: 4780.77\n",
      "Epoch [1598], train_loss: 5205.77 with loss1: 379.97, loss2: 45.08 and loss3: 4780.72\n",
      "Epoch [1599], train_loss: 5208.32 with loss1: 382.28, loss2: 45.38 and loss3: 4780.67\n",
      "Epoch [1600], train_loss: 5205.33 with loss1: 379.72, loss2: 45.00 and loss3: 4780.62\n",
      "Epoch [1601], train_loss: 5207.43 with loss1: 381.71, loss2: 45.16 and loss3: 4780.57\n",
      "Epoch [1602], train_loss: 5205.73 with loss1: 380.01, loss2: 45.20 and loss3: 4780.52\n",
      "Epoch [1603], train_loss: 5208.49 with loss1: 382.90, loss2: 45.12 and loss3: 4780.47\n",
      "Epoch [1604], train_loss: 5205.46 with loss1: 380.01, loss2: 45.02 and loss3: 4780.42\n",
      "Epoch [1605], train_loss: 5208.25 with loss1: 382.75, loss2: 45.12 and loss3: 4780.38\n",
      "Epoch [1606], train_loss: 5205.70 with loss1: 380.42, loss2: 44.95 and loss3: 4780.33\n",
      "Epoch [1607], train_loss: 5207.34 with loss1: 382.11, loss2: 44.96 and loss3: 4780.28\n",
      "Epoch [1608], train_loss: 5205.11 with loss1: 379.97, loss2: 44.91 and loss3: 4780.23\n",
      "Epoch [1609], train_loss: 5206.88 with loss1: 381.65, loss2: 45.06 and loss3: 4780.18\n",
      "Epoch [1610], train_loss: 5204.44 with loss1: 379.31, loss2: 44.99 and loss3: 4780.13\n",
      "Epoch [1611], train_loss: 5208.67 with loss1: 383.33, loss2: 45.26 and loss3: 4780.08\n",
      "Epoch [1612], train_loss: 5204.20 with loss1: 379.19, loss2: 44.97 and loss3: 4780.03\n",
      "Epoch [1613], train_loss: 5206.06 with loss1: 380.96, loss2: 45.12 and loss3: 4779.98\n",
      "Epoch [1614], train_loss: 5203.62 with loss1: 378.74, loss2: 44.95 and loss3: 4779.93\n",
      "Epoch [1615], train_loss: 5205.70 with loss1: 380.72, loss2: 45.10 and loss3: 4779.88\n",
      "Epoch [1616], train_loss: 5203.18 with loss1: 378.36, loss2: 44.99 and loss3: 4779.83\n",
      "Epoch [1617], train_loss: 5204.58 with loss1: 379.40, loss2: 45.39 and loss3: 4779.79\n",
      "Epoch [1618], train_loss: 5201.24 with loss1: 376.69, loss2: 44.82 and loss3: 4779.74\n",
      "Epoch [1619], train_loss: 5203.36 with loss1: 378.67, loss2: 45.01 and loss3: 4779.69\n",
      "Epoch [1620], train_loss: 5201.67 with loss1: 377.14, loss2: 44.89 and loss3: 4779.64\n",
      "Epoch [1621], train_loss: 5202.55 with loss1: 378.04, loss2: 44.92 and loss3: 4779.59\n",
      "Epoch [1622], train_loss: 5200.41 with loss1: 376.04, loss2: 44.83 and loss3: 4779.54\n",
      "Epoch [1623], train_loss: 5201.36 with loss1: 376.91, loss2: 44.96 and loss3: 4779.49\n",
      "Epoch [1624], train_loss: 5199.73 with loss1: 375.60, loss2: 44.69 and loss3: 4779.44\n",
      "Epoch [1625], train_loss: 5201.16 with loss1: 377.00, loss2: 44.77 and loss3: 4779.39\n",
      "Epoch [1626], train_loss: 5200.12 with loss1: 375.83, loss2: 44.95 and loss3: 4779.34\n",
      "Epoch [1627], train_loss: 5202.89 with loss1: 378.26, loss2: 45.34 and loss3: 4779.29\n",
      "Epoch [1628], train_loss: 5199.88 with loss1: 375.84, loss2: 44.79 and loss3: 4779.24\n",
      "Epoch [1629], train_loss: 5202.38 with loss1: 378.12, loss2: 45.07 and loss3: 4779.19\n",
      "Epoch [1630], train_loss: 5200.36 with loss1: 376.46, loss2: 44.76 and loss3: 4779.15\n",
      "Epoch [1631], train_loss: 5203.04 with loss1: 378.99, loss2: 44.96 and loss3: 4779.10\n",
      "Epoch [1632], train_loss: 5200.40 with loss1: 376.65, loss2: 44.70 and loss3: 4779.05\n",
      "Epoch [1633], train_loss: 5202.15 with loss1: 378.12, loss2: 45.03 and loss3: 4779.00\n",
      "Epoch [1634], train_loss: 5200.75 with loss1: 376.78, loss2: 45.02 and loss3: 4778.95\n",
      "Epoch [1635], train_loss: 5202.30 with loss1: 378.51, loss2: 44.88 and loss3: 4778.90\n",
      "Epoch [1636], train_loss: 5201.56 with loss1: 378.08, loss2: 44.63 and loss3: 4778.85\n",
      "Epoch [1637], train_loss: 5203.46 with loss1: 379.42, loss2: 45.24 and loss3: 4778.80\n",
      "Epoch [1638], train_loss: 5200.53 with loss1: 377.07, loss2: 44.71 and loss3: 4778.75\n",
      "Epoch [1639], train_loss: 5202.74 with loss1: 379.17, loss2: 44.87 and loss3: 4778.70\n",
      "Epoch [1640], train_loss: 5201.37 with loss1: 377.86, loss2: 44.85 and loss3: 4778.65\n",
      "Epoch [1641], train_loss: 5203.61 with loss1: 380.29, loss2: 44.72 and loss3: 4778.60\n",
      "Epoch [1642], train_loss: 5200.72 with loss1: 377.43, loss2: 44.74 and loss3: 4778.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1643], train_loss: 5205.28 with loss1: 381.87, loss2: 44.90 and loss3: 4778.51\n",
      "Epoch [1644], train_loss: 5202.81 with loss1: 379.25, loss2: 45.11 and loss3: 4778.46\n",
      "Epoch [1645], train_loss: 5205.86 with loss1: 382.72, loss2: 44.74 and loss3: 4778.41\n",
      "Epoch [1646], train_loss: 5203.74 with loss1: 380.76, loss2: 44.63 and loss3: 4778.36\n",
      "Epoch [1647], train_loss: 5204.79 with loss1: 381.82, loss2: 44.66 and loss3: 4778.31\n",
      "Epoch [1648], train_loss: 5203.68 with loss1: 380.87, loss2: 44.55 and loss3: 4778.26\n",
      "Epoch [1649], train_loss: 5205.91 with loss1: 382.87, loss2: 44.83 and loss3: 4778.21\n",
      "Epoch [1650], train_loss: 5202.86 with loss1: 380.16, loss2: 44.54 and loss3: 4778.16\n",
      "Epoch [1651], train_loss: 5206.84 with loss1: 383.95, loss2: 44.78 and loss3: 4778.11\n",
      "Epoch [1652], train_loss: 5206.31 with loss1: 383.67, loss2: 44.57 and loss3: 4778.06\n",
      "Epoch [1653], train_loss: 5207.53 with loss1: 384.71, loss2: 44.80 and loss3: 4778.01\n",
      "Epoch [1654], train_loss: 5205.23 with loss1: 382.80, loss2: 44.46 and loss3: 4777.96\n",
      "Epoch [1655], train_loss: 5208.26 with loss1: 385.66, loss2: 44.68 and loss3: 4777.92\n",
      "Epoch [1656], train_loss: 5205.88 with loss1: 383.62, loss2: 44.39 and loss3: 4777.87\n",
      "Epoch [1657], train_loss: 5209.56 with loss1: 386.89, loss2: 44.85 and loss3: 4777.82\n",
      "Epoch [1658], train_loss: 5206.69 with loss1: 384.42, loss2: 44.50 and loss3: 4777.77\n",
      "Epoch [1659], train_loss: 5209.38 with loss1: 386.94, loss2: 44.72 and loss3: 4777.72\n",
      "Epoch [1660], train_loss: 5208.27 with loss1: 386.01, loss2: 44.59 and loss3: 4777.67\n",
      "Epoch [1661], train_loss: 5211.08 with loss1: 388.68, loss2: 44.78 and loss3: 4777.62\n",
      "Epoch [1662], train_loss: 5208.14 with loss1: 386.16, loss2: 44.41 and loss3: 4777.57\n",
      "Epoch [1663], train_loss: 5210.78 with loss1: 388.65, loss2: 44.61 and loss3: 4777.52\n",
      "Epoch [1664], train_loss: 5209.21 with loss1: 386.92, loss2: 44.81 and loss3: 4777.47\n",
      "Epoch [1665], train_loss: 5212.66 with loss1: 390.61, loss2: 44.62 and loss3: 4777.42\n",
      "Epoch [1666], train_loss: 5208.89 with loss1: 387.25, loss2: 44.27 and loss3: 4777.37\n",
      "Epoch [1667], train_loss: 5211.25 with loss1: 389.34, loss2: 44.58 and loss3: 4777.32\n",
      "Epoch [1668], train_loss: 5208.72 with loss1: 387.09, loss2: 44.36 and loss3: 4777.28\n",
      "Epoch [1669], train_loss: 5212.08 with loss1: 390.46, loss2: 44.39 and loss3: 4777.23\n",
      "Epoch [1670], train_loss: 5208.84 with loss1: 387.41, loss2: 44.25 and loss3: 4777.18\n",
      "Epoch [1671], train_loss: 5211.59 with loss1: 390.01, loss2: 44.45 and loss3: 4777.13\n",
      "Epoch [1672], train_loss: 5209.20 with loss1: 387.56, loss2: 44.56 and loss3: 4777.08\n",
      "Epoch [1673], train_loss: 5212.32 with loss1: 390.74, loss2: 44.55 and loss3: 4777.03\n",
      "Epoch [1674], train_loss: 5208.55 with loss1: 387.25, loss2: 44.33 and loss3: 4776.98\n",
      "Epoch [1675], train_loss: 5209.97 with loss1: 388.52, loss2: 44.52 and loss3: 4776.93\n",
      "Epoch [1676], train_loss: 5208.55 with loss1: 387.41, loss2: 44.26 and loss3: 4776.88\n",
      "Epoch [1677], train_loss: 5211.54 with loss1: 390.10, loss2: 44.61 and loss3: 4776.83\n",
      "Epoch [1678], train_loss: 5207.53 with loss1: 386.49, loss2: 44.25 and loss3: 4776.78\n",
      "Epoch [1679], train_loss: 5209.79 with loss1: 388.28, loss2: 44.78 and loss3: 4776.73\n",
      "Epoch [1680], train_loss: 5207.93 with loss1: 386.64, loss2: 44.60 and loss3: 4776.68\n",
      "Epoch [1681], train_loss: 5209.39 with loss1: 388.09, loss2: 44.66 and loss3: 4776.64\n",
      "Epoch [1682], train_loss: 5206.45 with loss1: 385.62, loss2: 44.24 and loss3: 4776.59\n",
      "Epoch [1683], train_loss: 5208.36 with loss1: 387.43, loss2: 44.39 and loss3: 4776.54\n",
      "Epoch [1684], train_loss: 5205.98 with loss1: 385.26, loss2: 44.23 and loss3: 4776.49\n",
      "Epoch [1685], train_loss: 5207.54 with loss1: 386.64, loss2: 44.46 and loss3: 4776.44\n",
      "Epoch [1686], train_loss: 5204.67 with loss1: 384.16, loss2: 44.12 and loss3: 4776.39\n",
      "Epoch [1687], train_loss: 5207.09 with loss1: 386.33, loss2: 44.42 and loss3: 4776.34\n",
      "Epoch [1688], train_loss: 5203.54 with loss1: 383.10, loss2: 44.15 and loss3: 4776.29\n",
      "Epoch [1689], train_loss: 5206.23 with loss1: 385.57, loss2: 44.42 and loss3: 4776.24\n",
      "Epoch [1690], train_loss: 5203.48 with loss1: 383.01, loss2: 44.28 and loss3: 4776.19\n",
      "Epoch [1691], train_loss: 5205.07 with loss1: 384.68, loss2: 44.25 and loss3: 4776.14\n",
      "Epoch [1692], train_loss: 5202.64 with loss1: 382.23, loss2: 44.32 and loss3: 4776.09\n",
      "Epoch [1693], train_loss: 5203.75 with loss1: 383.40, loss2: 44.31 and loss3: 4776.04\n",
      "Epoch [1694], train_loss: 5200.57 with loss1: 380.47, loss2: 44.10 and loss3: 4776.00\n",
      "Epoch [1695], train_loss: 5202.64 with loss1: 382.41, loss2: 44.29 and loss3: 4775.95\n",
      "Epoch [1696], train_loss: 5199.75 with loss1: 379.59, loss2: 44.26 and loss3: 4775.90\n",
      "Epoch [1697], train_loss: 5200.65 with loss1: 380.49, loss2: 44.30 and loss3: 4775.85\n",
      "Epoch [1698], train_loss: 5198.31 with loss1: 378.33, loss2: 44.19 and loss3: 4775.80\n",
      "Epoch [1699], train_loss: 5200.39 with loss1: 380.41, loss2: 44.23 and loss3: 4775.75\n",
      "Epoch [1700], train_loss: 5197.63 with loss1: 377.84, loss2: 44.08 and loss3: 4775.70\n",
      "Epoch [1701], train_loss: 5198.96 with loss1: 378.77, loss2: 44.55 and loss3: 4775.65\n",
      "Epoch [1702], train_loss: 5196.03 with loss1: 376.28, loss2: 44.15 and loss3: 4775.60\n",
      "Epoch [1703], train_loss: 5198.27 with loss1: 378.51, loss2: 44.21 and loss3: 4775.55\n",
      "Epoch [1704], train_loss: 5193.73 with loss1: 374.21, loss2: 44.02 and loss3: 4775.50\n",
      "Epoch [1705], train_loss: 5195.95 with loss1: 376.28, loss2: 44.21 and loss3: 4775.45\n",
      "Epoch [1706], train_loss: 5192.62 with loss1: 373.11, loss2: 44.11 and loss3: 4775.41\n",
      "Epoch [1707], train_loss: 5193.25 with loss1: 373.52, loss2: 44.37 and loss3: 4775.36\n",
      "Epoch [1708], train_loss: 5190.85 with loss1: 371.71, loss2: 43.83 and loss3: 4775.31\n",
      "Epoch [1709], train_loss: 5193.22 with loss1: 373.83, loss2: 44.13 and loss3: 4775.26\n",
      "Epoch [1710], train_loss: 5190.38 with loss1: 371.19, loss2: 43.98 and loss3: 4775.21\n",
      "Epoch [1711], train_loss: 5192.04 with loss1: 372.66, loss2: 44.23 and loss3: 4775.16\n",
      "Epoch [1712], train_loss: 5189.37 with loss1: 370.27, loss2: 43.98 and loss3: 4775.11\n",
      "Epoch [1713], train_loss: 5191.62 with loss1: 372.33, loss2: 44.23 and loss3: 4775.06\n",
      "Epoch [1714], train_loss: 5188.11 with loss1: 369.08, loss2: 44.02 and loss3: 4775.01\n",
      "Epoch [1715], train_loss: 5189.50 with loss1: 370.41, loss2: 44.13 and loss3: 4774.96\n",
      "Epoch [1716], train_loss: 5187.82 with loss1: 368.96, loss2: 43.94 and loss3: 4774.91\n",
      "Epoch [1717], train_loss: 5189.13 with loss1: 370.03, loss2: 44.23 and loss3: 4774.86\n",
      "Epoch [1718], train_loss: 5186.83 with loss1: 367.95, loss2: 44.06 and loss3: 4774.81\n",
      "Epoch [1719], train_loss: 5188.56 with loss1: 369.63, loss2: 44.16 and loss3: 4774.77\n",
      "Epoch [1720], train_loss: 5186.02 with loss1: 367.34, loss2: 43.97 and loss3: 4774.72\n",
      "Epoch [1721], train_loss: 5187.58 with loss1: 368.92, loss2: 44.00 and loss3: 4774.67\n",
      "Epoch [1722], train_loss: 5187.39 with loss1: 368.86, loss2: 43.92 and loss3: 4774.62\n",
      "Epoch [1723], train_loss: 5187.54 with loss1: 368.86, loss2: 44.11 and loss3: 4774.57\n",
      "Epoch [1724], train_loss: 5186.12 with loss1: 367.56, loss2: 44.04 and loss3: 4774.52\n",
      "Epoch [1725], train_loss: 5187.05 with loss1: 368.32, loss2: 44.26 and loss3: 4774.47\n",
      "Epoch [1726], train_loss: 5186.60 with loss1: 368.28, loss2: 43.90 and loss3: 4774.42\n",
      "Epoch [1727], train_loss: 5187.26 with loss1: 368.77, loss2: 44.12 and loss3: 4774.37\n",
      "Epoch [1728], train_loss: 5186.58 with loss1: 368.10, loss2: 44.16 and loss3: 4774.32\n",
      "Epoch [1729], train_loss: 5187.55 with loss1: 369.34, loss2: 43.94 and loss3: 4774.27\n",
      "Epoch [1730], train_loss: 5186.27 with loss1: 368.13, loss2: 43.92 and loss3: 4774.22\n",
      "Epoch [1731], train_loss: 5188.17 with loss1: 369.93, loss2: 44.06 and loss3: 4774.18\n",
      "Epoch [1732], train_loss: 5186.28 with loss1: 368.43, loss2: 43.72 and loss3: 4774.13\n",
      "Epoch [1733], train_loss: 5188.23 with loss1: 370.13, loss2: 44.02 and loss3: 4774.08\n",
      "Epoch [1734], train_loss: 5187.14 with loss1: 369.36, loss2: 43.76 and loss3: 4774.03\n",
      "Epoch [1735], train_loss: 5188.26 with loss1: 370.24, loss2: 44.04 and loss3: 4773.98\n",
      "Epoch [1736], train_loss: 5187.62 with loss1: 369.87, loss2: 43.82 and loss3: 4773.93\n",
      "Epoch [1737], train_loss: 5188.97 with loss1: 371.19, loss2: 43.90 and loss3: 4773.88\n",
      "Epoch [1738], train_loss: 5186.48 with loss1: 368.85, loss2: 43.80 and loss3: 4773.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1739], train_loss: 5189.02 with loss1: 371.39, loss2: 43.84 and loss3: 4773.78\n",
      "Epoch [1740], train_loss: 5186.92 with loss1: 369.53, loss2: 43.65 and loss3: 4773.73\n",
      "Epoch [1741], train_loss: 5189.78 with loss1: 372.24, loss2: 43.86 and loss3: 4773.68\n",
      "Epoch [1742], train_loss: 5188.21 with loss1: 370.58, loss2: 44.00 and loss3: 4773.63\n",
      "Epoch [1743], train_loss: 5190.34 with loss1: 373.01, loss2: 43.75 and loss3: 4773.58\n",
      "Epoch [1744], train_loss: 5188.86 with loss1: 371.47, loss2: 43.86 and loss3: 4773.54\n",
      "Epoch [1745], train_loss: 5191.95 with loss1: 374.67, loss2: 43.80 and loss3: 4773.49\n",
      "Epoch [1746], train_loss: 5189.69 with loss1: 372.67, loss2: 43.58 and loss3: 4773.44\n",
      "Epoch [1747], train_loss: 5192.45 with loss1: 375.15, loss2: 43.91 and loss3: 4773.39\n",
      "Epoch [1748], train_loss: 5192.60 with loss1: 375.62, loss2: 43.64 and loss3: 4773.34\n",
      "Epoch [1749], train_loss: 5193.84 with loss1: 376.62, loss2: 43.93 and loss3: 4773.29\n",
      "Epoch [1750], train_loss: 5190.36 with loss1: 373.51, loss2: 43.61 and loss3: 4773.24\n",
      "Epoch [1751], train_loss: 5193.80 with loss1: 376.77, loss2: 43.84 and loss3: 4773.19\n",
      "Epoch [1752], train_loss: 5191.41 with loss1: 374.59, loss2: 43.68 and loss3: 4773.14\n",
      "Epoch [1753], train_loss: 5192.76 with loss1: 375.94, loss2: 43.73 and loss3: 4773.09\n",
      "Epoch [1754], train_loss: 5190.50 with loss1: 373.80, loss2: 43.66 and loss3: 4773.04\n",
      "Epoch [1755], train_loss: 5193.35 with loss1: 376.33, loss2: 44.03 and loss3: 4772.99\n",
      "Epoch [1756], train_loss: 5191.19 with loss1: 374.68, loss2: 43.56 and loss3: 4772.94\n",
      "Epoch [1757], train_loss: 5193.42 with loss1: 376.76, loss2: 43.77 and loss3: 4772.90\n",
      "Epoch [1758], train_loss: 5191.85 with loss1: 375.34, loss2: 43.66 and loss3: 4772.85\n",
      "Epoch [1759], train_loss: 5193.43 with loss1: 376.95, loss2: 43.69 and loss3: 4772.80\n",
      "Epoch [1760], train_loss: 5191.56 with loss1: 375.21, loss2: 43.60 and loss3: 4772.75\n",
      "Epoch [1761], train_loss: 5193.39 with loss1: 376.92, loss2: 43.78 and loss3: 4772.70\n",
      "Epoch [1762], train_loss: 5190.42 with loss1: 374.24, loss2: 43.53 and loss3: 4772.65\n",
      "Epoch [1763], train_loss: 5193.35 with loss1: 376.94, loss2: 43.82 and loss3: 4772.60\n",
      "Epoch [1764], train_loss: 5190.29 with loss1: 374.06, loss2: 43.68 and loss3: 4772.55\n",
      "Epoch [1765], train_loss: 5192.71 with loss1: 376.63, loss2: 43.58 and loss3: 4772.50\n",
      "Epoch [1766], train_loss: 5191.09 with loss1: 374.86, loss2: 43.78 and loss3: 4772.45\n",
      "Epoch [1767], train_loss: 5191.54 with loss1: 375.40, loss2: 43.74 and loss3: 4772.40\n",
      "Epoch [1768], train_loss: 5190.38 with loss1: 374.32, loss2: 43.71 and loss3: 4772.35\n",
      "Epoch [1769], train_loss: 5190.29 with loss1: 374.38, loss2: 43.60 and loss3: 4772.31\n",
      "Epoch [1770], train_loss: 5188.53 with loss1: 372.83, loss2: 43.44 and loss3: 4772.26\n",
      "Epoch [1771], train_loss: 5190.62 with loss1: 374.57, loss2: 43.84 and loss3: 4772.21\n",
      "Epoch [1772], train_loss: 5187.54 with loss1: 371.96, loss2: 43.42 and loss3: 4772.16\n",
      "Epoch [1773], train_loss: 5189.48 with loss1: 373.52, loss2: 43.86 and loss3: 4772.11\n",
      "Epoch [1774], train_loss: 5186.72 with loss1: 371.26, loss2: 43.40 and loss3: 4772.06\n",
      "Epoch [1775], train_loss: 5188.93 with loss1: 373.22, loss2: 43.70 and loss3: 4772.01\n",
      "Epoch [1776], train_loss: 5186.37 with loss1: 370.94, loss2: 43.47 and loss3: 4771.96\n",
      "Epoch [1777], train_loss: 5187.64 with loss1: 372.09, loss2: 43.63 and loss3: 4771.91\n",
      "Epoch [1778], train_loss: 5185.90 with loss1: 370.48, loss2: 43.56 and loss3: 4771.86\n",
      "Epoch [1779], train_loss: 5187.31 with loss1: 372.01, loss2: 43.48 and loss3: 4771.81\n",
      "Epoch [1780], train_loss: 5184.41 with loss1: 369.27, loss2: 43.38 and loss3: 4771.76\n",
      "Epoch [1781], train_loss: 5185.93 with loss1: 370.57, loss2: 43.64 and loss3: 4771.72\n",
      "Epoch [1782], train_loss: 5185.90 with loss1: 370.82, loss2: 43.41 and loss3: 4771.67\n",
      "Epoch [1783], train_loss: 5187.04 with loss1: 371.89, loss2: 43.54 and loss3: 4771.62\n",
      "Epoch [1784], train_loss: 5184.81 with loss1: 369.69, loss2: 43.55 and loss3: 4771.57\n",
      "Epoch [1785], train_loss: 5186.27 with loss1: 371.26, loss2: 43.49 and loss3: 4771.52\n",
      "Epoch [1786], train_loss: 5185.62 with loss1: 370.76, loss2: 43.39 and loss3: 4771.47\n",
      "Epoch [1787], train_loss: 5187.60 with loss1: 372.48, loss2: 43.71 and loss3: 4771.42\n",
      "Epoch [1788], train_loss: 5185.56 with loss1: 370.79, loss2: 43.39 and loss3: 4771.37\n",
      "Epoch [1789], train_loss: 5187.22 with loss1: 372.29, loss2: 43.60 and loss3: 4771.32\n",
      "Epoch [1790], train_loss: 5184.95 with loss1: 370.34, loss2: 43.34 and loss3: 4771.27\n",
      "Epoch [1791], train_loss: 5190.50 with loss1: 375.90, loss2: 43.38 and loss3: 4771.22\n",
      "Epoch [1792], train_loss: 5186.25 with loss1: 371.45, loss2: 43.63 and loss3: 4771.17\n",
      "Epoch [1793], train_loss: 5188.38 with loss1: 374.03, loss2: 43.22 and loss3: 4771.13\n",
      "Epoch [1794], train_loss: 5185.60 with loss1: 371.28, loss2: 43.24 and loss3: 4771.08\n",
      "Epoch [1795], train_loss: 5188.26 with loss1: 373.93, loss2: 43.30 and loss3: 4771.03\n",
      "Epoch [1796], train_loss: 5186.16 with loss1: 371.83, loss2: 43.35 and loss3: 4770.98\n",
      "Epoch [1797], train_loss: 5188.88 with loss1: 374.57, loss2: 43.38 and loss3: 4770.93\n",
      "Epoch [1798], train_loss: 5187.45 with loss1: 373.38, loss2: 43.19 and loss3: 4770.88\n",
      "Epoch [1799], train_loss: 5189.67 with loss1: 375.57, loss2: 43.27 and loss3: 4770.83\n",
      "Epoch [1800], train_loss: 5188.08 with loss1: 373.88, loss2: 43.41 and loss3: 4770.78\n",
      "Epoch [1801], train_loss: 5190.31 with loss1: 376.13, loss2: 43.45 and loss3: 4770.73\n",
      "Epoch [1802], train_loss: 5189.11 with loss1: 375.27, loss2: 43.16 and loss3: 4770.68\n",
      "Epoch [1803], train_loss: 5192.33 with loss1: 378.27, loss2: 43.42 and loss3: 4770.63\n",
      "Epoch [1804], train_loss: 5189.42 with loss1: 375.72, loss2: 43.11 and loss3: 4770.58\n",
      "Epoch [1805], train_loss: 5191.98 with loss1: 378.07, loss2: 43.37 and loss3: 4770.54\n",
      "Epoch [1806], train_loss: 5191.04 with loss1: 377.47, loss2: 43.08 and loss3: 4770.49\n",
      "Epoch [1807], train_loss: 5191.46 with loss1: 377.63, loss2: 43.39 and loss3: 4770.44\n",
      "Epoch [1808], train_loss: 5189.30 with loss1: 375.79, loss2: 43.13 and loss3: 4770.39\n",
      "Epoch [1809], train_loss: 5192.89 with loss1: 379.28, loss2: 43.28 and loss3: 4770.34\n",
      "Epoch [1810], train_loss: 5189.38 with loss1: 376.06, loss2: 43.03 and loss3: 4770.29\n",
      "Epoch [1811], train_loss: 5192.88 with loss1: 379.41, loss2: 43.23 and loss3: 4770.24\n",
      "Epoch [1812], train_loss: 5190.93 with loss1: 377.26, loss2: 43.48 and loss3: 4770.19\n",
      "Epoch [1813], train_loss: 5193.08 with loss1: 379.61, loss2: 43.33 and loss3: 4770.14\n",
      "Epoch [1814], train_loss: 5190.67 with loss1: 377.26, loss2: 43.32 and loss3: 4770.09\n",
      "Epoch [1815], train_loss: 5193.47 with loss1: 380.15, loss2: 43.27 and loss3: 4770.04\n",
      "Epoch [1816], train_loss: 5191.48 with loss1: 378.32, loss2: 43.17 and loss3: 4769.99\n",
      "Epoch [1817], train_loss: 5192.82 with loss1: 379.55, loss2: 43.32 and loss3: 4769.94\n",
      "Epoch [1818], train_loss: 5191.18 with loss1: 377.99, loss2: 43.29 and loss3: 4769.90\n",
      "Epoch [1819], train_loss: 5192.29 with loss1: 379.11, loss2: 43.34 and loss3: 4769.85\n",
      "Epoch [1820], train_loss: 5190.32 with loss1: 377.50, loss2: 43.02 and loss3: 4769.80\n",
      "Epoch [1821], train_loss: 5193.23 with loss1: 380.28, loss2: 43.20 and loss3: 4769.75\n",
      "Epoch [1822], train_loss: 5190.36 with loss1: 377.66, loss2: 43.00 and loss3: 4769.70\n",
      "Epoch [1823], train_loss: 5192.99 with loss1: 380.10, loss2: 43.25 and loss3: 4769.65\n",
      "Epoch [1824], train_loss: 5190.52 with loss1: 377.98, loss2: 42.95 and loss3: 4769.60\n",
      "Epoch [1825], train_loss: 5193.25 with loss1: 380.35, loss2: 43.35 and loss3: 4769.55\n",
      "Epoch [1826], train_loss: 5189.72 with loss1: 377.33, loss2: 42.89 and loss3: 4769.50\n",
      "Epoch [1827], train_loss: 5193.59 with loss1: 380.70, loss2: 43.44 and loss3: 4769.45\n",
      "Epoch [1828], train_loss: 5191.01 with loss1: 378.71, loss2: 42.90 and loss3: 4769.40\n",
      "Epoch [1829], train_loss: 5193.27 with loss1: 380.83, loss2: 43.08 and loss3: 4769.35\n",
      "Epoch [1830], train_loss: 5190.65 with loss1: 378.39, loss2: 42.96 and loss3: 4769.31\n",
      "Epoch [1831], train_loss: 5193.78 with loss1: 381.39, loss2: 43.13 and loss3: 4769.26\n",
      "Epoch [1832], train_loss: 5190.88 with loss1: 378.68, loss2: 42.99 and loss3: 4769.21\n",
      "Epoch [1833], train_loss: 5193.18 with loss1: 380.90, loss2: 43.12 and loss3: 4769.16\n",
      "Epoch [1834], train_loss: 5190.90 with loss1: 378.80, loss2: 42.99 and loss3: 4769.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1835], train_loss: 5193.21 with loss1: 381.04, loss2: 43.11 and loss3: 4769.06\n",
      "Epoch [1836], train_loss: 5191.23 with loss1: 379.18, loss2: 43.03 and loss3: 4769.01\n",
      "Epoch [1837], train_loss: 5193.11 with loss1: 381.01, loss2: 43.14 and loss3: 4768.96\n",
      "Epoch [1838], train_loss: 5190.38 with loss1: 378.37, loss2: 43.10 and loss3: 4768.91\n",
      "Epoch [1839], train_loss: 5194.50 with loss1: 382.35, loss2: 43.28 and loss3: 4768.86\n",
      "Epoch [1840], train_loss: 5190.22 with loss1: 378.65, loss2: 42.76 and loss3: 4768.81\n",
      "Epoch [1841], train_loss: 5193.81 with loss1: 382.16, loss2: 42.89 and loss3: 4768.76\n",
      "Epoch [1842], train_loss: 5189.08 with loss1: 377.54, loss2: 42.83 and loss3: 4768.72\n",
      "Epoch [1843], train_loss: 5191.97 with loss1: 380.27, loss2: 43.03 and loss3: 4768.67\n",
      "Epoch [1844], train_loss: 5188.29 with loss1: 376.80, loss2: 42.88 and loss3: 4768.62\n",
      "Epoch [1845], train_loss: 5191.22 with loss1: 379.49, loss2: 43.16 and loss3: 4768.57\n",
      "Epoch [1846], train_loss: 5188.51 with loss1: 377.19, loss2: 42.80 and loss3: 4768.52\n",
      "Epoch [1847], train_loss: 5190.78 with loss1: 379.35, loss2: 42.96 and loss3: 4768.47\n",
      "Epoch [1848], train_loss: 5186.97 with loss1: 375.77, loss2: 42.77 and loss3: 4768.42\n",
      "Epoch [1849], train_loss: 5187.86 with loss1: 376.50, loss2: 42.98 and loss3: 4768.37\n",
      "Epoch [1850], train_loss: 5184.22 with loss1: 373.16, loss2: 42.74 and loss3: 4768.32\n",
      "Epoch [1851], train_loss: 5187.01 with loss1: 375.77, loss2: 42.96 and loss3: 4768.27\n",
      "Epoch [1852], train_loss: 5183.26 with loss1: 372.30, loss2: 42.73 and loss3: 4768.22\n",
      "Epoch [1853], train_loss: 5184.94 with loss1: 373.80, loss2: 42.97 and loss3: 4768.17\n",
      "Epoch [1854], train_loss: 5182.61 with loss1: 371.36, loss2: 43.12 and loss3: 4768.13\n",
      "Epoch [1855], train_loss: 5183.68 with loss1: 372.71, loss2: 42.90 and loss3: 4768.08\n",
      "Epoch [1856], train_loss: 5180.72 with loss1: 369.25, loss2: 43.45 and loss3: 4768.03\n",
      "Epoch [1857], train_loss: 5181.99 with loss1: 371.24, loss2: 42.78 and loss3: 4767.98\n",
      "Epoch [1858], train_loss: 5178.55 with loss1: 367.73, loss2: 42.89 and loss3: 4767.93\n",
      "Epoch [1859], train_loss: 5180.24 with loss1: 369.43, loss2: 42.93 and loss3: 4767.88\n",
      "Epoch [1860], train_loss: 5177.42 with loss1: 366.76, loss2: 42.83 and loss3: 4767.83\n",
      "Epoch [1861], train_loss: 5176.76 with loss1: 366.19, loss2: 42.79 and loss3: 4767.78\n",
      "Epoch [1862], train_loss: 5174.87 with loss1: 364.25, loss2: 42.89 and loss3: 4767.73\n",
      "Epoch [1863], train_loss: 5176.65 with loss1: 365.79, loss2: 43.17 and loss3: 4767.68\n",
      "Epoch [1864], train_loss: 5172.60 with loss1: 362.30, loss2: 42.66 and loss3: 4767.63\n",
      "Epoch [1865], train_loss: 5173.09 with loss1: 362.68, loss2: 42.82 and loss3: 4767.59\n",
      "Epoch [1866], train_loss: 5171.61 with loss1: 361.40, loss2: 42.68 and loss3: 4767.54\n",
      "Epoch [1867], train_loss: 5171.95 with loss1: 361.59, loss2: 42.87 and loss3: 4767.49\n",
      "Epoch [1868], train_loss: 5169.76 with loss1: 359.44, loss2: 42.88 and loss3: 4767.44\n",
      "Epoch [1869], train_loss: 5170.31 with loss1: 359.88, loss2: 43.04 and loss3: 4767.39\n",
      "Epoch [1870], train_loss: 5168.21 with loss1: 358.12, loss2: 42.75 and loss3: 4767.34\n",
      "Epoch [1871], train_loss: 5168.36 with loss1: 358.36, loss2: 42.71 and loss3: 4767.29\n",
      "Epoch [1872], train_loss: 5166.70 with loss1: 356.77, loss2: 42.69 and loss3: 4767.24\n",
      "Epoch [1873], train_loss: 5168.01 with loss1: 358.12, loss2: 42.70 and loss3: 4767.19\n",
      "Epoch [1874], train_loss: 5166.31 with loss1: 356.56, loss2: 42.61 and loss3: 4767.14\n",
      "Epoch [1875], train_loss: 5165.85 with loss1: 355.94, loss2: 42.82 and loss3: 4767.09\n",
      "Epoch [1876], train_loss: 5163.63 with loss1: 354.05, loss2: 42.54 and loss3: 4767.04\n",
      "Epoch [1877], train_loss: 5164.21 with loss1: 354.49, loss2: 42.72 and loss3: 4767.00\n",
      "Epoch [1878], train_loss: 5162.77 with loss1: 353.16, loss2: 42.66 and loss3: 4766.95\n",
      "Epoch [1879], train_loss: 5164.27 with loss1: 354.60, loss2: 42.76 and loss3: 4766.90\n",
      "Epoch [1880], train_loss: 5162.12 with loss1: 352.72, loss2: 42.55 and loss3: 4766.85\n",
      "Epoch [1881], train_loss: 5162.40 with loss1: 352.88, loss2: 42.72 and loss3: 4766.80\n",
      "Epoch [1882], train_loss: 5160.76 with loss1: 351.45, loss2: 42.56 and loss3: 4766.75\n",
      "Epoch [1883], train_loss: 5161.78 with loss1: 352.21, loss2: 42.87 and loss3: 4766.70\n",
      "Epoch [1884], train_loss: 5160.74 with loss1: 351.51, loss2: 42.57 and loss3: 4766.65\n",
      "Epoch [1885], train_loss: 5162.32 with loss1: 353.04, loss2: 42.68 and loss3: 4766.60\n",
      "Epoch [1886], train_loss: 5159.84 with loss1: 350.73, loss2: 42.56 and loss3: 4766.55\n",
      "Epoch [1887], train_loss: 5159.88 with loss1: 350.86, loss2: 42.51 and loss3: 4766.50\n",
      "Epoch [1888], train_loss: 5160.06 with loss1: 351.00, loss2: 42.61 and loss3: 4766.46\n",
      "Epoch [1889], train_loss: 5160.44 with loss1: 351.49, loss2: 42.54 and loss3: 4766.41\n",
      "Epoch [1890], train_loss: 5161.10 with loss1: 352.37, loss2: 42.38 and loss3: 4766.36\n",
      "Epoch [1891], train_loss: 5160.52 with loss1: 351.69, loss2: 42.52 and loss3: 4766.31\n",
      "Epoch [1892], train_loss: 5160.01 with loss1: 351.22, loss2: 42.54 and loss3: 4766.26\n",
      "Epoch [1893], train_loss: 5160.91 with loss1: 352.09, loss2: 42.61 and loss3: 4766.21\n",
      "Epoch [1894], train_loss: 5159.34 with loss1: 350.77, loss2: 42.41 and loss3: 4766.16\n",
      "Epoch [1895], train_loss: 5160.92 with loss1: 352.08, loss2: 42.73 and loss3: 4766.11\n",
      "Epoch [1896], train_loss: 5160.77 with loss1: 352.05, loss2: 42.67 and loss3: 4766.06\n",
      "Epoch [1897], train_loss: 5161.89 with loss1: 353.25, loss2: 42.62 and loss3: 4766.01\n",
      "Epoch [1898], train_loss: 5160.35 with loss1: 352.03, loss2: 42.36 and loss3: 4765.96\n",
      "Epoch [1899], train_loss: 5162.68 with loss1: 354.32, loss2: 42.44 and loss3: 4765.92\n",
      "Epoch [1900], train_loss: 5161.17 with loss1: 352.75, loss2: 42.55 and loss3: 4765.87\n",
      "Epoch [1901], train_loss: 5162.83 with loss1: 354.59, loss2: 42.42 and loss3: 4765.82\n",
      "Epoch [1902], train_loss: 5161.95 with loss1: 353.68, loss2: 42.50 and loss3: 4765.77\n",
      "Epoch [1903], train_loss: 5163.76 with loss1: 355.44, loss2: 42.61 and loss3: 4765.72\n",
      "Epoch [1904], train_loss: 5162.74 with loss1: 354.67, loss2: 42.40 and loss3: 4765.67\n",
      "Epoch [1905], train_loss: 5164.36 with loss1: 356.30, loss2: 42.43 and loss3: 4765.62\n",
      "Epoch [1906], train_loss: 5164.91 with loss1: 357.01, loss2: 42.33 and loss3: 4765.57\n",
      "Epoch [1907], train_loss: 5166.22 with loss1: 358.36, loss2: 42.33 and loss3: 4765.52\n",
      "Epoch [1908], train_loss: 5166.25 with loss1: 358.58, loss2: 42.20 and loss3: 4765.47\n",
      "Epoch [1909], train_loss: 5168.26 with loss1: 360.51, loss2: 42.32 and loss3: 4765.42\n",
      "Epoch [1910], train_loss: 5167.99 with loss1: 360.28, loss2: 42.33 and loss3: 4765.37\n",
      "Epoch [1911], train_loss: 5169.83 with loss1: 362.15, loss2: 42.36 and loss3: 4765.33\n",
      "Epoch [1912], train_loss: 5170.06 with loss1: 362.45, loss2: 42.33 and loss3: 4765.28\n",
      "Epoch [1913], train_loss: 5172.45 with loss1: 364.80, loss2: 42.42 and loss3: 4765.23\n",
      "Epoch [1914], train_loss: 5170.48 with loss1: 363.03, loss2: 42.28 and loss3: 4765.18\n",
      "Epoch [1915], train_loss: 5173.53 with loss1: 366.02, loss2: 42.38 and loss3: 4765.13\n",
      "Epoch [1916], train_loss: 5173.11 with loss1: 365.68, loss2: 42.35 and loss3: 4765.08\n",
      "Epoch [1917], train_loss: 5176.65 with loss1: 369.22, loss2: 42.40 and loss3: 4765.03\n",
      "Epoch [1918], train_loss: 5176.46 with loss1: 369.26, loss2: 42.22 and loss3: 4764.98\n",
      "Epoch [1919], train_loss: 5179.70 with loss1: 372.25, loss2: 42.51 and loss3: 4764.93\n",
      "Epoch [1920], train_loss: 5178.84 with loss1: 371.69, loss2: 42.27 and loss3: 4764.88\n",
      "Epoch [1921], train_loss: 5182.01 with loss1: 374.80, loss2: 42.38 and loss3: 4764.83\n",
      "Epoch [1922], train_loss: 5180.82 with loss1: 373.84, loss2: 42.19 and loss3: 4764.79\n",
      "Epoch [1923], train_loss: 5185.98 with loss1: 378.50, loss2: 42.74 and loss3: 4764.74\n",
      "Epoch [1924], train_loss: 5184.90 with loss1: 378.04, loss2: 42.18 and loss3: 4764.69\n",
      "Epoch [1925], train_loss: 5188.33 with loss1: 381.35, loss2: 42.34 and loss3: 4764.64\n",
      "Epoch [1926], train_loss: 5186.06 with loss1: 379.34, loss2: 42.13 and loss3: 4764.59\n",
      "Epoch [1927], train_loss: 5191.08 with loss1: 384.23, loss2: 42.31 and loss3: 4764.54\n",
      "Epoch [1928], train_loss: 5189.72 with loss1: 382.98, loss2: 42.25 and loss3: 4764.49\n",
      "Epoch [1929], train_loss: 5193.57 with loss1: 386.84, loss2: 42.29 and loss3: 4764.44\n",
      "Epoch [1930], train_loss: 5190.80 with loss1: 384.29, loss2: 42.12 and loss3: 4764.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1931], train_loss: 5196.49 with loss1: 389.88, loss2: 42.26 and loss3: 4764.34\n",
      "Epoch [1932], train_loss: 5194.25 with loss1: 387.73, loss2: 42.22 and loss3: 4764.29\n",
      "Epoch [1933], train_loss: 5198.68 with loss1: 392.07, loss2: 42.37 and loss3: 4764.25\n",
      "Epoch [1934], train_loss: 5195.74 with loss1: 389.45, loss2: 42.09 and loss3: 4764.20\n",
      "Epoch [1935], train_loss: 5200.94 with loss1: 394.52, loss2: 42.28 and loss3: 4764.15\n",
      "Epoch [1936], train_loss: 5196.52 with loss1: 390.47, loss2: 41.95 and loss3: 4764.10\n",
      "Epoch [1937], train_loss: 5200.52 with loss1: 394.18, loss2: 42.30 and loss3: 4764.05\n",
      "Epoch [1938], train_loss: 5197.03 with loss1: 390.92, loss2: 42.11 and loss3: 4764.00\n",
      "Epoch [1939], train_loss: 5200.67 with loss1: 394.43, loss2: 42.29 and loss3: 4763.95\n",
      "Epoch [1940], train_loss: 5195.78 with loss1: 390.01, loss2: 41.87 and loss3: 4763.90\n",
      "Epoch [1941], train_loss: 5199.07 with loss1: 392.91, loss2: 42.31 and loss3: 4763.85\n",
      "Epoch [1942], train_loss: 5195.94 with loss1: 390.05, loss2: 42.09 and loss3: 4763.80\n",
      "Epoch [1943], train_loss: 5198.82 with loss1: 392.91, loss2: 42.15 and loss3: 4763.75\n",
      "Epoch [1944], train_loss: 5193.35 with loss1: 387.62, loss2: 42.02 and loss3: 4763.70\n",
      "Epoch [1945], train_loss: 5196.48 with loss1: 390.58, loss2: 42.25 and loss3: 4763.65\n",
      "Epoch [1946], train_loss: 5191.90 with loss1: 386.26, loss2: 42.03 and loss3: 4763.61\n",
      "Epoch [1947], train_loss: 5192.99 with loss1: 387.24, loss2: 42.19 and loss3: 4763.56\n",
      "Epoch [1948], train_loss: 5188.41 with loss1: 382.86, loss2: 42.04 and loss3: 4763.51\n",
      "Epoch [1949], train_loss: 5189.13 with loss1: 383.35, loss2: 42.33 and loss3: 4763.46\n",
      "Epoch [1950], train_loss: 5186.01 with loss1: 380.48, loss2: 42.13 and loss3: 4763.41\n",
      "Epoch [1951], train_loss: 5186.82 with loss1: 381.39, loss2: 42.07 and loss3: 4763.36\n",
      "Epoch [1952], train_loss: 5183.11 with loss1: 377.53, loss2: 42.27 and loss3: 4763.31\n",
      "Epoch [1953], train_loss: 5184.06 with loss1: 378.60, loss2: 42.19 and loss3: 4763.26\n",
      "Epoch [1954], train_loss: 5181.40 with loss1: 376.03, loss2: 42.15 and loss3: 4763.21\n",
      "Epoch [1955], train_loss: 5181.03 with loss1: 375.55, loss2: 42.31 and loss3: 4763.16\n",
      "Epoch [1956], train_loss: 5177.06 with loss1: 372.00, loss2: 41.94 and loss3: 4763.11\n",
      "Epoch [1957], train_loss: 5178.23 with loss1: 373.14, loss2: 42.02 and loss3: 4763.07\n",
      "Epoch [1958], train_loss: 5174.79 with loss1: 369.89, loss2: 41.88 and loss3: 4763.02\n",
      "Epoch [1959], train_loss: 5176.05 with loss1: 370.94, loss2: 42.14 and loss3: 4762.97\n",
      "Epoch [1960], train_loss: 5173.28 with loss1: 368.38, loss2: 41.97 and loss3: 4762.92\n",
      "Epoch [1961], train_loss: 5174.14 with loss1: 369.22, loss2: 42.05 and loss3: 4762.87\n",
      "Epoch [1962], train_loss: 5172.43 with loss1: 367.52, loss2: 42.08 and loss3: 4762.82\n",
      "Epoch [1963], train_loss: 5173.69 with loss1: 368.64, loss2: 42.27 and loss3: 4762.77\n",
      "Epoch [1964], train_loss: 5169.77 with loss1: 365.19, loss2: 41.86 and loss3: 4762.72\n",
      "Epoch [1965], train_loss: 5170.83 with loss1: 366.22, loss2: 41.93 and loss3: 4762.67\n",
      "Epoch [1966], train_loss: 5170.28 with loss1: 365.56, loss2: 42.10 and loss3: 4762.62\n",
      "Epoch [1967], train_loss: 5170.44 with loss1: 365.88, loss2: 41.99 and loss3: 4762.57\n",
      "Epoch [1968], train_loss: 5168.62 with loss1: 364.17, loss2: 41.92 and loss3: 4762.53\n",
      "Epoch [1969], train_loss: 5169.27 with loss1: 364.91, loss2: 41.89 and loss3: 4762.48\n",
      "Epoch [1970], train_loss: 5166.72 with loss1: 362.48, loss2: 41.82 and loss3: 4762.43\n",
      "Epoch [1971], train_loss: 5168.66 with loss1: 364.17, loss2: 42.11 and loss3: 4762.38\n",
      "Epoch [1972], train_loss: 5167.26 with loss1: 363.07, loss2: 41.86 and loss3: 4762.33\n",
      "Epoch [1973], train_loss: 5167.96 with loss1: 363.76, loss2: 41.92 and loss3: 4762.28\n",
      "Epoch [1974], train_loss: 5166.83 with loss1: 362.64, loss2: 41.96 and loss3: 4762.23\n",
      "Epoch [1975], train_loss: 5167.72 with loss1: 363.60, loss2: 41.94 and loss3: 4762.18\n",
      "Epoch [1976], train_loss: 5165.69 with loss1: 361.68, loss2: 41.87 and loss3: 4762.13\n",
      "Epoch [1977], train_loss: 5166.98 with loss1: 363.00, loss2: 41.89 and loss3: 4762.08\n",
      "Epoch [1978], train_loss: 5166.29 with loss1: 362.37, loss2: 41.88 and loss3: 4762.03\n",
      "Epoch [1979], train_loss: 5168.75 with loss1: 364.66, loss2: 42.10 and loss3: 4761.98\n",
      "Epoch [1980], train_loss: 5167.26 with loss1: 363.05, loss2: 42.27 and loss3: 4761.94\n",
      "Epoch [1981], train_loss: 5167.25 with loss1: 363.53, loss2: 41.83 and loss3: 4761.89\n",
      "Epoch [1982], train_loss: 5165.84 with loss1: 362.32, loss2: 41.69 and loss3: 4761.84\n",
      "Epoch [1983], train_loss: 5168.52 with loss1: 364.89, loss2: 41.85 and loss3: 4761.79\n",
      "Epoch [1984], train_loss: 5167.49 with loss1: 363.97, loss2: 41.78 and loss3: 4761.74\n",
      "Epoch [1985], train_loss: 5168.54 with loss1: 364.62, loss2: 42.23 and loss3: 4761.69\n",
      "Epoch [1986], train_loss: 5166.77 with loss1: 363.45, loss2: 41.67 and loss3: 4761.64\n",
      "Epoch [1987], train_loss: 5168.46 with loss1: 365.01, loss2: 41.86 and loss3: 4761.59\n",
      "Epoch [1988], train_loss: 5167.81 with loss1: 364.48, loss2: 41.79 and loss3: 4761.54\n",
      "Epoch [1989], train_loss: 5170.60 with loss1: 366.89, loss2: 42.22 and loss3: 4761.49\n",
      "Epoch [1990], train_loss: 5168.52 with loss1: 365.32, loss2: 41.76 and loss3: 4761.44\n",
      "Epoch [1991], train_loss: 5170.09 with loss1: 366.96, loss2: 41.74 and loss3: 4761.40\n",
      "Epoch [1992], train_loss: 5168.59 with loss1: 365.51, loss2: 41.73 and loss3: 4761.35\n",
      "Epoch [1993], train_loss: 5171.12 with loss1: 367.98, loss2: 41.85 and loss3: 4761.30\n",
      "Epoch [1994], train_loss: 5170.05 with loss1: 366.57, loss2: 42.23 and loss3: 4761.25\n",
      "Epoch [1995], train_loss: 5170.88 with loss1: 367.72, loss2: 41.97 and loss3: 4761.20\n",
      "Epoch [1996], train_loss: 5169.67 with loss1: 366.76, loss2: 41.75 and loss3: 4761.15\n",
      "Epoch [1997], train_loss: 5171.50 with loss1: 368.52, loss2: 41.87 and loss3: 4761.10\n",
      "Epoch [1998], train_loss: 5169.08 with loss1: 366.40, loss2: 41.63 and loss3: 4761.05\n",
      "Epoch [1999], train_loss: 5170.73 with loss1: 367.95, loss2: 41.78 and loss3: 4761.00\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1., lamb=1.\n",
    "# model version 3.5\n",
    "# with loss2 and loss3\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history = fit(epochs=2000, lr=5e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8493f8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 411619.00 with loss1: 363459.47, loss2: 550.01 and loss3: 47609.53\n",
      "Epoch [1], train_loss: 266158.97 with loss1: 218017.52, loss2: 536.78 and loss3: 47604.67\n",
      "Epoch [2], train_loss: 187249.59 with loss1: 139064.62, loss2: 585.38 and loss3: 47599.60\n",
      "Epoch [3], train_loss: 113174.75 with loss1: 64740.56, loss2: 839.46 and loss3: 47594.73\n",
      "Epoch [4], train_loss: 83393.62 with loss1: 35176.15, loss2: 627.85 and loss3: 47589.62\n",
      "Epoch [5], train_loss: 74817.36 with loss1: 26591.66, loss2: 640.95 and loss3: 47584.76\n",
      "Epoch [6], train_loss: 69917.67 with loss1: 21653.63, loss2: 684.34 and loss3: 47579.71\n",
      "Epoch [7], train_loss: 66237.04 with loss1: 17969.59, loss2: 692.61 and loss3: 47574.84\n",
      "Epoch [8], train_loss: 63233.22 with loss1: 14931.50, loss2: 731.91 and loss3: 47569.81\n",
      "Epoch [9], train_loss: 61371.78 with loss1: 13050.34, loss2: 756.53 and loss3: 47564.92\n",
      "Epoch [10], train_loss: 60069.26 with loss1: 11736.72, loss2: 772.60 and loss3: 47559.94\n",
      "Epoch [11], train_loss: 59050.37 with loss1: 10675.92, loss2: 819.42 and loss3: 47555.03\n",
      "Epoch [12], train_loss: 58202.64 with loss1: 9830.04, loss2: 822.53 and loss3: 47550.07\n",
      "Epoch [13], train_loss: 57519.70 with loss1: 9115.51, loss2: 859.03 and loss3: 47545.16\n",
      "Epoch [14], train_loss: 56928.55 with loss1: 8496.85, loss2: 891.49 and loss3: 47540.21\n",
      "Epoch [15], train_loss: 56522.61 with loss1: 8090.69, loss2: 896.63 and loss3: 47535.29\n",
      "Epoch [16], train_loss: 56219.38 with loss1: 7795.93, loss2: 893.10 and loss3: 47530.35\n",
      "Epoch [17], train_loss: 55993.44 with loss1: 7567.68, loss2: 900.32 and loss3: 47525.43\n",
      "Epoch [18], train_loss: 56184.52 with loss1: 7764.28, loss2: 899.74 and loss3: 47520.51\n",
      "Epoch [19], train_loss: 56854.89 with loss1: 8420.41, loss2: 918.90 and loss3: 47515.59\n",
      "Epoch [20], train_loss: 58760.38 with loss1: 10344.56, loss2: 905.16 and loss3: 47510.66\n",
      "Epoch [21], train_loss: 57469.71 with loss1: 9067.11, loss2: 896.86 and loss3: 47505.74\n",
      "Epoch [22], train_loss: 57329.16 with loss1: 8966.67, loss2: 861.68 and loss3: 47500.80\n",
      "Epoch [23], train_loss: 56068.46 with loss1: 7705.44, loss2: 867.14 and loss3: 47495.89\n",
      "Epoch [24], train_loss: 55694.68 with loss1: 7362.06, loss2: 841.63 and loss3: 47490.98\n",
      "Epoch [25], train_loss: 55228.04 with loss1: 6915.51, loss2: 826.47 and loss3: 47486.05\n",
      "Epoch [26], train_loss: 55172.32 with loss1: 6883.81, loss2: 807.36 and loss3: 47481.14\n",
      "Epoch [27], train_loss: 55016.55 with loss1: 6726.85, loss2: 813.47 and loss3: 47476.23\n",
      "Epoch [28], train_loss: 55069.87 with loss1: 6799.74, loss2: 798.81 and loss3: 47471.31\n",
      "Epoch [29], train_loss: 54871.70 with loss1: 6606.18, loss2: 799.12 and loss3: 47466.40\n",
      "Epoch [30], train_loss: 55011.09 with loss1: 6759.33, loss2: 790.28 and loss3: 47461.48\n",
      "Epoch [31], train_loss: 54752.46 with loss1: 6512.86, loss2: 783.04 and loss3: 47456.57\n",
      "Epoch [32], train_loss: 54857.12 with loss1: 6645.90, loss2: 759.58 and loss3: 47451.65\n",
      "Epoch [33], train_loss: 54548.06 with loss1: 6339.20, loss2: 762.10 and loss3: 47446.75\n",
      "Epoch [34], train_loss: 54633.05 with loss1: 6442.61, loss2: 748.60 and loss3: 47441.84\n",
      "Epoch [35], train_loss: 54379.29 with loss1: 6194.17, loss2: 748.20 and loss3: 47436.92\n",
      "Epoch [36], train_loss: 54437.95 with loss1: 6272.88, loss2: 733.06 and loss3: 47432.02\n",
      "Epoch [37], train_loss: 54233.31 with loss1: 6073.24, loss2: 732.96 and loss3: 47427.11\n",
      "Epoch [38], train_loss: 54305.41 with loss1: 6163.67, loss2: 719.53 and loss3: 47422.20\n",
      "Epoch [39], train_loss: 54069.43 with loss1: 5926.77, loss2: 725.38 and loss3: 47417.29\n",
      "Epoch [40], train_loss: 54130.30 with loss1: 6021.15, loss2: 696.77 and loss3: 47412.38\n",
      "Epoch [41], train_loss: 53932.35 with loss1: 5818.03, loss2: 706.84 and loss3: 47407.48\n",
      "Epoch [42], train_loss: 53983.66 with loss1: 5896.56, loss2: 684.53 and loss3: 47402.57\n",
      "Epoch [43], train_loss: 53844.96 with loss1: 5749.47, loss2: 697.82 and loss3: 47397.67\n",
      "Epoch [44], train_loss: 53922.86 with loss1: 5841.99, loss2: 688.11 and loss3: 47392.76\n",
      "Epoch [45], train_loss: 53748.16 with loss1: 5669.42, loss2: 690.88 and loss3: 47387.86\n",
      "Epoch [46], train_loss: 53809.01 with loss1: 5751.57, loss2: 674.50 and loss3: 47382.95\n",
      "Epoch [47], train_loss: 53640.17 with loss1: 5586.99, loss2: 675.13 and loss3: 47378.05\n",
      "Epoch [48], train_loss: 53674.88 with loss1: 5641.11, loss2: 660.63 and loss3: 47373.14\n",
      "Epoch [49], train_loss: 53508.52 with loss1: 5475.70, loss2: 664.58 and loss3: 47368.24\n",
      "Epoch [50], train_loss: 53548.41 with loss1: 5533.99, loss2: 651.08 and loss3: 47363.34\n",
      "Epoch [51], train_loss: 53374.30 with loss1: 5370.13, loss2: 645.73 and loss3: 47358.44\n",
      "Epoch [52], train_loss: 53465.58 with loss1: 5470.67, loss2: 641.37 and loss3: 47353.54\n",
      "Epoch [53], train_loss: 53314.88 with loss1: 5317.19, loss2: 649.05 and loss3: 47348.64\n",
      "Epoch [54], train_loss: 53381.80 with loss1: 5403.47, loss2: 634.60 and loss3: 47343.73\n",
      "Epoch [55], train_loss: 53214.20 with loss1: 5248.38, loss2: 626.98 and loss3: 47338.84\n",
      "Epoch [56], train_loss: 53287.16 with loss1: 5327.14, loss2: 626.08 and loss3: 47333.94\n",
      "Epoch [57], train_loss: 53169.28 with loss1: 5210.31, loss2: 629.94 and loss3: 47329.03\n",
      "Epoch [58], train_loss: 53219.55 with loss1: 5281.52, loss2: 613.89 and loss3: 47324.14\n",
      "Epoch [59], train_loss: 53093.50 with loss1: 5166.92, loss2: 607.34 and loss3: 47319.24\n",
      "Epoch [60], train_loss: 53155.99 with loss1: 5230.22, loss2: 611.44 and loss3: 47314.34\n",
      "Epoch [61], train_loss: 52986.08 with loss1: 5077.46, loss2: 599.16 and loss3: 47309.45\n",
      "Epoch [62], train_loss: 53085.84 with loss1: 5179.61, loss2: 601.68 and loss3: 47304.55\n",
      "Epoch [63], train_loss: 52916.25 with loss1: 5026.34, loss2: 590.25 and loss3: 47299.66\n",
      "Epoch [64], train_loss: 52993.43 with loss1: 5107.05, loss2: 591.62 and loss3: 47294.76\n",
      "Epoch [65], train_loss: 52857.64 with loss1: 4983.70, loss2: 584.07 and loss3: 47289.87\n",
      "Epoch [66], train_loss: 52912.46 with loss1: 5039.60, loss2: 587.89 and loss3: 47284.97\n",
      "Epoch [67], train_loss: 52776.82 with loss1: 4924.61, loss2: 572.13 and loss3: 47280.08\n",
      "Epoch [68], train_loss: 52839.19 with loss1: 4983.29, loss2: 580.70 and loss3: 47275.19\n",
      "Epoch [69], train_loss: 52688.05 with loss1: 4853.15, loss2: 564.61 and loss3: 47270.29\n",
      "Epoch [70], train_loss: 52763.52 with loss1: 4930.11, loss2: 568.01 and loss3: 47265.41\n",
      "Epoch [71], train_loss: 52645.07 with loss1: 4826.99, loss2: 557.58 and loss3: 47260.51\n",
      "Epoch [72], train_loss: 52689.84 with loss1: 4870.19, loss2: 564.02 and loss3: 47255.62\n",
      "Epoch [73], train_loss: 52567.40 with loss1: 4768.68, loss2: 548.00 and loss3: 47250.73\n",
      "Epoch [74], train_loss: 52607.43 with loss1: 4816.12, loss2: 545.47 and loss3: 47245.84\n",
      "Epoch [75], train_loss: 52512.04 with loss1: 4729.40, loss2: 541.70 and loss3: 47240.95\n",
      "Epoch [76], train_loss: 52582.04 with loss1: 4799.15, loss2: 546.82 and loss3: 47236.07\n",
      "Epoch [77], train_loss: 52434.98 with loss1: 4672.66, loss2: 531.15 and loss3: 47231.17\n",
      "Epoch [78], train_loss: 52542.69 with loss1: 4769.94, loss2: 546.46 and loss3: 47226.29\n",
      "Epoch [79], train_loss: 52394.57 with loss1: 4646.42, loss2: 526.76 and loss3: 47221.39\n",
      "Epoch [80], train_loss: 52448.44 with loss1: 4702.20, loss2: 529.73 and loss3: 47216.51\n",
      "Epoch [81], train_loss: 52332.02 with loss1: 4603.39, loss2: 517.02 and loss3: 47211.61\n",
      "Epoch [82], train_loss: 52399.12 with loss1: 4668.90, loss2: 523.50 and loss3: 47206.73\n",
      "Epoch [83], train_loss: 52256.92 with loss1: 4544.05, loss2: 511.04 and loss3: 47201.84\n",
      "Epoch [84], train_loss: 52304.40 with loss1: 4584.27, loss2: 523.18 and loss3: 47196.95\n",
      "Epoch [85], train_loss: 52157.78 with loss1: 4462.10, loss2: 503.62 and loss3: 47192.06\n",
      "Epoch [86], train_loss: 52195.66 with loss1: 4500.22, loss2: 508.26 and loss3: 47187.18\n",
      "Epoch [87], train_loss: 52079.95 with loss1: 4397.92, loss2: 499.74 and loss3: 47182.29\n",
      "Epoch [88], train_loss: 52134.43 with loss1: 4444.90, loss2: 512.12 and loss3: 47177.40\n",
      "Epoch [89], train_loss: 52038.58 with loss1: 4368.84, loss2: 497.22 and loss3: 47172.52\n",
      "Epoch [90], train_loss: 52103.23 with loss1: 4431.31, loss2: 504.29 and loss3: 47167.62\n",
      "Epoch [91], train_loss: 51975.47 with loss1: 4325.12, loss2: 487.61 and loss3: 47162.74\n",
      "Epoch [92], train_loss: 52053.17 with loss1: 4394.83, loss2: 500.49 and loss3: 47157.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [93], train_loss: 51906.13 with loss1: 4272.65, loss2: 480.50 and loss3: 47152.98\n",
      "Epoch [94], train_loss: 51989.13 with loss1: 4354.85, loss2: 486.19 and loss3: 47148.09\n",
      "Epoch [95], train_loss: 51843.12 with loss1: 4217.88, loss2: 482.04 and loss3: 47143.21\n",
      "Epoch [96], train_loss: 51913.59 with loss1: 4287.82, loss2: 487.45 and loss3: 47138.33\n",
      "Epoch [97], train_loss: 51777.40 with loss1: 4175.71, loss2: 468.24 and loss3: 47133.45\n",
      "Epoch [98], train_loss: 51866.68 with loss1: 4252.02, loss2: 486.09 and loss3: 47128.56\n",
      "Epoch [99], train_loss: 51655.41 with loss1: 4061.36, loss2: 470.36 and loss3: 47123.69\n",
      "Epoch [100], train_loss: 51697.19 with loss1: 4101.36, loss2: 477.03 and loss3: 47118.80\n",
      "Epoch [101], train_loss: 51503.18 with loss1: 3921.25, loss2: 468.00 and loss3: 47113.93\n",
      "Epoch [102], train_loss: 51512.01 with loss1: 3929.83, loss2: 473.15 and loss3: 47109.03\n",
      "Epoch [103], train_loss: 51412.37 with loss1: 3842.96, loss2: 465.25 and loss3: 47104.16\n",
      "Epoch [104], train_loss: 51427.90 with loss1: 3857.06, loss2: 471.57 and loss3: 47099.27\n",
      "Epoch [105], train_loss: 51353.40 with loss1: 3801.31, loss2: 457.70 and loss3: 47094.39\n",
      "Epoch [106], train_loss: 51402.60 with loss1: 3845.06, loss2: 468.02 and loss3: 47089.52\n",
      "Epoch [107], train_loss: 51365.36 with loss1: 3825.28, loss2: 455.43 and loss3: 47084.64\n",
      "Epoch [108], train_loss: 51442.15 with loss1: 3896.88, loss2: 465.51 and loss3: 47079.76\n",
      "Epoch [109], train_loss: 51394.60 with loss1: 3857.20, loss2: 462.50 and loss3: 47074.89\n",
      "Epoch [110], train_loss: 51491.44 with loss1: 3958.60, loss2: 462.83 and loss3: 47070.00\n",
      "Epoch [111], train_loss: 51380.60 with loss1: 3866.23, loss2: 449.24 and loss3: 47065.13\n",
      "Epoch [112], train_loss: 51522.20 with loss1: 4008.66, loss2: 453.29 and loss3: 47060.25\n",
      "Epoch [113], train_loss: 51363.97 with loss1: 3854.38, loss2: 454.21 and loss3: 47055.38\n",
      "Epoch [114], train_loss: 51442.16 with loss1: 3939.91, loss2: 451.75 and loss3: 47050.50\n",
      "Epoch [115], train_loss: 51276.47 with loss1: 3782.19, loss2: 448.65 and loss3: 47045.63\n",
      "Epoch [116], train_loss: 51383.37 with loss1: 3889.61, loss2: 453.00 and loss3: 47040.76\n",
      "Epoch [117], train_loss: 51195.19 with loss1: 3718.61, loss2: 440.69 and loss3: 47035.89\n",
      "Epoch [118], train_loss: 51286.27 with loss1: 3809.24, loss2: 446.02 and loss3: 47031.01\n",
      "Epoch [119], train_loss: 51115.43 with loss1: 3648.54, loss2: 440.75 and loss3: 47026.14\n",
      "Epoch [120], train_loss: 51188.27 with loss1: 3725.68, loss2: 441.32 and loss3: 47021.27\n",
      "Epoch [121], train_loss: 51038.93 with loss1: 3583.53, loss2: 438.99 and loss3: 47016.40\n",
      "Epoch [122], train_loss: 51063.08 with loss1: 3611.94, loss2: 439.61 and loss3: 47011.52\n",
      "Epoch [123], train_loss: 50940.91 with loss1: 3495.83, loss2: 438.41 and loss3: 47006.66\n",
      "Epoch [124], train_loss: 50964.46 with loss1: 3521.09, loss2: 441.59 and loss3: 47001.78\n",
      "Epoch [125], train_loss: 50848.71 with loss1: 3422.51, loss2: 429.29 and loss3: 46996.91\n",
      "Epoch [126], train_loss: 50895.14 with loss1: 3468.05, loss2: 435.06 and loss3: 46992.04\n",
      "Epoch [127], train_loss: 50780.77 with loss1: 3366.03, loss2: 427.56 and loss3: 46987.17\n",
      "Epoch [128], train_loss: 50807.98 with loss1: 3393.48, loss2: 432.20 and loss3: 46982.30\n",
      "Epoch [129], train_loss: 50718.72 with loss1: 3317.11, loss2: 424.18 and loss3: 46977.43\n",
      "Epoch [130], train_loss: 50756.95 with loss1: 3353.19, loss2: 431.19 and loss3: 46972.57\n",
      "Epoch [131], train_loss: 50655.35 with loss1: 3265.15, loss2: 422.51 and loss3: 46967.69\n",
      "Epoch [132], train_loss: 50714.51 with loss1: 3325.72, loss2: 425.97 and loss3: 46962.82\n",
      "Epoch [133], train_loss: 50641.58 with loss1: 3257.82, loss2: 425.81 and loss3: 46957.95\n",
      "Epoch [134], train_loss: 50675.82 with loss1: 3296.70, loss2: 426.04 and loss3: 46953.09\n",
      "Epoch [135], train_loss: 50595.02 with loss1: 3229.75, loss2: 417.06 and loss3: 46948.21\n",
      "Epoch [136], train_loss: 50650.09 with loss1: 3281.53, loss2: 425.20 and loss3: 46943.36\n",
      "Epoch [137], train_loss: 50578.20 with loss1: 3218.66, loss2: 421.05 and loss3: 46938.48\n",
      "Epoch [138], train_loss: 50638.52 with loss1: 3280.62, loss2: 424.29 and loss3: 46933.62\n",
      "Epoch [139], train_loss: 50549.03 with loss1: 3207.75, loss2: 412.53 and loss3: 46928.75\n",
      "Epoch [140], train_loss: 50624.05 with loss1: 3278.13, loss2: 422.03 and loss3: 46923.89\n",
      "Epoch [141], train_loss: 50514.32 with loss1: 3179.14, loss2: 416.16 and loss3: 46919.02\n",
      "Epoch [142], train_loss: 50570.32 with loss1: 3236.62, loss2: 419.55 and loss3: 46914.15\n",
      "Epoch [143], train_loss: 50479.20 with loss1: 3159.41, loss2: 410.48 and loss3: 46909.30\n",
      "Epoch [144], train_loss: 50535.13 with loss1: 3212.05, loss2: 418.66 and loss3: 46904.42\n",
      "Epoch [145], train_loss: 50436.27 with loss1: 3124.01, loss2: 412.69 and loss3: 46899.57\n",
      "Epoch [146], train_loss: 50479.63 with loss1: 3173.51, loss2: 411.42 and loss3: 46894.70\n",
      "Epoch [147], train_loss: 50394.62 with loss1: 3097.30, loss2: 407.47 and loss3: 46889.84\n",
      "Epoch [148], train_loss: 50428.27 with loss1: 3132.39, loss2: 410.90 and loss3: 46884.98\n",
      "Epoch [149], train_loss: 50336.65 with loss1: 3049.22, loss2: 407.31 and loss3: 46880.12\n",
      "Epoch [150], train_loss: 50414.67 with loss1: 3130.84, loss2: 408.57 and loss3: 46875.26\n",
      "Epoch [151], train_loss: 50329.50 with loss1: 3054.37, loss2: 404.74 and loss3: 46870.40\n",
      "Epoch [152], train_loss: 50390.39 with loss1: 3113.99, loss2: 410.85 and loss3: 46865.54\n",
      "Epoch [153], train_loss: 50294.69 with loss1: 3032.03, loss2: 401.98 and loss3: 46860.68\n",
      "Epoch [154], train_loss: 50352.51 with loss1: 3089.24, loss2: 407.45 and loss3: 46855.82\n",
      "Epoch [155], train_loss: 50245.70 with loss1: 2992.13, loss2: 402.60 and loss3: 46850.97\n",
      "Epoch [156], train_loss: 50312.13 with loss1: 3060.36, loss2: 405.65 and loss3: 46846.11\n",
      "Epoch [157], train_loss: 50189.79 with loss1: 2944.97, loss2: 403.56 and loss3: 46841.26\n",
      "Epoch [158], train_loss: 50234.06 with loss1: 2996.78, loss2: 400.88 and loss3: 46836.41\n",
      "Epoch [159], train_loss: 50100.49 with loss1: 2870.60, loss2: 398.35 and loss3: 46831.54\n",
      "Epoch [160], train_loss: 50129.19 with loss1: 2902.05, loss2: 400.44 and loss3: 46826.70\n",
      "Epoch [161], train_loss: 50039.03 with loss1: 2819.35, loss2: 397.84 and loss3: 46821.83\n",
      "Epoch [162], train_loss: 50055.86 with loss1: 2839.50, loss2: 399.37 and loss3: 46816.98\n",
      "Epoch [163], train_loss: 49979.09 with loss1: 2769.88, loss2: 397.09 and loss3: 46812.13\n",
      "Epoch [164], train_loss: 49996.52 with loss1: 2792.10, loss2: 397.16 and loss3: 46807.27\n",
      "Epoch [165], train_loss: 49947.86 with loss1: 2751.30, loss2: 394.14 and loss3: 46802.42\n",
      "Epoch [166], train_loss: 49978.38 with loss1: 2782.76, loss2: 398.06 and loss3: 46797.56\n",
      "Epoch [167], train_loss: 49925.09 with loss1: 2742.00, loss2: 390.38 and loss3: 46792.71\n",
      "Epoch [168], train_loss: 49980.20 with loss1: 2795.45, loss2: 396.90 and loss3: 46787.85\n",
      "Epoch [169], train_loss: 49927.09 with loss1: 2751.76, loss2: 392.33 and loss3: 46783.00\n",
      "Epoch [170], train_loss: 49965.04 with loss1: 2790.41, loss2: 396.49 and loss3: 46778.14\n",
      "Epoch [171], train_loss: 49911.19 with loss1: 2746.07, loss2: 391.82 and loss3: 46773.30\n",
      "Epoch [172], train_loss: 49943.65 with loss1: 2783.98, loss2: 391.23 and loss3: 46768.44\n",
      "Epoch [173], train_loss: 49890.79 with loss1: 2741.25, loss2: 385.95 and loss3: 46763.59\n",
      "Epoch [174], train_loss: 49944.57 with loss1: 2795.65, loss2: 390.19 and loss3: 46758.73\n",
      "Epoch [175], train_loss: 49880.04 with loss1: 2734.70, loss2: 391.46 and loss3: 46753.89\n",
      "Epoch [176], train_loss: 49943.49 with loss1: 2810.50, loss2: 383.97 and loss3: 46749.02\n",
      "Epoch [177], train_loss: 49875.04 with loss1: 2741.53, loss2: 389.32 and loss3: 46744.18\n",
      "Epoch [178], train_loss: 49939.32 with loss1: 2813.19, loss2: 386.81 and loss3: 46739.32\n",
      "Epoch [179], train_loss: 49833.80 with loss1: 2711.53, loss2: 387.80 and loss3: 46734.47\n",
      "Epoch [180], train_loss: 49881.43 with loss1: 2766.68, loss2: 385.13 and loss3: 46729.62\n",
      "Epoch [181], train_loss: 49794.24 with loss1: 2683.42, loss2: 386.05 and loss3: 46724.77\n",
      "Epoch [182], train_loss: 49852.00 with loss1: 2751.40, loss2: 380.68 and loss3: 46719.92\n",
      "Epoch [183], train_loss: 49786.54 with loss1: 2684.23, loss2: 387.23 and loss3: 46715.08\n",
      "Epoch [184], train_loss: 49870.58 with loss1: 2780.88, loss2: 379.48 and loss3: 46710.22\n",
      "Epoch [185], train_loss: 49776.43 with loss1: 2686.28, loss2: 384.77 and loss3: 46705.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [186], train_loss: 49838.91 with loss1: 2762.87, loss2: 375.52 and loss3: 46700.53\n",
      "Epoch [187], train_loss: 49746.40 with loss1: 2670.38, loss2: 380.34 and loss3: 46695.68\n",
      "Epoch [188], train_loss: 49815.61 with loss1: 2748.43, loss2: 376.34 and loss3: 46690.84\n",
      "Epoch [189], train_loss: 49720.62 with loss1: 2653.81, loss2: 380.83 and loss3: 46685.98\n",
      "Epoch [190], train_loss: 49792.79 with loss1: 2741.45, loss2: 370.20 and loss3: 46681.14\n",
      "Epoch [191], train_loss: 49710.95 with loss1: 2654.31, loss2: 380.34 and loss3: 46676.29\n",
      "Epoch [192], train_loss: 49796.15 with loss1: 2753.32, loss2: 371.38 and loss3: 46671.45\n",
      "Epoch [193], train_loss: 49696.27 with loss1: 2652.63, loss2: 377.04 and loss3: 46666.60\n",
      "Epoch [194], train_loss: 49750.28 with loss1: 2718.12, loss2: 370.40 and loss3: 46661.76\n",
      "Epoch [195], train_loss: 49657.52 with loss1: 2624.36, loss2: 376.26 and loss3: 46656.90\n",
      "Epoch [196], train_loss: 49668.12 with loss1: 2644.82, loss2: 371.22 and loss3: 46652.07\n",
      "Epoch [197], train_loss: 49577.63 with loss1: 2551.03, loss2: 379.38 and loss3: 46647.22\n",
      "Epoch [198], train_loss: 49562.34 with loss1: 2548.43, loss2: 371.54 and loss3: 46642.38\n",
      "Epoch [199], train_loss: 49489.78 with loss1: 2477.56, loss2: 374.69 and loss3: 46637.53\n",
      "Epoch [200], train_loss: 49475.94 with loss1: 2475.28, loss2: 367.97 and loss3: 46632.69\n",
      "Epoch [201], train_loss: 49407.19 with loss1: 2410.22, loss2: 369.13 and loss3: 46627.85\n",
      "Epoch [202], train_loss: 49407.00 with loss1: 2420.03, loss2: 363.97 and loss3: 46623.00\n",
      "Epoch [203], train_loss: 49370.02 with loss1: 2382.45, loss2: 369.41 and loss3: 46618.16\n",
      "Epoch [204], train_loss: 49362.96 with loss1: 2386.80, loss2: 362.84 and loss3: 46613.32\n",
      "Epoch [205], train_loss: 49335.12 with loss1: 2358.15, loss2: 368.49 and loss3: 46608.48\n",
      "Epoch [206], train_loss: 49343.94 with loss1: 2377.39, loss2: 362.92 and loss3: 46603.63\n",
      "Epoch [207], train_loss: 49308.37 with loss1: 2343.45, loss2: 366.12 and loss3: 46598.80\n",
      "Epoch [208], train_loss: 49326.50 with loss1: 2370.50, loss2: 362.05 and loss3: 46593.95\n",
      "Epoch [209], train_loss: 49298.04 with loss1: 2345.64, loss2: 363.28 and loss3: 46589.12\n",
      "Epoch [210], train_loss: 49319.21 with loss1: 2374.97, loss2: 359.96 and loss3: 46584.28\n",
      "Epoch [211], train_loss: 49294.85 with loss1: 2350.46, loss2: 364.95 and loss3: 46579.43\n",
      "Epoch [212], train_loss: 49316.38 with loss1: 2381.94, loss2: 359.83 and loss3: 46574.60\n",
      "Epoch [213], train_loss: 49294.64 with loss1: 2362.76, loss2: 362.12 and loss3: 46569.76\n",
      "Epoch [214], train_loss: 49352.26 with loss1: 2430.02, loss2: 357.32 and loss3: 46564.92\n",
      "Epoch [215], train_loss: 49311.49 with loss1: 2389.23, loss2: 362.19 and loss3: 46560.08\n",
      "Epoch [216], train_loss: 49384.50 with loss1: 2471.39, loss2: 357.87 and loss3: 46555.24\n",
      "Epoch [217], train_loss: 49344.88 with loss1: 2433.52, loss2: 360.95 and loss3: 46550.41\n",
      "Epoch [218], train_loss: 49437.44 with loss1: 2535.30, loss2: 356.58 and loss3: 46545.56\n",
      "Epoch [219], train_loss: 49380.83 with loss1: 2485.72, loss2: 354.38 and loss3: 46540.73\n",
      "Epoch [220], train_loss: 49508.02 with loss1: 2611.34, loss2: 360.78 and loss3: 46535.89\n",
      "Epoch [221], train_loss: 49414.32 with loss1: 2529.24, loss2: 354.03 and loss3: 46531.05\n",
      "Epoch [222], train_loss: 49541.61 with loss1: 2658.72, loss2: 356.68 and loss3: 46526.21\n",
      "Epoch [223], train_loss: 49422.00 with loss1: 2553.21, loss2: 347.41 and loss3: 46521.38\n",
      "Epoch [224], train_loss: 49524.68 with loss1: 2654.21, loss2: 353.93 and loss3: 46516.54\n",
      "Epoch [225], train_loss: 49377.96 with loss1: 2520.06, loss2: 346.20 and loss3: 46511.71\n",
      "Epoch [226], train_loss: 49438.84 with loss1: 2576.14, loss2: 355.84 and loss3: 46506.87\n",
      "Epoch [227], train_loss: 49295.33 with loss1: 2448.01, loss2: 345.27 and loss3: 46502.04\n",
      "Epoch [228], train_loss: 49333.72 with loss1: 2481.44, loss2: 355.09 and loss3: 46497.20\n",
      "Epoch [229], train_loss: 49215.41 with loss1: 2380.56, loss2: 342.49 and loss3: 46492.36\n",
      "Epoch [230], train_loss: 49241.34 with loss1: 2401.56, loss2: 352.25 and loss3: 46487.52\n",
      "Epoch [231], train_loss: 49156.87 with loss1: 2330.74, loss2: 343.44 and loss3: 46482.70\n",
      "Epoch [232], train_loss: 49166.86 with loss1: 2337.52, loss2: 351.49 and loss3: 46477.85\n",
      "Epoch [233], train_loss: 49098.77 with loss1: 2283.22, loss2: 342.53 and loss3: 46473.02\n",
      "Epoch [234], train_loss: 49110.30 with loss1: 2295.08, loss2: 347.04 and loss3: 46468.18\n",
      "Epoch [235], train_loss: 49062.49 with loss1: 2259.86, loss2: 339.28 and loss3: 46463.35\n",
      "Epoch [236], train_loss: 49078.38 with loss1: 2274.84, loss2: 345.02 and loss3: 46458.52\n",
      "Epoch [237], train_loss: 49026.23 with loss1: 2230.74, loss2: 341.81 and loss3: 46453.68\n",
      "Epoch [238], train_loss: 49049.48 with loss1: 2257.51, loss2: 343.11 and loss3: 46448.85\n",
      "Epoch [239], train_loss: 49006.88 with loss1: 2222.86, loss2: 340.00 and loss3: 46444.02\n",
      "Epoch [240], train_loss: 49040.85 with loss1: 2262.79, loss2: 338.87 and loss3: 46439.19\n",
      "Epoch [241], train_loss: 49007.82 with loss1: 2238.00, loss2: 335.47 and loss3: 46434.36\n",
      "Epoch [242], train_loss: 49044.66 with loss1: 2274.30, loss2: 340.83 and loss3: 46429.52\n",
      "Epoch [243], train_loss: 48996.68 with loss1: 2236.49, loss2: 335.49 and loss3: 46424.70\n",
      "Epoch [244], train_loss: 49048.08 with loss1: 2289.20, loss2: 339.01 and loss3: 46419.88\n",
      "Epoch [245], train_loss: 48990.91 with loss1: 2239.75, loss2: 336.12 and loss3: 46415.04\n",
      "Epoch [246], train_loss: 49020.46 with loss1: 2271.63, loss2: 338.61 and loss3: 46410.21\n",
      "Epoch [247], train_loss: 48959.01 with loss1: 2218.10, loss2: 335.52 and loss3: 46405.39\n",
      "Epoch [248], train_loss: 49007.33 with loss1: 2269.44, loss2: 337.33 and loss3: 46400.55\n",
      "Epoch [249], train_loss: 48926.13 with loss1: 2198.30, loss2: 332.10 and loss3: 46395.73\n",
      "Epoch [250], train_loss: 48945.14 with loss1: 2218.96, loss2: 335.28 and loss3: 46390.90\n",
      "Epoch [251], train_loss: 48888.44 with loss1: 2167.80, loss2: 334.56 and loss3: 46386.07\n",
      "Epoch [252], train_loss: 48915.15 with loss1: 2198.84, loss2: 335.07 and loss3: 46381.25\n",
      "Epoch [253], train_loss: 48858.50 with loss1: 2151.50, loss2: 330.58 and loss3: 46376.42\n",
      "Epoch [254], train_loss: 48888.81 with loss1: 2184.90, loss2: 332.32 and loss3: 46371.59\n",
      "Epoch [255], train_loss: 48830.80 with loss1: 2133.34, loss2: 330.70 and loss3: 46366.77\n",
      "Epoch [256], train_loss: 48866.29 with loss1: 2172.50, loss2: 331.85 and loss3: 46361.93\n",
      "Epoch [257], train_loss: 48818.10 with loss1: 2131.64, loss2: 329.35 and loss3: 46357.11\n",
      "Epoch [258], train_loss: 48861.00 with loss1: 2180.26, loss2: 328.47 and loss3: 46352.28\n",
      "Epoch [259], train_loss: 48809.12 with loss1: 2132.03, loss2: 329.63 and loss3: 46347.46\n",
      "Epoch [260], train_loss: 48845.21 with loss1: 2174.97, loss2: 327.62 and loss3: 46342.62\n",
      "Epoch [261], train_loss: 48802.68 with loss1: 2137.19, loss2: 327.68 and loss3: 46337.81\n",
      "Epoch [262], train_loss: 48840.35 with loss1: 2182.13, loss2: 325.24 and loss3: 46332.98\n",
      "Epoch [263], train_loss: 48789.18 with loss1: 2134.23, loss2: 326.80 and loss3: 46328.16\n",
      "Epoch [264], train_loss: 48839.26 with loss1: 2192.26, loss2: 323.66 and loss3: 46323.34\n",
      "Epoch [265], train_loss: 48783.28 with loss1: 2138.38, loss2: 326.39 and loss3: 46318.51\n",
      "Epoch [266], train_loss: 48818.86 with loss1: 2182.74, loss2: 322.43 and loss3: 46313.69\n",
      "Epoch [267], train_loss: 48758.75 with loss1: 2122.92, loss2: 326.96 and loss3: 46308.87\n",
      "Epoch [268], train_loss: 48796.50 with loss1: 2169.82, loss2: 322.64 and loss3: 46304.04\n",
      "Epoch [269], train_loss: 48732.32 with loss1: 2110.93, loss2: 322.17 and loss3: 46299.22\n",
      "Epoch [270], train_loss: 48768.91 with loss1: 2153.68, loss2: 320.83 and loss3: 46294.40\n",
      "Epoch [271], train_loss: 48706.11 with loss1: 2092.89, loss2: 323.64 and loss3: 46289.58\n",
      "Epoch [272], train_loss: 48733.50 with loss1: 2127.79, loss2: 320.96 and loss3: 46284.76\n",
      "Epoch [273], train_loss: 48676.14 with loss1: 2074.79, loss2: 321.42 and loss3: 46279.93\n",
      "Epoch [274], train_loss: 48696.59 with loss1: 2104.89, loss2: 316.59 and loss3: 46275.12\n",
      "Epoch [275], train_loss: 48645.05 with loss1: 2053.47, loss2: 321.29 and loss3: 46270.29\n",
      "Epoch [276], train_loss: 48663.80 with loss1: 2083.20, loss2: 315.13 and loss3: 46265.48\n",
      "Epoch [277], train_loss: 48614.85 with loss1: 2036.72, loss2: 317.47 and loss3: 46260.66\n",
      "Epoch [278], train_loss: 48628.26 with loss1: 2058.64, loss2: 313.79 and loss3: 46255.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [279], train_loss: 48588.16 with loss1: 2019.14, loss2: 318.00 and loss3: 46251.02\n",
      "Epoch [280], train_loss: 48613.11 with loss1: 2051.32, loss2: 315.59 and loss3: 46246.20\n",
      "Epoch [281], train_loss: 48576.34 with loss1: 2018.22, loss2: 316.75 and loss3: 46241.37\n",
      "Epoch [282], train_loss: 48587.30 with loss1: 2037.42, loss2: 313.32 and loss3: 46236.56\n",
      "Epoch [283], train_loss: 48543.25 with loss1: 1998.50, loss2: 313.01 and loss3: 46231.74\n",
      "Epoch [284], train_loss: 48576.97 with loss1: 2035.42, loss2: 314.63 and loss3: 46226.92\n",
      "Epoch [285], train_loss: 48549.18 with loss1: 2013.14, loss2: 313.94 and loss3: 46222.10\n",
      "Epoch [286], train_loss: 48568.57 with loss1: 2038.30, loss2: 312.99 and loss3: 46217.29\n",
      "Epoch [287], train_loss: 48535.91 with loss1: 2010.74, loss2: 312.70 and loss3: 46212.46\n",
      "Epoch [288], train_loss: 48566.18 with loss1: 2049.94, loss2: 308.59 and loss3: 46207.65\n",
      "Epoch [289], train_loss: 48544.59 with loss1: 2030.20, loss2: 311.56 and loss3: 46202.83\n",
      "Epoch [290], train_loss: 48597.35 with loss1: 2087.47, loss2: 311.87 and loss3: 46198.01\n",
      "Epoch [291], train_loss: 48549.57 with loss1: 2046.62, loss2: 309.76 and loss3: 46193.20\n",
      "Epoch [292], train_loss: 48600.61 with loss1: 2105.48, loss2: 306.74 and loss3: 46188.38\n",
      "Epoch [293], train_loss: 48547.36 with loss1: 2053.19, loss2: 310.61 and loss3: 46183.56\n",
      "Epoch [294], train_loss: 48581.10 with loss1: 2093.17, loss2: 309.18 and loss3: 46178.74\n",
      "Epoch [295], train_loss: 48513.70 with loss1: 2030.68, loss2: 309.09 and loss3: 46173.93\n",
      "Epoch [296], train_loss: 48550.52 with loss1: 2075.09, loss2: 306.33 and loss3: 46169.10\n",
      "Epoch [297], train_loss: 48483.89 with loss1: 2011.31, loss2: 308.29 and loss3: 46164.30\n",
      "Epoch [298], train_loss: 48526.82 with loss1: 2060.77, loss2: 306.57 and loss3: 46159.47\n",
      "Epoch [299], train_loss: 48484.18 with loss1: 2022.08, loss2: 307.43 and loss3: 46154.67\n",
      "Epoch [300], train_loss: 48534.38 with loss1: 2078.85, loss2: 305.69 and loss3: 46149.85\n",
      "Epoch [301], train_loss: 48467.45 with loss1: 2018.86, loss2: 303.55 and loss3: 46145.04\n",
      "Epoch [302], train_loss: 48515.59 with loss1: 2070.29, loss2: 305.07 and loss3: 46140.22\n",
      "Epoch [303], train_loss: 48460.22 with loss1: 2019.07, loss2: 305.74 and loss3: 46135.41\n",
      "Epoch [304], train_loss: 48499.52 with loss1: 2065.72, loss2: 303.21 and loss3: 46130.59\n",
      "Epoch [305], train_loss: 48433.95 with loss1: 2004.47, loss2: 303.69 and loss3: 46125.78\n",
      "Epoch [306], train_loss: 48478.12 with loss1: 2053.66, loss2: 303.50 and loss3: 46120.96\n",
      "Epoch [307], train_loss: 48398.88 with loss1: 1981.36, loss2: 301.35 and loss3: 46116.16\n",
      "Epoch [308], train_loss: 48423.20 with loss1: 2008.83, loss2: 303.03 and loss3: 46111.34\n",
      "Epoch [309], train_loss: 48354.07 with loss1: 1948.11, loss2: 299.43 and loss3: 46106.53\n",
      "Epoch [310], train_loss: 48367.29 with loss1: 1964.94, loss2: 300.63 and loss3: 46101.72\n",
      "Epoch [311], train_loss: 48318.90 with loss1: 1922.73, loss2: 299.26 and loss3: 46096.91\n",
      "Epoch [312], train_loss: 48333.48 with loss1: 1942.37, loss2: 299.02 and loss3: 46092.10\n",
      "Epoch [313], train_loss: 48289.63 with loss1: 1905.65, loss2: 296.69 and loss3: 46087.29\n",
      "Epoch [314], train_loss: 48314.17 with loss1: 1932.69, loss2: 298.99 and loss3: 46082.48\n",
      "Epoch [315], train_loss: 48266.62 with loss1: 1891.22, loss2: 297.74 and loss3: 46077.66\n",
      "Epoch [316], train_loss: 48277.55 with loss1: 1907.58, loss2: 297.11 and loss3: 46072.86\n",
      "Epoch [317], train_loss: 48254.93 with loss1: 1888.97, loss2: 297.91 and loss3: 46068.05\n",
      "Epoch [318], train_loss: 48270.52 with loss1: 1912.49, loss2: 294.78 and loss3: 46063.25\n",
      "Epoch [319], train_loss: 48242.78 with loss1: 1887.64, loss2: 296.71 and loss3: 46058.43\n",
      "Epoch [320], train_loss: 48252.21 with loss1: 1903.47, loss2: 295.11 and loss3: 46053.63\n",
      "Epoch [321], train_loss: 48222.62 with loss1: 1879.57, loss2: 294.24 and loss3: 46048.81\n",
      "Epoch [322], train_loss: 48234.71 with loss1: 1896.57, loss2: 294.12 and loss3: 46044.02\n",
      "Epoch [323], train_loss: 48209.60 with loss1: 1878.81, loss2: 291.59 and loss3: 46039.20\n",
      "Epoch [324], train_loss: 48224.90 with loss1: 1897.90, loss2: 292.61 and loss3: 46034.39\n",
      "Epoch [325], train_loss: 48191.59 with loss1: 1871.11, loss2: 290.90 and loss3: 46029.58\n",
      "Epoch [326], train_loss: 48216.51 with loss1: 1900.82, loss2: 290.91 and loss3: 46024.78\n",
      "Epoch [327], train_loss: 48185.56 with loss1: 1875.13, loss2: 290.46 and loss3: 46019.97\n",
      "Epoch [328], train_loss: 48215.47 with loss1: 1908.40, loss2: 291.91 and loss3: 46015.16\n",
      "Epoch [329], train_loss: 48178.75 with loss1: 1879.04, loss2: 289.35 and loss3: 46010.36\n",
      "Epoch [330], train_loss: 48208.36 with loss1: 1913.60, loss2: 289.20 and loss3: 46005.55\n",
      "Epoch [331], train_loss: 48166.34 with loss1: 1875.14, loss2: 290.45 and loss3: 46000.75\n",
      "Epoch [332], train_loss: 48197.88 with loss1: 1912.64, loss2: 289.30 and loss3: 45995.94\n",
      "Epoch [333], train_loss: 48152.46 with loss1: 1873.73, loss2: 287.59 and loss3: 45991.14\n",
      "Epoch [334], train_loss: 48190.62 with loss1: 1913.79, loss2: 290.50 and loss3: 45986.33\n",
      "Epoch [335], train_loss: 48154.37 with loss1: 1885.37, loss2: 287.48 and loss3: 45981.53\n",
      "Epoch [336], train_loss: 48184.20 with loss1: 1918.73, loss2: 288.74 and loss3: 45976.73\n",
      "Epoch [337], train_loss: 48133.94 with loss1: 1877.02, loss2: 285.00 and loss3: 45971.92\n",
      "Epoch [338], train_loss: 48164.00 with loss1: 1907.95, loss2: 288.93 and loss3: 45967.12\n",
      "Epoch [339], train_loss: 48113.73 with loss1: 1867.46, loss2: 283.96 and loss3: 45962.31\n",
      "Epoch [340], train_loss: 48141.95 with loss1: 1895.52, loss2: 288.91 and loss3: 45957.52\n",
      "Epoch [341], train_loss: 48091.83 with loss1: 1854.20, loss2: 284.92 and loss3: 45952.70\n",
      "Epoch [342], train_loss: 48109.06 with loss1: 1874.57, loss2: 286.58 and loss3: 45947.91\n",
      "Epoch [343], train_loss: 48057.43 with loss1: 1830.06, loss2: 284.26 and loss3: 45943.10\n",
      "Epoch [344], train_loss: 48071.84 with loss1: 1847.81, loss2: 285.72 and loss3: 45938.31\n",
      "Epoch [345], train_loss: 48018.18 with loss1: 1803.44, loss2: 281.24 and loss3: 45933.50\n",
      "Epoch [346], train_loss: 48035.59 with loss1: 1822.89, loss2: 284.00 and loss3: 45928.70\n",
      "Epoch [347], train_loss: 47982.52 with loss1: 1777.86, loss2: 280.77 and loss3: 45923.90\n",
      "Epoch [348], train_loss: 47983.11 with loss1: 1782.66, loss2: 281.35 and loss3: 45919.10\n",
      "Epoch [349], train_loss: 47950.09 with loss1: 1756.99, loss2: 278.79 and loss3: 45914.30\n",
      "Epoch [350], train_loss: 47949.29 with loss1: 1760.68, loss2: 279.12 and loss3: 45909.50\n",
      "Epoch [351], train_loss: 47931.77 with loss1: 1746.15, loss2: 280.92 and loss3: 45904.70\n",
      "Epoch [352], train_loss: 47938.52 with loss1: 1757.75, loss2: 280.87 and loss3: 45899.90\n",
      "Epoch [353], train_loss: 47921.77 with loss1: 1745.79, loss2: 280.87 and loss3: 45895.11\n",
      "Epoch [354], train_loss: 47922.99 with loss1: 1754.36, loss2: 278.32 and loss3: 45890.30\n",
      "Epoch [355], train_loss: 47901.19 with loss1: 1736.78, loss2: 278.90 and loss3: 45885.50\n",
      "Epoch [356], train_loss: 47915.84 with loss1: 1756.82, loss2: 278.32 and loss3: 45880.71\n",
      "Epoch [357], train_loss: 47896.68 with loss1: 1742.25, loss2: 278.53 and loss3: 45875.90\n",
      "Epoch [358], train_loss: 47912.72 with loss1: 1762.11, loss2: 279.50 and loss3: 45871.11\n",
      "Epoch [359], train_loss: 47879.61 with loss1: 1737.38, loss2: 275.92 and loss3: 45866.31\n",
      "Epoch [360], train_loss: 47906.37 with loss1: 1769.62, loss2: 275.23 and loss3: 45861.52\n",
      "Epoch [361], train_loss: 47882.33 with loss1: 1750.32, loss2: 275.30 and loss3: 45856.72\n",
      "Epoch [362], train_loss: 47906.31 with loss1: 1779.55, loss2: 274.85 and loss3: 45851.91\n",
      "Epoch [363], train_loss: 47882.42 with loss1: 1759.24, loss2: 276.06 and loss3: 45847.13\n",
      "Epoch [364], train_loss: 47915.71 with loss1: 1796.95, loss2: 276.45 and loss3: 45842.32\n",
      "Epoch [365], train_loss: 47878.04 with loss1: 1764.44, loss2: 276.07 and loss3: 45837.52\n",
      "Epoch [366], train_loss: 47906.77 with loss1: 1798.61, loss2: 275.43 and loss3: 45832.73\n",
      "Epoch [367], train_loss: 47872.33 with loss1: 1770.34, loss2: 274.05 and loss3: 45827.93\n",
      "Epoch [368], train_loss: 47902.14 with loss1: 1807.05, loss2: 271.97 and loss3: 45823.13\n",
      "Epoch [369], train_loss: 47853.57 with loss1: 1761.68, loss2: 273.55 and loss3: 45818.34\n",
      "Epoch [370], train_loss: 47885.22 with loss1: 1800.06, loss2: 271.61 and loss3: 45813.55\n",
      "Epoch [371], train_loss: 47826.05 with loss1: 1744.17, loss2: 273.13 and loss3: 45808.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [372], train_loss: 47848.80 with loss1: 1772.50, loss2: 272.37 and loss3: 45803.95\n",
      "Epoch [373], train_loss: 47809.39 with loss1: 1737.43, loss2: 272.81 and loss3: 45799.15\n",
      "Epoch [374], train_loss: 47824.18 with loss1: 1759.34, loss2: 270.48 and loss3: 45794.36\n",
      "Epoch [375], train_loss: 47775.91 with loss1: 1715.65, loss2: 270.71 and loss3: 45789.55\n",
      "Epoch [376], train_loss: 47790.32 with loss1: 1737.73, loss2: 267.82 and loss3: 45784.77\n",
      "Epoch [377], train_loss: 47747.27 with loss1: 1697.32, loss2: 269.99 and loss3: 45779.96\n",
      "Epoch [378], train_loss: 47762.42 with loss1: 1718.61, loss2: 268.63 and loss3: 45775.18\n",
      "Epoch [379], train_loss: 47726.27 with loss1: 1686.16, loss2: 269.73 and loss3: 45770.37\n",
      "Epoch [380], train_loss: 47732.88 with loss1: 1698.89, loss2: 268.40 and loss3: 45765.58\n",
      "Epoch [381], train_loss: 47704.49 with loss1: 1674.74, loss2: 268.98 and loss3: 45760.78\n",
      "Epoch [382], train_loss: 47710.99 with loss1: 1686.28, loss2: 268.73 and loss3: 45755.98\n",
      "Epoch [383], train_loss: 47680.69 with loss1: 1661.68, loss2: 267.82 and loss3: 45751.19\n",
      "Epoch [384], train_loss: 47686.55 with loss1: 1673.65, loss2: 266.50 and loss3: 45746.41\n",
      "Epoch [385], train_loss: 47664.16 with loss1: 1656.65, loss2: 265.91 and loss3: 45741.60\n",
      "Epoch [386], train_loss: 47678.48 with loss1: 1676.71, loss2: 264.95 and loss3: 45736.82\n",
      "Epoch [387], train_loss: 47656.69 with loss1: 1659.82, loss2: 264.85 and loss3: 45732.01\n",
      "Epoch [388], train_loss: 47670.33 with loss1: 1678.77, loss2: 264.33 and loss3: 45727.23\n",
      "Epoch [389], train_loss: 47649.09 with loss1: 1660.48, loss2: 266.18 and loss3: 45722.44\n",
      "Epoch [390], train_loss: 47684.70 with loss1: 1702.29, loss2: 264.77 and loss3: 45717.64\n",
      "Epoch [391], train_loss: 47652.65 with loss1: 1673.47, loss2: 266.33 and loss3: 45712.85\n",
      "Epoch [392], train_loss: 47685.75 with loss1: 1712.16, loss2: 265.53 and loss3: 45708.06\n",
      "Epoch [393], train_loss: 47662.39 with loss1: 1696.09, loss2: 263.02 and loss3: 45703.27\n",
      "Epoch [394], train_loss: 47712.23 with loss1: 1752.13, loss2: 261.62 and loss3: 45698.48\n",
      "Epoch [395], train_loss: 47661.65 with loss1: 1706.62, loss2: 261.34 and loss3: 45693.69\n",
      "Epoch [396], train_loss: 47718.89 with loss1: 1765.15, loss2: 264.83 and loss3: 45688.90\n",
      "Epoch [397], train_loss: 47664.63 with loss1: 1719.34, loss2: 261.17 and loss3: 45684.12\n",
      "Epoch [398], train_loss: 47711.12 with loss1: 1770.37, loss2: 261.43 and loss3: 45679.32\n",
      "Epoch [399], train_loss: 47654.20 with loss1: 1716.85, loss2: 262.81 and loss3: 45674.54\n",
      "Epoch [400], train_loss: 47694.82 with loss1: 1762.98, loss2: 262.11 and loss3: 45669.74\n",
      "Epoch [401], train_loss: 47623.23 with loss1: 1697.90, loss2: 260.37 and loss3: 45664.96\n",
      "Epoch [402], train_loss: 47660.22 with loss1: 1738.85, loss2: 261.21 and loss3: 45660.16\n",
      "Epoch [403], train_loss: 47597.49 with loss1: 1682.62, loss2: 259.48 and loss3: 45655.39\n",
      "Epoch [404], train_loss: 47620.48 with loss1: 1708.92, loss2: 260.97 and loss3: 45650.59\n",
      "Epoch [405], train_loss: 47564.79 with loss1: 1659.04, loss2: 259.94 and loss3: 45645.81\n",
      "Epoch [406], train_loss: 47582.99 with loss1: 1682.09, loss2: 259.89 and loss3: 45641.01\n",
      "Epoch [407], train_loss: 47539.21 with loss1: 1646.91, loss2: 256.07 and loss3: 45636.23\n",
      "Epoch [408], train_loss: 47562.68 with loss1: 1670.30, loss2: 260.95 and loss3: 45631.44\n",
      "Epoch [409], train_loss: 47514.86 with loss1: 1629.71, loss2: 258.49 and loss3: 45626.66\n",
      "Epoch [410], train_loss: 47538.58 with loss1: 1656.95, loss2: 259.76 and loss3: 45621.87\n",
      "Epoch [411], train_loss: 47504.64 with loss1: 1630.65, loss2: 256.92 and loss3: 45617.08\n",
      "Epoch [412], train_loss: 47532.35 with loss1: 1663.37, loss2: 256.68 and loss3: 45612.30\n",
      "Epoch [413], train_loss: 47496.07 with loss1: 1632.48, loss2: 256.08 and loss3: 45607.51\n",
      "Epoch [414], train_loss: 47522.21 with loss1: 1662.74, loss2: 256.75 and loss3: 45602.73\n",
      "Epoch [415], train_loss: 47485.79 with loss1: 1631.74, loss2: 256.12 and loss3: 45597.94\n",
      "Epoch [416], train_loss: 47511.40 with loss1: 1662.57, loss2: 255.68 and loss3: 45593.16\n",
      "Epoch [417], train_loss: 47493.22 with loss1: 1651.09, loss2: 253.76 and loss3: 45588.38\n",
      "Epoch [418], train_loss: 47516.75 with loss1: 1676.85, loss2: 256.31 and loss3: 45583.59\n",
      "Epoch [419], train_loss: 47479.38 with loss1: 1646.83, loss2: 253.74 and loss3: 45578.80\n",
      "Epoch [420], train_loss: 47508.71 with loss1: 1679.39, loss2: 255.32 and loss3: 45574.01\n",
      "Epoch [421], train_loss: 47462.30 with loss1: 1641.19, loss2: 251.87 and loss3: 45569.24\n",
      "Epoch [422], train_loss: 47490.40 with loss1: 1671.46, loss2: 254.50 and loss3: 45564.45\n",
      "Epoch [423], train_loss: 47446.70 with loss1: 1634.71, loss2: 252.31 and loss3: 45559.68\n",
      "Epoch [424], train_loss: 47477.21 with loss1: 1667.97, loss2: 254.36 and loss3: 45554.88\n",
      "Epoch [425], train_loss: 47437.00 with loss1: 1634.97, loss2: 251.92 and loss3: 45550.12\n",
      "Epoch [426], train_loss: 47459.24 with loss1: 1660.95, loss2: 252.97 and loss3: 45545.32\n",
      "Epoch [427], train_loss: 47416.44 with loss1: 1625.17, loss2: 250.72 and loss3: 45540.55\n",
      "Epoch [428], train_loss: 47441.64 with loss1: 1653.78, loss2: 252.10 and loss3: 45535.76\n",
      "Epoch [429], train_loss: 47406.03 with loss1: 1624.39, loss2: 250.67 and loss3: 45530.98\n",
      "Epoch [430], train_loss: 47425.55 with loss1: 1648.01, loss2: 251.33 and loss3: 45526.20\n",
      "Epoch [431], train_loss: 47389.25 with loss1: 1618.77, loss2: 249.07 and loss3: 45521.41\n",
      "Epoch [432], train_loss: 47409.10 with loss1: 1642.99, loss2: 249.46 and loss3: 45516.65\n",
      "Epoch [433], train_loss: 47381.98 with loss1: 1619.45, loss2: 250.67 and loss3: 45511.86\n",
      "Epoch [434], train_loss: 47397.23 with loss1: 1640.10, loss2: 250.04 and loss3: 45507.09\n",
      "Epoch [435], train_loss: 47368.29 with loss1: 1617.06, loss2: 248.93 and loss3: 45502.30\n",
      "Epoch [436], train_loss: 47387.02 with loss1: 1641.00, loss2: 248.49 and loss3: 45497.53\n",
      "Epoch [437], train_loss: 47356.59 with loss1: 1614.77, loss2: 249.07 and loss3: 45492.74\n",
      "Epoch [438], train_loss: 47367.54 with loss1: 1631.15, loss2: 248.42 and loss3: 45487.97\n",
      "Epoch [439], train_loss: 47335.39 with loss1: 1604.50, loss2: 247.69 and loss3: 45483.20\n",
      "Epoch [440], train_loss: 47348.22 with loss1: 1622.98, loss2: 246.83 and loss3: 45478.42\n",
      "Epoch [441], train_loss: 47304.93 with loss1: 1583.87, loss2: 247.43 and loss3: 45473.63\n",
      "Epoch [442], train_loss: 47316.07 with loss1: 1601.96, loss2: 245.25 and loss3: 45468.86\n",
      "Epoch [443], train_loss: 47282.45 with loss1: 1571.53, loss2: 246.84 and loss3: 45464.08\n",
      "Epoch [444], train_loss: 47284.93 with loss1: 1579.22, loss2: 246.40 and loss3: 45459.30\n",
      "Epoch [445], train_loss: 47252.26 with loss1: 1552.14, loss2: 245.60 and loss3: 45454.52\n",
      "Epoch [446], train_loss: 47254.34 with loss1: 1560.40, loss2: 244.19 and loss3: 45449.75\n",
      "Epoch [447], train_loss: 47226.63 with loss1: 1535.48, loss2: 246.18 and loss3: 45444.97\n",
      "Epoch [448], train_loss: 47229.17 with loss1: 1544.75, loss2: 244.23 and loss3: 45440.20\n",
      "Epoch [449], train_loss: 47210.95 with loss1: 1527.10, loss2: 248.43 and loss3: 45435.41\n",
      "Epoch [450], train_loss: 47212.73 with loss1: 1537.28, loss2: 244.81 and loss3: 45430.63\n",
      "Epoch [451], train_loss: 47193.13 with loss1: 1522.47, loss2: 244.80 and loss3: 45425.86\n",
      "Epoch [452], train_loss: 47196.47 with loss1: 1532.64, loss2: 242.73 and loss3: 45421.09\n",
      "Epoch [453], train_loss: 47177.69 with loss1: 1516.03, loss2: 245.34 and loss3: 45416.31\n",
      "Epoch [454], train_loss: 47176.94 with loss1: 1521.78, loss2: 243.63 and loss3: 45411.53\n",
      "Epoch [455], train_loss: 47163.25 with loss1: 1513.06, loss2: 243.43 and loss3: 45406.76\n",
      "Epoch [456], train_loss: 47175.70 with loss1: 1531.91, loss2: 241.80 and loss3: 45401.99\n",
      "Epoch [457], train_loss: 47145.12 with loss1: 1505.24, loss2: 242.68 and loss3: 45397.21\n",
      "Epoch [458], train_loss: 47152.53 with loss1: 1518.37, loss2: 241.71 and loss3: 45392.44\n",
      "Epoch [459], train_loss: 47136.96 with loss1: 1507.44, loss2: 241.86 and loss3: 45387.65\n",
      "Epoch [460], train_loss: 47142.25 with loss1: 1518.14, loss2: 241.22 and loss3: 45382.89\n",
      "Epoch [461], train_loss: 47128.61 with loss1: 1508.81, loss2: 241.69 and loss3: 45378.11\n",
      "Epoch [462], train_loss: 47138.77 with loss1: 1524.37, loss2: 241.06 and loss3: 45373.34\n",
      "Epoch [463], train_loss: 47113.12 with loss1: 1504.92, loss2: 239.65 and loss3: 45368.55\n",
      "Epoch [464], train_loss: 47126.93 with loss1: 1523.38, loss2: 239.76 and loss3: 45363.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [465], train_loss: 47105.16 with loss1: 1507.37, loss2: 238.78 and loss3: 45359.00\n",
      "Epoch [466], train_loss: 47127.74 with loss1: 1533.15, loss2: 240.35 and loss3: 45354.24\n",
      "Epoch [467], train_loss: 47095.38 with loss1: 1507.92, loss2: 238.00 and loss3: 45349.46\n",
      "Epoch [468], train_loss: 47109.62 with loss1: 1525.68, loss2: 239.25 and loss3: 45344.69\n",
      "Epoch [469], train_loss: 47082.12 with loss1: 1503.51, loss2: 238.71 and loss3: 45339.91\n",
      "Epoch [470], train_loss: 47091.10 with loss1: 1517.59, loss2: 238.36 and loss3: 45335.15\n",
      "Epoch [471], train_loss: 47071.81 with loss1: 1501.44, loss2: 240.01 and loss3: 45330.36\n",
      "Epoch [472], train_loss: 47075.20 with loss1: 1510.40, loss2: 239.21 and loss3: 45325.59\n",
      "Epoch [473], train_loss: 47054.46 with loss1: 1495.97, loss2: 237.67 and loss3: 45320.82\n",
      "Epoch [474], train_loss: 47057.68 with loss1: 1504.27, loss2: 237.36 and loss3: 45316.05\n",
      "Epoch [475], train_loss: 47039.96 with loss1: 1492.99, loss2: 235.69 and loss3: 45311.28\n",
      "Epoch [476], train_loss: 47049.41 with loss1: 1506.52, loss2: 236.39 and loss3: 45306.50\n",
      "Epoch [477], train_loss: 47026.12 with loss1: 1487.53, loss2: 236.86 and loss3: 45301.73\n",
      "Epoch [478], train_loss: 47031.45 with loss1: 1497.25, loss2: 237.23 and loss3: 45296.97\n",
      "Epoch [479], train_loss: 47007.51 with loss1: 1481.27, loss2: 234.04 and loss3: 45292.19\n",
      "Epoch [480], train_loss: 47020.60 with loss1: 1497.81, loss2: 235.37 and loss3: 45287.43\n",
      "Epoch [481], train_loss: 46988.19 with loss1: 1469.90, loss2: 235.63 and loss3: 45282.66\n",
      "Epoch [482], train_loss: 46996.45 with loss1: 1483.72, loss2: 234.85 and loss3: 45277.89\n",
      "Epoch [483], train_loss: 46973.45 with loss1: 1463.59, loss2: 236.74 and loss3: 45273.12\n",
      "Epoch [484], train_loss: 46979.59 with loss1: 1477.05, loss2: 234.20 and loss3: 45268.34\n",
      "Epoch [485], train_loss: 46956.38 with loss1: 1458.98, loss2: 233.83 and loss3: 45263.57\n",
      "Epoch [486], train_loss: 46965.84 with loss1: 1473.81, loss2: 233.23 and loss3: 45258.80\n",
      "Epoch [487], train_loss: 46944.50 with loss1: 1455.89, loss2: 234.58 and loss3: 45254.03\n",
      "Epoch [488], train_loss: 46944.40 with loss1: 1460.47, loss2: 234.65 and loss3: 45249.27\n",
      "Epoch [489], train_loss: 46925.80 with loss1: 1449.08, loss2: 232.22 and loss3: 45244.50\n",
      "Epoch [490], train_loss: 46932.35 with loss1: 1459.55, loss2: 233.08 and loss3: 45239.73\n",
      "Epoch [491], train_loss: 46916.96 with loss1: 1450.07, loss2: 231.93 and loss3: 45234.96\n",
      "Epoch [492], train_loss: 46919.96 with loss1: 1457.48, loss2: 232.28 and loss3: 45230.20\n",
      "Epoch [493], train_loss: 46907.32 with loss1: 1449.00, loss2: 232.90 and loss3: 45225.43\n",
      "Epoch [494], train_loss: 46908.94 with loss1: 1456.17, loss2: 232.11 and loss3: 45220.66\n",
      "Epoch [495], train_loss: 46889.55 with loss1: 1442.90, loss2: 230.76 and loss3: 45215.90\n",
      "Epoch [496], train_loss: 46902.31 with loss1: 1459.22, loss2: 231.96 and loss3: 45211.13\n",
      "Epoch [497], train_loss: 46883.30 with loss1: 1445.62, loss2: 231.30 and loss3: 45206.38\n",
      "Epoch [498], train_loss: 46893.78 with loss1: 1460.56, loss2: 231.61 and loss3: 45201.60\n",
      "Epoch [499], train_loss: 46877.11 with loss1: 1449.05, loss2: 231.21 and loss3: 45196.84\n",
      "Epoch [500], train_loss: 46892.61 with loss1: 1469.87, loss2: 230.66 and loss3: 45192.08\n",
      "Epoch [501], train_loss: 46866.96 with loss1: 1449.85, loss2: 229.81 and loss3: 45187.31\n",
      "Epoch [502], train_loss: 46887.14 with loss1: 1473.71, loss2: 230.88 and loss3: 45182.55\n",
      "Epoch [503], train_loss: 46866.68 with loss1: 1459.20, loss2: 229.69 and loss3: 45177.79\n",
      "Epoch [504], train_loss: 46883.42 with loss1: 1479.48, loss2: 230.90 and loss3: 45173.03\n",
      "Epoch [505], train_loss: 46860.13 with loss1: 1463.76, loss2: 228.10 and loss3: 45168.27\n",
      "Epoch [506], train_loss: 46887.26 with loss1: 1492.33, loss2: 231.43 and loss3: 45163.50\n",
      "Epoch [507], train_loss: 46860.15 with loss1: 1473.61, loss2: 227.79 and loss3: 45158.75\n",
      "Epoch [508], train_loss: 46889.61 with loss1: 1505.02, loss2: 230.61 and loss3: 45153.98\n",
      "Epoch [509], train_loss: 46855.33 with loss1: 1478.48, loss2: 227.64 and loss3: 45149.22\n",
      "Epoch [510], train_loss: 46890.30 with loss1: 1516.18, loss2: 229.66 and loss3: 45144.46\n",
      "Epoch [511], train_loss: 46852.93 with loss1: 1486.43, loss2: 226.80 and loss3: 45139.70\n",
      "Epoch [512], train_loss: 46889.38 with loss1: 1525.08, loss2: 229.36 and loss3: 45134.94\n",
      "Epoch [513], train_loss: 46846.47 with loss1: 1489.28, loss2: 227.01 and loss3: 45130.18\n",
      "Epoch [514], train_loss: 46872.35 with loss1: 1519.12, loss2: 227.81 and loss3: 45125.42\n",
      "Epoch [515], train_loss: 46825.55 with loss1: 1479.81, loss2: 225.09 and loss3: 45120.66\n",
      "Epoch [516], train_loss: 46840.54 with loss1: 1496.38, loss2: 228.26 and loss3: 45115.90\n",
      "Epoch [517], train_loss: 46791.43 with loss1: 1453.97, loss2: 226.32 and loss3: 45111.14\n",
      "Epoch [518], train_loss: 46803.27 with loss1: 1470.22, loss2: 226.67 and loss3: 45106.39\n",
      "Epoch [519], train_loss: 46758.51 with loss1: 1431.01, loss2: 225.87 and loss3: 45101.62\n",
      "Epoch [520], train_loss: 46770.11 with loss1: 1446.57, loss2: 226.66 and loss3: 45096.88\n",
      "Epoch [521], train_loss: 46725.20 with loss1: 1408.73, loss2: 224.36 and loss3: 45092.11\n",
      "Epoch [522], train_loss: 46730.46 with loss1: 1416.84, loss2: 226.28 and loss3: 45087.35\n",
      "Epoch [523], train_loss: 46696.73 with loss1: 1389.36, loss2: 224.77 and loss3: 45082.60\n",
      "Epoch [524], train_loss: 46697.72 with loss1: 1394.48, loss2: 225.40 and loss3: 45077.84\n",
      "Epoch [525], train_loss: 46672.93 with loss1: 1376.04, loss2: 223.81 and loss3: 45073.09\n",
      "Epoch [526], train_loss: 46680.43 with loss1: 1387.13, loss2: 224.96 and loss3: 45068.34\n",
      "Epoch [527], train_loss: 46655.64 with loss1: 1368.69, loss2: 223.37 and loss3: 45063.57\n",
      "Epoch [528], train_loss: 46660.93 with loss1: 1378.52, loss2: 223.59 and loss3: 45058.82\n",
      "Epoch [529], train_loss: 46644.14 with loss1: 1365.12, loss2: 224.96 and loss3: 45054.06\n",
      "Epoch [530], train_loss: 46654.48 with loss1: 1381.30, loss2: 223.86 and loss3: 45049.31\n",
      "Epoch [531], train_loss: 46629.58 with loss1: 1362.08, loss2: 222.95 and loss3: 45044.55\n",
      "Epoch [532], train_loss: 46637.66 with loss1: 1375.50, loss2: 222.36 and loss3: 45039.80\n",
      "Epoch [533], train_loss: 46615.07 with loss1: 1357.11, loss2: 222.91 and loss3: 45035.04\n",
      "Epoch [534], train_loss: 46626.07 with loss1: 1373.19, loss2: 222.58 and loss3: 45030.30\n",
      "Epoch [535], train_loss: 46610.00 with loss1: 1362.40, loss2: 222.08 and loss3: 45025.53\n",
      "Epoch [536], train_loss: 46626.62 with loss1: 1384.59, loss2: 221.26 and loss3: 45020.78\n",
      "Epoch [537], train_loss: 46607.44 with loss1: 1368.52, loss2: 222.90 and loss3: 45016.02\n",
      "Epoch [538], train_loss: 46629.29 with loss1: 1396.94, loss2: 221.07 and loss3: 45011.28\n",
      "Epoch [539], train_loss: 46612.05 with loss1: 1383.91, loss2: 221.62 and loss3: 45006.52\n",
      "Epoch [540], train_loss: 46628.70 with loss1: 1406.15, loss2: 220.78 and loss3: 45001.77\n",
      "Epoch [541], train_loss: 46602.50 with loss1: 1385.60, loss2: 219.90 and loss3: 44997.01\n",
      "Epoch [542], train_loss: 46632.81 with loss1: 1419.81, loss2: 220.74 and loss3: 44992.27\n",
      "Epoch [543], train_loss: 46608.65 with loss1: 1400.57, loss2: 220.59 and loss3: 44987.50\n",
      "Epoch [544], train_loss: 46628.58 with loss1: 1425.12, loss2: 220.71 and loss3: 44982.76\n",
      "Epoch [545], train_loss: 46602.57 with loss1: 1405.34, loss2: 219.25 and loss3: 44977.99\n",
      "Epoch [546], train_loss: 46620.56 with loss1: 1427.75, loss2: 219.56 and loss3: 44973.25\n",
      "Epoch [547], train_loss: 46582.55 with loss1: 1393.46, loss2: 220.60 and loss3: 44968.48\n",
      "Epoch [548], train_loss: 46596.66 with loss1: 1413.36, loss2: 219.55 and loss3: 44963.75\n",
      "Epoch [549], train_loss: 46562.71 with loss1: 1385.42, loss2: 218.30 and loss3: 44958.98\n",
      "Epoch [550], train_loss: 46573.66 with loss1: 1400.28, loss2: 219.14 and loss3: 44954.24\n",
      "Epoch [551], train_loss: 46532.02 with loss1: 1364.13, loss2: 218.40 and loss3: 44949.48\n",
      "Epoch [552], train_loss: 46538.32 with loss1: 1376.50, loss2: 217.08 and loss3: 44944.74\n",
      "Epoch [553], train_loss: 46501.67 with loss1: 1344.43, loss2: 217.26 and loss3: 44939.98\n",
      "Epoch [554], train_loss: 46505.17 with loss1: 1351.96, loss2: 217.97 and loss3: 44935.24\n",
      "Epoch [555], train_loss: 46482.68 with loss1: 1335.24, loss2: 216.96 and loss3: 44930.48\n",
      "Epoch [556], train_loss: 46480.91 with loss1: 1337.86, loss2: 217.32 and loss3: 44925.73\n",
      "Epoch [557], train_loss: 46461.56 with loss1: 1322.66, loss2: 217.93 and loss3: 44920.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [558], train_loss: 46460.02 with loss1: 1328.30, loss2: 215.49 and loss3: 44916.23\n",
      "Epoch [559], train_loss: 46439.05 with loss1: 1311.32, loss2: 216.25 and loss3: 44911.48\n",
      "Epoch [560], train_loss: 46447.00 with loss1: 1324.72, loss2: 215.55 and loss3: 44906.73\n",
      "Epoch [561], train_loss: 46425.07 with loss1: 1306.72, loss2: 216.37 and loss3: 44901.98\n",
      "Epoch [562], train_loss: 46434.09 with loss1: 1320.86, loss2: 216.00 and loss3: 44897.23\n",
      "Epoch [563], train_loss: 46411.78 with loss1: 1303.84, loss2: 215.45 and loss3: 44892.48\n",
      "Epoch [564], train_loss: 46413.27 with loss1: 1310.39, loss2: 215.14 and loss3: 44887.73\n",
      "Epoch [565], train_loss: 46400.92 with loss1: 1303.08, loss2: 214.85 and loss3: 44882.99\n",
      "Epoch [566], train_loss: 46404.70 with loss1: 1312.10, loss2: 214.35 and loss3: 44878.24\n",
      "Epoch [567], train_loss: 46392.82 with loss1: 1304.95, loss2: 214.39 and loss3: 44873.48\n",
      "Epoch [568], train_loss: 46396.68 with loss1: 1314.09, loss2: 213.84 and loss3: 44868.75\n",
      "Epoch [569], train_loss: 46387.89 with loss1: 1309.85, loss2: 214.05 and loss3: 44863.99\n",
      "Epoch [570], train_loss: 46394.44 with loss1: 1322.46, loss2: 212.72 and loss3: 44859.26\n",
      "Epoch [571], train_loss: 46382.64 with loss1: 1314.93, loss2: 213.21 and loss3: 44854.50\n",
      "Epoch [572], train_loss: 46392.37 with loss1: 1328.72, loss2: 213.87 and loss3: 44849.77\n",
      "Epoch [573], train_loss: 46378.01 with loss1: 1319.49, loss2: 213.51 and loss3: 44845.01\n",
      "Epoch [574], train_loss: 46391.60 with loss1: 1337.23, loss2: 214.08 and loss3: 44840.28\n",
      "Epoch [575], train_loss: 46373.75 with loss1: 1325.80, loss2: 212.42 and loss3: 44835.52\n",
      "Epoch [576], train_loss: 46382.36 with loss1: 1339.09, loss2: 212.49 and loss3: 44830.79\n",
      "Epoch [577], train_loss: 46371.86 with loss1: 1332.64, loss2: 213.19 and loss3: 44826.04\n",
      "Epoch [578], train_loss: 46387.97 with loss1: 1354.82, loss2: 211.85 and loss3: 44821.30\n",
      "Epoch [579], train_loss: 46368.30 with loss1: 1339.31, loss2: 212.45 and loss3: 44816.55\n",
      "Epoch [580], train_loss: 46382.49 with loss1: 1358.43, loss2: 212.25 and loss3: 44811.80\n",
      "Epoch [581], train_loss: 46364.77 with loss1: 1346.38, loss2: 211.33 and loss3: 44807.06\n",
      "Epoch [582], train_loss: 46378.07 with loss1: 1364.47, loss2: 211.27 and loss3: 44802.32\n",
      "Epoch [583], train_loss: 46352.42 with loss1: 1344.18, loss2: 210.67 and loss3: 44797.57\n",
      "Epoch [584], train_loss: 46365.32 with loss1: 1361.08, loss2: 211.41 and loss3: 44792.83\n",
      "Epoch [585], train_loss: 46343.45 with loss1: 1344.99, loss2: 210.38 and loss3: 44788.08\n",
      "Epoch [586], train_loss: 46354.43 with loss1: 1360.06, loss2: 211.02 and loss3: 44783.35\n",
      "Epoch [587], train_loss: 46333.41 with loss1: 1345.49, loss2: 209.32 and loss3: 44778.59\n",
      "Epoch [588], train_loss: 46342.81 with loss1: 1358.74, loss2: 210.21 and loss3: 44773.86\n",
      "Epoch [589], train_loss: 46322.37 with loss1: 1343.26, loss2: 210.00 and loss3: 44769.11\n",
      "Epoch [590], train_loss: 46328.02 with loss1: 1352.73, loss2: 210.92 and loss3: 44764.37\n",
      "Epoch [591], train_loss: 46304.13 with loss1: 1335.44, loss2: 209.06 and loss3: 44759.62\n",
      "Epoch [592], train_loss: 46311.61 with loss1: 1346.76, loss2: 209.96 and loss3: 44754.89\n",
      "Epoch [593], train_loss: 46288.96 with loss1: 1329.28, loss2: 209.53 and loss3: 44750.15\n",
      "Epoch [594], train_loss: 46293.03 with loss1: 1337.48, loss2: 210.15 and loss3: 44745.40\n",
      "Epoch [595], train_loss: 46275.93 with loss1: 1325.59, loss2: 209.67 and loss3: 44740.66\n",
      "Epoch [596], train_loss: 46277.50 with loss1: 1332.77, loss2: 208.80 and loss3: 44735.92\n",
      "Epoch [597], train_loss: 46262.47 with loss1: 1322.10, loss2: 209.19 and loss3: 44731.18\n",
      "Epoch [598], train_loss: 46270.50 with loss1: 1335.99, loss2: 208.06 and loss3: 44726.44\n",
      "Epoch [599], train_loss: 46251.64 with loss1: 1322.75, loss2: 207.18 and loss3: 44721.70\n",
      "Epoch [600], train_loss: 46257.35 with loss1: 1332.13, loss2: 208.26 and loss3: 44716.96\n",
      "Epoch [601], train_loss: 46238.09 with loss1: 1318.50, loss2: 207.36 and loss3: 44712.23\n",
      "Epoch [602], train_loss: 46246.03 with loss1: 1329.55, loss2: 208.99 and loss3: 44707.48\n",
      "Epoch [603], train_loss: 46226.16 with loss1: 1315.46, loss2: 207.94 and loss3: 44702.75\n",
      "Epoch [604], train_loss: 46230.08 with loss1: 1323.95, loss2: 208.12 and loss3: 44698.01\n",
      "Epoch [605], train_loss: 46210.78 with loss1: 1311.19, loss2: 206.32 and loss3: 44693.27\n",
      "Epoch [606], train_loss: 46219.39 with loss1: 1324.00, loss2: 206.86 and loss3: 44688.52\n",
      "Epoch [607], train_loss: 46196.87 with loss1: 1306.63, loss2: 206.45 and loss3: 44683.80\n",
      "Epoch [608], train_loss: 46202.32 with loss1: 1317.16, loss2: 206.10 and loss3: 44679.05\n",
      "Epoch [609], train_loss: 46181.77 with loss1: 1300.26, loss2: 207.18 and loss3: 44674.33\n",
      "Epoch [610], train_loss: 46192.20 with loss1: 1317.36, loss2: 205.26 and loss3: 44669.57\n",
      "Epoch [611], train_loss: 46172.31 with loss1: 1302.02, loss2: 205.45 and loss3: 44664.84\n",
      "Epoch [612], train_loss: 46189.02 with loss1: 1323.10, loss2: 205.81 and loss3: 44660.11\n",
      "Epoch [613], train_loss: 46169.76 with loss1: 1309.53, loss2: 204.86 and loss3: 44655.37\n",
      "Epoch [614], train_loss: 46185.79 with loss1: 1329.62, loss2: 205.54 and loss3: 44650.63\n",
      "Epoch [615], train_loss: 46165.98 with loss1: 1315.43, loss2: 204.65 and loss3: 44645.89\n",
      "Epoch [616], train_loss: 46180.19 with loss1: 1335.17, loss2: 203.85 and loss3: 44641.16\n",
      "Epoch [617], train_loss: 46163.46 with loss1: 1323.40, loss2: 203.65 and loss3: 44636.41\n",
      "Epoch [618], train_loss: 46173.96 with loss1: 1337.91, loss2: 204.37 and loss3: 44631.68\n",
      "Epoch [619], train_loss: 46153.63 with loss1: 1322.33, loss2: 204.35 and loss3: 44626.95\n",
      "Epoch [620], train_loss: 46156.16 with loss1: 1330.01, loss2: 203.94 and loss3: 44622.21\n",
      "Epoch [621], train_loss: 46123.68 with loss1: 1302.57, loss2: 203.64 and loss3: 44617.47\n",
      "Epoch [622], train_loss: 46122.52 with loss1: 1305.17, loss2: 204.61 and loss3: 44612.74\n",
      "Epoch [623], train_loss: 46091.94 with loss1: 1280.26, loss2: 203.67 and loss3: 44608.01\n",
      "Epoch [624], train_loss: 46081.57 with loss1: 1275.02, loss2: 203.28 and loss3: 44603.27\n",
      "Epoch [625], train_loss: 46059.43 with loss1: 1258.32, loss2: 202.57 and loss3: 44598.54\n",
      "Epoch [626], train_loss: 46050.45 with loss1: 1253.81, loss2: 202.83 and loss3: 44593.80\n",
      "Epoch [627], train_loss: 46026.30 with loss1: 1235.83, loss2: 201.41 and loss3: 44589.07\n",
      "Epoch [628], train_loss: 46020.06 with loss1: 1233.00, loss2: 202.73 and loss3: 44584.34\n",
      "Epoch [629], train_loss: 46002.12 with loss1: 1220.10, loss2: 202.43 and loss3: 44579.60\n",
      "Epoch [630], train_loss: 45998.24 with loss1: 1221.49, loss2: 201.87 and loss3: 44574.88\n",
      "Epoch [631], train_loss: 45982.07 with loss1: 1209.29, loss2: 202.64 and loss3: 44570.14\n",
      "Epoch [632], train_loss: 45975.05 with loss1: 1208.65, loss2: 200.99 and loss3: 44565.41\n",
      "Epoch [633], train_loss: 45962.96 with loss1: 1201.25, loss2: 201.04 and loss3: 44560.68\n",
      "Epoch [634], train_loss: 45961.63 with loss1: 1203.81, loss2: 201.86 and loss3: 44555.96\n",
      "Epoch [635], train_loss: 45948.60 with loss1: 1196.81, loss2: 200.57 and loss3: 44551.22\n",
      "Epoch [636], train_loss: 45948.75 with loss1: 1200.16, loss2: 202.09 and loss3: 44546.50\n",
      "Epoch [637], train_loss: 45936.57 with loss1: 1194.05, loss2: 200.75 and loss3: 44541.77\n",
      "Epoch [638], train_loss: 45943.53 with loss1: 1205.87, loss2: 200.62 and loss3: 44537.04\n",
      "Epoch [639], train_loss: 45932.15 with loss1: 1199.34, loss2: 200.50 and loss3: 44532.31\n",
      "Epoch [640], train_loss: 45936.51 with loss1: 1207.94, loss2: 200.98 and loss3: 44527.59\n",
      "Epoch [641], train_loss: 45928.84 with loss1: 1205.51, loss2: 200.48 and loss3: 44522.86\n",
      "Epoch [642], train_loss: 45931.93 with loss1: 1213.79, loss2: 200.01 and loss3: 44518.13\n",
      "Epoch [643], train_loss: 45930.88 with loss1: 1217.28, loss2: 200.20 and loss3: 44513.40\n",
      "Epoch [644], train_loss: 45938.89 with loss1: 1230.48, loss2: 199.73 and loss3: 44508.68\n",
      "Epoch [645], train_loss: 45928.37 with loss1: 1225.33, loss2: 199.09 and loss3: 44503.95\n",
      "Epoch [646], train_loss: 45940.09 with loss1: 1241.94, loss2: 198.93 and loss3: 44499.22\n",
      "Epoch [647], train_loss: 45932.82 with loss1: 1238.44, loss2: 199.88 and loss3: 44494.49\n",
      "Epoch [648], train_loss: 45951.71 with loss1: 1262.85, loss2: 199.10 and loss3: 44489.77\n",
      "Epoch [649], train_loss: 45940.65 with loss1: 1257.26, loss2: 198.36 and loss3: 44485.04\n",
      "Epoch [650], train_loss: 45962.03 with loss1: 1282.40, loss2: 199.31 and loss3: 44480.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [651], train_loss: 45949.27 with loss1: 1275.97, loss2: 197.71 and loss3: 44475.59\n",
      "Epoch [652], train_loss: 45974.68 with loss1: 1304.99, loss2: 198.82 and loss3: 44470.87\n",
      "Epoch [653], train_loss: 45944.29 with loss1: 1281.17, loss2: 196.98 and loss3: 44466.13\n",
      "Epoch [654], train_loss: 45969.11 with loss1: 1309.53, loss2: 198.17 and loss3: 44461.41\n",
      "Epoch [655], train_loss: 45936.50 with loss1: 1282.69, loss2: 197.13 and loss3: 44456.68\n",
      "Epoch [656], train_loss: 45951.67 with loss1: 1302.23, loss2: 197.47 and loss3: 44451.97\n",
      "Epoch [657], train_loss: 45920.26 with loss1: 1275.54, loss2: 197.49 and loss3: 44447.23\n",
      "Epoch [658], train_loss: 45926.01 with loss1: 1285.26, loss2: 198.23 and loss3: 44442.52\n",
      "Epoch [659], train_loss: 45885.69 with loss1: 1250.53, loss2: 197.38 and loss3: 44437.78\n",
      "Epoch [660], train_loss: 45897.40 with loss1: 1268.00, loss2: 196.33 and loss3: 44433.07\n",
      "Epoch [661], train_loss: 45856.80 with loss1: 1231.91, loss2: 196.56 and loss3: 44428.34\n",
      "Epoch [662], train_loss: 45863.21 with loss1: 1243.45, loss2: 196.14 and loss3: 44423.62\n",
      "Epoch [663], train_loss: 45836.17 with loss1: 1221.39, loss2: 195.90 and loss3: 44418.89\n",
      "Epoch [664], train_loss: 45835.84 with loss1: 1226.32, loss2: 195.35 and loss3: 44414.17\n",
      "Epoch [665], train_loss: 45809.02 with loss1: 1205.30, loss2: 194.27 and loss3: 44409.45\n",
      "Epoch [666], train_loss: 45814.93 with loss1: 1214.44, loss2: 195.76 and loss3: 44404.73\n",
      "Epoch [667], train_loss: 45793.03 with loss1: 1198.48, loss2: 194.55 and loss3: 44400.00\n",
      "Epoch [668], train_loss: 45793.61 with loss1: 1203.47, loss2: 194.86 and loss3: 44395.28\n",
      "Epoch [669], train_loss: 45776.13 with loss1: 1191.53, loss2: 194.05 and loss3: 44390.55\n",
      "Epoch [670], train_loss: 45779.80 with loss1: 1199.31, loss2: 194.65 and loss3: 44385.84\n",
      "Epoch [671], train_loss: 45759.54 with loss1: 1184.79, loss2: 193.64 and loss3: 44381.11\n",
      "Epoch [672], train_loss: 45765.06 with loss1: 1194.30, loss2: 194.37 and loss3: 44376.39\n",
      "Epoch [673], train_loss: 45741.49 with loss1: 1176.11, loss2: 193.71 and loss3: 44371.67\n",
      "Epoch [674], train_loss: 45744.23 with loss1: 1182.64, loss2: 194.64 and loss3: 44366.95\n",
      "Epoch [675], train_loss: 45731.01 with loss1: 1176.84, loss2: 191.94 and loss3: 44362.23\n",
      "Epoch [676], train_loss: 45734.27 with loss1: 1182.34, loss2: 194.42 and loss3: 44357.50\n",
      "Epoch [677], train_loss: 45722.08 with loss1: 1175.37, loss2: 193.93 and loss3: 44352.78\n",
      "Epoch [678], train_loss: 45720.23 with loss1: 1178.14, loss2: 194.03 and loss3: 44348.05\n",
      "Epoch [679], train_loss: 45710.89 with loss1: 1174.54, loss2: 193.00 and loss3: 44343.35\n",
      "Epoch [680], train_loss: 45714.73 with loss1: 1183.62, loss2: 192.49 and loss3: 44338.62\n",
      "Epoch [681], train_loss: 45707.38 with loss1: 1180.68, loss2: 192.79 and loss3: 44333.91\n",
      "Epoch [682], train_loss: 45708.14 with loss1: 1186.33, loss2: 192.63 and loss3: 44329.18\n",
      "Epoch [683], train_loss: 45695.60 with loss1: 1178.94, loss2: 192.19 and loss3: 44324.47\n",
      "Epoch [684], train_loss: 45697.49 with loss1: 1184.99, loss2: 192.76 and loss3: 44319.75\n",
      "Epoch [685], train_loss: 45683.11 with loss1: 1175.55, loss2: 192.52 and loss3: 44315.04\n",
      "Epoch [686], train_loss: 45690.33 with loss1: 1188.21, loss2: 191.81 and loss3: 44310.31\n",
      "Epoch [687], train_loss: 45679.21 with loss1: 1181.99, loss2: 191.63 and loss3: 44305.59\n",
      "Epoch [688], train_loss: 45682.13 with loss1: 1188.88, loss2: 192.39 and loss3: 44300.87\n",
      "Epoch [689], train_loss: 45668.64 with loss1: 1182.01, loss2: 190.48 and loss3: 44296.16\n",
      "Epoch [690], train_loss: 45675.49 with loss1: 1192.41, loss2: 191.65 and loss3: 44291.44\n",
      "Epoch [691], train_loss: 45661.66 with loss1: 1184.27, loss2: 190.66 and loss3: 44286.72\n",
      "Epoch [692], train_loss: 45673.61 with loss1: 1199.78, loss2: 191.82 and loss3: 44282.00\n",
      "Epoch [693], train_loss: 45653.62 with loss1: 1186.07, loss2: 190.26 and loss3: 44277.29\n",
      "Epoch [694], train_loss: 45666.70 with loss1: 1203.92, loss2: 190.21 and loss3: 44272.57\n",
      "Epoch [695], train_loss: 45652.66 with loss1: 1195.15, loss2: 189.65 and loss3: 44267.86\n",
      "Epoch [696], train_loss: 45663.16 with loss1: 1209.64, loss2: 190.39 and loss3: 44263.13\n",
      "Epoch [697], train_loss: 45649.37 with loss1: 1201.13, loss2: 189.82 and loss3: 44258.42\n",
      "Epoch [698], train_loss: 45660.41 with loss1: 1216.81, loss2: 189.89 and loss3: 44253.70\n",
      "Epoch [699], train_loss: 45643.10 with loss1: 1204.68, loss2: 189.43 and loss3: 44248.99\n",
      "Epoch [700], train_loss: 45660.25 with loss1: 1226.21, loss2: 189.77 and loss3: 44244.27\n",
      "Epoch [701], train_loss: 45634.65 with loss1: 1206.27, loss2: 188.81 and loss3: 44239.56\n",
      "Epoch [702], train_loss: 45652.05 with loss1: 1227.90, loss2: 189.30 and loss3: 44234.84\n",
      "Epoch [703], train_loss: 45628.87 with loss1: 1210.11, loss2: 188.63 and loss3: 44230.14\n",
      "Epoch [704], train_loss: 45637.17 with loss1: 1222.81, loss2: 188.95 and loss3: 44225.41\n",
      "Epoch [705], train_loss: 45614.17 with loss1: 1204.50, loss2: 188.96 and loss3: 44220.71\n",
      "Epoch [706], train_loss: 45628.47 with loss1: 1221.79, loss2: 190.70 and loss3: 44215.98\n",
      "Epoch [707], train_loss: 45598.87 with loss1: 1199.28, loss2: 188.31 and loss3: 44211.28\n",
      "Epoch [708], train_loss: 45603.25 with loss1: 1209.56, loss2: 187.13 and loss3: 44206.56\n",
      "Epoch [709], train_loss: 45574.61 with loss1: 1183.79, loss2: 188.96 and loss3: 44201.87\n",
      "Epoch [710], train_loss: 45581.34 with loss1: 1196.06, loss2: 188.15 and loss3: 44197.14\n",
      "Epoch [711], train_loss: 45558.25 with loss1: 1177.32, loss2: 188.49 and loss3: 44192.44\n",
      "Epoch [712], train_loss: 45559.49 with loss1: 1183.85, loss2: 187.92 and loss3: 44187.71\n",
      "Epoch [713], train_loss: 45538.22 with loss1: 1167.74, loss2: 187.46 and loss3: 44183.02\n",
      "Epoch [714], train_loss: 45545.59 with loss1: 1179.66, loss2: 187.64 and loss3: 44178.29\n",
      "Epoch [715], train_loss: 45516.67 with loss1: 1156.17, loss2: 186.90 and loss3: 44173.59\n",
      "Epoch [716], train_loss: 45519.78 with loss1: 1164.64, loss2: 186.27 and loss3: 44168.87\n",
      "Epoch [717], train_loss: 45495.14 with loss1: 1143.78, loss2: 187.18 and loss3: 44164.18\n",
      "Epoch [718], train_loss: 45502.70 with loss1: 1156.79, loss2: 186.46 and loss3: 44159.45\n",
      "Epoch [719], train_loss: 45478.70 with loss1: 1137.51, loss2: 186.43 and loss3: 44154.76\n",
      "Epoch [720], train_loss: 45476.05 with loss1: 1139.95, loss2: 186.05 and loss3: 44150.04\n",
      "Epoch [721], train_loss: 45462.16 with loss1: 1131.74, loss2: 185.08 and loss3: 44145.34\n",
      "Epoch [722], train_loss: 45460.60 with loss1: 1133.73, loss2: 186.25 and loss3: 44140.62\n",
      "Epoch [723], train_loss: 45446.60 with loss1: 1124.36, loss2: 186.31 and loss3: 44135.93\n",
      "Epoch [724], train_loss: 45445.43 with loss1: 1130.02, loss2: 184.20 and loss3: 44131.21\n",
      "Epoch [725], train_loss: 45432.05 with loss1: 1119.19, loss2: 186.35 and loss3: 44126.51\n",
      "Epoch [726], train_loss: 45430.83 with loss1: 1123.94, loss2: 185.09 and loss3: 44121.80\n",
      "Epoch [727], train_loss: 45415.94 with loss1: 1113.04, loss2: 185.80 and loss3: 44117.10\n",
      "Epoch [728], train_loss: 45414.90 with loss1: 1118.42, loss2: 184.09 and loss3: 44112.39\n",
      "Epoch [729], train_loss: 45398.05 with loss1: 1104.82, loss2: 185.54 and loss3: 44107.69\n",
      "Epoch [730], train_loss: 45398.21 with loss1: 1111.21, loss2: 184.03 and loss3: 44102.98\n",
      "Epoch [731], train_loss: 45386.58 with loss1: 1102.86, loss2: 185.44 and loss3: 44098.27\n",
      "Epoch [732], train_loss: 45385.15 with loss1: 1106.95, loss2: 184.64 and loss3: 44093.56\n",
      "Epoch [733], train_loss: 45374.88 with loss1: 1101.15, loss2: 184.86 and loss3: 44088.86\n",
      "Epoch [734], train_loss: 45375.27 with loss1: 1106.68, loss2: 184.43 and loss3: 44084.16\n",
      "Epoch [735], train_loss: 45361.11 with loss1: 1097.51, loss2: 184.16 and loss3: 44079.45\n",
      "Epoch [736], train_loss: 45358.05 with loss1: 1099.69, loss2: 183.61 and loss3: 44074.75\n",
      "Epoch [737], train_loss: 45346.71 with loss1: 1093.41, loss2: 183.26 and loss3: 44070.04\n",
      "Epoch [738], train_loss: 45348.16 with loss1: 1099.07, loss2: 183.75 and loss3: 44065.34\n",
      "Epoch [739], train_loss: 45337.84 with loss1: 1093.80, loss2: 183.41 and loss3: 44060.63\n",
      "Epoch [740], train_loss: 45336.61 with loss1: 1097.25, loss2: 183.43 and loss3: 44055.93\n",
      "Epoch [741], train_loss: 45327.21 with loss1: 1092.51, loss2: 183.48 and loss3: 44051.22\n",
      "Epoch [742], train_loss: 45328.25 with loss1: 1099.83, loss2: 181.89 and loss3: 44046.52\n",
      "Epoch [743], train_loss: 45317.74 with loss1: 1092.70, loss2: 183.22 and loss3: 44041.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [744], train_loss: 45317.49 with loss1: 1098.21, loss2: 182.16 and loss3: 44037.12\n",
      "Epoch [745], train_loss: 45305.70 with loss1: 1090.13, loss2: 183.15 and loss3: 44032.41\n",
      "Epoch [746], train_loss: 45312.32 with loss1: 1102.07, loss2: 182.53 and loss3: 44027.72\n",
      "Epoch [747], train_loss: 45297.79 with loss1: 1092.34, loss2: 182.45 and loss3: 44023.00\n",
      "Epoch [748], train_loss: 45300.89 with loss1: 1100.42, loss2: 182.15 and loss3: 44018.32\n",
      "Epoch [749], train_loss: 45292.27 with loss1: 1097.18, loss2: 181.49 and loss3: 44013.60\n",
      "Epoch [750], train_loss: 45301.90 with loss1: 1111.02, loss2: 181.96 and loss3: 44008.92\n",
      "Epoch [751], train_loss: 45284.96 with loss1: 1099.29, loss2: 181.46 and loss3: 44004.20\n",
      "Epoch [752], train_loss: 45292.44 with loss1: 1111.96, loss2: 180.96 and loss3: 43999.52\n",
      "Epoch [753], train_loss: 45278.80 with loss1: 1102.91, loss2: 181.08 and loss3: 43994.81\n",
      "Epoch [754], train_loss: 45285.66 with loss1: 1114.40, loss2: 181.14 and loss3: 43990.12\n",
      "Epoch [755], train_loss: 45276.23 with loss1: 1109.36, loss2: 181.46 and loss3: 43985.41\n",
      "Epoch [756], train_loss: 45285.48 with loss1: 1124.35, loss2: 180.41 and loss3: 43980.72\n",
      "Epoch [757], train_loss: 45273.39 with loss1: 1116.68, loss2: 180.70 and loss3: 43976.02\n",
      "Epoch [758], train_loss: 45282.71 with loss1: 1130.31, loss2: 181.07 and loss3: 43971.33\n",
      "Epoch [759], train_loss: 45273.82 with loss1: 1125.55, loss2: 181.64 and loss3: 43966.62\n",
      "Epoch [760], train_loss: 45283.48 with loss1: 1141.35, loss2: 180.20 and loss3: 43961.93\n",
      "Epoch [761], train_loss: 45269.18 with loss1: 1130.70, loss2: 181.25 and loss3: 43957.23\n",
      "Epoch [762], train_loss: 45278.55 with loss1: 1145.44, loss2: 180.57 and loss3: 43952.54\n",
      "Epoch [763], train_loss: 45266.43 with loss1: 1138.96, loss2: 179.64 and loss3: 43947.83\n",
      "Epoch [764], train_loss: 45276.91 with loss1: 1154.52, loss2: 179.24 and loss3: 43943.15\n",
      "Epoch [765], train_loss: 45259.81 with loss1: 1141.74, loss2: 179.62 and loss3: 43938.44\n",
      "Epoch [766], train_loss: 45273.49 with loss1: 1160.44, loss2: 179.30 and loss3: 43933.75\n",
      "Epoch [767], train_loss: 45257.21 with loss1: 1148.43, loss2: 179.73 and loss3: 43929.04\n",
      "Epoch [768], train_loss: 45270.34 with loss1: 1166.27, loss2: 179.70 and loss3: 43924.37\n",
      "Epoch [769], train_loss: 45255.04 with loss1: 1156.40, loss2: 178.98 and loss3: 43919.66\n",
      "Epoch [770], train_loss: 45265.57 with loss1: 1171.36, loss2: 179.24 and loss3: 43914.97\n",
      "Epoch [771], train_loss: 45250.36 with loss1: 1161.92, loss2: 178.17 and loss3: 43910.27\n",
      "Epoch [772], train_loss: 45262.46 with loss1: 1177.70, loss2: 179.17 and loss3: 43905.59\n",
      "Epoch [773], train_loss: 45241.38 with loss1: 1161.64, loss2: 178.86 and loss3: 43900.88\n",
      "Epoch [774], train_loss: 45258.77 with loss1: 1184.71, loss2: 177.87 and loss3: 43896.20\n",
      "Epoch [775], train_loss: 45243.07 with loss1: 1174.02, loss2: 177.56 and loss3: 43891.50\n",
      "Epoch [776], train_loss: 45255.92 with loss1: 1190.48, loss2: 178.64 and loss3: 43886.80\n",
      "Epoch [777], train_loss: 45237.32 with loss1: 1177.25, loss2: 177.96 and loss3: 43882.11\n",
      "Epoch [778], train_loss: 45247.51 with loss1: 1192.75, loss2: 177.34 and loss3: 43877.42\n",
      "Epoch [779], train_loss: 45229.00 with loss1: 1179.04, loss2: 177.22 and loss3: 43872.73\n",
      "Epoch [780], train_loss: 45237.21 with loss1: 1191.13, loss2: 178.05 and loss3: 43868.03\n",
      "Epoch [781], train_loss: 45217.74 with loss1: 1177.49, loss2: 176.91 and loss3: 43863.34\n",
      "Epoch [782], train_loss: 45231.97 with loss1: 1196.24, loss2: 177.10 and loss3: 43858.64\n",
      "Epoch [783], train_loss: 45213.54 with loss1: 1182.01, loss2: 177.58 and loss3: 43853.95\n",
      "Epoch [784], train_loss: 45226.89 with loss1: 1200.34, loss2: 177.29 and loss3: 43849.27\n",
      "Epoch [785], train_loss: 45196.80 with loss1: 1176.13, loss2: 176.10 and loss3: 43844.57\n",
      "Epoch [786], train_loss: 45206.88 with loss1: 1189.62, loss2: 177.38 and loss3: 43839.88\n",
      "Epoch [787], train_loss: 45179.79 with loss1: 1167.28, loss2: 177.31 and loss3: 43835.20\n",
      "Epoch [788], train_loss: 45184.05 with loss1: 1176.32, loss2: 177.23 and loss3: 43830.51\n",
      "Epoch [789], train_loss: 45152.84 with loss1: 1150.33, loss2: 176.70 and loss3: 43825.82\n",
      "Epoch [790], train_loss: 45152.80 with loss1: 1155.47, loss2: 176.20 and loss3: 43821.13\n",
      "Epoch [791], train_loss: 45120.07 with loss1: 1128.09, loss2: 175.54 and loss3: 43816.44\n",
      "Epoch [792], train_loss: 45117.86 with loss1: 1130.48, loss2: 175.62 and loss3: 43811.75\n",
      "Epoch [793], train_loss: 45090.42 with loss1: 1107.77, loss2: 175.59 and loss3: 43807.06\n",
      "Epoch [794], train_loss: 45089.65 with loss1: 1111.58, loss2: 175.70 and loss3: 43802.38\n",
      "Epoch [795], train_loss: 45063.48 with loss1: 1091.31, loss2: 174.48 and loss3: 43797.69\n",
      "Epoch [796], train_loss: 45061.22 with loss1: 1093.33, loss2: 174.89 and loss3: 43793.00\n",
      "Epoch [797], train_loss: 45040.07 with loss1: 1076.20, loss2: 175.56 and loss3: 43788.32\n",
      "Epoch [798], train_loss: 45036.77 with loss1: 1078.54, loss2: 174.60 and loss3: 43783.62\n",
      "Epoch [799], train_loss: 45021.07 with loss1: 1067.43, loss2: 174.70 and loss3: 43778.94\n",
      "Epoch [800], train_loss: 45018.36 with loss1: 1068.93, loss2: 175.16 and loss3: 43774.26\n",
      "Epoch [801], train_loss: 45003.95 with loss1: 1059.80, loss2: 174.58 and loss3: 43769.57\n",
      "Epoch [802], train_loss: 45003.91 with loss1: 1064.66, loss2: 174.37 and loss3: 43764.88\n",
      "Epoch [803], train_loss: 44990.54 with loss1: 1056.10, loss2: 174.25 and loss3: 43760.19\n",
      "Epoch [804], train_loss: 44989.95 with loss1: 1060.38, loss2: 174.06 and loss3: 43755.52\n",
      "Epoch [805], train_loss: 44975.02 with loss1: 1050.68, loss2: 173.52 and loss3: 43750.82\n",
      "Epoch [806], train_loss: 44981.06 with loss1: 1060.26, loss2: 174.66 and loss3: 43746.14\n",
      "Epoch [807], train_loss: 44961.32 with loss1: 1045.50, loss2: 174.37 and loss3: 43741.45\n",
      "Epoch [808], train_loss: 44960.52 with loss1: 1050.48, loss2: 173.27 and loss3: 43736.77\n",
      "Epoch [809], train_loss: 44951.26 with loss1: 1046.45, loss2: 172.72 and loss3: 43732.09\n",
      "Epoch [810], train_loss: 44952.52 with loss1: 1051.87, loss2: 173.25 and loss3: 43727.40\n",
      "Epoch [811], train_loss: 44941.65 with loss1: 1046.04, loss2: 172.89 and loss3: 43722.73\n",
      "Epoch [812], train_loss: 44943.10 with loss1: 1053.08, loss2: 171.99 and loss3: 43718.03\n",
      "Epoch [813], train_loss: 44934.34 with loss1: 1049.32, loss2: 171.66 and loss3: 43713.36\n",
      "Epoch [814], train_loss: 44940.75 with loss1: 1059.50, loss2: 172.58 and loss3: 43708.67\n",
      "Epoch [815], train_loss: 44930.54 with loss1: 1054.62, loss2: 171.93 and loss3: 43703.99\n",
      "Epoch [816], train_loss: 44934.25 with loss1: 1062.23, loss2: 172.72 and loss3: 43699.30\n",
      "Epoch [817], train_loss: 44926.00 with loss1: 1060.20, loss2: 171.18 and loss3: 43694.63\n",
      "Epoch [818], train_loss: 44930.72 with loss1: 1069.19, loss2: 171.59 and loss3: 43689.94\n",
      "Epoch [819], train_loss: 44924.72 with loss1: 1067.72, loss2: 171.72 and loss3: 43685.27\n",
      "Epoch [820], train_loss: 44929.33 with loss1: 1075.99, loss2: 172.76 and loss3: 43680.58\n",
      "Epoch [821], train_loss: 44921.67 with loss1: 1074.70, loss2: 171.06 and loss3: 43675.91\n",
      "Epoch [822], train_loss: 44920.16 with loss1: 1076.09, loss2: 172.85 and loss3: 43671.22\n",
      "Epoch [823], train_loss: 44909.77 with loss1: 1072.65, loss2: 170.57 and loss3: 43666.55\n",
      "Epoch [824], train_loss: 44916.68 with loss1: 1083.08, loss2: 171.74 and loss3: 43661.87\n",
      "Epoch [825], train_loss: 44900.77 with loss1: 1071.94, loss2: 171.64 and loss3: 43657.19\n",
      "Epoch [826], train_loss: 44912.15 with loss1: 1087.90, loss2: 171.75 and loss3: 43652.50\n",
      "Epoch [827], train_loss: 44894.96 with loss1: 1076.52, loss2: 170.61 and loss3: 43647.83\n",
      "Epoch [828], train_loss: 44901.38 with loss1: 1087.25, loss2: 170.98 and loss3: 43643.14\n",
      "Epoch [829], train_loss: 44882.22 with loss1: 1074.29, loss2: 169.45 and loss3: 43638.47\n",
      "Epoch [830], train_loss: 44888.29 with loss1: 1084.03, loss2: 170.47 and loss3: 43633.79\n",
      "Epoch [831], train_loss: 44871.14 with loss1: 1071.08, loss2: 170.95 and loss3: 43629.12\n",
      "Epoch [832], train_loss: 44874.78 with loss1: 1080.65, loss2: 169.71 and loss3: 43624.42\n",
      "Epoch [833], train_loss: 44859.55 with loss1: 1069.97, loss2: 169.82 and loss3: 43619.76\n",
      "Epoch [834], train_loss: 44861.35 with loss1: 1076.49, loss2: 169.79 and loss3: 43615.07\n",
      "Epoch [835], train_loss: 44840.02 with loss1: 1060.29, loss2: 169.33 and loss3: 43610.41\n",
      "Epoch [836], train_loss: 44845.16 with loss1: 1069.41, loss2: 170.03 and loss3: 43605.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [837], train_loss: 44828.23 with loss1: 1058.28, loss2: 168.90 and loss3: 43601.05\n",
      "Epoch [838], train_loss: 44829.35 with loss1: 1063.67, loss2: 169.30 and loss3: 43596.38\n",
      "Epoch [839], train_loss: 44813.30 with loss1: 1052.79, loss2: 168.81 and loss3: 43591.70\n",
      "Epoch [840], train_loss: 44813.65 with loss1: 1057.09, loss2: 169.54 and loss3: 43587.03\n",
      "Epoch [841], train_loss: 44798.21 with loss1: 1047.81, loss2: 168.05 and loss3: 43582.35\n",
      "Epoch [842], train_loss: 44794.57 with loss1: 1048.04, loss2: 168.86 and loss3: 43577.68\n",
      "Epoch [843], train_loss: 44780.73 with loss1: 1040.36, loss2: 167.37 and loss3: 43573.00\n",
      "Epoch [844], train_loss: 44784.91 with loss1: 1047.89, loss2: 168.69 and loss3: 43568.33\n",
      "Epoch [845], train_loss: 44767.68 with loss1: 1036.52, loss2: 167.51 and loss3: 43563.66\n",
      "Epoch [846], train_loss: 44771.95 with loss1: 1044.24, loss2: 168.74 and loss3: 43558.98\n",
      "Epoch [847], train_loss: 44752.16 with loss1: 1029.99, loss2: 167.86 and loss3: 43554.32\n",
      "Epoch [848], train_loss: 44758.16 with loss1: 1040.57, loss2: 167.94 and loss3: 43549.64\n",
      "Epoch [849], train_loss: 44741.86 with loss1: 1029.52, loss2: 167.37 and loss3: 43544.97\n",
      "Epoch [850], train_loss: 44743.07 with loss1: 1035.37, loss2: 167.41 and loss3: 43540.29\n",
      "Epoch [851], train_loss: 44729.93 with loss1: 1027.32, loss2: 166.99 and loss3: 43535.62\n",
      "Epoch [852], train_loss: 44739.27 with loss1: 1040.72, loss2: 167.60 and loss3: 43530.95\n",
      "Epoch [853], train_loss: 44719.93 with loss1: 1026.80, loss2: 166.85 and loss3: 43526.28\n",
      "Epoch [854], train_loss: 44726.22 with loss1: 1036.77, loss2: 167.84 and loss3: 43521.61\n",
      "Epoch [855], train_loss: 44711.28 with loss1: 1028.16, loss2: 166.19 and loss3: 43516.94\n",
      "Epoch [856], train_loss: 44716.26 with loss1: 1036.93, loss2: 167.06 and loss3: 43512.27\n",
      "Epoch [857], train_loss: 44705.75 with loss1: 1031.94, loss2: 166.21 and loss3: 43507.60\n",
      "Epoch [858], train_loss: 44712.79 with loss1: 1042.89, loss2: 166.98 and loss3: 43502.93\n",
      "Epoch [859], train_loss: 44694.75 with loss1: 1030.17, loss2: 166.32 and loss3: 43498.26\n",
      "Epoch [860], train_loss: 44699.16 with loss1: 1039.41, loss2: 166.17 and loss3: 43493.59\n",
      "Epoch [861], train_loss: 44686.26 with loss1: 1031.07, loss2: 166.26 and loss3: 43488.92\n",
      "Epoch [862], train_loss: 44692.57 with loss1: 1042.19, loss2: 166.14 and loss3: 43484.24\n",
      "Epoch [863], train_loss: 44677.81 with loss1: 1033.24, loss2: 164.99 and loss3: 43479.58\n",
      "Epoch [864], train_loss: 44683.62 with loss1: 1042.30, loss2: 166.42 and loss3: 43474.90\n",
      "Epoch [865], train_loss: 44666.44 with loss1: 1031.08, loss2: 165.12 and loss3: 43470.24\n",
      "Epoch [866], train_loss: 44678.57 with loss1: 1046.82, loss2: 166.18 and loss3: 43465.57\n",
      "Epoch [867], train_loss: 44659.17 with loss1: 1033.33, loss2: 164.93 and loss3: 43460.91\n",
      "Epoch [868], train_loss: 44666.09 with loss1: 1043.64, loss2: 166.22 and loss3: 43456.23\n",
      "Epoch [869], train_loss: 44648.86 with loss1: 1032.61, loss2: 164.68 and loss3: 43451.57\n",
      "Epoch [870], train_loss: 44655.79 with loss1: 1042.99, loss2: 165.91 and loss3: 43446.90\n",
      "Epoch [871], train_loss: 44636.60 with loss1: 1029.96, loss2: 164.41 and loss3: 43442.23\n",
      "Epoch [872], train_loss: 44639.37 with loss1: 1037.13, loss2: 164.67 and loss3: 43437.57\n",
      "Epoch [873], train_loss: 44622.30 with loss1: 1025.48, loss2: 163.91 and loss3: 43432.91\n",
      "Epoch [874], train_loss: 44627.79 with loss1: 1033.64, loss2: 165.91 and loss3: 43428.23\n",
      "Epoch [875], train_loss: 44610.92 with loss1: 1022.48, loss2: 164.86 and loss3: 43423.57\n",
      "Epoch [876], train_loss: 44611.33 with loss1: 1028.41, loss2: 164.01 and loss3: 43418.91\n",
      "Epoch [877], train_loss: 44596.67 with loss1: 1018.56, loss2: 163.86 and loss3: 43414.25\n",
      "Epoch [878], train_loss: 44601.59 with loss1: 1027.41, loss2: 164.61 and loss3: 43409.58\n",
      "Epoch [879], train_loss: 44588.25 with loss1: 1019.10, loss2: 164.23 and loss3: 43404.92\n",
      "Epoch [880], train_loss: 44592.50 with loss1: 1028.35, loss2: 163.90 and loss3: 43400.25\n",
      "Epoch [881], train_loss: 44573.86 with loss1: 1014.63, loss2: 163.63 and loss3: 43395.59\n",
      "Epoch [882], train_loss: 44577.31 with loss1: 1023.04, loss2: 163.34 and loss3: 43390.93\n",
      "Epoch [883], train_loss: 44561.77 with loss1: 1012.17, loss2: 163.33 and loss3: 43386.27\n",
      "Epoch [884], train_loss: 44569.45 with loss1: 1024.39, loss2: 163.45 and loss3: 43381.61\n",
      "Epoch [885], train_loss: 44553.89 with loss1: 1014.43, loss2: 162.51 and loss3: 43376.95\n",
      "Epoch [886], train_loss: 44558.59 with loss1: 1023.29, loss2: 163.01 and loss3: 43372.29\n",
      "Epoch [887], train_loss: 44548.57 with loss1: 1017.12, loss2: 163.82 and loss3: 43367.62\n",
      "Epoch [888], train_loss: 44555.36 with loss1: 1028.97, loss2: 163.42 and loss3: 43362.96\n",
      "Epoch [889], train_loss: 44540.44 with loss1: 1019.34, loss2: 162.80 and loss3: 43358.30\n",
      "Epoch [890], train_loss: 44551.38 with loss1: 1035.18, loss2: 162.56 and loss3: 43353.64\n",
      "Epoch [891], train_loss: 44531.66 with loss1: 1020.33, loss2: 162.34 and loss3: 43348.99\n",
      "Epoch [892], train_loss: 44539.54 with loss1: 1032.52, loss2: 162.70 and loss3: 43344.32\n",
      "Epoch [893], train_loss: 44521.04 with loss1: 1019.28, loss2: 162.09 and loss3: 43339.67\n",
      "Epoch [894], train_loss: 44529.39 with loss1: 1031.79, loss2: 162.60 and loss3: 43335.00\n",
      "Epoch [895], train_loss: 44508.18 with loss1: 1015.29, loss2: 162.54 and loss3: 43330.35\n",
      "Epoch [896], train_loss: 44511.48 with loss1: 1024.20, loss2: 161.61 and loss3: 43325.67\n",
      "Epoch [897], train_loss: 44495.64 with loss1: 1012.98, loss2: 161.63 and loss3: 43321.04\n",
      "Epoch [898], train_loss: 44500.72 with loss1: 1022.68, loss2: 161.68 and loss3: 43316.36\n",
      "Epoch [899], train_loss: 44479.50 with loss1: 1005.80, loss2: 161.99 and loss3: 43311.71\n",
      "Epoch [900], train_loss: 44476.54 with loss1: 1008.87, loss2: 160.63 and loss3: 43307.04\n",
      "Epoch [901], train_loss: 44459.21 with loss1: 995.43, loss2: 161.38 and loss3: 43302.39\n",
      "Epoch [902], train_loss: 44459.80 with loss1: 1000.89, loss2: 161.18 and loss3: 43297.73\n",
      "Epoch [903], train_loss: 44442.83 with loss1: 988.69, loss2: 161.06 and loss3: 43293.08\n",
      "Epoch [904], train_loss: 44439.79 with loss1: 990.77, loss2: 160.61 and loss3: 43288.41\n",
      "Epoch [905], train_loss: 44429.09 with loss1: 984.82, loss2: 160.52 and loss3: 43283.75\n",
      "Epoch [906], train_loss: 44423.63 with loss1: 983.96, loss2: 160.58 and loss3: 43279.09\n",
      "Epoch [907], train_loss: 44410.34 with loss1: 975.20, loss2: 160.71 and loss3: 43274.43\n",
      "Epoch [908], train_loss: 44409.27 with loss1: 980.01, loss2: 159.49 and loss3: 43269.77\n",
      "Epoch [909], train_loss: 44397.96 with loss1: 971.78, loss2: 161.07 and loss3: 43265.12\n",
      "Epoch [910], train_loss: 44395.16 with loss1: 974.44, loss2: 160.26 and loss3: 43260.46\n",
      "Epoch [911], train_loss: 44384.29 with loss1: 968.14, loss2: 160.36 and loss3: 43255.80\n",
      "Epoch [912], train_loss: 44377.20 with loss1: 966.57, loss2: 159.49 and loss3: 43251.14\n",
      "Epoch [913], train_loss: 44367.95 with loss1: 961.23, loss2: 160.23 and loss3: 43246.48\n",
      "Epoch [914], train_loss: 44368.01 with loss1: 966.74, loss2: 159.44 and loss3: 43241.83\n",
      "Epoch [915], train_loss: 44357.06 with loss1: 960.32, loss2: 159.58 and loss3: 43237.17\n",
      "Epoch [916], train_loss: 44357.62 with loss1: 965.52, loss2: 159.59 and loss3: 43232.51\n",
      "Epoch [917], train_loss: 44348.24 with loss1: 960.84, loss2: 159.55 and loss3: 43227.86\n",
      "Epoch [918], train_loss: 44346.21 with loss1: 964.43, loss2: 158.57 and loss3: 43223.20\n",
      "Epoch [919], train_loss: 44334.30 with loss1: 956.61, loss2: 159.14 and loss3: 43218.55\n",
      "Epoch [920], train_loss: 44335.70 with loss1: 962.49, loss2: 159.32 and loss3: 43213.89\n",
      "Epoch [921], train_loss: 44327.42 with loss1: 958.27, loss2: 159.92 and loss3: 43209.23\n",
      "Epoch [922], train_loss: 44325.95 with loss1: 962.31, loss2: 159.06 and loss3: 43204.58\n",
      "Epoch [923], train_loss: 44316.77 with loss1: 957.69, loss2: 159.16 and loss3: 43199.91\n",
      "Epoch [924], train_loss: 44317.30 with loss1: 963.23, loss2: 158.80 and loss3: 43195.27\n",
      "Epoch [925], train_loss: 44305.26 with loss1: 955.44, loss2: 159.22 and loss3: 43190.60\n",
      "Epoch [926], train_loss: 44307.13 with loss1: 962.61, loss2: 158.55 and loss3: 43185.97\n",
      "Epoch [927], train_loss: 44297.28 with loss1: 958.02, loss2: 157.97 and loss3: 43181.29\n",
      "Epoch [928], train_loss: 44296.21 with loss1: 961.85, loss2: 157.70 and loss3: 43176.66\n",
      "Epoch [929], train_loss: 44286.52 with loss1: 955.97, loss2: 158.56 and loss3: 43171.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [930], train_loss: 44285.50 with loss1: 960.32, loss2: 157.83 and loss3: 43167.34\n",
      "Epoch [931], train_loss: 44281.86 with loss1: 961.11, loss2: 158.06 and loss3: 43162.69\n",
      "Epoch [932], train_loss: 44277.81 with loss1: 962.43, loss2: 157.34 and loss3: 43158.04\n",
      "Epoch [933], train_loss: 44272.72 with loss1: 960.77, loss2: 158.58 and loss3: 43153.38\n",
      "Epoch [934], train_loss: 44272.42 with loss1: 966.78, loss2: 156.91 and loss3: 43148.73\n",
      "Epoch [935], train_loss: 44264.70 with loss1: 962.91, loss2: 157.72 and loss3: 43144.07\n",
      "Epoch [936], train_loss: 44268.62 with loss1: 971.88, loss2: 157.31 and loss3: 43139.42\n",
      "Epoch [937], train_loss: 44259.69 with loss1: 967.62, loss2: 157.31 and loss3: 43134.77\n",
      "Epoch [938], train_loss: 44261.68 with loss1: 974.87, loss2: 156.69 and loss3: 43130.11\n",
      "Epoch [939], train_loss: 44252.74 with loss1: 969.98, loss2: 157.30 and loss3: 43125.46\n",
      "Epoch [940], train_loss: 44259.11 with loss1: 982.27, loss2: 156.03 and loss3: 43120.81\n",
      "Epoch [941], train_loss: 44246.21 with loss1: 973.59, loss2: 156.47 and loss3: 43116.15\n",
      "Epoch [942], train_loss: 44256.69 with loss1: 988.60, loss2: 156.58 and loss3: 43111.51\n",
      "Epoch [943], train_loss: 44243.79 with loss1: 980.13, loss2: 156.80 and loss3: 43106.86\n",
      "Epoch [944], train_loss: 44253.68 with loss1: 994.85, loss2: 156.62 and loss3: 43102.21\n",
      "Epoch [945], train_loss: 44242.21 with loss1: 987.69, loss2: 156.96 and loss3: 43097.55\n",
      "Epoch [946], train_loss: 44251.19 with loss1: 1003.08, loss2: 155.20 and loss3: 43092.91\n",
      "Epoch [947], train_loss: 44237.30 with loss1: 993.06, loss2: 155.99 and loss3: 43088.25\n",
      "Epoch [948], train_loss: 44250.37 with loss1: 1011.14, loss2: 155.62 and loss3: 43083.61\n",
      "Epoch [949], train_loss: 44241.98 with loss1: 1006.82, loss2: 156.21 and loss3: 43078.95\n",
      "Epoch [950], train_loss: 44251.54 with loss1: 1021.87, loss2: 155.36 and loss3: 43074.30\n",
      "Epoch [951], train_loss: 44239.36 with loss1: 1014.29, loss2: 155.42 and loss3: 43069.65\n",
      "Epoch [952], train_loss: 44253.39 with loss1: 1032.98, loss2: 155.41 and loss3: 43065.00\n",
      "Epoch [953], train_loss: 44239.86 with loss1: 1023.18, loss2: 156.34 and loss3: 43060.35\n",
      "Epoch [954], train_loss: 44256.11 with loss1: 1045.38, loss2: 155.02 and loss3: 43055.71\n",
      "Epoch [955], train_loss: 44239.95 with loss1: 1033.46, loss2: 155.44 and loss3: 43051.05\n",
      "Epoch [956], train_loss: 44257.43 with loss1: 1054.68, loss2: 156.34 and loss3: 43046.41\n",
      "Epoch [957], train_loss: 44237.33 with loss1: 1039.89, loss2: 155.68 and loss3: 43041.76\n",
      "Epoch [958], train_loss: 44247.89 with loss1: 1056.70, loss2: 154.07 and loss3: 43037.11\n",
      "Epoch [959], train_loss: 44227.42 with loss1: 1040.05, loss2: 154.91 and loss3: 43032.46\n",
      "Epoch [960], train_loss: 44240.19 with loss1: 1057.62, loss2: 154.76 and loss3: 43027.81\n",
      "Epoch [961], train_loss: 44212.55 with loss1: 1034.56, loss2: 154.83 and loss3: 43023.16\n",
      "Epoch [962], train_loss: 44221.39 with loss1: 1048.34, loss2: 154.53 and loss3: 43018.52\n",
      "Epoch [963], train_loss: 44201.45 with loss1: 1033.14, loss2: 154.44 and loss3: 43013.88\n",
      "Epoch [964], train_loss: 44208.62 with loss1: 1045.40, loss2: 154.00 and loss3: 43009.22\n",
      "Epoch [965], train_loss: 44186.97 with loss1: 1027.88, loss2: 154.52 and loss3: 43004.58\n",
      "Epoch [966], train_loss: 44194.12 with loss1: 1040.26, loss2: 153.93 and loss3: 42999.93\n",
      "Epoch [967], train_loss: 44174.62 with loss1: 1025.28, loss2: 154.06 and loss3: 42995.28\n",
      "Epoch [968], train_loss: 44177.83 with loss1: 1032.71, loss2: 154.49 and loss3: 42990.63\n",
      "Epoch [969], train_loss: 44156.02 with loss1: 1016.12, loss2: 153.91 and loss3: 42985.99\n",
      "Epoch [970], train_loss: 44157.84 with loss1: 1022.53, loss2: 153.97 and loss3: 42981.35\n",
      "Epoch [971], train_loss: 44136.40 with loss1: 1006.42, loss2: 153.28 and loss3: 42976.70\n",
      "Epoch [972], train_loss: 44138.00 with loss1: 1013.01, loss2: 152.93 and loss3: 42972.06\n",
      "Epoch [973], train_loss: 44115.62 with loss1: 995.11, loss2: 153.11 and loss3: 42967.40\n",
      "Epoch [974], train_loss: 44119.36 with loss1: 1002.87, loss2: 153.72 and loss3: 42962.77\n",
      "Epoch [975], train_loss: 44095.80 with loss1: 984.66, loss2: 153.01 and loss3: 42958.12\n",
      "Epoch [976], train_loss: 44100.94 with loss1: 993.76, loss2: 153.71 and loss3: 42953.48\n",
      "Epoch [977], train_loss: 44080.97 with loss1: 979.52, loss2: 152.62 and loss3: 42948.83\n",
      "Epoch [978], train_loss: 44082.69 with loss1: 985.12, loss2: 153.38 and loss3: 42944.19\n",
      "Epoch [979], train_loss: 44064.21 with loss1: 972.63, loss2: 152.04 and loss3: 42939.54\n",
      "Epoch [980], train_loss: 44065.12 with loss1: 977.47, loss2: 152.75 and loss3: 42934.90\n",
      "Epoch [981], train_loss: 44051.00 with loss1: 968.77, loss2: 151.97 and loss3: 42930.27\n",
      "Epoch [982], train_loss: 44054.39 with loss1: 975.97, loss2: 152.81 and loss3: 42925.62\n",
      "Epoch [983], train_loss: 44043.03 with loss1: 969.48, loss2: 152.57 and loss3: 42920.97\n",
      "Epoch [984], train_loss: 44035.71 with loss1: 966.73, loss2: 152.64 and loss3: 42916.34\n",
      "Epoch [985], train_loss: 44019.94 with loss1: 956.75, loss2: 151.50 and loss3: 42911.69\n",
      "Epoch [986], train_loss: 44020.50 with loss1: 960.73, loss2: 152.71 and loss3: 42907.05\n",
      "Epoch [987], train_loss: 44004.92 with loss1: 951.18, loss2: 151.33 and loss3: 42902.40\n",
      "Epoch [988], train_loss: 44004.37 with loss1: 954.10, loss2: 152.49 and loss3: 42897.78\n",
      "Epoch [989], train_loss: 43989.95 with loss1: 945.40, loss2: 151.42 and loss3: 42893.12\n",
      "Epoch [990], train_loss: 43986.35 with loss1: 945.97, loss2: 151.89 and loss3: 42888.49\n",
      "Epoch [991], train_loss: 43977.62 with loss1: 943.07, loss2: 150.70 and loss3: 42883.85\n",
      "Epoch [992], train_loss: 43973.56 with loss1: 942.80, loss2: 151.54 and loss3: 42879.21\n",
      "Epoch [993], train_loss: 43957.83 with loss1: 932.04, loss2: 151.21 and loss3: 42874.57\n",
      "Epoch [994], train_loss: 43955.57 with loss1: 934.30, loss2: 151.34 and loss3: 42869.93\n",
      "Epoch [995], train_loss: 43945.38 with loss1: 928.71, loss2: 151.38 and loss3: 42865.29\n",
      "Epoch [996], train_loss: 43939.85 with loss1: 928.27, loss2: 150.93 and loss3: 42860.64\n",
      "Epoch [997], train_loss: 43928.92 with loss1: 922.40, loss2: 150.51 and loss3: 42856.02\n",
      "Epoch [998], train_loss: 43926.31 with loss1: 923.84, loss2: 151.10 and loss3: 42851.37\n",
      "Epoch [999], train_loss: 43914.30 with loss1: 917.18, loss2: 150.40 and loss3: 42846.73\n",
      "Epoch [1000], train_loss: 43912.29 with loss1: 919.22, loss2: 150.98 and loss3: 42842.09\n",
      "Epoch [1001], train_loss: 43901.39 with loss1: 914.17, loss2: 149.77 and loss3: 42837.45\n",
      "Epoch [1002], train_loss: 43901.82 with loss1: 918.62, loss2: 150.38 and loss3: 42832.82\n",
      "Epoch [1003], train_loss: 43893.54 with loss1: 915.76, loss2: 149.61 and loss3: 42828.17\n",
      "Epoch [1004], train_loss: 43891.59 with loss1: 918.30, loss2: 149.74 and loss3: 42823.54\n",
      "Epoch [1005], train_loss: 43879.68 with loss1: 910.73, loss2: 150.04 and loss3: 42818.90\n",
      "Epoch [1006], train_loss: 43881.49 with loss1: 916.70, loss2: 150.52 and loss3: 42814.27\n",
      "Epoch [1007], train_loss: 43869.99 with loss1: 910.91, loss2: 149.46 and loss3: 42809.62\n",
      "Epoch [1008], train_loss: 43871.34 with loss1: 916.06, loss2: 150.29 and loss3: 42804.98\n",
      "Epoch [1009], train_loss: 43861.62 with loss1: 912.35, loss2: 148.92 and loss3: 42800.35\n",
      "Epoch [1010], train_loss: 43861.03 with loss1: 915.97, loss2: 149.35 and loss3: 42795.71\n",
      "Epoch [1011], train_loss: 43850.46 with loss1: 910.85, loss2: 148.54 and loss3: 42791.07\n",
      "Epoch [1012], train_loss: 43856.34 with loss1: 920.75, loss2: 149.17 and loss3: 42786.43\n",
      "Epoch [1013], train_loss: 43845.58 with loss1: 915.13, loss2: 148.65 and loss3: 42781.80\n",
      "Epoch [1014], train_loss: 43843.95 with loss1: 917.86, loss2: 148.94 and loss3: 42777.15\n",
      "Epoch [1015], train_loss: 43835.90 with loss1: 914.47, loss2: 148.91 and loss3: 42772.52\n",
      "Epoch [1016], train_loss: 43840.14 with loss1: 923.41, loss2: 148.87 and loss3: 42767.88\n",
      "Epoch [1017], train_loss: 43826.52 with loss1: 915.10, loss2: 148.17 and loss3: 42763.24\n",
      "Epoch [1018], train_loss: 43825.42 with loss1: 917.98, loss2: 148.84 and loss3: 42758.60\n",
      "Epoch [1019], train_loss: 43817.52 with loss1: 915.27, loss2: 148.28 and loss3: 42753.96\n",
      "Epoch [1020], train_loss: 43819.51 with loss1: 921.97, loss2: 148.21 and loss3: 42749.33\n",
      "Epoch [1021], train_loss: 43813.50 with loss1: 920.27, loss2: 148.54 and loss3: 42744.69\n",
      "Epoch [1022], train_loss: 43810.98 with loss1: 923.07, loss2: 147.85 and loss3: 42740.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1023], train_loss: 43801.20 with loss1: 918.39, loss2: 147.39 and loss3: 42735.41\n",
      "Epoch [1024], train_loss: 43805.60 with loss1: 927.05, loss2: 147.77 and loss3: 42730.78\n",
      "Epoch [1025], train_loss: 43790.26 with loss1: 916.13, loss2: 147.99 and loss3: 42726.14\n",
      "Epoch [1026], train_loss: 43792.47 with loss1: 922.82, loss2: 148.14 and loss3: 42721.51\n",
      "Epoch [1027], train_loss: 43784.00 with loss1: 919.74, loss2: 147.38 and loss3: 42716.87\n",
      "Epoch [1028], train_loss: 43785.88 with loss1: 925.87, loss2: 147.77 and loss3: 42712.23\n",
      "Epoch [1029], train_loss: 43776.50 with loss1: 921.07, loss2: 147.83 and loss3: 42707.60\n",
      "Epoch [1030], train_loss: 43775.69 with loss1: 925.26, loss2: 147.46 and loss3: 42702.97\n",
      "Epoch [1031], train_loss: 43767.26 with loss1: 920.91, loss2: 148.02 and loss3: 42698.34\n",
      "Epoch [1032], train_loss: 43771.27 with loss1: 930.52, loss2: 147.05 and loss3: 42693.69\n",
      "Epoch [1033], train_loss: 43758.34 with loss1: 922.94, loss2: 146.33 and loss3: 42689.06\n",
      "Epoch [1034], train_loss: 43763.18 with loss1: 931.03, loss2: 147.73 and loss3: 42684.42\n",
      "Epoch [1035], train_loss: 43750.01 with loss1: 922.90, loss2: 147.31 and loss3: 42679.80\n",
      "Epoch [1036], train_loss: 43750.69 with loss1: 929.22, loss2: 146.30 and loss3: 42675.16\n",
      "Epoch [1037], train_loss: 43741.27 with loss1: 924.02, loss2: 146.72 and loss3: 42670.52\n",
      "Epoch [1038], train_loss: 43739.04 with loss1: 926.65, loss2: 146.51 and loss3: 42665.89\n",
      "Epoch [1039], train_loss: 43725.30 with loss1: 917.85, loss2: 146.19 and loss3: 42661.26\n",
      "Epoch [1040], train_loss: 43727.30 with loss1: 924.28, loss2: 146.40 and loss3: 42656.62\n",
      "Epoch [1041], train_loss: 43721.52 with loss1: 923.55, loss2: 145.98 and loss3: 42651.99\n",
      "Epoch [1042], train_loss: 43722.75 with loss1: 929.22, loss2: 146.17 and loss3: 42647.36\n",
      "Epoch [1043], train_loss: 43711.07 with loss1: 922.99, loss2: 145.36 and loss3: 42642.73\n",
      "Epoch [1044], train_loss: 43715.85 with loss1: 931.64, loss2: 146.11 and loss3: 42638.09\n",
      "Epoch [1045], train_loss: 43702.37 with loss1: 923.14, loss2: 145.77 and loss3: 42633.46\n",
      "Epoch [1046], train_loss: 43706.62 with loss1: 931.30, loss2: 146.49 and loss3: 42628.83\n",
      "Epoch [1047], train_loss: 43693.90 with loss1: 924.31, loss2: 145.39 and loss3: 42624.20\n",
      "Epoch [1048], train_loss: 43694.24 with loss1: 928.82, loss2: 145.85 and loss3: 42619.57\n",
      "Epoch [1049], train_loss: 43683.49 with loss1: 923.01, loss2: 145.56 and loss3: 42614.93\n",
      "Epoch [1050], train_loss: 43679.05 with loss1: 923.49, loss2: 145.24 and loss3: 42610.31\n",
      "Epoch [1051], train_loss: 43669.25 with loss1: 918.29, loss2: 145.29 and loss3: 42605.67\n",
      "Epoch [1052], train_loss: 43673.95 with loss1: 927.99, loss2: 144.92 and loss3: 42601.05\n",
      "Epoch [1053], train_loss: 43654.25 with loss1: 913.00, loss2: 144.84 and loss3: 42596.41\n",
      "Epoch [1054], train_loss: 43656.36 with loss1: 919.87, loss2: 144.71 and loss3: 42591.78\n",
      "Epoch [1055], train_loss: 43642.27 with loss1: 910.48, loss2: 144.64 and loss3: 42587.15\n",
      "Epoch [1056], train_loss: 43641.80 with loss1: 914.55, loss2: 144.73 and loss3: 42582.52\n",
      "Epoch [1057], train_loss: 43628.98 with loss1: 906.50, loss2: 144.59 and loss3: 42577.89\n",
      "Epoch [1058], train_loss: 43634.76 with loss1: 917.43, loss2: 144.07 and loss3: 42573.26\n",
      "Epoch [1059], train_loss: 43623.80 with loss1: 910.76, loss2: 144.41 and loss3: 42568.64\n",
      "Epoch [1060], train_loss: 43622.17 with loss1: 913.41, loss2: 144.76 and loss3: 42564.00\n",
      "Epoch [1061], train_loss: 43608.58 with loss1: 905.48, loss2: 143.72 and loss3: 42559.38\n",
      "Epoch [1062], train_loss: 43609.61 with loss1: 910.33, loss2: 144.55 and loss3: 42554.73\n",
      "Epoch [1063], train_loss: 43596.79 with loss1: 902.52, loss2: 144.15 and loss3: 42550.12\n",
      "Epoch [1064], train_loss: 43596.46 with loss1: 907.22, loss2: 143.77 and loss3: 42545.48\n",
      "Epoch [1065], train_loss: 43586.18 with loss1: 901.40, loss2: 143.91 and loss3: 42540.87\n",
      "Epoch [1066], train_loss: 43588.87 with loss1: 908.38, loss2: 144.27 and loss3: 42536.23\n",
      "Epoch [1067], train_loss: 43573.05 with loss1: 898.28, loss2: 143.15 and loss3: 42531.61\n",
      "Epoch [1068], train_loss: 43574.11 with loss1: 903.32, loss2: 143.82 and loss3: 42526.97\n",
      "Epoch [1069], train_loss: 43557.98 with loss1: 892.54, loss2: 143.09 and loss3: 42522.36\n",
      "Epoch [1070], train_loss: 43559.82 with loss1: 898.95, loss2: 143.15 and loss3: 42517.72\n",
      "Epoch [1071], train_loss: 43547.99 with loss1: 891.70, loss2: 143.19 and loss3: 42513.10\n",
      "Epoch [1072], train_loss: 43548.26 with loss1: 895.76, loss2: 144.03 and loss3: 42508.47\n",
      "Epoch [1073], train_loss: 43535.57 with loss1: 889.07, loss2: 142.66 and loss3: 42503.85\n",
      "Epoch [1074], train_loss: 43541.25 with loss1: 898.69, loss2: 143.35 and loss3: 42499.21\n",
      "Epoch [1075], train_loss: 43531.29 with loss1: 893.80, loss2: 142.89 and loss3: 42494.60\n",
      "Epoch [1076], train_loss: 43528.74 with loss1: 896.09, loss2: 142.69 and loss3: 42489.96\n",
      "Epoch [1077], train_loss: 43515.61 with loss1: 887.92, loss2: 142.35 and loss3: 42485.35\n",
      "Epoch [1078], train_loss: 43516.74 with loss1: 893.14, loss2: 142.88 and loss3: 42480.71\n",
      "Epoch [1079], train_loss: 43507.21 with loss1: 889.15, loss2: 141.97 and loss3: 42476.09\n",
      "Epoch [1080], train_loss: 43508.76 with loss1: 894.00, loss2: 143.30 and loss3: 42471.46\n",
      "Epoch [1081], train_loss: 43497.36 with loss1: 888.36, loss2: 142.15 and loss3: 42466.85\n",
      "Epoch [1082], train_loss: 43498.13 with loss1: 893.68, loss2: 142.24 and loss3: 42462.21\n",
      "Epoch [1083], train_loss: 43486.34 with loss1: 887.36, loss2: 141.39 and loss3: 42457.60\n",
      "Epoch [1084], train_loss: 43487.96 with loss1: 892.50, loss2: 142.49 and loss3: 42452.97\n",
      "Epoch [1085], train_loss: 43474.91 with loss1: 885.56, loss2: 141.00 and loss3: 42448.35\n",
      "Epoch [1086], train_loss: 43479.12 with loss1: 893.09, loss2: 142.29 and loss3: 42443.73\n",
      "Epoch [1087], train_loss: 43466.82 with loss1: 886.22, loss2: 141.50 and loss3: 42439.10\n",
      "Epoch [1088], train_loss: 43466.99 with loss1: 890.62, loss2: 141.89 and loss3: 42434.48\n",
      "Epoch [1089], train_loss: 43456.32 with loss1: 885.06, loss2: 141.41 and loss3: 42429.84\n",
      "Epoch [1090], train_loss: 43456.89 with loss1: 890.02, loss2: 141.64 and loss3: 42425.23\n",
      "Epoch [1091], train_loss: 43447.45 with loss1: 885.43, loss2: 141.42 and loss3: 42420.61\n",
      "Epoch [1092], train_loss: 43446.33 with loss1: 889.15, loss2: 141.19 and loss3: 42415.98\n",
      "Epoch [1093], train_loss: 43436.01 with loss1: 883.93, loss2: 140.72 and loss3: 42411.36\n",
      "Epoch [1094], train_loss: 43441.64 with loss1: 892.91, loss2: 142.00 and loss3: 42406.74\n",
      "Epoch [1095], train_loss: 43429.93 with loss1: 886.43, loss2: 141.39 and loss3: 42402.11\n",
      "Epoch [1096], train_loss: 43434.63 with loss1: 895.92, loss2: 141.22 and loss3: 42397.49\n",
      "Epoch [1097], train_loss: 43422.22 with loss1: 889.29, loss2: 140.06 and loss3: 42392.87\n",
      "Epoch [1098], train_loss: 43425.09 with loss1: 895.84, loss2: 141.01 and loss3: 42388.25\n",
      "Epoch [1099], train_loss: 43415.42 with loss1: 891.29, loss2: 140.49 and loss3: 42383.63\n",
      "Epoch [1100], train_loss: 43416.36 with loss1: 896.46, loss2: 140.89 and loss3: 42379.01\n",
      "Epoch [1101], train_loss: 43406.11 with loss1: 891.87, loss2: 139.85 and loss3: 42374.39\n",
      "Epoch [1102], train_loss: 43408.63 with loss1: 898.57, loss2: 140.30 and loss3: 42369.77\n",
      "Epoch [1103], train_loss: 43394.67 with loss1: 889.90, loss2: 139.63 and loss3: 42365.15\n",
      "Epoch [1104], train_loss: 43404.31 with loss1: 902.51, loss2: 141.28 and loss3: 42360.53\n",
      "Epoch [1105], train_loss: 43391.91 with loss1: 896.99, loss2: 139.02 and loss3: 42355.90\n",
      "Epoch [1106], train_loss: 43393.95 with loss1: 902.29, loss2: 140.38 and loss3: 42351.29\n",
      "Epoch [1107], train_loss: 43380.75 with loss1: 894.58, loss2: 139.50 and loss3: 42346.67\n",
      "Epoch [1108], train_loss: 43385.75 with loss1: 903.14, loss2: 140.56 and loss3: 42342.05\n",
      "Epoch [1109], train_loss: 43374.50 with loss1: 897.86, loss2: 139.21 and loss3: 42337.43\n",
      "Epoch [1110], train_loss: 43382.71 with loss1: 909.68, loss2: 140.23 and loss3: 42332.80\n",
      "Epoch [1111], train_loss: 43369.05 with loss1: 901.85, loss2: 139.01 and loss3: 42328.20\n",
      "Epoch [1112], train_loss: 43375.81 with loss1: 912.11, loss2: 140.13 and loss3: 42323.57\n",
      "Epoch [1113], train_loss: 43362.58 with loss1: 903.50, loss2: 140.12 and loss3: 42318.95\n",
      "Epoch [1114], train_loss: 43366.40 with loss1: 911.76, loss2: 140.31 and loss3: 42314.34\n",
      "Epoch [1115], train_loss: 43354.25 with loss1: 906.11, loss2: 138.42 and loss3: 42309.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1116], train_loss: 43354.42 with loss1: 909.24, loss2: 140.08 and loss3: 42305.09\n",
      "Epoch [1117], train_loss: 43339.22 with loss1: 899.84, loss2: 138.90 and loss3: 42300.48\n",
      "Epoch [1118], train_loss: 43338.65 with loss1: 903.07, loss2: 139.73 and loss3: 42295.85\n",
      "Epoch [1119], train_loss: 43324.35 with loss1: 893.88, loss2: 139.22 and loss3: 42291.25\n",
      "Epoch [1120], train_loss: 43319.08 with loss1: 893.23, loss2: 139.23 and loss3: 42286.62\n",
      "Epoch [1121], train_loss: 43299.83 with loss1: 879.71, loss2: 138.10 and loss3: 42282.01\n",
      "Epoch [1122], train_loss: 43294.53 with loss1: 877.88, loss2: 139.26 and loss3: 42277.39\n",
      "Epoch [1123], train_loss: 43281.05 with loss1: 870.07, loss2: 138.21 and loss3: 42272.77\n",
      "Epoch [1124], train_loss: 43276.16 with loss1: 868.97, loss2: 139.03 and loss3: 42268.16\n",
      "Epoch [1125], train_loss: 43261.54 with loss1: 860.24, loss2: 137.76 and loss3: 42263.54\n",
      "Epoch [1126], train_loss: 43255.01 with loss1: 857.70, loss2: 138.38 and loss3: 42258.93\n",
      "Epoch [1127], train_loss: 43242.81 with loss1: 850.43, loss2: 138.08 and loss3: 42254.30\n",
      "Epoch [1128], train_loss: 43241.93 with loss1: 852.73, loss2: 139.51 and loss3: 42249.69\n",
      "Epoch [1129], train_loss: 43234.26 with loss1: 851.29, loss2: 137.89 and loss3: 42245.07\n",
      "Epoch [1130], train_loss: 43226.43 with loss1: 847.73, loss2: 138.24 and loss3: 42240.45\n",
      "Epoch [1131], train_loss: 43215.95 with loss1: 842.12, loss2: 137.97 and loss3: 42235.85\n",
      "Epoch [1132], train_loss: 43214.88 with loss1: 845.30, loss2: 138.36 and loss3: 42231.23\n",
      "Epoch [1133], train_loss: 43204.46 with loss1: 840.29, loss2: 137.57 and loss3: 42226.61\n",
      "Epoch [1134], train_loss: 43203.74 with loss1: 843.96, loss2: 137.78 and loss3: 42222.00\n",
      "Epoch [1135], train_loss: 43198.54 with loss1: 844.05, loss2: 137.11 and loss3: 42217.38\n",
      "Epoch [1136], train_loss: 43197.25 with loss1: 846.36, loss2: 138.11 and loss3: 42212.77\n",
      "Epoch [1137], train_loss: 43191.89 with loss1: 846.03, loss2: 137.70 and loss3: 42208.16\n",
      "Epoch [1138], train_loss: 43191.40 with loss1: 850.32, loss2: 137.54 and loss3: 42203.54\n",
      "Epoch [1139], train_loss: 43183.48 with loss1: 847.51, loss2: 137.04 and loss3: 42198.93\n",
      "Epoch [1140], train_loss: 43188.68 with loss1: 856.82, loss2: 137.54 and loss3: 42194.32\n",
      "Epoch [1141], train_loss: 43179.69 with loss1: 853.07, loss2: 136.93 and loss3: 42189.70\n",
      "Epoch [1142], train_loss: 43185.56 with loss1: 863.16, loss2: 137.31 and loss3: 42185.09\n",
      "Epoch [1143], train_loss: 43179.71 with loss1: 862.11, loss2: 137.13 and loss3: 42180.48\n",
      "Epoch [1144], train_loss: 43190.38 with loss1: 877.48, loss2: 137.02 and loss3: 42175.88\n",
      "Epoch [1145], train_loss: 43183.49 with loss1: 874.57, loss2: 137.66 and loss3: 42171.26\n",
      "Epoch [1146], train_loss: 43193.60 with loss1: 890.48, loss2: 136.47 and loss3: 42166.65\n",
      "Epoch [1147], train_loss: 43183.61 with loss1: 884.86, loss2: 136.71 and loss3: 42162.04\n",
      "Epoch [1148], train_loss: 43200.40 with loss1: 906.68, loss2: 136.29 and loss3: 42157.43\n",
      "Epoch [1149], train_loss: 43188.08 with loss1: 897.73, loss2: 137.54 and loss3: 42152.81\n",
      "Epoch [1150], train_loss: 43202.83 with loss1: 918.36, loss2: 136.24 and loss3: 42148.22\n",
      "Epoch [1151], train_loss: 43186.90 with loss1: 907.22, loss2: 136.08 and loss3: 42143.59\n",
      "Epoch [1152], train_loss: 43201.00 with loss1: 925.95, loss2: 136.04 and loss3: 42139.00\n",
      "Epoch [1153], train_loss: 43178.62 with loss1: 907.52, loss2: 136.73 and loss3: 42134.38\n",
      "Epoch [1154], train_loss: 43187.56 with loss1: 921.80, loss2: 135.97 and loss3: 42129.79\n",
      "Epoch [1155], train_loss: 43160.23 with loss1: 899.16, loss2: 135.91 and loss3: 42125.16\n",
      "Epoch [1156], train_loss: 43166.12 with loss1: 909.85, loss2: 135.72 and loss3: 42120.57\n",
      "Epoch [1157], train_loss: 43140.64 with loss1: 888.45, loss2: 136.25 and loss3: 42115.95\n",
      "Epoch [1158], train_loss: 43140.06 with loss1: 893.30, loss2: 135.41 and loss3: 42111.35\n",
      "Epoch [1159], train_loss: 43117.62 with loss1: 874.88, loss2: 136.01 and loss3: 42106.73\n",
      "Epoch [1160], train_loss: 43116.97 with loss1: 878.64, loss2: 136.20 and loss3: 42102.13\n",
      "Epoch [1161], train_loss: 43096.45 with loss1: 863.19, loss2: 135.74 and loss3: 42097.52\n",
      "Epoch [1162], train_loss: 43091.30 with loss1: 863.12, loss2: 135.25 and loss3: 42092.93\n",
      "Epoch [1163], train_loss: 43073.38 with loss1: 849.75, loss2: 135.33 and loss3: 42088.30\n",
      "Epoch [1164], train_loss: 43072.51 with loss1: 853.61, loss2: 135.17 and loss3: 42083.72\n",
      "Epoch [1165], train_loss: 43058.43 with loss1: 843.81, loss2: 135.54 and loss3: 42079.08\n",
      "Epoch [1166], train_loss: 43057.34 with loss1: 847.76, loss2: 135.08 and loss3: 42074.50\n",
      "Epoch [1167], train_loss: 43043.30 with loss1: 838.72, loss2: 134.70 and loss3: 42069.88\n",
      "Epoch [1168], train_loss: 43040.58 with loss1: 840.76, loss2: 134.53 and loss3: 42065.29\n",
      "Epoch [1169], train_loss: 43031.66 with loss1: 836.47, loss2: 134.52 and loss3: 42060.67\n",
      "Epoch [1170], train_loss: 43028.07 with loss1: 837.47, loss2: 134.51 and loss3: 42056.09\n",
      "Epoch [1171], train_loss: 43017.42 with loss1: 830.92, loss2: 135.05 and loss3: 42051.45\n",
      "Epoch [1172], train_loss: 43019.26 with loss1: 838.25, loss2: 134.14 and loss3: 42046.88\n",
      "Epoch [1173], train_loss: 43010.11 with loss1: 833.68, loss2: 134.18 and loss3: 42042.25\n",
      "Epoch [1174], train_loss: 43009.62 with loss1: 836.57, loss2: 135.39 and loss3: 42037.66\n",
      "Epoch [1175], train_loss: 42998.79 with loss1: 830.71, loss2: 135.04 and loss3: 42033.04\n",
      "Epoch [1176], train_loss: 43002.50 with loss1: 839.41, loss2: 134.62 and loss3: 42028.47\n",
      "Epoch [1177], train_loss: 42988.54 with loss1: 830.70, loss2: 134.00 and loss3: 42023.84\n",
      "Epoch [1178], train_loss: 42991.94 with loss1: 838.32, loss2: 134.36 and loss3: 42019.26\n",
      "Epoch [1179], train_loss: 42983.55 with loss1: 834.66, loss2: 134.25 and loss3: 42014.64\n",
      "Epoch [1180], train_loss: 42987.34 with loss1: 843.45, loss2: 133.83 and loss3: 42010.06\n",
      "Epoch [1181], train_loss: 42978.95 with loss1: 839.41, loss2: 134.10 and loss3: 42005.43\n",
      "Epoch [1182], train_loss: 42981.04 with loss1: 846.31, loss2: 133.87 and loss3: 42000.86\n",
      "Epoch [1183], train_loss: 42969.84 with loss1: 839.70, loss2: 133.91 and loss3: 41996.23\n",
      "Epoch [1184], train_loss: 42975.24 with loss1: 849.28, loss2: 134.31 and loss3: 41991.65\n",
      "Epoch [1185], train_loss: 42966.95 with loss1: 846.57, loss2: 133.35 and loss3: 41987.03\n",
      "Epoch [1186], train_loss: 42969.40 with loss1: 853.11, loss2: 133.85 and loss3: 41982.45\n",
      "Epoch [1187], train_loss: 42958.79 with loss1: 846.94, loss2: 134.02 and loss3: 41977.83\n",
      "Epoch [1188], train_loss: 42964.27 with loss1: 857.17, loss2: 133.85 and loss3: 41973.25\n",
      "Epoch [1189], train_loss: 42954.08 with loss1: 851.63, loss2: 133.81 and loss3: 41968.64\n",
      "Epoch [1190], train_loss: 42957.55 with loss1: 859.92, loss2: 133.58 and loss3: 41964.05\n",
      "Epoch [1191], train_loss: 42949.20 with loss1: 856.73, loss2: 133.02 and loss3: 41959.45\n",
      "Epoch [1192], train_loss: 42952.32 with loss1: 863.93, loss2: 133.54 and loss3: 41954.85\n",
      "Epoch [1193], train_loss: 42942.52 with loss1: 858.75, loss2: 133.52 and loss3: 41950.24\n",
      "Epoch [1194], train_loss: 42947.73 with loss1: 869.64, loss2: 132.43 and loss3: 41945.66\n",
      "Epoch [1195], train_loss: 42935.69 with loss1: 860.96, loss2: 133.68 and loss3: 41941.05\n",
      "Epoch [1196], train_loss: 42942.18 with loss1: 871.87, loss2: 133.85 and loss3: 41936.46\n",
      "Epoch [1197], train_loss: 42928.07 with loss1: 863.92, loss2: 132.28 and loss3: 41931.87\n",
      "Epoch [1198], train_loss: 42930.73 with loss1: 870.78, loss2: 132.69 and loss3: 41927.27\n",
      "Epoch [1199], train_loss: 42925.64 with loss1: 870.20, loss2: 132.77 and loss3: 41922.68\n",
      "Epoch [1200], train_loss: 42927.41 with loss1: 877.48, loss2: 131.85 and loss3: 41918.08\n",
      "Epoch [1201], train_loss: 42914.49 with loss1: 868.86, loss2: 132.15 and loss3: 41913.48\n",
      "Epoch [1202], train_loss: 42919.09 with loss1: 878.16, loss2: 132.05 and loss3: 41908.89\n",
      "Epoch [1203], train_loss: 42904.18 with loss1: 867.93, loss2: 131.96 and loss3: 41904.29\n",
      "Epoch [1204], train_loss: 42902.94 with loss1: 870.78, loss2: 132.46 and loss3: 41899.70\n",
      "Epoch [1205], train_loss: 42888.22 with loss1: 861.08, loss2: 132.03 and loss3: 41895.11\n",
      "Epoch [1206], train_loss: 42890.84 with loss1: 868.56, loss2: 131.77 and loss3: 41890.51\n",
      "Epoch [1207], train_loss: 42875.90 with loss1: 857.70, loss2: 132.27 and loss3: 41885.93\n",
      "Epoch [1208], train_loss: 42875.43 with loss1: 861.58, loss2: 132.52 and loss3: 41881.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1209], train_loss: 42862.27 with loss1: 854.06, loss2: 131.46 and loss3: 41876.74\n",
      "Epoch [1210], train_loss: 42860.21 with loss1: 856.00, loss2: 132.08 and loss3: 41872.14\n",
      "Epoch [1211], train_loss: 42845.66 with loss1: 845.62, loss2: 132.47 and loss3: 41867.56\n",
      "Epoch [1212], train_loss: 42844.82 with loss1: 850.23, loss2: 131.64 and loss3: 41862.95\n",
      "Epoch [1213], train_loss: 42835.57 with loss1: 845.70, loss2: 131.50 and loss3: 41858.37\n",
      "Epoch [1214], train_loss: 42832.70 with loss1: 847.60, loss2: 131.34 and loss3: 41853.77\n",
      "Epoch [1215], train_loss: 42823.44 with loss1: 842.77, loss2: 131.48 and loss3: 41849.18\n",
      "Epoch [1216], train_loss: 42823.29 with loss1: 847.26, loss2: 131.43 and loss3: 41844.59\n",
      "Epoch [1217], train_loss: 42811.70 with loss1: 841.16, loss2: 130.54 and loss3: 41840.00\n",
      "Epoch [1218], train_loss: 42811.39 with loss1: 844.43, loss2: 131.54 and loss3: 41835.41\n",
      "Epoch [1219], train_loss: 42798.59 with loss1: 837.12, loss2: 130.66 and loss3: 41830.82\n",
      "Epoch [1220], train_loss: 42798.26 with loss1: 841.33, loss2: 130.69 and loss3: 41826.23\n",
      "Epoch [1221], train_loss: 42790.25 with loss1: 838.20, loss2: 130.41 and loss3: 41821.64\n",
      "Epoch [1222], train_loss: 42790.72 with loss1: 842.41, loss2: 131.27 and loss3: 41817.04\n",
      "Epoch [1223], train_loss: 42780.79 with loss1: 837.76, loss2: 130.58 and loss3: 41812.46\n",
      "Epoch [1224], train_loss: 42783.04 with loss1: 844.68, loss2: 130.49 and loss3: 41807.87\n",
      "Epoch [1225], train_loss: 42772.36 with loss1: 838.46, loss2: 130.61 and loss3: 41803.28\n",
      "Epoch [1226], train_loss: 42772.21 with loss1: 843.26, loss2: 130.26 and loss3: 41798.69\n",
      "Epoch [1227], train_loss: 42765.50 with loss1: 840.53, loss2: 130.86 and loss3: 41794.10\n",
      "Epoch [1228], train_loss: 42765.34 with loss1: 845.54, loss2: 130.30 and loss3: 41789.51\n",
      "Epoch [1229], train_loss: 42755.92 with loss1: 839.81, loss2: 131.18 and loss3: 41784.92\n",
      "Epoch [1230], train_loss: 42752.21 with loss1: 841.73, loss2: 130.15 and loss3: 41780.33\n",
      "Epoch [1231], train_loss: 42743.62 with loss1: 837.76, loss2: 130.12 and loss3: 41775.74\n",
      "Epoch [1232], train_loss: 42742.55 with loss1: 840.97, loss2: 130.42 and loss3: 41771.15\n",
      "Epoch [1233], train_loss: 42729.54 with loss1: 833.09, loss2: 129.87 and loss3: 41766.57\n",
      "Epoch [1234], train_loss: 42729.87 with loss1: 837.75, loss2: 130.14 and loss3: 41761.98\n",
      "Epoch [1235], train_loss: 42715.54 with loss1: 827.90, loss2: 130.26 and loss3: 41757.39\n",
      "Epoch [1236], train_loss: 42714.82 with loss1: 831.99, loss2: 130.02 and loss3: 41752.81\n",
      "Epoch [1237], train_loss: 42704.81 with loss1: 826.53, loss2: 130.07 and loss3: 41748.22\n",
      "Epoch [1238], train_loss: 42700.14 with loss1: 827.13, loss2: 129.38 and loss3: 41743.63\n",
      "Epoch [1239], train_loss: 42686.09 with loss1: 817.68, loss2: 129.37 and loss3: 41739.05\n",
      "Epoch [1240], train_loss: 42685.14 with loss1: 821.39, loss2: 129.29 and loss3: 41734.46\n",
      "Epoch [1241], train_loss: 42673.89 with loss1: 814.24, loss2: 129.77 and loss3: 41729.88\n",
      "Epoch [1242], train_loss: 42675.14 with loss1: 820.76, loss2: 129.08 and loss3: 41725.29\n",
      "Epoch [1243], train_loss: 42660.30 with loss1: 810.69, loss2: 128.90 and loss3: 41720.71\n",
      "Epoch [1244], train_loss: 42657.30 with loss1: 812.21, loss2: 128.97 and loss3: 41716.12\n",
      "Epoch [1245], train_loss: 42641.61 with loss1: 801.44, loss2: 128.62 and loss3: 41711.55\n",
      "Epoch [1246], train_loss: 42640.86 with loss1: 805.17, loss2: 128.74 and loss3: 41706.96\n",
      "Epoch [1247], train_loss: 42627.96 with loss1: 796.17, loss2: 129.40 and loss3: 41702.38\n",
      "Epoch [1248], train_loss: 42623.62 with loss1: 797.17, loss2: 128.65 and loss3: 41697.80\n",
      "Epoch [1249], train_loss: 42612.88 with loss1: 790.94, loss2: 128.72 and loss3: 41693.22\n",
      "Epoch [1250], train_loss: 42610.03 with loss1: 792.19, loss2: 129.20 and loss3: 41688.64\n",
      "Epoch [1251], train_loss: 42602.30 with loss1: 789.74, loss2: 128.51 and loss3: 41684.06\n",
      "Epoch [1252], train_loss: 42593.93 with loss1: 785.64, loss2: 128.81 and loss3: 41679.47\n",
      "Epoch [1253], train_loss: 42588.14 with loss1: 784.20, loss2: 129.04 and loss3: 41674.90\n",
      "Epoch [1254], train_loss: 42583.44 with loss1: 785.03, loss2: 128.09 and loss3: 41670.31\n",
      "Epoch [1255], train_loss: 42572.69 with loss1: 778.82, loss2: 128.12 and loss3: 41665.75\n",
      "Epoch [1256], train_loss: 42571.68 with loss1: 782.32, loss2: 128.21 and loss3: 41661.15\n",
      "Epoch [1257], train_loss: 42563.57 with loss1: 778.93, loss2: 128.04 and loss3: 41656.60\n",
      "Epoch [1258], train_loss: 42559.47 with loss1: 779.59, loss2: 127.88 and loss3: 41652.00\n",
      "Epoch [1259], train_loss: 42551.73 with loss1: 776.86, loss2: 127.43 and loss3: 41647.44\n",
      "Epoch [1260], train_loss: 42551.90 with loss1: 779.92, loss2: 129.13 and loss3: 41642.85\n",
      "Epoch [1261], train_loss: 42541.29 with loss1: 775.81, loss2: 127.19 and loss3: 41638.29\n",
      "Epoch [1262], train_loss: 42540.87 with loss1: 779.42, loss2: 127.75 and loss3: 41633.70\n",
      "Epoch [1263], train_loss: 42536.20 with loss1: 779.27, loss2: 127.79 and loss3: 41629.14\n",
      "Epoch [1264], train_loss: 42534.96 with loss1: 782.42, loss2: 128.00 and loss3: 41624.55\n",
      "Epoch [1265], train_loss: 42524.25 with loss1: 777.36, loss2: 126.89 and loss3: 41620.00\n",
      "Epoch [1266], train_loss: 42524.13 with loss1: 780.97, loss2: 127.76 and loss3: 41615.40\n",
      "Epoch [1267], train_loss: 42516.08 with loss1: 777.83, loss2: 127.40 and loss3: 41610.84\n",
      "Epoch [1268], train_loss: 42515.41 with loss1: 781.68, loss2: 127.47 and loss3: 41606.25\n",
      "Epoch [1269], train_loss: 42506.04 with loss1: 777.44, loss2: 126.90 and loss3: 41601.70\n",
      "Epoch [1270], train_loss: 42505.86 with loss1: 781.62, loss2: 127.13 and loss3: 41597.11\n",
      "Epoch [1271], train_loss: 42501.89 with loss1: 782.09, loss2: 127.25 and loss3: 41592.55\n",
      "Epoch [1272], train_loss: 42500.53 with loss1: 785.02, loss2: 127.55 and loss3: 41587.97\n",
      "Epoch [1273], train_loss: 42494.21 with loss1: 784.02, loss2: 126.80 and loss3: 41583.40\n",
      "Epoch [1274], train_loss: 42494.39 with loss1: 789.20, loss2: 126.37 and loss3: 41578.82\n",
      "Epoch [1275], train_loss: 42487.47 with loss1: 786.76, loss2: 126.45 and loss3: 41574.26\n",
      "Epoch [1276], train_loss: 42488.37 with loss1: 791.71, loss2: 126.98 and loss3: 41569.68\n",
      "Epoch [1277], train_loss: 42483.29 with loss1: 791.89, loss2: 126.30 and loss3: 41565.11\n",
      "Epoch [1278], train_loss: 42482.11 with loss1: 794.77, loss2: 126.80 and loss3: 41560.54\n",
      "Epoch [1279], train_loss: 42479.95 with loss1: 797.75, loss2: 126.23 and loss3: 41555.97\n",
      "Epoch [1280], train_loss: 42480.69 with loss1: 802.97, loss2: 126.32 and loss3: 41551.40\n",
      "Epoch [1281], train_loss: 42473.89 with loss1: 801.47, loss2: 125.60 and loss3: 41546.82\n",
      "Epoch [1282], train_loss: 42478.46 with loss1: 809.60, loss2: 126.62 and loss3: 41542.25\n",
      "Epoch [1283], train_loss: 42472.64 with loss1: 809.19, loss2: 125.76 and loss3: 41537.69\n",
      "Epoch [1284], train_loss: 42482.31 with loss1: 822.78, loss2: 126.42 and loss3: 41533.11\n",
      "Epoch [1285], train_loss: 42474.19 with loss1: 820.07, loss2: 125.57 and loss3: 41528.55\n",
      "Epoch [1286], train_loss: 42484.87 with loss1: 834.96, loss2: 125.94 and loss3: 41523.98\n",
      "Epoch [1287], train_loss: 42473.87 with loss1: 828.75, loss2: 125.71 and loss3: 41519.41\n",
      "Epoch [1288], train_loss: 42479.80 with loss1: 837.79, loss2: 127.18 and loss3: 41514.84\n",
      "Epoch [1289], train_loss: 42468.98 with loss1: 833.43, loss2: 125.29 and loss3: 41510.27\n",
      "Epoch [1290], train_loss: 42477.46 with loss1: 845.75, loss2: 126.01 and loss3: 41505.70\n",
      "Epoch [1291], train_loss: 42465.19 with loss1: 839.25, loss2: 124.81 and loss3: 41501.13\n",
      "Epoch [1292], train_loss: 42475.45 with loss1: 853.01, loss2: 125.89 and loss3: 41496.55\n",
      "Epoch [1293], train_loss: 42458.52 with loss1: 841.18, loss2: 125.35 and loss3: 41491.99\n",
      "Epoch [1294], train_loss: 42466.48 with loss1: 852.97, loss2: 126.08 and loss3: 41487.42\n",
      "Epoch [1295], train_loss: 42448.48 with loss1: 840.28, loss2: 125.35 and loss3: 41482.85\n",
      "Epoch [1296], train_loss: 42456.89 with loss1: 852.64, loss2: 125.97 and loss3: 41478.29\n",
      "Epoch [1297], train_loss: 42439.25 with loss1: 841.04, loss2: 124.49 and loss3: 41473.71\n",
      "Epoch [1298], train_loss: 42443.19 with loss1: 848.50, loss2: 125.54 and loss3: 41469.15\n",
      "Epoch [1299], train_loss: 42423.77 with loss1: 834.82, loss2: 124.37 and loss3: 41464.58\n",
      "Epoch [1300], train_loss: 42427.51 with loss1: 842.26, loss2: 125.23 and loss3: 41460.02\n",
      "Epoch [1301], train_loss: 42410.73 with loss1: 829.97, loss2: 125.32 and loss3: 41455.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1302], train_loss: 42411.96 with loss1: 836.01, loss2: 125.06 and loss3: 41450.89\n",
      "Epoch [1303], train_loss: 42396.21 with loss1: 825.14, loss2: 124.75 and loss3: 41446.32\n",
      "Epoch [1304], train_loss: 42394.96 with loss1: 828.41, loss2: 124.79 and loss3: 41441.76\n",
      "Epoch [1305], train_loss: 42376.49 with loss1: 815.45, loss2: 123.86 and loss3: 41437.19\n",
      "Epoch [1306], train_loss: 42375.09 with loss1: 817.95, loss2: 124.51 and loss3: 41432.64\n",
      "Epoch [1307], train_loss: 42360.59 with loss1: 808.35, loss2: 124.17 and loss3: 41428.06\n",
      "Epoch [1308], train_loss: 42361.29 with loss1: 812.61, loss2: 125.17 and loss3: 41423.51\n",
      "Epoch [1309], train_loss: 42343.38 with loss1: 800.15, loss2: 124.29 and loss3: 41418.94\n",
      "Epoch [1310], train_loss: 42342.61 with loss1: 803.59, loss2: 124.64 and loss3: 41414.37\n",
      "Epoch [1311], train_loss: 42329.63 with loss1: 795.75, loss2: 124.06 and loss3: 41409.81\n",
      "Epoch [1312], train_loss: 42325.58 with loss1: 795.56, loss2: 124.78 and loss3: 41405.24\n",
      "Epoch [1313], train_loss: 42310.59 with loss1: 786.37, loss2: 123.53 and loss3: 41400.70\n",
      "Epoch [1314], train_loss: 42311.33 with loss1: 791.08, loss2: 124.13 and loss3: 41396.11\n",
      "Epoch [1315], train_loss: 42298.96 with loss1: 784.09, loss2: 123.31 and loss3: 41391.56\n",
      "Epoch [1316], train_loss: 42297.73 with loss1: 787.21, loss2: 123.53 and loss3: 41386.99\n",
      "Epoch [1317], train_loss: 42285.66 with loss1: 780.00, loss2: 123.21 and loss3: 41382.45\n",
      "Epoch [1318], train_loss: 42284.70 with loss1: 783.11, loss2: 123.73 and loss3: 41377.86\n",
      "Epoch [1319], train_loss: 42274.03 with loss1: 777.57, loss2: 123.14 and loss3: 41373.32\n",
      "Epoch [1320], train_loss: 42275.95 with loss1: 783.68, loss2: 123.52 and loss3: 41368.74\n",
      "Epoch [1321], train_loss: 42265.95 with loss1: 778.34, loss2: 123.42 and loss3: 41364.19\n",
      "Epoch [1322], train_loss: 42265.52 with loss1: 782.48, loss2: 123.41 and loss3: 41359.63\n",
      "Epoch [1323], train_loss: 42256.68 with loss1: 778.49, loss2: 123.12 and loss3: 41355.07\n",
      "Epoch [1324], train_loss: 42257.17 with loss1: 783.47, loss2: 123.19 and loss3: 41350.51\n",
      "Epoch [1325], train_loss: 42247.84 with loss1: 779.09, loss2: 122.80 and loss3: 41345.95\n",
      "Epoch [1326], train_loss: 42251.62 with loss1: 787.59, loss2: 122.65 and loss3: 41341.39\n",
      "Epoch [1327], train_loss: 42240.82 with loss1: 781.24, loss2: 122.74 and loss3: 41336.83\n",
      "Epoch [1328], train_loss: 42244.86 with loss1: 789.75, loss2: 122.85 and loss3: 41332.27\n",
      "Epoch [1329], train_loss: 42231.18 with loss1: 781.03, loss2: 122.43 and loss3: 41327.71\n",
      "Epoch [1330], train_loss: 42236.22 with loss1: 790.58, loss2: 122.49 and loss3: 41323.14\n",
      "Epoch [1331], train_loss: 42224.00 with loss1: 782.81, loss2: 122.60 and loss3: 41318.59\n",
      "Epoch [1332], train_loss: 42228.12 with loss1: 792.16, loss2: 121.93 and loss3: 41314.03\n",
      "Epoch [1333], train_loss: 42216.59 with loss1: 785.06, loss2: 122.06 and loss3: 41309.48\n",
      "Epoch [1334], train_loss: 42220.83 with loss1: 793.11, loss2: 122.80 and loss3: 41304.91\n",
      "Epoch [1335], train_loss: 42211.50 with loss1: 788.36, loss2: 122.78 and loss3: 41300.36\n",
      "Epoch [1336], train_loss: 42211.61 with loss1: 793.58, loss2: 122.22 and loss3: 41295.80\n",
      "Epoch [1337], train_loss: 42198.21 with loss1: 784.38, loss2: 122.59 and loss3: 41291.25\n",
      "Epoch [1338], train_loss: 42200.44 with loss1: 791.56, loss2: 122.19 and loss3: 41286.69\n",
      "Epoch [1339], train_loss: 42186.15 with loss1: 782.03, loss2: 122.00 and loss3: 41282.13\n",
      "Epoch [1340], train_loss: 42187.32 with loss1: 787.27, loss2: 122.48 and loss3: 41277.57\n",
      "Epoch [1341], train_loss: 42174.57 with loss1: 780.09, loss2: 121.45 and loss3: 41273.02\n",
      "Epoch [1342], train_loss: 42175.98 with loss1: 785.69, loss2: 121.84 and loss3: 41268.45\n",
      "Epoch [1343], train_loss: 42159.16 with loss1: 773.39, loss2: 121.87 and loss3: 41263.91\n",
      "Epoch [1344], train_loss: 42157.30 with loss1: 776.27, loss2: 121.70 and loss3: 41259.34\n",
      "Epoch [1345], train_loss: 42143.62 with loss1: 767.42, loss2: 121.40 and loss3: 41254.80\n",
      "Epoch [1346], train_loss: 42141.40 with loss1: 769.54, loss2: 121.62 and loss3: 41250.23\n",
      "Epoch [1347], train_loss: 42127.78 with loss1: 760.63, loss2: 121.47 and loss3: 41245.67\n",
      "Epoch [1348], train_loss: 42126.29 with loss1: 764.06, loss2: 121.12 and loss3: 41241.11\n",
      "Epoch [1349], train_loss: 42116.79 with loss1: 759.15, loss2: 121.07 and loss3: 41236.57\n",
      "Epoch [1350], train_loss: 42112.66 with loss1: 759.54, loss2: 121.12 and loss3: 41232.00\n",
      "Epoch [1351], train_loss: 42102.52 with loss1: 753.70, loss2: 121.36 and loss3: 41227.46\n",
      "Epoch [1352], train_loss: 42102.39 with loss1: 758.74, loss2: 120.76 and loss3: 41222.89\n",
      "Epoch [1353], train_loss: 42092.07 with loss1: 752.58, loss2: 121.14 and loss3: 41218.34\n",
      "Epoch [1354], train_loss: 42091.10 with loss1: 756.15, loss2: 121.18 and loss3: 41213.78\n",
      "Epoch [1355], train_loss: 42081.77 with loss1: 751.24, loss2: 121.28 and loss3: 41209.24\n",
      "Epoch [1356], train_loss: 42080.82 with loss1: 755.13, loss2: 121.01 and loss3: 41204.68\n",
      "Epoch [1357], train_loss: 42068.84 with loss1: 748.12, loss2: 120.58 and loss3: 41200.14\n",
      "Epoch [1358], train_loss: 42067.74 with loss1: 750.92, loss2: 121.25 and loss3: 41195.57\n",
      "Epoch [1359], train_loss: 42057.43 with loss1: 746.01, loss2: 120.39 and loss3: 41191.03\n",
      "Epoch [1360], train_loss: 42054.50 with loss1: 747.09, loss2: 120.94 and loss3: 41186.46\n",
      "Epoch [1361], train_loss: 42045.14 with loss1: 742.42, loss2: 120.79 and loss3: 41181.93\n",
      "Epoch [1362], train_loss: 42043.09 with loss1: 744.40, loss2: 121.33 and loss3: 41177.36\n",
      "Epoch [1363], train_loss: 42038.14 with loss1: 744.55, loss2: 120.76 and loss3: 41172.83\n",
      "Epoch [1364], train_loss: 42035.20 with loss1: 746.12, loss2: 120.83 and loss3: 41168.26\n",
      "Epoch [1365], train_loss: 42027.62 with loss1: 743.49, loss2: 120.40 and loss3: 41163.73\n",
      "Epoch [1366], train_loss: 42027.15 with loss1: 746.56, loss2: 121.43 and loss3: 41159.16\n",
      "Epoch [1367], train_loss: 42018.15 with loss1: 743.14, loss2: 120.38 and loss3: 41154.63\n",
      "Epoch [1368], train_loss: 42016.28 with loss1: 746.09, loss2: 120.13 and loss3: 41150.06\n",
      "Epoch [1369], train_loss: 42008.26 with loss1: 742.88, loss2: 119.85 and loss3: 41145.53\n",
      "Epoch [1370], train_loss: 42008.95 with loss1: 747.88, loss2: 120.12 and loss3: 41140.96\n",
      "Epoch [1371], train_loss: 42001.29 with loss1: 744.78, loss2: 120.08 and loss3: 41136.43\n",
      "Epoch [1372], train_loss: 42001.42 with loss1: 749.10, loss2: 120.46 and loss3: 41131.86\n",
      "Epoch [1373], train_loss: 41992.93 with loss1: 745.80, loss2: 119.80 and loss3: 41127.32\n",
      "Epoch [1374], train_loss: 41993.98 with loss1: 751.38, loss2: 119.84 and loss3: 41122.77\n",
      "Epoch [1375], train_loss: 41985.28 with loss1: 747.41, loss2: 119.65 and loss3: 41118.22\n",
      "Epoch [1376], train_loss: 41987.71 with loss1: 753.74, loss2: 120.30 and loss3: 41113.67\n",
      "Epoch [1377], train_loss: 41978.89 with loss1: 750.51, loss2: 119.24 and loss3: 41109.13\n",
      "Epoch [1378], train_loss: 41983.20 with loss1: 758.80, loss2: 119.83 and loss3: 41104.57\n",
      "Epoch [1379], train_loss: 41972.45 with loss1: 753.08, loss2: 119.33 and loss3: 41100.04\n",
      "Epoch [1380], train_loss: 41976.00 with loss1: 761.32, loss2: 119.21 and loss3: 41095.47\n",
      "Epoch [1381], train_loss: 41968.50 with loss1: 757.84, loss2: 119.72 and loss3: 41090.94\n",
      "Epoch [1382], train_loss: 41968.04 with loss1: 762.45, loss2: 119.21 and loss3: 41086.38\n",
      "Epoch [1383], train_loss: 41958.35 with loss1: 756.86, loss2: 119.65 and loss3: 41081.84\n",
      "Epoch [1384], train_loss: 41959.61 with loss1: 763.05, loss2: 119.28 and loss3: 41077.29\n",
      "Epoch [1385], train_loss: 41953.10 with loss1: 761.18, loss2: 119.17 and loss3: 41072.74\n",
      "Epoch [1386], train_loss: 41954.82 with loss1: 767.41, loss2: 119.21 and loss3: 41068.20\n",
      "Epoch [1387], train_loss: 41946.20 with loss1: 763.37, loss2: 119.17 and loss3: 41063.65\n",
      "Epoch [1388], train_loss: 41946.46 with loss1: 768.58, loss2: 118.77 and loss3: 41059.11\n",
      "Epoch [1389], train_loss: 41936.51 with loss1: 763.27, loss2: 118.68 and loss3: 41054.56\n",
      "Epoch [1390], train_loss: 41939.20 with loss1: 770.50, loss2: 118.68 and loss3: 41050.02\n",
      "Epoch [1391], train_loss: 41929.25 with loss1: 764.57, loss2: 119.20 and loss3: 41045.47\n",
      "Epoch [1392], train_loss: 41930.03 with loss1: 770.56, loss2: 118.54 and loss3: 41040.93\n",
      "Epoch [1393], train_loss: 41917.57 with loss1: 762.77, loss2: 118.41 and loss3: 41036.39\n",
      "Epoch [1394], train_loss: 41923.84 with loss1: 773.12, loss2: 118.88 and loss3: 41031.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1395], train_loss: 41911.12 with loss1: 765.47, loss2: 118.35 and loss3: 41027.30\n",
      "Epoch [1396], train_loss: 41910.14 with loss1: 769.53, loss2: 117.85 and loss3: 41022.75\n",
      "Epoch [1397], train_loss: 41903.37 with loss1: 766.25, loss2: 118.92 and loss3: 41018.21\n",
      "Epoch [1398], train_loss: 41903.62 with loss1: 771.67, loss2: 118.28 and loss3: 41013.67\n",
      "Epoch [1399], train_loss: 41890.96 with loss1: 763.73, loss2: 118.11 and loss3: 41009.12\n",
      "Epoch [1400], train_loss: 41894.07 with loss1: 771.77, loss2: 117.70 and loss3: 41004.59\n",
      "Epoch [1401], train_loss: 41882.88 with loss1: 764.92, loss2: 117.92 and loss3: 41000.04\n",
      "Epoch [1402], train_loss: 41884.35 with loss1: 771.13, loss2: 117.71 and loss3: 40995.50\n",
      "Epoch [1403], train_loss: 41875.98 with loss1: 767.25, loss2: 117.78 and loss3: 40990.95\n",
      "Epoch [1404], train_loss: 41878.91 with loss1: 775.08, loss2: 117.42 and loss3: 40986.41\n",
      "Epoch [1405], train_loss: 41869.01 with loss1: 769.23, loss2: 117.91 and loss3: 40981.87\n",
      "Epoch [1406], train_loss: 41871.60 with loss1: 776.39, loss2: 117.88 and loss3: 40977.32\n",
      "Epoch [1407], train_loss: 41860.88 with loss1: 770.37, loss2: 117.72 and loss3: 40972.78\n",
      "Epoch [1408], train_loss: 41860.70 with loss1: 774.80, loss2: 117.66 and loss3: 40968.24\n",
      "Epoch [1409], train_loss: 41848.32 with loss1: 767.35, loss2: 117.27 and loss3: 40963.70\n",
      "Epoch [1410], train_loss: 41850.35 with loss1: 773.81, loss2: 117.39 and loss3: 40959.15\n",
      "Epoch [1411], train_loss: 41841.39 with loss1: 769.07, loss2: 117.70 and loss3: 40954.62\n",
      "Epoch [1412], train_loss: 41844.29 with loss1: 776.76, loss2: 117.47 and loss3: 40950.06\n",
      "Epoch [1413], train_loss: 41835.95 with loss1: 772.70, loss2: 117.72 and loss3: 40945.54\n",
      "Epoch [1414], train_loss: 41837.59 with loss1: 779.63, loss2: 116.99 and loss3: 40940.98\n",
      "Epoch [1415], train_loss: 41829.53 with loss1: 775.94, loss2: 117.14 and loss3: 40936.45\n",
      "Epoch [1416], train_loss: 41830.99 with loss1: 781.98, loss2: 117.11 and loss3: 40931.90\n",
      "Epoch [1417], train_loss: 41820.16 with loss1: 775.32, loss2: 117.47 and loss3: 40927.37\n",
      "Epoch [1418], train_loss: 41827.34 with loss1: 787.65, loss2: 116.87 and loss3: 40922.82\n",
      "Epoch [1419], train_loss: 41812.54 with loss1: 777.04, loss2: 117.21 and loss3: 40918.29\n",
      "Epoch [1420], train_loss: 41820.12 with loss1: 789.44, loss2: 116.94 and loss3: 40913.73\n",
      "Epoch [1421], train_loss: 41807.88 with loss1: 781.25, loss2: 117.42 and loss3: 40909.21\n",
      "Epoch [1422], train_loss: 41810.93 with loss1: 789.68, loss2: 116.59 and loss3: 40904.66\n",
      "Epoch [1423], train_loss: 41800.07 with loss1: 783.05, loss2: 116.89 and loss3: 40900.14\n",
      "Epoch [1424], train_loss: 41802.94 with loss1: 790.66, loss2: 116.69 and loss3: 40895.58\n",
      "Epoch [1425], train_loss: 41789.63 with loss1: 781.35, loss2: 117.22 and loss3: 40891.06\n",
      "Epoch [1426], train_loss: 41791.40 with loss1: 788.20, loss2: 116.69 and loss3: 40886.52\n",
      "Epoch [1427], train_loss: 41779.30 with loss1: 779.93, loss2: 117.39 and loss3: 40881.98\n",
      "Epoch [1428], train_loss: 41780.90 with loss1: 787.01, loss2: 116.45 and loss3: 40877.44\n",
      "Epoch [1429], train_loss: 41765.06 with loss1: 775.20, loss2: 116.95 and loss3: 40872.91\n",
      "Epoch [1430], train_loss: 41761.65 with loss1: 777.15, loss2: 116.13 and loss3: 40868.37\n",
      "Epoch [1431], train_loss: 41750.14 with loss1: 769.84, loss2: 116.45 and loss3: 40863.85\n",
      "Epoch [1432], train_loss: 41745.49 with loss1: 769.98, loss2: 116.21 and loss3: 40859.30\n",
      "Epoch [1433], train_loss: 41731.34 with loss1: 760.03, loss2: 116.54 and loss3: 40854.78\n",
      "Epoch [1434], train_loss: 41722.90 with loss1: 755.88, loss2: 116.78 and loss3: 40850.23\n",
      "Epoch [1435], train_loss: 41711.15 with loss1: 748.95, loss2: 116.49 and loss3: 40845.71\n",
      "Epoch [1436], train_loss: 41705.23 with loss1: 748.41, loss2: 115.65 and loss3: 40841.17\n",
      "Epoch [1437], train_loss: 41696.63 with loss1: 743.83, loss2: 116.16 and loss3: 40836.64\n",
      "Epoch [1438], train_loss: 41688.14 with loss1: 740.55, loss2: 115.49 and loss3: 40832.11\n",
      "Epoch [1439], train_loss: 41677.87 with loss1: 734.06, loss2: 116.23 and loss3: 40827.57\n",
      "Epoch [1440], train_loss: 41672.41 with loss1: 733.58, loss2: 115.79 and loss3: 40823.04\n",
      "Epoch [1441], train_loss: 41661.65 with loss1: 727.05, loss2: 116.09 and loss3: 40818.51\n",
      "Epoch [1442], train_loss: 41655.96 with loss1: 726.22, loss2: 115.76 and loss3: 40813.98\n",
      "Epoch [1443], train_loss: 41647.54 with loss1: 722.49, loss2: 115.61 and loss3: 40809.45\n",
      "Epoch [1444], train_loss: 41643.63 with loss1: 722.74, loss2: 115.97 and loss3: 40804.91\n",
      "Epoch [1445], train_loss: 41636.27 with loss1: 720.20, loss2: 115.69 and loss3: 40800.38\n",
      "Epoch [1446], train_loss: 41632.26 with loss1: 720.93, loss2: 115.48 and loss3: 40795.85\n",
      "Epoch [1447], train_loss: 41625.57 with loss1: 718.85, loss2: 115.41 and loss3: 40791.31\n",
      "Epoch [1448], train_loss: 41622.72 with loss1: 720.57, loss2: 115.36 and loss3: 40786.79\n",
      "Epoch [1449], train_loss: 41613.02 with loss1: 715.67, loss2: 115.11 and loss3: 40782.25\n",
      "Epoch [1450], train_loss: 41610.68 with loss1: 717.89, loss2: 115.06 and loss3: 40777.72\n",
      "Epoch [1451], train_loss: 41606.80 with loss1: 718.40, loss2: 115.22 and loss3: 40773.18\n",
      "Epoch [1452], train_loss: 41607.01 with loss1: 722.98, loss2: 115.37 and loss3: 40768.66\n",
      "Epoch [1453], train_loss: 41600.92 with loss1: 721.37, loss2: 115.43 and loss3: 40764.12\n",
      "Epoch [1454], train_loss: 41602.98 with loss1: 728.52, loss2: 114.85 and loss3: 40759.61\n",
      "Epoch [1455], train_loss: 41600.70 with loss1: 730.26, loss2: 115.38 and loss3: 40755.06\n",
      "Epoch [1456], train_loss: 41602.59 with loss1: 736.76, loss2: 115.29 and loss3: 40750.54\n",
      "Epoch [1457], train_loss: 41595.83 with loss1: 735.50, loss2: 114.32 and loss3: 40746.01\n",
      "Epoch [1458], train_loss: 41599.23 with loss1: 743.28, loss2: 114.46 and loss3: 40741.48\n",
      "Epoch [1459], train_loss: 41594.90 with loss1: 743.26, loss2: 114.70 and loss3: 40736.95\n",
      "Epoch [1460], train_loss: 41598.60 with loss1: 751.20, loss2: 114.97 and loss3: 40732.43\n",
      "Epoch [1461], train_loss: 41589.83 with loss1: 747.67, loss2: 114.27 and loss3: 40727.89\n",
      "Epoch [1462], train_loss: 41595.88 with loss1: 757.92, loss2: 114.60 and loss3: 40723.37\n",
      "Epoch [1463], train_loss: 41585.90 with loss1: 752.55, loss2: 114.52 and loss3: 40718.83\n",
      "Epoch [1464], train_loss: 41590.15 with loss1: 760.71, loss2: 115.13 and loss3: 40714.31\n",
      "Epoch [1465], train_loss: 41579.58 with loss1: 755.61, loss2: 114.20 and loss3: 40709.77\n",
      "Epoch [1466], train_loss: 41581.06 with loss1: 760.97, loss2: 114.84 and loss3: 40705.25\n",
      "Epoch [1467], train_loss: 41564.34 with loss1: 749.42, loss2: 114.20 and loss3: 40700.72\n",
      "Epoch [1468], train_loss: 41562.43 with loss1: 751.80, loss2: 114.43 and loss3: 40696.20\n",
      "Epoch [1469], train_loss: 41547.85 with loss1: 741.75, loss2: 114.43 and loss3: 40691.66\n",
      "Epoch [1470], train_loss: 41544.32 with loss1: 743.26, loss2: 113.93 and loss3: 40687.14\n",
      "Epoch [1471], train_loss: 41532.91 with loss1: 736.34, loss2: 113.95 and loss3: 40682.61\n",
      "Epoch [1472], train_loss: 41529.84 with loss1: 737.46, loss2: 114.30 and loss3: 40678.08\n",
      "Epoch [1473], train_loss: 41514.84 with loss1: 727.74, loss2: 113.55 and loss3: 40673.56\n",
      "Epoch [1474], train_loss: 41510.86 with loss1: 727.48, loss2: 114.36 and loss3: 40669.03\n",
      "Epoch [1475], train_loss: 41502.23 with loss1: 724.53, loss2: 113.19 and loss3: 40664.50\n",
      "Epoch [1476], train_loss: 41499.57 with loss1: 725.16, loss2: 114.44 and loss3: 40659.98\n",
      "Epoch [1477], train_loss: 41488.89 with loss1: 720.19, loss2: 113.25 and loss3: 40655.45\n",
      "Epoch [1478], train_loss: 41490.24 with loss1: 724.78, loss2: 114.55 and loss3: 40650.92\n",
      "Epoch [1479], train_loss: 41478.11 with loss1: 718.41, loss2: 113.30 and loss3: 40646.39\n",
      "Epoch [1480], train_loss: 41475.11 with loss1: 719.05, loss2: 114.19 and loss3: 40641.87\n",
      "Epoch [1481], train_loss: 41466.05 with loss1: 715.44, loss2: 113.27 and loss3: 40637.34\n",
      "Epoch [1482], train_loss: 41463.08 with loss1: 716.82, loss2: 113.44 and loss3: 40632.82\n",
      "Epoch [1483], train_loss: 41457.91 with loss1: 716.41, loss2: 113.21 and loss3: 40628.29\n",
      "Epoch [1484], train_loss: 41457.33 with loss1: 719.60, loss2: 113.97 and loss3: 40623.77\n",
      "Epoch [1485], train_loss: 41448.12 with loss1: 715.77, loss2: 113.10 and loss3: 40619.24\n",
      "Epoch [1486], train_loss: 41448.84 with loss1: 720.47, loss2: 113.65 and loss3: 40614.71\n",
      "Epoch [1487], train_loss: 41439.63 with loss1: 716.70, loss2: 112.74 and loss3: 40610.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1488], train_loss: 41441.99 with loss1: 723.19, loss2: 113.14 and loss3: 40605.66\n",
      "Epoch [1489], train_loss: 41432.37 with loss1: 718.42, loss2: 112.81 and loss3: 40601.14\n",
      "Epoch [1490], train_loss: 41434.27 with loss1: 724.58, loss2: 113.07 and loss3: 40596.62\n",
      "Epoch [1491], train_loss: 41424.90 with loss1: 720.16, loss2: 112.64 and loss3: 40592.09\n",
      "Epoch [1492], train_loss: 41428.48 with loss1: 727.53, loss2: 113.38 and loss3: 40587.57\n",
      "Epoch [1493], train_loss: 41418.23 with loss1: 722.48, loss2: 112.70 and loss3: 40583.05\n",
      "Epoch [1494], train_loss: 41423.96 with loss1: 732.56, loss2: 112.87 and loss3: 40578.52\n",
      "Epoch [1495], train_loss: 41413.12 with loss1: 727.01, loss2: 112.10 and loss3: 40574.00\n",
      "Epoch [1496], train_loss: 41414.46 with loss1: 731.90, loss2: 113.08 and loss3: 40569.48\n",
      "Epoch [1497], train_loss: 41406.03 with loss1: 728.68, loss2: 112.39 and loss3: 40564.96\n",
      "Epoch [1498], train_loss: 41407.62 with loss1: 734.72, loss2: 112.47 and loss3: 40560.43\n",
      "Epoch [1499], train_loss: 41398.79 with loss1: 730.27, loss2: 112.61 and loss3: 40555.91\n",
      "Epoch [1500], train_loss: 41400.92 with loss1: 736.78, loss2: 112.74 and loss3: 40551.39\n",
      "Epoch [1501], train_loss: 41390.54 with loss1: 731.43, loss2: 112.23 and loss3: 40546.87\n",
      "Epoch [1502], train_loss: 41393.26 with loss1: 738.34, loss2: 112.57 and loss3: 40542.35\n",
      "Epoch [1503], train_loss: 41382.81 with loss1: 733.36, loss2: 111.61 and loss3: 40537.83\n",
      "Epoch [1504], train_loss: 41383.29 with loss1: 737.62, loss2: 112.36 and loss3: 40533.30\n",
      "Epoch [1505], train_loss: 41372.92 with loss1: 732.46, loss2: 111.67 and loss3: 40528.79\n",
      "Epoch [1506], train_loss: 41376.97 with loss1: 740.92, loss2: 111.79 and loss3: 40524.26\n",
      "Epoch [1507], train_loss: 41364.27 with loss1: 733.21, loss2: 111.31 and loss3: 40519.75\n",
      "Epoch [1508], train_loss: 41367.09 with loss1: 739.68, loss2: 112.18 and loss3: 40515.23\n",
      "Epoch [1509], train_loss: 41356.50 with loss1: 734.22, loss2: 111.56 and loss3: 40510.72\n",
      "Epoch [1510], train_loss: 41355.32 with loss1: 737.07, loss2: 112.06 and loss3: 40506.19\n",
      "Epoch [1511], train_loss: 41346.60 with loss1: 733.70, loss2: 111.21 and loss3: 40501.68\n",
      "Epoch [1512], train_loss: 41346.79 with loss1: 738.01, loss2: 111.62 and loss3: 40497.15\n",
      "Epoch [1513], train_loss: 41333.18 with loss1: 729.13, loss2: 111.40 and loss3: 40492.65\n",
      "Epoch [1514], train_loss: 41334.36 with loss1: 734.81, loss2: 111.43 and loss3: 40488.12\n",
      "Epoch [1515], train_loss: 41321.49 with loss1: 726.74, loss2: 111.15 and loss3: 40483.61\n",
      "Epoch [1516], train_loss: 41326.54 with loss1: 735.83, loss2: 111.62 and loss3: 40479.09\n",
      "Epoch [1517], train_loss: 41314.73 with loss1: 728.89, loss2: 111.27 and loss3: 40474.57\n",
      "Epoch [1518], train_loss: 41313.84 with loss1: 732.43, loss2: 111.36 and loss3: 40470.05\n",
      "Epoch [1519], train_loss: 41300.18 with loss1: 723.88, loss2: 110.76 and loss3: 40465.54\n",
      "Epoch [1520], train_loss: 41300.29 with loss1: 728.12, loss2: 111.15 and loss3: 40461.02\n",
      "Epoch [1521], train_loss: 41290.03 with loss1: 722.59, loss2: 110.93 and loss3: 40456.51\n",
      "Epoch [1522], train_loss: 41288.53 with loss1: 725.59, loss2: 110.95 and loss3: 40451.99\n",
      "Epoch [1523], train_loss: 41277.89 with loss1: 719.51, loss2: 110.90 and loss3: 40447.48\n",
      "Epoch [1524], train_loss: 41275.48 with loss1: 721.79, loss2: 110.73 and loss3: 40442.95\n",
      "Epoch [1525], train_loss: 41264.98 with loss1: 715.99, loss2: 110.54 and loss3: 40438.45\n",
      "Epoch [1526], train_loss: 41263.93 with loss1: 719.03, loss2: 110.98 and loss3: 40433.92\n",
      "Epoch [1527], train_loss: 41255.27 with loss1: 714.86, loss2: 111.00 and loss3: 40429.42\n",
      "Epoch [1528], train_loss: 41249.60 with loss1: 714.02, loss2: 110.69 and loss3: 40424.89\n",
      "Epoch [1529], train_loss: 41239.22 with loss1: 708.57, loss2: 110.26 and loss3: 40420.39\n",
      "Epoch [1530], train_loss: 41237.72 with loss1: 711.31, loss2: 110.55 and loss3: 40415.86\n",
      "Epoch [1531], train_loss: 41228.86 with loss1: 707.00, loss2: 110.50 and loss3: 40411.35\n",
      "Epoch [1532], train_loss: 41227.25 with loss1: 709.74, loss2: 110.67 and loss3: 40406.84\n",
      "Epoch [1533], train_loss: 41218.70 with loss1: 706.55, loss2: 109.82 and loss3: 40402.33\n",
      "Epoch [1534], train_loss: 41219.93 with loss1: 711.63, loss2: 110.49 and loss3: 40397.81\n",
      "Epoch [1535], train_loss: 41206.30 with loss1: 702.92, loss2: 110.08 and loss3: 40393.30\n",
      "Epoch [1536], train_loss: 41204.78 with loss1: 706.33, loss2: 109.67 and loss3: 40388.78\n",
      "Epoch [1537], train_loss: 41194.45 with loss1: 700.02, loss2: 110.15 and loss3: 40384.27\n",
      "Epoch [1538], train_loss: 41196.29 with loss1: 705.93, loss2: 110.60 and loss3: 40379.76\n",
      "Epoch [1539], train_loss: 41184.30 with loss1: 699.39, loss2: 109.68 and loss3: 40375.24\n",
      "Epoch [1540], train_loss: 41180.62 with loss1: 699.82, loss2: 110.07 and loss3: 40370.74\n",
      "Epoch [1541], train_loss: 41172.48 with loss1: 696.37, loss2: 109.89 and loss3: 40366.22\n",
      "Epoch [1542], train_loss: 41171.02 with loss1: 699.38, loss2: 109.94 and loss3: 40361.71\n",
      "Epoch [1543], train_loss: 41161.16 with loss1: 694.44, loss2: 109.52 and loss3: 40357.20\n",
      "Epoch [1544], train_loss: 41158.52 with loss1: 695.84, loss2: 110.00 and loss3: 40352.68\n",
      "Epoch [1545], train_loss: 41152.36 with loss1: 694.38, loss2: 109.79 and loss3: 40348.19\n",
      "Epoch [1546], train_loss: 41149.46 with loss1: 696.42, loss2: 109.39 and loss3: 40343.66\n",
      "Epoch [1547], train_loss: 41139.57 with loss1: 690.92, loss2: 109.49 and loss3: 40339.16\n",
      "Epoch [1548], train_loss: 41139.58 with loss1: 695.67, loss2: 109.26 and loss3: 40334.64\n",
      "Epoch [1549], train_loss: 41131.92 with loss1: 692.18, loss2: 109.60 and loss3: 40330.14\n",
      "Epoch [1550], train_loss: 41128.44 with loss1: 693.56, loss2: 109.26 and loss3: 40325.62\n",
      "Epoch [1551], train_loss: 41123.00 with loss1: 692.77, loss2: 109.11 and loss3: 40321.12\n",
      "Epoch [1552], train_loss: 41121.65 with loss1: 695.81, loss2: 109.24 and loss3: 40316.60\n",
      "Epoch [1553], train_loss: 41112.73 with loss1: 691.78, loss2: 108.85 and loss3: 40312.11\n",
      "Epoch [1554], train_loss: 41112.95 with loss1: 696.36, loss2: 109.00 and loss3: 40307.59\n",
      "Epoch [1555], train_loss: 41104.94 with loss1: 692.79, loss2: 109.06 and loss3: 40303.09\n",
      "Epoch [1556], train_loss: 41103.61 with loss1: 696.08, loss2: 108.96 and loss3: 40298.57\n",
      "Epoch [1557], train_loss: 41096.50 with loss1: 693.51, loss2: 108.92 and loss3: 40294.07\n",
      "Epoch [1558], train_loss: 41099.29 with loss1: 700.93, loss2: 108.80 and loss3: 40289.55\n",
      "Epoch [1559], train_loss: 41088.66 with loss1: 695.03, loss2: 108.58 and loss3: 40285.05\n",
      "Epoch [1560], train_loss: 41089.76 with loss1: 700.57, loss2: 108.64 and loss3: 40280.54\n",
      "Epoch [1561], train_loss: 41080.81 with loss1: 695.83, loss2: 108.95 and loss3: 40276.03\n",
      "Epoch [1562], train_loss: 41082.48 with loss1: 702.17, loss2: 108.80 and loss3: 40271.52\n",
      "Epoch [1563], train_loss: 41075.18 with loss1: 699.36, loss2: 108.79 and loss3: 40267.02\n",
      "Epoch [1564], train_loss: 41078.21 with loss1: 707.37, loss2: 108.33 and loss3: 40262.50\n",
      "Epoch [1565], train_loss: 41070.38 with loss1: 704.17, loss2: 108.21 and loss3: 40258.00\n",
      "Epoch [1566], train_loss: 41072.34 with loss1: 710.68, loss2: 108.17 and loss3: 40253.49\n",
      "Epoch [1567], train_loss: 41065.47 with loss1: 707.81, loss2: 108.67 and loss3: 40248.99\n",
      "Epoch [1568], train_loss: 41068.60 with loss1: 715.12, loss2: 109.00 and loss3: 40244.48\n",
      "Epoch [1569], train_loss: 41059.18 with loss1: 711.01, loss2: 108.19 and loss3: 40239.98\n",
      "Epoch [1570], train_loss: 41061.02 with loss1: 717.19, loss2: 108.37 and loss3: 40235.46\n",
      "Epoch [1571], train_loss: 41050.71 with loss1: 712.11, loss2: 107.64 and loss3: 40230.96\n",
      "Epoch [1572], train_loss: 41053.07 with loss1: 718.86, loss2: 107.76 and loss3: 40226.45\n",
      "Epoch [1573], train_loss: 41043.20 with loss1: 713.12, loss2: 108.12 and loss3: 40221.95\n",
      "Epoch [1574], train_loss: 41046.52 with loss1: 721.23, loss2: 107.86 and loss3: 40217.44\n",
      "Epoch [1575], train_loss: 41030.61 with loss1: 709.58, loss2: 108.09 and loss3: 40212.95\n",
      "Epoch [1576], train_loss: 41033.32 with loss1: 717.32, loss2: 107.58 and loss3: 40208.43\n",
      "Epoch [1577], train_loss: 41017.23 with loss1: 705.56, loss2: 107.73 and loss3: 40203.94\n",
      "Epoch [1578], train_loss: 41014.40 with loss1: 707.28, loss2: 107.70 and loss3: 40199.43\n",
      "Epoch [1579], train_loss: 41002.57 with loss1: 699.72, loss2: 107.93 and loss3: 40194.93\n",
      "Epoch [1580], train_loss: 40997.10 with loss1: 699.09, loss2: 107.59 and loss3: 40190.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1581], train_loss: 40985.67 with loss1: 692.16, loss2: 107.59 and loss3: 40185.92\n",
      "Epoch [1582], train_loss: 40983.18 with loss1: 694.06, loss2: 107.70 and loss3: 40181.42\n",
      "Epoch [1583], train_loss: 40969.99 with loss1: 685.68, loss2: 107.40 and loss3: 40176.91\n",
      "Epoch [1584], train_loss: 40965.54 with loss1: 685.54, loss2: 107.59 and loss3: 40172.42\n",
      "Epoch [1585], train_loss: 40954.35 with loss1: 678.96, loss2: 107.48 and loss3: 40167.91\n",
      "Epoch [1586], train_loss: 40950.13 with loss1: 679.60, loss2: 107.12 and loss3: 40163.41\n",
      "Epoch [1587], train_loss: 40941.41 with loss1: 675.13, loss2: 107.37 and loss3: 40158.91\n",
      "Epoch [1588], train_loss: 40935.65 with loss1: 673.97, loss2: 107.27 and loss3: 40154.41\n",
      "Epoch [1589], train_loss: 40926.57 with loss1: 669.77, loss2: 106.89 and loss3: 40149.91\n",
      "Epoch [1590], train_loss: 40925.11 with loss1: 672.82, loss2: 106.88 and loss3: 40145.41\n",
      "Epoch [1591], train_loss: 40915.65 with loss1: 667.30, loss2: 107.44 and loss3: 40140.91\n",
      "Epoch [1592], train_loss: 40911.23 with loss1: 667.82, loss2: 107.00 and loss3: 40136.41\n",
      "Epoch [1593], train_loss: 40903.55 with loss1: 664.58, loss2: 107.06 and loss3: 40131.91\n",
      "Epoch [1594], train_loss: 40902.60 with loss1: 668.53, loss2: 106.66 and loss3: 40127.41\n",
      "Epoch [1595], train_loss: 40893.23 with loss1: 663.52, loss2: 106.79 and loss3: 40122.91\n",
      "Epoch [1596], train_loss: 40891.20 with loss1: 665.94, loss2: 106.84 and loss3: 40118.42\n",
      "Epoch [1597], train_loss: 40881.07 with loss1: 660.64, loss2: 106.52 and loss3: 40113.91\n",
      "Epoch [1598], train_loss: 40879.58 with loss1: 663.50, loss2: 106.65 and loss3: 40109.43\n",
      "Epoch [1599], train_loss: 40871.95 with loss1: 660.99, loss2: 106.05 and loss3: 40104.92\n",
      "Epoch [1600], train_loss: 40870.82 with loss1: 664.10, loss2: 106.29 and loss3: 40100.43\n",
      "Epoch [1601], train_loss: 40864.54 with loss1: 662.10, loss2: 106.52 and loss3: 40095.92\n",
      "Epoch [1602], train_loss: 40860.89 with loss1: 662.73, loss2: 106.72 and loss3: 40091.44\n",
      "Epoch [1603], train_loss: 40855.72 with loss1: 662.36, loss2: 106.44 and loss3: 40086.93\n",
      "Epoch [1604], train_loss: 40852.95 with loss1: 664.33, loss2: 106.17 and loss3: 40082.45\n",
      "Epoch [1605], train_loss: 40848.50 with loss1: 664.06, loss2: 106.50 and loss3: 40077.95\n",
      "Epoch [1606], train_loss: 40847.14 with loss1: 667.36, loss2: 106.32 and loss3: 40073.46\n",
      "Epoch [1607], train_loss: 40838.80 with loss1: 663.87, loss2: 105.98 and loss3: 40068.95\n",
      "Epoch [1608], train_loss: 40837.05 with loss1: 667.04, loss2: 105.54 and loss3: 40064.47\n",
      "Epoch [1609], train_loss: 40831.95 with loss1: 665.82, loss2: 106.16 and loss3: 40059.97\n",
      "Epoch [1610], train_loss: 40831.29 with loss1: 669.59, loss2: 106.21 and loss3: 40055.48\n",
      "Epoch [1611], train_loss: 40824.20 with loss1: 666.98, loss2: 106.24 and loss3: 40050.98\n",
      "Epoch [1612], train_loss: 40822.99 with loss1: 671.25, loss2: 105.24 and loss3: 40046.50\n",
      "Epoch [1613], train_loss: 40817.23 with loss1: 669.40, loss2: 105.85 and loss3: 40041.99\n",
      "Epoch [1614], train_loss: 40818.97 with loss1: 676.16, loss2: 105.29 and loss3: 40037.52\n",
      "Epoch [1615], train_loss: 40814.64 with loss1: 675.78, loss2: 105.87 and loss3: 40033.00\n",
      "Epoch [1616], train_loss: 40814.21 with loss1: 680.05, loss2: 105.64 and loss3: 40028.53\n",
      "Epoch [1617], train_loss: 40807.71 with loss1: 677.97, loss2: 105.72 and loss3: 40024.02\n",
      "Epoch [1618], train_loss: 40809.51 with loss1: 684.55, loss2: 105.42 and loss3: 40019.55\n",
      "Epoch [1619], train_loss: 40805.50 with loss1: 684.62, loss2: 105.84 and loss3: 40015.04\n",
      "Epoch [1620], train_loss: 40806.35 with loss1: 690.38, loss2: 105.42 and loss3: 40010.55\n",
      "Epoch [1621], train_loss: 40800.13 with loss1: 688.58, loss2: 105.49 and loss3: 40006.06\n",
      "Epoch [1622], train_loss: 40805.70 with loss1: 698.75, loss2: 105.39 and loss3: 40001.57\n",
      "Epoch [1623], train_loss: 40796.22 with loss1: 693.55, loss2: 105.59 and loss3: 39997.08\n",
      "Epoch [1624], train_loss: 40802.79 with loss1: 704.77, loss2: 105.43 and loss3: 39992.59\n",
      "Epoch [1625], train_loss: 40794.48 with loss1: 700.76, loss2: 105.62 and loss3: 39988.10\n",
      "Epoch [1626], train_loss: 40798.30 with loss1: 709.66, loss2: 105.03 and loss3: 39983.60\n",
      "Epoch [1627], train_loss: 40789.57 with loss1: 705.25, loss2: 105.19 and loss3: 39979.12\n",
      "Epoch [1628], train_loss: 40792.96 with loss1: 713.32, loss2: 105.02 and loss3: 39974.62\n",
      "Epoch [1629], train_loss: 40783.81 with loss1: 708.64, loss2: 105.02 and loss3: 39970.15\n",
      "Epoch [1630], train_loss: 40785.98 with loss1: 715.18, loss2: 105.16 and loss3: 39965.64\n",
      "Epoch [1631], train_loss: 40774.77 with loss1: 708.73, loss2: 104.87 and loss3: 39961.17\n",
      "Epoch [1632], train_loss: 40775.75 with loss1: 714.24, loss2: 104.85 and loss3: 39956.66\n",
      "Epoch [1633], train_loss: 40763.03 with loss1: 705.41, loss2: 105.42 and loss3: 39952.19\n",
      "Epoch [1634], train_loss: 40766.69 with loss1: 714.40, loss2: 104.61 and loss3: 39947.68\n",
      "Epoch [1635], train_loss: 40754.07 with loss1: 705.49, loss2: 105.37 and loss3: 39943.21\n",
      "Epoch [1636], train_loss: 40758.48 with loss1: 715.32, loss2: 104.46 and loss3: 39938.71\n",
      "Epoch [1637], train_loss: 40745.51 with loss1: 705.96, loss2: 105.31 and loss3: 39934.24\n",
      "Epoch [1638], train_loss: 40744.91 with loss1: 710.86, loss2: 104.32 and loss3: 39929.73\n",
      "Epoch [1639], train_loss: 40731.75 with loss1: 701.69, loss2: 104.81 and loss3: 39925.26\n",
      "Epoch [1640], train_loss: 40732.84 with loss1: 706.70, loss2: 105.39 and loss3: 39920.76\n",
      "Epoch [1641], train_loss: 40719.13 with loss1: 698.44, loss2: 104.42 and loss3: 39916.28\n",
      "Epoch [1642], train_loss: 40720.95 with loss1: 704.99, loss2: 104.17 and loss3: 39911.79\n",
      "Epoch [1643], train_loss: 40711.01 with loss1: 699.01, loss2: 104.69 and loss3: 39907.30\n",
      "Epoch [1644], train_loss: 40707.45 with loss1: 700.59, loss2: 104.04 and loss3: 39902.81\n",
      "Epoch [1645], train_loss: 40697.62 with loss1: 695.16, loss2: 104.13 and loss3: 39898.33\n",
      "Epoch [1646], train_loss: 40697.98 with loss1: 700.21, loss2: 103.92 and loss3: 39893.84\n",
      "Epoch [1647], train_loss: 40685.12 with loss1: 691.27, loss2: 104.50 and loss3: 39889.36\n",
      "Epoch [1648], train_loss: 40684.53 with loss1: 695.51, loss2: 104.15 and loss3: 39884.87\n",
      "Epoch [1649], train_loss: 40676.37 with loss1: 691.86, loss2: 104.12 and loss3: 39880.39\n",
      "Epoch [1650], train_loss: 40678.56 with loss1: 698.78, loss2: 103.88 and loss3: 39875.89\n",
      "Epoch [1651], train_loss: 40669.41 with loss1: 693.58, loss2: 104.41 and loss3: 39871.41\n",
      "Epoch [1652], train_loss: 40667.87 with loss1: 697.04, loss2: 103.91 and loss3: 39866.92\n",
      "Epoch [1653], train_loss: 40659.94 with loss1: 693.16, loss2: 104.34 and loss3: 39862.45\n",
      "Epoch [1654], train_loss: 40660.71 with loss1: 698.66, loss2: 104.10 and loss3: 39857.95\n",
      "Epoch [1655], train_loss: 40650.23 with loss1: 692.96, loss2: 103.80 and loss3: 39853.47\n",
      "Epoch [1656], train_loss: 40649.87 with loss1: 696.81, loss2: 104.08 and loss3: 39848.98\n",
      "Epoch [1657], train_loss: 40641.57 with loss1: 693.45, loss2: 103.62 and loss3: 39844.50\n",
      "Epoch [1658], train_loss: 40643.16 with loss1: 699.37, loss2: 103.77 and loss3: 39840.02\n",
      "Epoch [1659], train_loss: 40633.24 with loss1: 693.81, loss2: 103.90 and loss3: 39835.53\n",
      "Epoch [1660], train_loss: 40631.33 with loss1: 696.83, loss2: 103.45 and loss3: 39831.05\n",
      "Epoch [1661], train_loss: 40621.14 with loss1: 690.97, loss2: 103.60 and loss3: 39826.56\n",
      "Epoch [1662], train_loss: 40622.46 with loss1: 697.01, loss2: 103.38 and loss3: 39822.08\n",
      "Epoch [1663], train_loss: 40613.92 with loss1: 693.12, loss2: 103.20 and loss3: 39817.60\n",
      "Epoch [1664], train_loss: 40610.61 with loss1: 693.76, loss2: 103.73 and loss3: 39813.12\n",
      "Epoch [1665], train_loss: 40598.57 with loss1: 686.65, loss2: 103.29 and loss3: 39808.63\n",
      "Epoch [1666], train_loss: 40599.51 with loss1: 692.01, loss2: 103.34 and loss3: 39804.16\n",
      "Epoch [1667], train_loss: 40588.38 with loss1: 685.56, loss2: 103.15 and loss3: 39799.67\n",
      "Epoch [1668], train_loss: 40586.83 with loss1: 688.23, loss2: 103.41 and loss3: 39795.19\n",
      "Epoch [1669], train_loss: 40576.16 with loss1: 682.02, loss2: 103.44 and loss3: 39790.71\n",
      "Epoch [1670], train_loss: 40575.77 with loss1: 686.52, loss2: 103.03 and loss3: 39786.22\n",
      "Epoch [1671], train_loss: 40566.01 with loss1: 681.12, loss2: 103.14 and loss3: 39781.75\n",
      "Epoch [1672], train_loss: 40565.61 with loss1: 685.06, loss2: 103.30 and loss3: 39777.25\n",
      "Epoch [1673], train_loss: 40556.61 with loss1: 681.16, loss2: 102.65 and loss3: 39772.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1674], train_loss: 40560.86 with loss1: 689.63, loss2: 102.94 and loss3: 39768.29\n",
      "Epoch [1675], train_loss: 40550.03 with loss1: 683.29, loss2: 102.91 and loss3: 39763.83\n",
      "Epoch [1676], train_loss: 40549.27 with loss1: 686.99, loss2: 102.94 and loss3: 39759.34\n",
      "Epoch [1677], train_loss: 40539.28 with loss1: 681.73, loss2: 102.68 and loss3: 39754.86\n",
      "Epoch [1678], train_loss: 40540.92 with loss1: 687.75, loss2: 102.79 and loss3: 39750.38\n",
      "Epoch [1679], train_loss: 40530.47 with loss1: 682.06, loss2: 102.51 and loss3: 39745.90\n",
      "Epoch [1680], train_loss: 40534.00 with loss1: 689.70, loss2: 102.88 and loss3: 39741.42\n",
      "Epoch [1681], train_loss: 40519.42 with loss1: 680.29, loss2: 102.19 and loss3: 39736.94\n",
      "Epoch [1682], train_loss: 40519.71 with loss1: 684.82, loss2: 102.44 and loss3: 39732.46\n",
      "Epoch [1683], train_loss: 40509.01 with loss1: 679.03, loss2: 102.00 and loss3: 39727.97\n",
      "Epoch [1684], train_loss: 40509.76 with loss1: 683.41, loss2: 102.85 and loss3: 39723.50\n",
      "Epoch [1685], train_loss: 40496.92 with loss1: 675.43, loss2: 102.47 and loss3: 39719.02\n",
      "Epoch [1686], train_loss: 40497.40 with loss1: 679.87, loss2: 102.98 and loss3: 39714.55\n",
      "Epoch [1687], train_loss: 40483.05 with loss1: 671.05, loss2: 101.95 and loss3: 39710.05\n",
      "Epoch [1688], train_loss: 40483.71 with loss1: 675.47, loss2: 102.66 and loss3: 39705.59\n",
      "Epoch [1689], train_loss: 40476.27 with loss1: 673.07, loss2: 102.11 and loss3: 39701.09\n",
      "Epoch [1690], train_loss: 40472.48 with loss1: 673.41, loss2: 102.44 and loss3: 39696.63\n",
      "Epoch [1691], train_loss: 40458.60 with loss1: 664.56, loss2: 101.89 and loss3: 39692.14\n",
      "Epoch [1692], train_loss: 40460.62 with loss1: 670.76, loss2: 102.20 and loss3: 39687.67\n",
      "Epoch [1693], train_loss: 40447.84 with loss1: 662.63, loss2: 102.03 and loss3: 39683.19\n",
      "Epoch [1694], train_loss: 40445.91 with loss1: 664.27, loss2: 102.93 and loss3: 39678.71\n",
      "Epoch [1695], train_loss: 40435.21 with loss1: 659.48, loss2: 101.50 and loss3: 39674.23\n",
      "Epoch [1696], train_loss: 40434.00 with loss1: 662.01, loss2: 102.25 and loss3: 39669.75\n",
      "Epoch [1697], train_loss: 40422.72 with loss1: 655.82, loss2: 101.61 and loss3: 39665.28\n",
      "Epoch [1698], train_loss: 40422.16 with loss1: 659.61, loss2: 101.76 and loss3: 39660.80\n",
      "Epoch [1699], train_loss: 40415.44 with loss1: 657.33, loss2: 101.78 and loss3: 39656.33\n",
      "Epoch [1700], train_loss: 40409.20 with loss1: 655.70, loss2: 101.67 and loss3: 39651.84\n",
      "Epoch [1701], train_loss: 40403.66 with loss1: 654.60, loss2: 101.68 and loss3: 39647.38\n",
      "Epoch [1702], train_loss: 40401.55 with loss1: 656.36, loss2: 102.30 and loss3: 39642.89\n",
      "Epoch [1703], train_loss: 40392.03 with loss1: 652.08, loss2: 101.52 and loss3: 39638.42\n",
      "Epoch [1704], train_loss: 40391.84 with loss1: 655.69, loss2: 102.21 and loss3: 39633.94\n",
      "Epoch [1705], train_loss: 40381.45 with loss1: 650.90, loss2: 101.08 and loss3: 39629.47\n",
      "Epoch [1706], train_loss: 40381.27 with loss1: 654.64, loss2: 101.65 and loss3: 39624.98\n",
      "Epoch [1707], train_loss: 40375.02 with loss1: 653.17, loss2: 101.34 and loss3: 39620.51\n",
      "Epoch [1708], train_loss: 40371.11 with loss1: 653.86, loss2: 101.22 and loss3: 39616.03\n",
      "Epoch [1709], train_loss: 40364.52 with loss1: 652.21, loss2: 100.75 and loss3: 39611.56\n",
      "Epoch [1710], train_loss: 40364.36 with loss1: 656.04, loss2: 101.24 and loss3: 39607.07\n",
      "Epoch [1711], train_loss: 40356.19 with loss1: 652.57, loss2: 101.01 and loss3: 39602.61\n",
      "Epoch [1712], train_loss: 40354.38 with loss1: 654.91, loss2: 101.34 and loss3: 39598.12\n",
      "Epoch [1713], train_loss: 40347.58 with loss1: 652.69, loss2: 101.23 and loss3: 39593.66\n",
      "Epoch [1714], train_loss: 40347.05 with loss1: 656.52, loss2: 101.36 and loss3: 39589.17\n",
      "Epoch [1715], train_loss: 40337.70 with loss1: 652.01, loss2: 100.98 and loss3: 39584.71\n",
      "Epoch [1716], train_loss: 40339.27 with loss1: 657.66, loss2: 101.38 and loss3: 39580.23\n",
      "Epoch [1717], train_loss: 40330.00 with loss1: 653.51, loss2: 100.73 and loss3: 39575.76\n",
      "Epoch [1718], train_loss: 40328.24 with loss1: 656.09, loss2: 100.87 and loss3: 39571.29\n",
      "Epoch [1719], train_loss: 40321.02 with loss1: 653.79, loss2: 100.41 and loss3: 39566.82\n",
      "Epoch [1720], train_loss: 40321.55 with loss1: 658.18, loss2: 101.03 and loss3: 39562.34\n",
      "Epoch [1721], train_loss: 40313.48 with loss1: 654.88, loss2: 100.74 and loss3: 39557.87\n",
      "Epoch [1722], train_loss: 40312.48 with loss1: 658.23, loss2: 100.85 and loss3: 39553.39\n",
      "Epoch [1723], train_loss: 40304.81 with loss1: 655.59, loss2: 100.30 and loss3: 39548.92\n",
      "Epoch [1724], train_loss: 40307.24 with loss1: 662.33, loss2: 100.46 and loss3: 39544.45\n",
      "Epoch [1725], train_loss: 40296.73 with loss1: 656.53, loss2: 100.22 and loss3: 39539.98\n",
      "Epoch [1726], train_loss: 40297.22 with loss1: 660.82, loss2: 100.90 and loss3: 39535.50\n",
      "Epoch [1727], train_loss: 40287.39 with loss1: 656.33, loss2: 100.02 and loss3: 39531.04\n",
      "Epoch [1728], train_loss: 40288.68 with loss1: 661.72, loss2: 100.39 and loss3: 39526.57\n",
      "Epoch [1729], train_loss: 40279.00 with loss1: 656.67, loss2: 100.23 and loss3: 39522.09\n",
      "Epoch [1730], train_loss: 40279.66 with loss1: 661.37, loss2: 100.67 and loss3: 39517.62\n",
      "Epoch [1731], train_loss: 40269.59 with loss1: 656.63, loss2: 99.80 and loss3: 39513.16\n",
      "Epoch [1732], train_loss: 40269.19 with loss1: 660.30, loss2: 100.20 and loss3: 39508.68\n",
      "Epoch [1733], train_loss: 40257.01 with loss1: 653.50, loss2: 99.29 and loss3: 39504.22\n",
      "Epoch [1734], train_loss: 40257.54 with loss1: 657.76, loss2: 100.03 and loss3: 39499.75\n",
      "Epoch [1735], train_loss: 40247.06 with loss1: 651.93, loss2: 99.85 and loss3: 39495.28\n",
      "Epoch [1736], train_loss: 40245.18 with loss1: 654.28, loss2: 100.09 and loss3: 39490.81\n",
      "Epoch [1737], train_loss: 40236.26 with loss1: 650.50, loss2: 99.41 and loss3: 39486.34\n",
      "Epoch [1738], train_loss: 40238.13 with loss1: 655.98, loss2: 100.26 and loss3: 39481.88\n",
      "Epoch [1739], train_loss: 40228.45 with loss1: 651.67, loss2: 99.38 and loss3: 39477.41\n",
      "Epoch [1740], train_loss: 40231.70 with loss1: 659.14, loss2: 99.63 and loss3: 39472.94\n",
      "Epoch [1741], train_loss: 40218.53 with loss1: 650.83, loss2: 99.23 and loss3: 39468.47\n",
      "Epoch [1742], train_loss: 40217.69 with loss1: 654.06, loss2: 99.63 and loss3: 39464.00\n",
      "Epoch [1743], train_loss: 40206.55 with loss1: 647.83, loss2: 99.18 and loss3: 39459.54\n",
      "Epoch [1744], train_loss: 40205.05 with loss1: 650.33, loss2: 99.65 and loss3: 39455.07\n",
      "Epoch [1745], train_loss: 40195.69 with loss1: 645.58, loss2: 99.51 and loss3: 39450.61\n",
      "Epoch [1746], train_loss: 40195.56 with loss1: 650.13, loss2: 99.29 and loss3: 39446.14\n",
      "Epoch [1747], train_loss: 40187.19 with loss1: 646.84, loss2: 98.68 and loss3: 39441.68\n",
      "Epoch [1748], train_loss: 40184.78 with loss1: 648.23, loss2: 99.34 and loss3: 39437.21\n",
      "Epoch [1749], train_loss: 40174.41 with loss1: 642.69, loss2: 98.98 and loss3: 39432.74\n",
      "Epoch [1750], train_loss: 40175.50 with loss1: 648.20, loss2: 99.02 and loss3: 39428.28\n",
      "Epoch [1751], train_loss: 40165.36 with loss1: 642.71, loss2: 98.84 and loss3: 39423.81\n",
      "Epoch [1752], train_loss: 40164.67 with loss1: 645.96, loss2: 99.35 and loss3: 39419.35\n",
      "Epoch [1753], train_loss: 40155.86 with loss1: 641.93, loss2: 99.05 and loss3: 39414.89\n",
      "Epoch [1754], train_loss: 40153.04 with loss1: 643.34, loss2: 99.28 and loss3: 39410.42\n",
      "Epoch [1755], train_loss: 40142.48 with loss1: 638.08, loss2: 98.45 and loss3: 39405.96\n",
      "Epoch [1756], train_loss: 40142.86 with loss1: 642.21, loss2: 99.17 and loss3: 39401.49\n",
      "Epoch [1757], train_loss: 40132.54 with loss1: 636.63, loss2: 98.88 and loss3: 39397.04\n",
      "Epoch [1758], train_loss: 40131.10 with loss1: 640.10, loss2: 98.44 and loss3: 39392.56\n",
      "Epoch [1759], train_loss: 40120.38 with loss1: 633.89, loss2: 98.37 and loss3: 39388.11\n",
      "Epoch [1760], train_loss: 40119.85 with loss1: 637.55, loss2: 98.66 and loss3: 39383.64\n",
      "Epoch [1761], train_loss: 40113.12 with loss1: 635.59, loss2: 98.34 and loss3: 39379.18\n",
      "Epoch [1762], train_loss: 40108.48 with loss1: 635.24, loss2: 98.51 and loss3: 39374.72\n",
      "Epoch [1763], train_loss: 40099.99 with loss1: 631.38, loss2: 98.34 and loss3: 39370.27\n",
      "Epoch [1764], train_loss: 40096.93 with loss1: 632.81, loss2: 98.32 and loss3: 39365.80\n",
      "Epoch [1765], train_loss: 40090.23 with loss1: 630.94, loss2: 97.95 and loss3: 39361.34\n",
      "Epoch [1766], train_loss: 40088.09 with loss1: 633.09, loss2: 98.12 and loss3: 39356.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1767], train_loss: 40080.87 with loss1: 630.54, loss2: 97.91 and loss3: 39352.43\n",
      "Epoch [1768], train_loss: 40079.37 with loss1: 633.34, loss2: 98.06 and loss3: 39347.96\n",
      "Epoch [1769], train_loss: 40071.95 with loss1: 630.30, loss2: 98.14 and loss3: 39343.51\n",
      "Epoch [1770], train_loss: 40068.87 with loss1: 631.67, loss2: 98.15 and loss3: 39339.05\n",
      "Epoch [1771], train_loss: 40061.79 with loss1: 629.32, loss2: 97.88 and loss3: 39334.59\n",
      "Epoch [1772], train_loss: 40061.36 with loss1: 633.08, loss2: 98.14 and loss3: 39330.14\n",
      "Epoch [1773], train_loss: 40053.80 with loss1: 630.51, loss2: 97.62 and loss3: 39325.67\n",
      "Epoch [1774], train_loss: 40051.30 with loss1: 631.87, loss2: 98.21 and loss3: 39321.22\n",
      "Epoch [1775], train_loss: 40043.58 with loss1: 629.23, loss2: 97.59 and loss3: 39316.76\n",
      "Epoch [1776], train_loss: 40042.30 with loss1: 632.10, loss2: 97.89 and loss3: 39312.30\n",
      "Epoch [1777], train_loss: 40035.19 with loss1: 629.87, loss2: 97.47 and loss3: 39307.84\n",
      "Epoch [1778], train_loss: 40034.39 with loss1: 632.91, loss2: 98.08 and loss3: 39303.39\n",
      "Epoch [1779], train_loss: 40028.88 with loss1: 632.40, loss2: 97.55 and loss3: 39298.93\n",
      "Epoch [1780], train_loss: 40026.25 with loss1: 633.88, loss2: 97.89 and loss3: 39294.48\n",
      "Epoch [1781], train_loss: 40017.90 with loss1: 630.53, loss2: 97.35 and loss3: 39290.02\n",
      "Epoch [1782], train_loss: 40017.51 with loss1: 634.49, loss2: 97.46 and loss3: 39285.56\n",
      "Epoch [1783], train_loss: 40008.29 with loss1: 629.70, loss2: 97.48 and loss3: 39281.11\n",
      "Epoch [1784], train_loss: 40010.30 with loss1: 635.95, loss2: 97.70 and loss3: 39276.65\n",
      "Epoch [1785], train_loss: 40000.73 with loss1: 631.31, loss2: 97.23 and loss3: 39272.20\n",
      "Epoch [1786], train_loss: 40000.81 with loss1: 635.90, loss2: 97.17 and loss3: 39267.74\n",
      "Epoch [1787], train_loss: 39994.53 with loss1: 634.25, loss2: 96.99 and loss3: 39263.28\n",
      "Epoch [1788], train_loss: 39995.01 with loss1: 638.89, loss2: 97.29 and loss3: 39258.83\n",
      "Epoch [1789], train_loss: 39987.47 with loss1: 635.90, loss2: 97.19 and loss3: 39254.38\n",
      "Epoch [1790], train_loss: 39986.41 with loss1: 638.92, loss2: 97.56 and loss3: 39249.92\n",
      "Epoch [1791], train_loss: 39979.48 with loss1: 637.25, loss2: 96.76 and loss3: 39245.47\n",
      "Epoch [1792], train_loss: 39980.97 with loss1: 643.19, loss2: 96.77 and loss3: 39241.02\n",
      "Epoch [1793], train_loss: 39975.95 with loss1: 641.94, loss2: 97.45 and loss3: 39236.56\n",
      "Epoch [1794], train_loss: 39975.80 with loss1: 646.71, loss2: 96.99 and loss3: 39232.11\n",
      "Epoch [1795], train_loss: 39969.74 with loss1: 645.05, loss2: 97.04 and loss3: 39227.66\n",
      "Epoch [1796], train_loss: 39972.83 with loss1: 652.78, loss2: 96.86 and loss3: 39223.20\n",
      "Epoch [1797], train_loss: 39961.77 with loss1: 646.17, loss2: 96.85 and loss3: 39218.75\n",
      "Epoch [1798], train_loss: 39966.30 with loss1: 655.25, loss2: 96.76 and loss3: 39214.29\n",
      "Epoch [1799], train_loss: 39956.19 with loss1: 649.40, loss2: 96.95 and loss3: 39209.84\n",
      "Epoch [1800], train_loss: 39961.49 with loss1: 658.97, loss2: 97.13 and loss3: 39205.39\n",
      "Epoch [1801], train_loss: 39952.22 with loss1: 654.38, loss2: 96.90 and loss3: 39200.94\n",
      "Epoch [1802], train_loss: 39955.15 with loss1: 661.49, loss2: 97.17 and loss3: 39196.48\n",
      "Epoch [1803], train_loss: 39943.84 with loss1: 655.55, loss2: 96.26 and loss3: 39192.04\n",
      "Epoch [1804], train_loss: 39946.79 with loss1: 662.55, loss2: 96.66 and loss3: 39187.58\n",
      "Epoch [1805], train_loss: 39935.22 with loss1: 655.30, loss2: 96.79 and loss3: 39183.13\n",
      "Epoch [1806], train_loss: 39935.97 with loss1: 660.74, loss2: 96.54 and loss3: 39178.69\n",
      "Epoch [1807], train_loss: 39924.49 with loss1: 653.52, loss2: 96.73 and loss3: 39174.24\n",
      "Epoch [1808], train_loss: 39923.73 with loss1: 657.48, loss2: 96.47 and loss3: 39169.79\n",
      "Epoch [1809], train_loss: 39911.79 with loss1: 650.07, loss2: 96.38 and loss3: 39165.34\n",
      "Epoch [1810], train_loss: 39910.65 with loss1: 653.24, loss2: 96.52 and loss3: 39160.89\n",
      "Epoch [1811], train_loss: 39893.22 with loss1: 640.34, loss2: 96.44 and loss3: 39156.44\n",
      "Epoch [1812], train_loss: 39891.07 with loss1: 642.68, loss2: 96.40 and loss3: 39151.99\n",
      "Epoch [1813], train_loss: 39878.21 with loss1: 634.68, loss2: 95.98 and loss3: 39147.55\n",
      "Epoch [1814], train_loss: 39873.45 with loss1: 634.09, loss2: 96.27 and loss3: 39143.09\n",
      "Epoch [1815], train_loss: 39862.64 with loss1: 627.65, loss2: 96.34 and loss3: 39138.66\n",
      "Epoch [1816], train_loss: 39858.57 with loss1: 628.39, loss2: 95.99 and loss3: 39134.20\n",
      "Epoch [1817], train_loss: 39847.30 with loss1: 621.68, loss2: 95.87 and loss3: 39129.76\n",
      "Epoch [1818], train_loss: 39844.29 with loss1: 622.82, loss2: 96.16 and loss3: 39125.30\n",
      "Epoch [1819], train_loss: 39833.85 with loss1: 617.07, loss2: 95.92 and loss3: 39120.86\n",
      "Epoch [1820], train_loss: 39830.68 with loss1: 618.35, loss2: 95.91 and loss3: 39116.41\n",
      "Epoch [1821], train_loss: 39820.45 with loss1: 612.66, loss2: 95.83 and loss3: 39111.96\n",
      "Epoch [1822], train_loss: 39817.12 with loss1: 613.90, loss2: 95.70 and loss3: 39107.52\n",
      "Epoch [1823], train_loss: 39809.58 with loss1: 610.66, loss2: 95.84 and loss3: 39103.08\n",
      "Epoch [1824], train_loss: 39803.43 with loss1: 609.14, loss2: 95.67 and loss3: 39098.62\n",
      "Epoch [1825], train_loss: 39795.54 with loss1: 605.48, loss2: 95.88 and loss3: 39094.18\n",
      "Epoch [1826], train_loss: 39793.03 with loss1: 607.64, loss2: 95.66 and loss3: 39089.73\n",
      "Epoch [1827], train_loss: 39784.64 with loss1: 603.75, loss2: 95.59 and loss3: 39085.29\n",
      "Epoch [1828], train_loss: 39781.32 with loss1: 604.95, loss2: 95.53 and loss3: 39080.84\n",
      "Epoch [1829], train_loss: 39774.18 with loss1: 602.19, loss2: 95.58 and loss3: 39076.40\n",
      "Epoch [1830], train_loss: 39772.42 with loss1: 604.83, loss2: 95.64 and loss3: 39071.95\n",
      "Epoch [1831], train_loss: 39767.34 with loss1: 604.24, loss2: 95.59 and loss3: 39067.51\n",
      "Epoch [1832], train_loss: 39762.13 with loss1: 603.70, loss2: 95.37 and loss3: 39063.06\n",
      "Epoch [1833], train_loss: 39755.79 with loss1: 602.24, loss2: 94.92 and loss3: 39058.62\n",
      "Epoch [1834], train_loss: 39750.98 with loss1: 601.64, loss2: 95.16 and loss3: 39054.17\n",
      "Epoch [1835], train_loss: 39744.93 with loss1: 600.04, loss2: 95.15 and loss3: 39049.73\n",
      "Epoch [1836], train_loss: 39744.18 with loss1: 603.86, loss2: 95.04 and loss3: 39045.28\n",
      "Epoch [1837], train_loss: 39736.16 with loss1: 600.08, loss2: 95.24 and loss3: 39040.84\n",
      "Epoch [1838], train_loss: 39734.75 with loss1: 603.20, loss2: 95.17 and loss3: 39036.39\n",
      "Epoch [1839], train_loss: 39726.05 with loss1: 599.05, loss2: 95.04 and loss3: 39031.95\n",
      "Epoch [1840], train_loss: 39723.18 with loss1: 600.68, loss2: 95.00 and loss3: 39027.50\n",
      "Epoch [1841], train_loss: 39717.96 with loss1: 600.22, loss2: 94.68 and loss3: 39023.07\n",
      "Epoch [1842], train_loss: 39717.86 with loss1: 604.56, loss2: 94.67 and loss3: 39018.62\n",
      "Epoch [1843], train_loss: 39712.05 with loss1: 603.04, loss2: 94.82 and loss3: 39014.18\n",
      "Epoch [1844], train_loss: 39708.02 with loss1: 603.21, loss2: 95.08 and loss3: 39009.73\n",
      "Epoch [1845], train_loss: 39704.99 with loss1: 605.17, loss2: 94.51 and loss3: 39005.30\n",
      "Epoch [1846], train_loss: 39700.98 with loss1: 605.61, loss2: 94.52 and loss3: 39000.86\n",
      "Epoch [1847], train_loss: 39697.79 with loss1: 606.42, loss2: 94.95 and loss3: 38996.42\n",
      "Epoch [1848], train_loss: 39696.06 with loss1: 609.76, loss2: 94.33 and loss3: 38991.97\n",
      "Epoch [1849], train_loss: 39689.88 with loss1: 607.21, loss2: 95.12 and loss3: 38987.54\n",
      "Epoch [1850], train_loss: 39690.20 with loss1: 612.97, loss2: 94.13 and loss3: 38983.09\n",
      "Epoch [1851], train_loss: 39685.64 with loss1: 612.63, loss2: 94.34 and loss3: 38978.66\n",
      "Epoch [1852], train_loss: 39686.64 with loss1: 618.16, loss2: 94.26 and loss3: 38974.22\n",
      "Epoch [1853], train_loss: 39679.72 with loss1: 615.70, loss2: 94.24 and loss3: 38969.78\n",
      "Epoch [1854], train_loss: 39680.95 with loss1: 621.18, loss2: 94.43 and loss3: 38965.34\n",
      "Epoch [1855], train_loss: 39674.41 with loss1: 619.45, loss2: 94.05 and loss3: 38960.91\n",
      "Epoch [1856], train_loss: 39675.84 with loss1: 624.95, loss2: 94.42 and loss3: 38956.46\n",
      "Epoch [1857], train_loss: 39670.32 with loss1: 624.14, loss2: 94.14 and loss3: 38952.03\n",
      "Epoch [1858], train_loss: 39670.13 with loss1: 628.39, loss2: 94.16 and loss3: 38947.59\n",
      "Epoch [1859], train_loss: 39662.89 with loss1: 625.20, loss2: 94.54 and loss3: 38943.15\n",
      "Epoch [1860], train_loss: 39665.58 with loss1: 632.85, loss2: 94.02 and loss3: 38938.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1861], train_loss: 39658.18 with loss1: 629.78, loss2: 94.12 and loss3: 38934.27\n",
      "Epoch [1862], train_loss: 39666.23 with loss1: 641.88, loss2: 94.51 and loss3: 38929.84\n",
      "Epoch [1863], train_loss: 39656.96 with loss1: 637.94, loss2: 93.64 and loss3: 38925.39\n",
      "Epoch [1864], train_loss: 39659.92 with loss1: 645.04, loss2: 93.91 and loss3: 38920.97\n",
      "Epoch [1865], train_loss: 39653.04 with loss1: 642.86, loss2: 93.66 and loss3: 38916.52\n",
      "Epoch [1866], train_loss: 39658.03 with loss1: 652.07, loss2: 93.87 and loss3: 38912.09\n",
      "Epoch [1867], train_loss: 39650.86 with loss1: 649.12, loss2: 94.08 and loss3: 38907.65\n",
      "Epoch [1868], train_loss: 39652.41 with loss1: 655.69, loss2: 93.50 and loss3: 38903.22\n",
      "Epoch [1869], train_loss: 39647.50 with loss1: 654.88, loss2: 93.84 and loss3: 38898.79\n",
      "Epoch [1870], train_loss: 39651.25 with loss1: 663.01, loss2: 93.89 and loss3: 38894.34\n",
      "Epoch [1871], train_loss: 39641.36 with loss1: 657.96, loss2: 93.49 and loss3: 38889.91\n",
      "Epoch [1872], train_loss: 39647.62 with loss1: 668.34, loss2: 93.79 and loss3: 38885.48\n",
      "Epoch [1873], train_loss: 39637.32 with loss1: 662.53, loss2: 93.76 and loss3: 38881.04\n",
      "Epoch [1874], train_loss: 39641.45 with loss1: 671.27, loss2: 93.57 and loss3: 38876.61\n",
      "Epoch [1875], train_loss: 39630.73 with loss1: 664.96, loss2: 93.59 and loss3: 38872.17\n",
      "Epoch [1876], train_loss: 39635.92 with loss1: 674.85, loss2: 93.33 and loss3: 38867.74\n",
      "Epoch [1877], train_loss: 39624.09 with loss1: 667.17, loss2: 93.62 and loss3: 38863.30\n",
      "Epoch [1878], train_loss: 39626.57 with loss1: 674.27, loss2: 93.42 and loss3: 38858.88\n",
      "Epoch [1879], train_loss: 39614.00 with loss1: 666.23, loss2: 93.34 and loss3: 38854.43\n",
      "Epoch [1880], train_loss: 39616.80 with loss1: 673.40, loss2: 93.39 and loss3: 38850.02\n",
      "Epoch [1881], train_loss: 39604.05 with loss1: 665.20, loss2: 93.29 and loss3: 38845.57\n",
      "Epoch [1882], train_loss: 39606.05 with loss1: 671.11, loss2: 93.79 and loss3: 38841.15\n",
      "Epoch [1883], train_loss: 39590.56 with loss1: 660.31, loss2: 93.55 and loss3: 38836.70\n",
      "Epoch [1884], train_loss: 39591.35 with loss1: 665.70, loss2: 93.37 and loss3: 38832.29\n",
      "Epoch [1885], train_loss: 39576.88 with loss1: 655.61, loss2: 93.42 and loss3: 38827.84\n",
      "Epoch [1886], train_loss: 39577.05 with loss1: 660.65, loss2: 92.97 and loss3: 38823.42\n",
      "Epoch [1887], train_loss: 39560.63 with loss1: 648.43, loss2: 93.22 and loss3: 38818.98\n",
      "Epoch [1888], train_loss: 39560.51 with loss1: 652.89, loss2: 93.06 and loss3: 38814.57\n",
      "Epoch [1889], train_loss: 39546.26 with loss1: 642.87, loss2: 93.27 and loss3: 38810.12\n",
      "Epoch [1890], train_loss: 39543.73 with loss1: 645.13, loss2: 92.90 and loss3: 38805.71\n",
      "Epoch [1891], train_loss: 39529.56 with loss1: 635.42, loss2: 92.89 and loss3: 38801.25\n",
      "Epoch [1892], train_loss: 39525.87 with loss1: 636.10, loss2: 92.93 and loss3: 38796.84\n",
      "Epoch [1893], train_loss: 39512.62 with loss1: 627.21, loss2: 93.01 and loss3: 38792.40\n",
      "Epoch [1894], train_loss: 39510.70 with loss1: 629.89, loss2: 92.83 and loss3: 38787.98\n",
      "Epoch [1895], train_loss: 39500.10 with loss1: 624.13, loss2: 92.44 and loss3: 38783.54\n",
      "Epoch [1896], train_loss: 39495.40 with loss1: 623.60, loss2: 92.68 and loss3: 38779.12\n",
      "Epoch [1897], train_loss: 39483.33 with loss1: 616.18, loss2: 92.48 and loss3: 38774.68\n",
      "Epoch [1898], train_loss: 39480.86 with loss1: 618.05, loss2: 92.55 and loss3: 38770.27\n",
      "Epoch [1899], train_loss: 39469.70 with loss1: 611.36, loss2: 92.52 and loss3: 38765.82\n",
      "Epoch [1900], train_loss: 39465.90 with loss1: 611.66, loss2: 92.83 and loss3: 38761.41\n",
      "Epoch [1901], train_loss: 39454.17 with loss1: 604.76, loss2: 92.44 and loss3: 38756.97\n",
      "Epoch [1902], train_loss: 39450.77 with loss1: 605.74, loss2: 92.48 and loss3: 38752.55\n",
      "Epoch [1903], train_loss: 39441.12 with loss1: 600.74, loss2: 92.26 and loss3: 38748.12\n",
      "Epoch [1904], train_loss: 39436.70 with loss1: 600.95, loss2: 92.05 and loss3: 38743.70\n",
      "Epoch [1905], train_loss: 39429.52 with loss1: 598.16, loss2: 92.10 and loss3: 38739.26\n",
      "Epoch [1906], train_loss: 39427.54 with loss1: 600.38, loss2: 92.31 and loss3: 38734.85\n",
      "Epoch [1907], train_loss: 39417.33 with loss1: 594.72, loss2: 92.20 and loss3: 38730.41\n",
      "Epoch [1908], train_loss: 39413.93 with loss1: 595.69, loss2: 92.25 and loss3: 38726.00\n",
      "Epoch [1909], train_loss: 39405.91 with loss1: 592.65, loss2: 91.71 and loss3: 38721.55\n",
      "Epoch [1910], train_loss: 39401.41 with loss1: 592.20, loss2: 92.05 and loss3: 38717.15\n",
      "Epoch [1911], train_loss: 39397.45 with loss1: 592.70, loss2: 92.04 and loss3: 38712.70\n",
      "Epoch [1912], train_loss: 39395.23 with loss1: 594.05, loss2: 92.90 and loss3: 38708.29\n",
      "Epoch [1913], train_loss: 39387.73 with loss1: 592.00, loss2: 91.86 and loss3: 38703.86\n",
      "Epoch [1914], train_loss: 39383.18 with loss1: 591.48, loss2: 92.27 and loss3: 38699.44\n",
      "Epoch [1915], train_loss: 39377.65 with loss1: 590.80, loss2: 91.84 and loss3: 38695.02\n",
      "Epoch [1916], train_loss: 39374.66 with loss1: 591.98, loss2: 92.09 and loss3: 38690.59\n",
      "Epoch [1917], train_loss: 39368.43 with loss1: 590.48, loss2: 91.78 and loss3: 38686.16\n",
      "Epoch [1918], train_loss: 39366.60 with loss1: 593.02, loss2: 91.84 and loss3: 38681.74\n",
      "Epoch [1919], train_loss: 39360.75 with loss1: 591.76, loss2: 91.68 and loss3: 38677.31\n",
      "Epoch [1920], train_loss: 39357.12 with loss1: 592.08, loss2: 92.14 and loss3: 38672.90\n",
      "Epoch [1921], train_loss: 39349.52 with loss1: 589.38, loss2: 91.67 and loss3: 38668.47\n",
      "Epoch [1922], train_loss: 39349.00 with loss1: 593.11, loss2: 91.84 and loss3: 38664.05\n",
      "Epoch [1923], train_loss: 39342.04 with loss1: 590.99, loss2: 91.43 and loss3: 38659.62\n",
      "Epoch [1924], train_loss: 39342.21 with loss1: 595.46, loss2: 91.54 and loss3: 38655.20\n",
      "Epoch [1925], train_loss: 39334.88 with loss1: 592.91, loss2: 91.20 and loss3: 38650.78\n",
      "Epoch [1926], train_loss: 39334.28 with loss1: 596.33, loss2: 91.59 and loss3: 38646.36\n",
      "Epoch [1927], train_loss: 39329.77 with loss1: 596.70, loss2: 91.13 and loss3: 38641.93\n",
      "Epoch [1928], train_loss: 39328.70 with loss1: 599.69, loss2: 91.50 and loss3: 38637.52\n",
      "Epoch [1929], train_loss: 39322.46 with loss1: 598.29, loss2: 91.09 and loss3: 38633.09\n",
      "Epoch [1930], train_loss: 39323.61 with loss1: 603.14, loss2: 91.79 and loss3: 38628.68\n",
      "Epoch [1931], train_loss: 39315.38 with loss1: 600.11, loss2: 91.03 and loss3: 38624.24\n",
      "Epoch [1932], train_loss: 39318.72 with loss1: 607.10, loss2: 91.78 and loss3: 38619.84\n",
      "Epoch [1933], train_loss: 39311.97 with loss1: 605.62, loss2: 90.95 and loss3: 38615.40\n",
      "Epoch [1934], train_loss: 39311.56 with loss1: 608.89, loss2: 91.68 and loss3: 38610.99\n",
      "Epoch [1935], train_loss: 39306.69 with loss1: 609.21, loss2: 90.91 and loss3: 38606.57\n",
      "Epoch [1936], train_loss: 39309.02 with loss1: 615.18, loss2: 91.69 and loss3: 38602.15\n",
      "Epoch [1937], train_loss: 39301.27 with loss1: 613.06, loss2: 90.48 and loss3: 38597.73\n",
      "Epoch [1938], train_loss: 39304.15 with loss1: 619.40, loss2: 91.44 and loss3: 38593.31\n",
      "Epoch [1939], train_loss: 39296.21 with loss1: 616.00, loss2: 91.32 and loss3: 38588.89\n",
      "Epoch [1940], train_loss: 39296.12 with loss1: 620.70, loss2: 90.94 and loss3: 38584.48\n",
      "Epoch [1941], train_loss: 39288.82 with loss1: 618.12, loss2: 90.64 and loss3: 38580.06\n",
      "Epoch [1942], train_loss: 39287.78 with loss1: 621.11, loss2: 91.03 and loss3: 38575.64\n",
      "Epoch [1943], train_loss: 39277.93 with loss1: 616.14, loss2: 90.57 and loss3: 38571.23\n",
      "Epoch [1944], train_loss: 39278.98 with loss1: 620.84, loss2: 91.32 and loss3: 38566.82\n",
      "Epoch [1945], train_loss: 39270.52 with loss1: 617.55, loss2: 90.58 and loss3: 38562.40\n",
      "Epoch [1946], train_loss: 39270.32 with loss1: 621.48, loss2: 90.85 and loss3: 38557.99\n",
      "Epoch [1947], train_loss: 39260.38 with loss1: 616.35, loss2: 90.46 and loss3: 38553.57\n",
      "Epoch [1948], train_loss: 39259.50 with loss1: 619.58, loss2: 90.75 and loss3: 38549.16\n",
      "Epoch [1949], train_loss: 39249.16 with loss1: 614.12, loss2: 90.30 and loss3: 38544.74\n",
      "Epoch [1950], train_loss: 39249.96 with loss1: 618.54, loss2: 91.07 and loss3: 38540.34\n",
      "Epoch [1951], train_loss: 39240.08 with loss1: 613.61, loss2: 90.56 and loss3: 38535.91\n",
      "Epoch [1952], train_loss: 39238.06 with loss1: 615.67, loss2: 90.87 and loss3: 38531.52\n",
      "Epoch [1953], train_loss: 39229.72 with loss1: 612.42, loss2: 90.20 and loss3: 38527.09\n",
      "Epoch [1954], train_loss: 39228.43 with loss1: 615.22, loss2: 90.51 and loss3: 38522.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1955], train_loss: 39219.29 with loss1: 611.20, loss2: 89.81 and loss3: 38518.28\n",
      "Epoch [1956], train_loss: 39215.50 with loss1: 611.04, loss2: 90.59 and loss3: 38513.87\n",
      "Epoch [1957], train_loss: 39209.70 with loss1: 609.89, loss2: 90.35 and loss3: 38509.46\n",
      "Epoch [1958], train_loss: 39206.31 with loss1: 610.88, loss2: 90.39 and loss3: 38505.04\n",
      "Epoch [1959], train_loss: 39196.02 with loss1: 605.27, loss2: 90.12 and loss3: 38500.64\n",
      "Epoch [1960], train_loss: 39194.01 with loss1: 607.07, loss2: 90.71 and loss3: 38496.23\n",
      "Epoch [1961], train_loss: 39186.08 with loss1: 604.26, loss2: 90.00 and loss3: 38491.82\n",
      "Epoch [1962], train_loss: 39184.43 with loss1: 606.53, loss2: 90.49 and loss3: 38487.41\n",
      "Epoch [1963], train_loss: 39177.29 with loss1: 604.43, loss2: 89.86 and loss3: 38483.00\n",
      "Epoch [1964], train_loss: 39174.10 with loss1: 605.47, loss2: 90.04 and loss3: 38478.60\n",
      "Epoch [1965], train_loss: 39166.00 with loss1: 601.82, loss2: 90.00 and loss3: 38474.18\n",
      "Epoch [1966], train_loss: 39164.94 with loss1: 605.37, loss2: 89.78 and loss3: 38469.79\n",
      "Epoch [1967], train_loss: 39155.05 with loss1: 600.00, loss2: 89.69 and loss3: 38465.36\n",
      "Epoch [1968], train_loss: 39154.86 with loss1: 604.11, loss2: 89.79 and loss3: 38460.97\n",
      "Epoch [1969], train_loss: 39145.84 with loss1: 599.64, loss2: 89.65 and loss3: 38456.55\n",
      "Epoch [1970], train_loss: 39146.40 with loss1: 604.31, loss2: 89.95 and loss3: 38452.14\n",
      "Epoch [1971], train_loss: 39138.28 with loss1: 600.58, loss2: 89.97 and loss3: 38447.73\n",
      "Epoch [1972], train_loss: 39137.22 with loss1: 603.79, loss2: 90.10 and loss3: 38443.33\n",
      "Epoch [1973], train_loss: 39126.96 with loss1: 598.44, loss2: 89.60 and loss3: 38438.91\n",
      "Epoch [1974], train_loss: 39126.81 with loss1: 602.51, loss2: 89.79 and loss3: 38434.52\n",
      "Epoch [1975], train_loss: 39119.35 with loss1: 599.82, loss2: 89.43 and loss3: 38430.10\n",
      "Epoch [1976], train_loss: 39115.20 with loss1: 599.95, loss2: 89.55 and loss3: 38425.70\n",
      "Epoch [1977], train_loss: 39109.18 with loss1: 598.53, loss2: 89.37 and loss3: 38421.29\n",
      "Epoch [1978], train_loss: 39106.45 with loss1: 600.06, loss2: 89.50 and loss3: 38416.89\n",
      "Epoch [1979], train_loss: 39099.15 with loss1: 597.36, loss2: 89.31 and loss3: 38412.47\n",
      "Epoch [1980], train_loss: 39096.75 with loss1: 599.34, loss2: 89.33 and loss3: 38408.08\n",
      "Epoch [1981], train_loss: 39088.87 with loss1: 595.92, loss2: 89.29 and loss3: 38403.66\n",
      "Epoch [1982], train_loss: 39087.07 with loss1: 598.50, loss2: 89.30 and loss3: 38399.27\n",
      "Epoch [1983], train_loss: 39080.37 with loss1: 596.41, loss2: 89.10 and loss3: 38394.85\n",
      "Epoch [1984], train_loss: 39079.32 with loss1: 599.00, loss2: 89.86 and loss3: 38390.46\n",
      "Epoch [1985], train_loss: 39069.73 with loss1: 594.94, loss2: 88.75 and loss3: 38386.04\n",
      "Epoch [1986], train_loss: 39069.37 with loss1: 598.51, loss2: 89.21 and loss3: 38381.64\n",
      "Epoch [1987], train_loss: 39057.79 with loss1: 591.73, loss2: 88.83 and loss3: 38377.23\n",
      "Epoch [1988], train_loss: 39057.01 with loss1: 595.18, loss2: 88.99 and loss3: 38372.84\n",
      "Epoch [1989], train_loss: 39048.77 with loss1: 591.64, loss2: 88.71 and loss3: 38368.42\n",
      "Epoch [1990], train_loss: 39048.26 with loss1: 595.29, loss2: 88.94 and loss3: 38364.02\n",
      "Epoch [1991], train_loss: 39039.71 with loss1: 591.37, loss2: 88.72 and loss3: 38359.61\n",
      "Epoch [1992], train_loss: 39037.00 with loss1: 592.79, loss2: 89.00 and loss3: 38355.21\n",
      "Epoch [1993], train_loss: 39028.32 with loss1: 588.98, loss2: 88.54 and loss3: 38350.80\n",
      "Epoch [1994], train_loss: 39026.85 with loss1: 591.73, loss2: 88.70 and loss3: 38346.41\n",
      "Epoch [1995], train_loss: 39020.00 with loss1: 589.21, loss2: 88.79 and loss3: 38342.00\n",
      "Epoch [1996], train_loss: 39016.58 with loss1: 589.86, loss2: 89.11 and loss3: 38337.61\n",
      "Epoch [1997], train_loss: 39007.33 with loss1: 585.52, loss2: 88.62 and loss3: 38333.19\n",
      "Epoch [1998], train_loss: 39004.71 with loss1: 587.10, loss2: 88.82 and loss3: 38328.80\n",
      "Epoch [1999], train_loss: 38998.99 with loss1: 586.31, loss2: 88.30 and loss3: 38324.38\n",
      "Epoch [2000], train_loss: 38996.64 with loss1: 587.85, loss2: 88.79 and loss3: 38319.99\n",
      "Epoch [2001], train_loss: 38988.87 with loss1: 584.58, loss2: 88.71 and loss3: 38315.58\n",
      "Epoch [2002], train_loss: 38986.00 with loss1: 586.40, loss2: 88.41 and loss3: 38311.19\n",
      "Epoch [2003], train_loss: 38980.31 with loss1: 585.46, loss2: 88.08 and loss3: 38306.77\n",
      "Epoch [2004], train_loss: 38976.79 with loss1: 586.18, loss2: 88.23 and loss3: 38302.38\n",
      "Epoch [2005], train_loss: 38971.16 with loss1: 584.97, loss2: 88.21 and loss3: 38297.97\n",
      "Epoch [2006], train_loss: 38968.47 with loss1: 586.50, loss2: 88.39 and loss3: 38293.58\n",
      "Epoch [2007], train_loss: 38961.50 with loss1: 584.30, loss2: 88.02 and loss3: 38289.17\n",
      "Epoch [2008], train_loss: 38961.91 with loss1: 588.04, loss2: 89.09 and loss3: 38284.78\n",
      "Epoch [2009], train_loss: 38953.83 with loss1: 585.20, loss2: 88.26 and loss3: 38280.38\n",
      "Epoch [2010], train_loss: 38950.37 with loss1: 586.41, loss2: 87.97 and loss3: 38275.98\n",
      "Epoch [2011], train_loss: 38943.19 with loss1: 583.72, loss2: 87.89 and loss3: 38271.58\n",
      "Epoch [2012], train_loss: 38944.01 with loss1: 588.63, loss2: 88.20 and loss3: 38267.18\n",
      "Epoch [2013], train_loss: 38935.57 with loss1: 584.95, loss2: 87.84 and loss3: 38262.78\n",
      "Epoch [2014], train_loss: 38932.57 with loss1: 586.16, loss2: 88.03 and loss3: 38258.38\n",
      "Epoch [2015], train_loss: 38925.17 with loss1: 583.39, loss2: 87.80 and loss3: 38253.98\n",
      "Epoch [2016], train_loss: 38924.39 with loss1: 587.03, loss2: 87.78 and loss3: 38249.59\n",
      "Epoch [2017], train_loss: 38918.57 with loss1: 585.56, loss2: 87.83 and loss3: 38245.19\n",
      "Epoch [2018], train_loss: 38916.77 with loss1: 588.38, loss2: 87.60 and loss3: 38240.79\n",
      "Epoch [2019], train_loss: 38909.96 with loss1: 585.63, loss2: 87.94 and loss3: 38236.39\n",
      "Epoch [2020], train_loss: 38910.16 with loss1: 590.50, loss2: 87.66 and loss3: 38232.00\n",
      "Epoch [2021], train_loss: 38902.30 with loss1: 587.06, loss2: 87.65 and loss3: 38227.59\n",
      "Epoch [2022], train_loss: 38903.65 with loss1: 593.02, loss2: 87.43 and loss3: 38223.20\n",
      "Epoch [2023], train_loss: 38895.01 with loss1: 588.40, loss2: 87.82 and loss3: 38218.79\n",
      "Epoch [2024], train_loss: 38895.53 with loss1: 593.32, loss2: 87.80 and loss3: 38214.41\n",
      "Epoch [2025], train_loss: 38885.09 with loss1: 587.85, loss2: 87.24 and loss3: 38210.00\n",
      "Epoch [2026], train_loss: 38886.70 with loss1: 593.80, loss2: 87.28 and loss3: 38205.62\n",
      "Epoch [2027], train_loss: 38878.54 with loss1: 589.92, loss2: 87.40 and loss3: 38201.21\n",
      "Epoch [2028], train_loss: 38880.35 with loss1: 596.04, loss2: 87.49 and loss3: 38196.82\n",
      "Epoch [2029], train_loss: 38871.71 with loss1: 591.86, loss2: 87.43 and loss3: 38192.42\n",
      "Epoch [2030], train_loss: 38872.07 with loss1: 596.80, loss2: 87.24 and loss3: 38188.03\n",
      "Epoch [2031], train_loss: 38861.09 with loss1: 590.25, loss2: 87.21 and loss3: 38183.63\n",
      "Epoch [2032], train_loss: 38862.45 with loss1: 596.07, loss2: 87.14 and loss3: 38179.24\n",
      "Epoch [2033], train_loss: 38851.02 with loss1: 588.65, loss2: 87.53 and loss3: 38174.84\n",
      "Epoch [2034], train_loss: 38853.23 with loss1: 595.78, loss2: 87.00 and loss3: 38170.45\n",
      "Epoch [2035], train_loss: 38845.20 with loss1: 591.89, loss2: 87.26 and loss3: 38166.05\n",
      "Epoch [2036], train_loss: 38843.25 with loss1: 594.53, loss2: 87.07 and loss3: 38161.66\n",
      "Epoch [2037], train_loss: 38832.43 with loss1: 587.82, loss2: 87.35 and loss3: 38157.27\n",
      "Epoch [2038], train_loss: 38832.11 with loss1: 592.22, loss2: 87.02 and loss3: 38152.87\n",
      "Epoch [2039], train_loss: 38820.30 with loss1: 584.58, loss2: 87.24 and loss3: 38148.48\n",
      "Epoch [2040], train_loss: 38819.41 with loss1: 588.46, loss2: 86.87 and loss3: 38144.08\n",
      "Epoch [2041], train_loss: 38809.25 with loss1: 582.47, loss2: 87.08 and loss3: 38139.69\n",
      "Epoch [2042], train_loss: 38807.00 with loss1: 584.93, loss2: 86.78 and loss3: 38135.29\n",
      "Epoch [2043], train_loss: 38798.66 with loss1: 580.62, loss2: 87.13 and loss3: 38130.91\n",
      "Epoch [2044], train_loss: 38795.20 with loss1: 581.92, loss2: 86.78 and loss3: 38126.50\n",
      "Epoch [2045], train_loss: 38785.95 with loss1: 576.80, loss2: 87.02 and loss3: 38122.12\n",
      "Epoch [2046], train_loss: 38781.99 with loss1: 577.84, loss2: 86.43 and loss3: 38117.71\n",
      "Epoch [2047], train_loss: 38775.16 with loss1: 574.73, loss2: 87.09 and loss3: 38113.34\n",
      "Epoch [2048], train_loss: 38770.97 with loss1: 575.78, loss2: 86.26 and loss3: 38108.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2049], train_loss: 38763.45 with loss1: 571.98, loss2: 86.91 and loss3: 38104.55\n",
      "Epoch [2050], train_loss: 38759.91 with loss1: 573.36, loss2: 86.39 and loss3: 38100.15\n",
      "Epoch [2051], train_loss: 38752.68 with loss1: 570.26, loss2: 86.64 and loss3: 38095.78\n",
      "Epoch [2052], train_loss: 38749.21 with loss1: 571.29, loss2: 86.54 and loss3: 38091.38\n",
      "Epoch [2053], train_loss: 38743.66 with loss1: 569.94, loss2: 86.74 and loss3: 38086.99\n",
      "Epoch [2054], train_loss: 38737.83 with loss1: 568.86, loss2: 86.36 and loss3: 38082.60\n",
      "Epoch [2055], train_loss: 38730.43 with loss1: 565.83, loss2: 86.39 and loss3: 38078.21\n",
      "Epoch [2056], train_loss: 38727.30 with loss1: 567.03, loss2: 86.45 and loss3: 38073.81\n",
      "Epoch [2057], train_loss: 38719.04 with loss1: 563.23, loss2: 86.39 and loss3: 38069.42\n",
      "Epoch [2058], train_loss: 38714.31 with loss1: 563.12, loss2: 86.16 and loss3: 38065.04\n",
      "Epoch [2059], train_loss: 38709.80 with loss1: 562.84, loss2: 86.33 and loss3: 38060.64\n",
      "Epoch [2060], train_loss: 38705.45 with loss1: 563.27, loss2: 85.92 and loss3: 38056.26\n",
      "Epoch [2061], train_loss: 38700.16 with loss1: 561.61, loss2: 86.69 and loss3: 38051.86\n",
      "Epoch [2062], train_loss: 38697.23 with loss1: 563.73, loss2: 86.02 and loss3: 38047.48\n",
      "Epoch [2063], train_loss: 38690.57 with loss1: 561.56, loss2: 85.93 and loss3: 38043.09\n",
      "Epoch [2064], train_loss: 38688.43 with loss1: 563.89, loss2: 85.85 and loss3: 38038.70\n",
      "Epoch [2065], train_loss: 38682.38 with loss1: 562.04, loss2: 86.03 and loss3: 38034.31\n",
      "Epoch [2066], train_loss: 38678.78 with loss1: 562.89, loss2: 85.97 and loss3: 38029.92\n",
      "Epoch [2067], train_loss: 38671.96 with loss1: 560.35, loss2: 86.07 and loss3: 38025.54\n",
      "Epoch [2068], train_loss: 38669.66 with loss1: 562.99, loss2: 85.53 and loss3: 38021.14\n",
      "Epoch [2069], train_loss: 38663.16 with loss1: 560.38, loss2: 86.02 and loss3: 38016.76\n",
      "Epoch [2070], train_loss: 38662.38 with loss1: 564.39, loss2: 85.62 and loss3: 38012.37\n",
      "Epoch [2071], train_loss: 38655.31 with loss1: 561.49, loss2: 85.84 and loss3: 38007.98\n",
      "Epoch [2072], train_loss: 38654.49 with loss1: 565.03, loss2: 85.87 and loss3: 38003.59\n",
      "Epoch [2073], train_loss: 38647.60 with loss1: 562.53, loss2: 85.87 and loss3: 37999.20\n",
      "Epoch [2074], train_loss: 38645.27 with loss1: 564.89, loss2: 85.56 and loss3: 37994.82\n",
      "Epoch [2075], train_loss: 38637.93 with loss1: 561.92, loss2: 85.58 and loss3: 37990.43\n",
      "Epoch [2076], train_loss: 38637.91 with loss1: 566.62, loss2: 85.25 and loss3: 37986.05\n",
      "Epoch [2077], train_loss: 38632.95 with loss1: 565.45, loss2: 85.84 and loss3: 37981.66\n",
      "Epoch [2078], train_loss: 38630.36 with loss1: 567.16, loss2: 85.93 and loss3: 37977.28\n",
      "Epoch [2079], train_loss: 38625.16 with loss1: 566.64, loss2: 85.63 and loss3: 37972.89\n",
      "Epoch [2080], train_loss: 38622.59 with loss1: 568.93, loss2: 85.15 and loss3: 37968.51\n",
      "Epoch [2081], train_loss: 38619.28 with loss1: 569.53, loss2: 85.63 and loss3: 37964.12\n",
      "Epoch [2082], train_loss: 38615.75 with loss1: 570.63, loss2: 85.38 and loss3: 37959.73\n",
      "Epoch [2083], train_loss: 38610.90 with loss1: 570.06, loss2: 85.49 and loss3: 37955.36\n",
      "Epoch [2084], train_loss: 38608.91 with loss1: 572.41, loss2: 85.54 and loss3: 37950.96\n",
      "Epoch [2085], train_loss: 38601.93 with loss1: 569.92, loss2: 85.42 and loss3: 37946.59\n",
      "Epoch [2086], train_loss: 38600.09 with loss1: 572.79, loss2: 85.10 and loss3: 37942.20\n",
      "Epoch [2087], train_loss: 38593.69 with loss1: 570.58, loss2: 85.29 and loss3: 37937.82\n",
      "Epoch [2088], train_loss: 38593.30 with loss1: 574.96, loss2: 84.90 and loss3: 37933.43\n",
      "Epoch [2089], train_loss: 38586.82 with loss1: 572.37, loss2: 85.39 and loss3: 37929.06\n",
      "Epoch [2090], train_loss: 38585.35 with loss1: 575.76, loss2: 84.92 and loss3: 37924.67\n",
      "Epoch [2091], train_loss: 38578.31 with loss1: 572.92, loss2: 85.11 and loss3: 37920.29\n",
      "Epoch [2092], train_loss: 38578.16 with loss1: 577.33, loss2: 84.93 and loss3: 37915.91\n",
      "Epoch [2093], train_loss: 38569.98 with loss1: 573.07, loss2: 85.39 and loss3: 37911.53\n",
      "Epoch [2094], train_loss: 38569.63 with loss1: 577.58, loss2: 84.91 and loss3: 37907.14\n",
      "Epoch [2095], train_loss: 38561.86 with loss1: 574.21, loss2: 84.88 and loss3: 37902.77\n",
      "Epoch [2096], train_loss: 38563.84 with loss1: 580.71, loss2: 84.74 and loss3: 37898.38\n",
      "Epoch [2097], train_loss: 38559.33 with loss1: 580.30, loss2: 85.03 and loss3: 37894.00\n",
      "Epoch [2098], train_loss: 38558.82 with loss1: 584.18, loss2: 85.02 and loss3: 37889.62\n",
      "Epoch [2099], train_loss: 38550.14 with loss1: 579.95, loss2: 84.95 and loss3: 37885.24\n",
      "Epoch [2100], train_loss: 38551.77 with loss1: 586.09, loss2: 84.81 and loss3: 37880.86\n",
      "Epoch [2101], train_loss: 38544.41 with loss1: 583.08, loss2: 84.85 and loss3: 37876.49\n",
      "Epoch [2102], train_loss: 38547.80 with loss1: 591.25, loss2: 84.45 and loss3: 37872.10\n",
      "Epoch [2103], train_loss: 38540.97 with loss1: 588.33, loss2: 84.92 and loss3: 37867.73\n",
      "Epoch [2104], train_loss: 38542.71 with loss1: 594.99, loss2: 84.38 and loss3: 37863.34\n",
      "Epoch [2105], train_loss: 38536.26 with loss1: 592.65, loss2: 84.64 and loss3: 37858.97\n",
      "Epoch [2106], train_loss: 38536.09 with loss1: 596.93, loss2: 84.58 and loss3: 37854.58\n",
      "Epoch [2107], train_loss: 38529.30 with loss1: 594.45, loss2: 84.64 and loss3: 37850.21\n",
      "Epoch [2108], train_loss: 38531.55 with loss1: 601.00, loss2: 84.73 and loss3: 37845.82\n",
      "Epoch [2109], train_loss: 38519.04 with loss1: 593.23, loss2: 84.35 and loss3: 37841.46\n",
      "Epoch [2110], train_loss: 38520.62 with loss1: 599.07, loss2: 84.48 and loss3: 37837.07\n",
      "Epoch [2111], train_loss: 38511.27 with loss1: 594.23, loss2: 84.32 and loss3: 37832.71\n",
      "Epoch [2112], train_loss: 38509.37 with loss1: 596.61, loss2: 84.43 and loss3: 37828.33\n",
      "Epoch [2113], train_loss: 38499.81 with loss1: 591.57, loss2: 84.27 and loss3: 37823.96\n",
      "Epoch [2114], train_loss: 38501.06 with loss1: 596.84, loss2: 84.64 and loss3: 37819.59\n",
      "Epoch [2115], train_loss: 38491.70 with loss1: 592.35, loss2: 84.14 and loss3: 37815.22\n",
      "Epoch [2116], train_loss: 38493.79 with loss1: 598.23, loss2: 84.72 and loss3: 37810.84\n",
      "Epoch [2117], train_loss: 38481.54 with loss1: 590.98, loss2: 84.09 and loss3: 37806.48\n",
      "Epoch [2118], train_loss: 38481.54 with loss1: 595.40, loss2: 84.04 and loss3: 37802.10\n",
      "Epoch [2119], train_loss: 38474.87 with loss1: 592.70, loss2: 84.44 and loss3: 37797.73\n",
      "Epoch [2120], train_loss: 38471.84 with loss1: 594.31, loss2: 84.17 and loss3: 37793.36\n",
      "Epoch [2121], train_loss: 38461.93 with loss1: 588.67, loss2: 84.26 and loss3: 37788.99\n",
      "Epoch [2122], train_loss: 38459.78 with loss1: 591.07, loss2: 84.10 and loss3: 37784.61\n",
      "Epoch [2123], train_loss: 38451.27 with loss1: 586.94, loss2: 84.07 and loss3: 37780.25\n",
      "Epoch [2124], train_loss: 38452.10 with loss1: 591.99, loss2: 84.23 and loss3: 37775.88\n",
      "Epoch [2125], train_loss: 38441.50 with loss1: 586.14, loss2: 83.86 and loss3: 37771.51\n",
      "Epoch [2126], train_loss: 38443.49 with loss1: 592.22, loss2: 84.14 and loss3: 37767.13\n",
      "Epoch [2127], train_loss: 38435.47 with loss1: 588.98, loss2: 83.73 and loss3: 37762.77\n",
      "Epoch [2128], train_loss: 38431.42 with loss1: 588.81, loss2: 84.22 and loss3: 37758.39\n",
      "Epoch [2129], train_loss: 38422.54 with loss1: 584.89, loss2: 83.61 and loss3: 37754.04\n",
      "Epoch [2130], train_loss: 38420.68 with loss1: 586.95, loss2: 84.07 and loss3: 37749.65\n",
      "Epoch [2131], train_loss: 38410.83 with loss1: 582.08, loss2: 83.45 and loss3: 37745.30\n",
      "Epoch [2132], train_loss: 38410.42 with loss1: 585.87, loss2: 83.64 and loss3: 37740.91\n",
      "Epoch [2133], train_loss: 38401.23 with loss1: 581.16, loss2: 83.51 and loss3: 37736.56\n",
      "Epoch [2134], train_loss: 38398.50 with loss1: 582.67, loss2: 83.66 and loss3: 37732.18\n",
      "Epoch [2135], train_loss: 38389.04 with loss1: 577.50, loss2: 83.72 and loss3: 37727.83\n",
      "Epoch [2136], train_loss: 38388.61 with loss1: 581.34, loss2: 83.82 and loss3: 37723.45\n",
      "Epoch [2137], train_loss: 38376.90 with loss1: 574.24, loss2: 83.57 and loss3: 37719.09\n",
      "Epoch [2138], train_loss: 38374.68 with loss1: 576.39, loss2: 83.58 and loss3: 37714.71\n",
      "Epoch [2139], train_loss: 38363.94 with loss1: 570.38, loss2: 83.20 and loss3: 37710.36\n",
      "Epoch [2140], train_loss: 38364.18 with loss1: 574.39, loss2: 83.82 and loss3: 37705.98\n",
      "Epoch [2141], train_loss: 38354.23 with loss1: 568.99, loss2: 83.61 and loss3: 37701.63\n",
      "Epoch [2142], train_loss: 38352.22 with loss1: 571.04, loss2: 83.93 and loss3: 37697.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2143], train_loss: 38342.99 with loss1: 566.98, loss2: 83.11 and loss3: 37692.90\n",
      "Epoch [2144], train_loss: 38341.89 with loss1: 569.87, loss2: 83.50 and loss3: 37688.52\n",
      "Epoch [2145], train_loss: 38331.15 with loss1: 563.57, loss2: 83.42 and loss3: 37684.17\n",
      "Epoch [2146], train_loss: 38328.36 with loss1: 565.22, loss2: 83.35 and loss3: 37679.79\n",
      "Epoch [2147], train_loss: 38319.85 with loss1: 561.28, loss2: 83.13 and loss3: 37675.45\n",
      "Epoch [2148], train_loss: 38319.94 with loss1: 565.46, loss2: 83.42 and loss3: 37671.06\n",
      "Epoch [2149], train_loss: 38311.45 with loss1: 561.96, loss2: 82.77 and loss3: 37666.72\n",
      "Epoch [2150], train_loss: 38311.22 with loss1: 565.62, loss2: 83.26 and loss3: 37662.34\n",
      "Epoch [2151], train_loss: 38301.32 with loss1: 560.58, loss2: 82.75 and loss3: 37657.99\n",
      "Epoch [2152], train_loss: 38299.29 with loss1: 562.64, loss2: 83.04 and loss3: 37653.61\n",
      "Epoch [2153], train_loss: 38291.59 with loss1: 559.49, loss2: 82.84 and loss3: 37649.26\n",
      "Epoch [2154], train_loss: 38288.61 with loss1: 560.74, loss2: 82.98 and loss3: 37644.89\n",
      "Epoch [2155], train_loss: 38282.11 with loss1: 558.55, loss2: 83.02 and loss3: 37640.54\n",
      "Epoch [2156], train_loss: 38278.92 with loss1: 559.57, loss2: 83.19 and loss3: 37636.16\n",
      "Epoch [2157], train_loss: 38272.14 with loss1: 557.76, loss2: 82.56 and loss3: 37631.81\n",
      "Epoch [2158], train_loss: 38270.58 with loss1: 559.77, loss2: 83.37 and loss3: 37627.44\n",
      "Epoch [2159], train_loss: 38262.70 with loss1: 557.21, loss2: 82.40 and loss3: 37623.09\n",
      "Epoch [2160], train_loss: 38263.35 with loss1: 561.56, loss2: 83.08 and loss3: 37618.71\n",
      "Epoch [2161], train_loss: 38256.77 with loss1: 560.04, loss2: 82.36 and loss3: 37614.36\n",
      "Epoch [2162], train_loss: 38255.29 with loss1: 562.35, loss2: 82.96 and loss3: 37609.99\n",
      "Epoch [2163], train_loss: 38247.45 with loss1: 559.40, loss2: 82.41 and loss3: 37605.64\n",
      "Epoch [2164], train_loss: 38246.56 with loss1: 562.43, loss2: 82.87 and loss3: 37601.27\n",
      "Epoch [2165], train_loss: 38239.92 with loss1: 560.52, loss2: 82.49 and loss3: 37596.91\n",
      "Epoch [2166], train_loss: 38240.19 with loss1: 564.51, loss2: 83.13 and loss3: 37592.55\n",
      "Epoch [2167], train_loss: 38232.18 with loss1: 561.43, loss2: 82.55 and loss3: 37588.20\n",
      "Epoch [2168], train_loss: 38232.61 with loss1: 565.88, loss2: 82.91 and loss3: 37583.82\n",
      "Epoch [2169], train_loss: 38224.33 with loss1: 562.26, loss2: 82.60 and loss3: 37579.47\n",
      "Epoch [2170], train_loss: 38224.45 with loss1: 566.87, loss2: 82.46 and loss3: 37575.11\n",
      "Epoch [2171], train_loss: 38218.43 with loss1: 565.50, loss2: 82.18 and loss3: 37570.75\n",
      "Epoch [2172], train_loss: 38215.82 with loss1: 566.78, loss2: 82.65 and loss3: 37566.39\n",
      "Epoch [2173], train_loss: 38209.12 with loss1: 564.58, loss2: 82.51 and loss3: 37562.03\n",
      "Epoch [2174], train_loss: 38208.03 with loss1: 567.80, loss2: 82.57 and loss3: 37557.66\n",
      "Epoch [2175], train_loss: 38200.52 with loss1: 564.85, loss2: 82.35 and loss3: 37553.31\n",
      "Epoch [2176], train_loss: 38198.56 with loss1: 567.30, loss2: 82.32 and loss3: 37548.95\n",
      "Epoch [2177], train_loss: 38191.12 with loss1: 564.38, loss2: 82.14 and loss3: 37544.60\n",
      "Epoch [2178], train_loss: 38189.34 with loss1: 566.83, loss2: 82.28 and loss3: 37540.23\n",
      "Epoch [2179], train_loss: 38182.26 with loss1: 564.27, loss2: 82.11 and loss3: 37535.88\n",
      "Epoch [2180], train_loss: 38182.12 with loss1: 568.11, loss2: 82.49 and loss3: 37531.52\n",
      "Epoch [2181], train_loss: 38171.91 with loss1: 563.00, loss2: 81.74 and loss3: 37527.16\n",
      "Epoch [2182], train_loss: 38171.36 with loss1: 566.32, loss2: 82.23 and loss3: 37522.80\n",
      "Epoch [2183], train_loss: 38162.33 with loss1: 561.69, loss2: 82.20 and loss3: 37518.45\n",
      "Epoch [2184], train_loss: 38162.51 with loss1: 566.18, loss2: 82.24 and loss3: 37514.09\n",
      "Epoch [2185], train_loss: 38151.88 with loss1: 560.35, loss2: 81.79 and loss3: 37509.73\n",
      "Epoch [2186], train_loss: 38150.13 with loss1: 562.63, loss2: 82.12 and loss3: 37505.38\n",
      "Epoch [2187], train_loss: 38142.39 with loss1: 559.61, loss2: 81.75 and loss3: 37501.02\n",
      "Epoch [2188], train_loss: 38142.38 with loss1: 563.61, loss2: 82.10 and loss3: 37496.66\n",
      "Epoch [2189], train_loss: 38133.32 with loss1: 559.40, loss2: 81.61 and loss3: 37492.31\n",
      "Epoch [2190], train_loss: 38130.30 with loss1: 560.63, loss2: 81.72 and loss3: 37487.95\n",
      "Epoch [2191], train_loss: 38124.16 with loss1: 558.90, loss2: 81.66 and loss3: 37483.60\n",
      "Epoch [2192], train_loss: 38122.20 with loss1: 560.92, loss2: 82.04 and loss3: 37479.24\n",
      "Epoch [2193], train_loss: 38114.40 with loss1: 558.06, loss2: 81.45 and loss3: 37474.89\n",
      "Epoch [2194], train_loss: 38111.20 with loss1: 558.91, loss2: 81.77 and loss3: 37470.53\n",
      "Epoch [2195], train_loss: 38103.75 with loss1: 555.51, loss2: 82.07 and loss3: 37466.18\n",
      "Epoch [2196], train_loss: 38101.36 with loss1: 558.06, loss2: 81.49 and loss3: 37461.82\n",
      "Epoch [2197], train_loss: 38094.12 with loss1: 555.43, loss2: 81.22 and loss3: 37457.47\n",
      "Epoch [2198], train_loss: 38092.09 with loss1: 557.28, loss2: 81.70 and loss3: 37453.11\n",
      "Epoch [2199], train_loss: 38082.78 with loss1: 552.39, loss2: 81.62 and loss3: 37448.77\n",
      "Epoch [2200], train_loss: 38082.79 with loss1: 556.91, loss2: 81.48 and loss3: 37444.40\n",
      "Epoch [2201], train_loss: 38072.75 with loss1: 551.52, loss2: 81.16 and loss3: 37440.06\n",
      "Epoch [2202], train_loss: 38072.18 with loss1: 554.83, loss2: 81.65 and loss3: 37435.69\n",
      "Epoch [2203], train_loss: 38063.95 with loss1: 551.13, loss2: 81.47 and loss3: 37431.35\n",
      "Epoch [2204], train_loss: 38061.92 with loss1: 553.55, loss2: 81.39 and loss3: 37426.98\n",
      "Epoch [2205], train_loss: 38052.82 with loss1: 549.19, loss2: 80.98 and loss3: 37422.65\n",
      "Epoch [2206], train_loss: 38050.88 with loss1: 551.49, loss2: 81.11 and loss3: 37418.28\n",
      "Epoch [2207], train_loss: 38043.55 with loss1: 548.53, loss2: 81.08 and loss3: 37413.94\n",
      "Epoch [2208], train_loss: 38040.42 with loss1: 549.46, loss2: 81.39 and loss3: 37409.57\n",
      "Epoch [2209], train_loss: 38032.76 with loss1: 546.68, loss2: 80.84 and loss3: 37405.24\n",
      "Epoch [2210], train_loss: 38031.43 with loss1: 549.39, loss2: 81.17 and loss3: 37400.87\n",
      "Epoch [2211], train_loss: 38022.81 with loss1: 545.26, loss2: 81.02 and loss3: 37396.53\n",
      "Epoch [2212], train_loss: 38020.27 with loss1: 547.22, loss2: 80.88 and loss3: 37392.17\n",
      "Epoch [2213], train_loss: 38014.31 with loss1: 545.65, loss2: 80.83 and loss3: 37387.83\n",
      "Epoch [2214], train_loss: 38010.73 with loss1: 546.32, loss2: 80.94 and loss3: 37383.47\n",
      "Epoch [2215], train_loss: 38004.68 with loss1: 544.74, loss2: 80.82 and loss3: 37379.12\n",
      "Epoch [2216], train_loss: 38003.62 with loss1: 547.88, loss2: 80.97 and loss3: 37374.77\n",
      "Epoch [2217], train_loss: 37994.67 with loss1: 543.65, loss2: 80.59 and loss3: 37370.42\n",
      "Epoch [2218], train_loss: 37993.91 with loss1: 546.94, loss2: 80.89 and loss3: 37366.08\n",
      "Epoch [2219], train_loss: 37986.23 with loss1: 544.01, loss2: 80.51 and loss3: 37361.72\n",
      "Epoch [2220], train_loss: 37983.63 with loss1: 545.23, loss2: 81.02 and loss3: 37357.38\n",
      "Epoch [2221], train_loss: 37977.64 with loss1: 543.99, loss2: 80.64 and loss3: 37353.01\n",
      "Epoch [2222], train_loss: 37975.89 with loss1: 546.55, loss2: 80.65 and loss3: 37348.69\n",
      "Epoch [2223], train_loss: 37966.18 with loss1: 541.45, loss2: 80.42 and loss3: 37344.31\n",
      "Epoch [2224], train_loss: 37964.33 with loss1: 543.40, loss2: 80.95 and loss3: 37339.98\n",
      "Epoch [2225], train_loss: 37958.46 with loss1: 542.56, loss2: 80.28 and loss3: 37335.62\n",
      "Epoch [2226], train_loss: 37956.60 with loss1: 544.28, loss2: 81.03 and loss3: 37331.29\n",
      "Epoch [2227], train_loss: 37950.32 with loss1: 543.07, loss2: 80.33 and loss3: 37326.92\n",
      "Epoch [2228], train_loss: 37950.42 with loss1: 547.55, loss2: 80.29 and loss3: 37322.58\n",
      "Epoch [2229], train_loss: 37943.35 with loss1: 544.45, loss2: 80.67 and loss3: 37318.23\n",
      "Epoch [2230], train_loss: 37939.71 with loss1: 545.40, loss2: 80.43 and loss3: 37313.88\n",
      "Epoch [2231], train_loss: 37933.29 with loss1: 543.69, loss2: 80.06 and loss3: 37309.54\n",
      "Epoch [2232], train_loss: 37933.11 with loss1: 547.63, loss2: 80.29 and loss3: 37305.19\n",
      "Epoch [2233], train_loss: 37925.66 with loss1: 544.58, loss2: 80.24 and loss3: 37300.85\n",
      "Epoch [2234], train_loss: 37924.41 with loss1: 547.75, loss2: 80.17 and loss3: 37296.49\n",
      "Epoch [2235], train_loss: 37917.24 with loss1: 545.22, loss2: 79.87 and loss3: 37292.16\n",
      "Epoch [2236], train_loss: 37916.75 with loss1: 548.64, loss2: 80.32 and loss3: 37287.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2237], train_loss: 37910.26 with loss1: 546.80, loss2: 80.00 and loss3: 37283.46\n",
      "Epoch [2238], train_loss: 37910.37 with loss1: 551.12, loss2: 80.14 and loss3: 37279.11\n",
      "Epoch [2239], train_loss: 37903.28 with loss1: 548.40, loss2: 80.11 and loss3: 37274.77\n",
      "Epoch [2240], train_loss: 37902.30 with loss1: 551.64, loss2: 80.25 and loss3: 37270.41\n",
      "Epoch [2241], train_loss: 37894.80 with loss1: 548.82, loss2: 79.90 and loss3: 37266.08\n",
      "Epoch [2242], train_loss: 37893.62 with loss1: 551.69, loss2: 80.21 and loss3: 37261.72\n",
      "Epoch [2243], train_loss: 37886.41 with loss1: 549.10, loss2: 79.93 and loss3: 37257.38\n",
      "Epoch [2244], train_loss: 37886.30 with loss1: 553.12, loss2: 80.14 and loss3: 37253.04\n",
      "Epoch [2245], train_loss: 37879.32 with loss1: 550.83, loss2: 79.80 and loss3: 37248.70\n",
      "Epoch [2246], train_loss: 37878.98 with loss1: 554.70, loss2: 79.93 and loss3: 37244.34\n",
      "Epoch [2247], train_loss: 37870.84 with loss1: 550.93, loss2: 79.90 and loss3: 37240.01\n",
      "Epoch [2248], train_loss: 37869.54 with loss1: 553.95, loss2: 79.94 and loss3: 37235.66\n",
      "Epoch [2249], train_loss: 37860.25 with loss1: 548.95, loss2: 79.98 and loss3: 37231.32\n",
      "Epoch [2250], train_loss: 37862.22 with loss1: 555.58, loss2: 79.66 and loss3: 37226.98\n",
      "Epoch [2251], train_loss: 37853.71 with loss1: 550.79, loss2: 80.30 and loss3: 37222.63\n",
      "Epoch [2252], train_loss: 37851.64 with loss1: 553.77, loss2: 79.58 and loss3: 37218.29\n",
      "Epoch [2253], train_loss: 37842.16 with loss1: 548.74, loss2: 79.48 and loss3: 37213.95\n",
      "Epoch [2254], train_loss: 37841.38 with loss1: 552.43, loss2: 79.35 and loss3: 37209.60\n",
      "Epoch [2255], train_loss: 37832.72 with loss1: 547.68, loss2: 79.78 and loss3: 37205.27\n",
      "Epoch [2256], train_loss: 37829.33 with loss1: 548.94, loss2: 79.47 and loss3: 37200.92\n",
      "Epoch [2257], train_loss: 37820.88 with loss1: 545.08, loss2: 79.21 and loss3: 37196.59\n",
      "Epoch [2258], train_loss: 37817.14 with loss1: 545.67, loss2: 79.23 and loss3: 37192.23\n",
      "Epoch [2259], train_loss: 37808.06 with loss1: 540.76, loss2: 79.40 and loss3: 37187.91\n",
      "Epoch [2260], train_loss: 37806.79 with loss1: 544.01, loss2: 79.22 and loss3: 37183.56\n",
      "Epoch [2261], train_loss: 37799.69 with loss1: 540.93, loss2: 79.53 and loss3: 37179.23\n",
      "Epoch [2262], train_loss: 37794.81 with loss1: 540.48, loss2: 79.45 and loss3: 37174.88\n",
      "Epoch [2263], train_loss: 37785.81 with loss1: 535.95, loss2: 79.31 and loss3: 37170.55\n",
      "Epoch [2264], train_loss: 37782.20 with loss1: 537.03, loss2: 78.96 and loss3: 37166.20\n",
      "Epoch [2265], train_loss: 37774.70 with loss1: 533.19, loss2: 79.64 and loss3: 37161.88\n",
      "Epoch [2266], train_loss: 37772.28 with loss1: 535.90, loss2: 78.86 and loss3: 37157.53\n",
      "Epoch [2267], train_loss: 37764.23 with loss1: 532.01, loss2: 79.02 and loss3: 37153.20\n",
      "Epoch [2268], train_loss: 37761.81 with loss1: 533.70, loss2: 79.26 and loss3: 37148.85\n",
      "Epoch [2269], train_loss: 37752.44 with loss1: 528.83, loss2: 79.10 and loss3: 37144.52\n",
      "Epoch [2270], train_loss: 37747.64 with loss1: 528.59, loss2: 78.87 and loss3: 37140.18\n",
      "Epoch [2271], train_loss: 37740.62 with loss1: 525.63, loss2: 79.15 and loss3: 37135.84\n",
      "Epoch [2272], train_loss: 37737.73 with loss1: 527.28, loss2: 78.95 and loss3: 37131.50\n",
      "Epoch [2273], train_loss: 37730.09 with loss1: 524.22, loss2: 78.71 and loss3: 37127.16\n",
      "Epoch [2274], train_loss: 37727.96 with loss1: 525.90, loss2: 79.23 and loss3: 37122.83\n",
      "Epoch [2275], train_loss: 37720.00 with loss1: 522.43, loss2: 79.07 and loss3: 37118.49\n",
      "Epoch [2276], train_loss: 37717.01 with loss1: 524.15, loss2: 78.71 and loss3: 37114.15\n",
      "Epoch [2277], train_loss: 37710.72 with loss1: 521.75, loss2: 79.15 and loss3: 37109.82\n",
      "Epoch [2278], train_loss: 37707.17 with loss1: 523.09, loss2: 78.59 and loss3: 37105.48\n",
      "Epoch [2279], train_loss: 37703.05 with loss1: 523.27, loss2: 78.63 and loss3: 37101.15\n",
      "Epoch [2280], train_loss: 37698.89 with loss1: 523.39, loss2: 78.68 and loss3: 37096.82\n",
      "Epoch [2281], train_loss: 37691.59 with loss1: 520.20, loss2: 78.91 and loss3: 37092.48\n",
      "Epoch [2282], train_loss: 37690.04 with loss1: 523.19, loss2: 78.70 and loss3: 37088.15\n",
      "Epoch [2283], train_loss: 37682.94 with loss1: 520.74, loss2: 78.40 and loss3: 37083.81\n",
      "Epoch [2284], train_loss: 37681.08 with loss1: 522.72, loss2: 78.88 and loss3: 37079.48\n",
      "Epoch [2285], train_loss: 37672.77 with loss1: 519.18, loss2: 78.45 and loss3: 37075.14\n",
      "Epoch [2286], train_loss: 37672.70 with loss1: 523.61, loss2: 78.27 and loss3: 37070.81\n",
      "Epoch [2287], train_loss: 37667.23 with loss1: 521.99, loss2: 78.76 and loss3: 37066.48\n",
      "Epoch [2288], train_loss: 37666.89 with loss1: 526.22, loss2: 78.53 and loss3: 37062.14\n",
      "Epoch [2289], train_loss: 37657.94 with loss1: 521.75, loss2: 78.38 and loss3: 37057.81\n",
      "Epoch [2290], train_loss: 37656.58 with loss1: 524.71, loss2: 78.39 and loss3: 37053.48\n",
      "Epoch [2291], train_loss: 37651.71 with loss1: 524.20, loss2: 78.36 and loss3: 37049.14\n",
      "Epoch [2292], train_loss: 37649.24 with loss1: 526.05, loss2: 78.37 and loss3: 37044.82\n",
      "Epoch [2293], train_loss: 37641.96 with loss1: 523.41, loss2: 78.08 and loss3: 37040.47\n",
      "Epoch [2294], train_loss: 37640.47 with loss1: 525.88, loss2: 78.43 and loss3: 37036.16\n",
      "Epoch [2295], train_loss: 37636.07 with loss1: 526.05, loss2: 78.21 and loss3: 37031.81\n",
      "Epoch [2296], train_loss: 37633.33 with loss1: 527.72, loss2: 78.11 and loss3: 37027.50\n",
      "Epoch [2297], train_loss: 37630.05 with loss1: 528.62, loss2: 78.28 and loss3: 37023.15\n",
      "Epoch [2298], train_loss: 37626.16 with loss1: 528.70, loss2: 78.63 and loss3: 37018.84\n",
      "Epoch [2299], train_loss: 37622.41 with loss1: 529.81, loss2: 78.10 and loss3: 37014.49\n",
      "Epoch [2300], train_loss: 37618.30 with loss1: 529.72, loss2: 78.40 and loss3: 37010.17\n",
      "Epoch [2301], train_loss: 37612.16 with loss1: 528.37, loss2: 77.95 and loss3: 37005.84\n",
      "Epoch [2302], train_loss: 37611.73 with loss1: 532.14, loss2: 78.08 and loss3: 37001.51\n",
      "Epoch [2303], train_loss: 37605.35 with loss1: 530.21, loss2: 77.96 and loss3: 36997.18\n",
      "Epoch [2304], train_loss: 37604.94 with loss1: 534.28, loss2: 77.81 and loss3: 36992.85\n",
      "Epoch [2305], train_loss: 37598.13 with loss1: 531.81, loss2: 77.79 and loss3: 36988.53\n",
      "Epoch [2306], train_loss: 37597.70 with loss1: 535.81, loss2: 77.69 and loss3: 36984.19\n",
      "Epoch [2307], train_loss: 37590.47 with loss1: 533.23, loss2: 77.36 and loss3: 36979.88\n",
      "Epoch [2308], train_loss: 37590.52 with loss1: 536.93, loss2: 78.06 and loss3: 36975.54\n",
      "Epoch [2309], train_loss: 37584.00 with loss1: 535.23, loss2: 77.54 and loss3: 36971.23\n",
      "Epoch [2310], train_loss: 37583.67 with loss1: 538.92, loss2: 77.87 and loss3: 36966.88\n",
      "Epoch [2311], train_loss: 37576.53 with loss1: 536.25, loss2: 77.72 and loss3: 36962.57\n",
      "Epoch [2312], train_loss: 37577.69 with loss1: 541.23, loss2: 78.23 and loss3: 36958.23\n",
      "Epoch [2313], train_loss: 37570.03 with loss1: 538.70, loss2: 77.41 and loss3: 36953.91\n",
      "Epoch [2314], train_loss: 37568.21 with loss1: 540.84, loss2: 77.79 and loss3: 36949.58\n",
      "Epoch [2315], train_loss: 37562.46 with loss1: 539.65, loss2: 77.54 and loss3: 36945.27\n",
      "Epoch [2316], train_loss: 37562.00 with loss1: 543.42, loss2: 77.65 and loss3: 36940.93\n",
      "Epoch [2317], train_loss: 37555.76 with loss1: 541.63, loss2: 77.52 and loss3: 36936.62\n",
      "Epoch [2318], train_loss: 37556.77 with loss1: 546.81, loss2: 77.68 and loss3: 36932.27\n",
      "Epoch [2319], train_loss: 37548.39 with loss1: 543.30, loss2: 77.12 and loss3: 36927.97\n",
      "Epoch [2320], train_loss: 37550.30 with loss1: 549.40, loss2: 77.27 and loss3: 36923.63\n",
      "Epoch [2321], train_loss: 37542.96 with loss1: 546.28, loss2: 77.36 and loss3: 36919.32\n",
      "Epoch [2322], train_loss: 37542.98 with loss1: 550.60, loss2: 77.41 and loss3: 36914.98\n",
      "Epoch [2323], train_loss: 37535.95 with loss1: 548.10, loss2: 77.18 and loss3: 36910.67\n",
      "Epoch [2324], train_loss: 37535.29 with loss1: 551.56, loss2: 77.39 and loss3: 36906.34\n",
      "Epoch [2325], train_loss: 37527.77 with loss1: 548.36, loss2: 77.39 and loss3: 36902.02\n",
      "Epoch [2326], train_loss: 37527.59 with loss1: 552.53, loss2: 77.36 and loss3: 36897.70\n",
      "Epoch [2327], train_loss: 37521.49 with loss1: 551.06, loss2: 77.06 and loss3: 36893.37\n",
      "Epoch [2328], train_loss: 37520.04 with loss1: 553.67, loss2: 77.32 and loss3: 36889.05\n",
      "Epoch [2329], train_loss: 37512.95 with loss1: 550.88, loss2: 77.34 and loss3: 36884.73\n",
      "Epoch [2330], train_loss: 37512.48 with loss1: 554.99, loss2: 77.08 and loss3: 36880.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2331], train_loss: 37504.05 with loss1: 550.87, loss2: 77.10 and loss3: 36876.09\n",
      "Epoch [2332], train_loss: 37504.79 with loss1: 555.79, loss2: 77.23 and loss3: 36871.76\n",
      "Epoch [2333], train_loss: 37497.63 with loss1: 553.26, loss2: 76.93 and loss3: 36867.44\n",
      "Epoch [2334], train_loss: 37497.06 with loss1: 556.81, loss2: 77.13 and loss3: 36863.12\n",
      "Epoch [2335], train_loss: 37488.93 with loss1: 552.94, loss2: 77.19 and loss3: 36858.80\n",
      "Epoch [2336], train_loss: 37489.73 with loss1: 558.22, loss2: 77.02 and loss3: 36854.48\n",
      "Epoch [2337], train_loss: 37481.20 with loss1: 554.05, loss2: 77.00 and loss3: 36850.15\n",
      "Epoch [2338], train_loss: 37481.88 with loss1: 558.80, loss2: 77.23 and loss3: 36845.84\n",
      "Epoch [2339], train_loss: 37473.14 with loss1: 554.55, loss2: 77.08 and loss3: 36841.52\n",
      "Epoch [2340], train_loss: 37475.00 with loss1: 560.69, loss2: 77.11 and loss3: 36837.21\n",
      "Epoch [2341], train_loss: 37466.19 with loss1: 556.37, loss2: 76.94 and loss3: 36832.88\n",
      "Epoch [2342], train_loss: 37467.12 with loss1: 561.54, loss2: 77.01 and loss3: 36828.57\n",
      "Epoch [2343], train_loss: 37457.20 with loss1: 556.29, loss2: 76.67 and loss3: 36824.24\n",
      "Epoch [2344], train_loss: 37460.52 with loss1: 563.43, loss2: 77.15 and loss3: 36819.94\n",
      "Epoch [2345], train_loss: 37449.09 with loss1: 556.82, loss2: 76.66 and loss3: 36815.61\n",
      "Epoch [2346], train_loss: 37452.61 with loss1: 564.58, loss2: 76.72 and loss3: 36811.30\n",
      "Epoch [2347], train_loss: 37441.73 with loss1: 558.01, loss2: 76.74 and loss3: 36806.97\n",
      "Epoch [2348], train_loss: 37443.79 with loss1: 564.20, loss2: 76.92 and loss3: 36802.66\n",
      "Epoch [2349], train_loss: 37432.80 with loss1: 557.82, loss2: 76.65 and loss3: 36798.34\n",
      "Epoch [2350], train_loss: 37433.64 with loss1: 562.84, loss2: 76.76 and loss3: 36794.04\n",
      "Epoch [2351], train_loss: 37424.39 with loss1: 558.30, loss2: 76.38 and loss3: 36789.71\n",
      "Epoch [2352], train_loss: 37423.67 with loss1: 561.32, loss2: 76.95 and loss3: 36785.41\n",
      "Epoch [2353], train_loss: 37409.95 with loss1: 552.33, loss2: 76.53 and loss3: 36781.09\n",
      "Epoch [2354], train_loss: 37410.66 with loss1: 557.10, loss2: 76.78 and loss3: 36776.77\n",
      "Epoch [2355], train_loss: 37399.12 with loss1: 550.11, loss2: 76.56 and loss3: 36772.45\n",
      "Epoch [2356], train_loss: 37398.10 with loss1: 553.31, loss2: 76.64 and loss3: 36768.15\n",
      "Epoch [2357], train_loss: 37386.19 with loss1: 546.15, loss2: 76.21 and loss3: 36763.83\n",
      "Epoch [2358], train_loss: 37384.07 with loss1: 548.01, loss2: 76.53 and loss3: 36759.52\n",
      "Epoch [2359], train_loss: 37371.62 with loss1: 540.28, loss2: 76.13 and loss3: 36755.20\n",
      "Epoch [2360], train_loss: 37370.12 with loss1: 542.82, loss2: 76.41 and loss3: 36750.89\n",
      "Epoch [2361], train_loss: 37359.79 with loss1: 536.77, loss2: 76.43 and loss3: 36746.59\n",
      "Epoch [2362], train_loss: 37356.00 with loss1: 537.35, loss2: 76.37 and loss3: 36742.27\n",
      "Epoch [2363], train_loss: 37348.18 with loss1: 533.85, loss2: 76.37 and loss3: 36737.96\n",
      "Epoch [2364], train_loss: 37343.12 with loss1: 533.06, loss2: 76.42 and loss3: 36733.65\n",
      "Epoch [2365], train_loss: 37334.24 with loss1: 528.71, loss2: 76.18 and loss3: 36729.34\n",
      "Epoch [2366], train_loss: 37329.17 with loss1: 527.92, loss2: 76.22 and loss3: 36725.03\n",
      "Epoch [2367], train_loss: 37322.00 with loss1: 525.35, loss2: 75.93 and loss3: 36720.73\n",
      "Epoch [2368], train_loss: 37318.50 with loss1: 525.61, loss2: 76.48 and loss3: 36716.41\n",
      "Epoch [2369], train_loss: 37310.46 with loss1: 522.20, loss2: 76.15 and loss3: 36712.11\n",
      "Epoch [2370], train_loss: 37304.19 with loss1: 520.27, loss2: 76.13 and loss3: 36707.79\n",
      "Epoch [2371], train_loss: 37296.68 with loss1: 517.21, loss2: 75.99 and loss3: 36703.48\n",
      "Epoch [2372], train_loss: 37293.09 with loss1: 517.97, loss2: 75.94 and loss3: 36699.18\n",
      "Epoch [2373], train_loss: 37285.70 with loss1: 514.76, loss2: 76.06 and loss3: 36694.87\n",
      "Epoch [2374], train_loss: 37280.97 with loss1: 514.50, loss2: 75.91 and loss3: 36690.56\n",
      "Epoch [2375], train_loss: 37276.20 with loss1: 514.03, loss2: 75.92 and loss3: 36686.25\n",
      "Epoch [2376], train_loss: 37269.45 with loss1: 511.64, loss2: 75.86 and loss3: 36681.95\n",
      "Epoch [2377], train_loss: 37265.52 with loss1: 512.17, loss2: 75.71 and loss3: 36677.64\n",
      "Epoch [2378], train_loss: 37260.25 with loss1: 510.73, loss2: 76.18 and loss3: 36673.34\n",
      "Epoch [2379], train_loss: 37252.27 with loss1: 507.49, loss2: 75.75 and loss3: 36669.03\n",
      "Epoch [2380], train_loss: 37248.02 with loss1: 507.57, loss2: 75.73 and loss3: 36664.72\n",
      "Epoch [2381], train_loss: 37242.37 with loss1: 506.29, loss2: 75.66 and loss3: 36660.41\n",
      "Epoch [2382], train_loss: 37238.50 with loss1: 506.55, loss2: 75.83 and loss3: 36656.11\n",
      "Epoch [2383], train_loss: 37232.92 with loss1: 505.60, loss2: 75.51 and loss3: 36651.80\n",
      "Epoch [2384], train_loss: 37228.55 with loss1: 505.10, loss2: 75.94 and loss3: 36647.50\n",
      "Epoch [2385], train_loss: 37222.68 with loss1: 503.90, loss2: 75.59 and loss3: 36643.20\n",
      "Epoch [2386], train_loss: 37220.15 with loss1: 505.57, loss2: 75.68 and loss3: 36638.90\n",
      "Epoch [2387], train_loss: 37212.44 with loss1: 502.45, loss2: 75.41 and loss3: 36634.58\n",
      "Epoch [2388], train_loss: 37209.06 with loss1: 502.96, loss2: 75.81 and loss3: 36630.29\n",
      "Epoch [2389], train_loss: 37204.03 with loss1: 502.73, loss2: 75.33 and loss3: 36625.97\n",
      "Epoch [2390], train_loss: 37200.98 with loss1: 503.63, loss2: 75.67 and loss3: 36621.68\n",
      "Epoch [2391], train_loss: 37194.87 with loss1: 502.12, loss2: 75.38 and loss3: 36617.37\n",
      "Epoch [2392], train_loss: 37191.31 with loss1: 502.49, loss2: 75.74 and loss3: 36613.08\n",
      "Epoch [2393], train_loss: 37185.25 with loss1: 501.00, loss2: 75.48 and loss3: 36608.77\n",
      "Epoch [2394], train_loss: 37181.06 with loss1: 501.27, loss2: 75.32 and loss3: 36604.47\n",
      "Epoch [2395], train_loss: 37176.98 with loss1: 501.61, loss2: 75.19 and loss3: 36600.17\n",
      "Epoch [2396], train_loss: 37173.34 with loss1: 502.10, loss2: 75.37 and loss3: 36595.86\n",
      "Epoch [2397], train_loss: 37167.44 with loss1: 500.62, loss2: 75.25 and loss3: 36591.57\n",
      "Epoch [2398], train_loss: 37165.06 with loss1: 502.41, loss2: 75.38 and loss3: 36587.27\n",
      "Epoch [2399], train_loss: 37158.56 with loss1: 500.54, loss2: 75.05 and loss3: 36582.97\n",
      "Epoch [2400], train_loss: 37156.92 with loss1: 502.98, loss2: 75.27 and loss3: 36578.67\n",
      "Epoch [2401], train_loss: 37149.30 with loss1: 500.09, loss2: 74.84 and loss3: 36574.37\n",
      "Epoch [2402], train_loss: 37146.40 with loss1: 501.26, loss2: 75.07 and loss3: 36570.07\n",
      "Epoch [2403], train_loss: 37143.18 with loss1: 502.50, loss2: 74.92 and loss3: 36565.77\n",
      "Epoch [2404], train_loss: 37138.54 with loss1: 501.83, loss2: 75.23 and loss3: 36561.48\n",
      "Epoch [2405], train_loss: 37133.50 with loss1: 501.29, loss2: 75.05 and loss3: 36557.16\n",
      "Epoch [2406], train_loss: 37131.02 with loss1: 503.16, loss2: 74.99 and loss3: 36552.88\n",
      "Epoch [2407], train_loss: 37126.57 with loss1: 503.03, loss2: 74.96 and loss3: 36548.57\n",
      "Epoch [2408], train_loss: 37123.62 with loss1: 504.56, loss2: 74.79 and loss3: 36544.27\n",
      "Epoch [2409], train_loss: 37119.16 with loss1: 504.45, loss2: 74.74 and loss3: 36539.98\n",
      "Epoch [2410], train_loss: 37116.86 with loss1: 506.30, loss2: 74.88 and loss3: 36535.68\n",
      "Epoch [2411], train_loss: 37109.58 with loss1: 503.55, loss2: 74.65 and loss3: 36531.38\n",
      "Epoch [2412], train_loss: 37107.20 with loss1: 505.32, loss2: 74.81 and loss3: 36527.08\n",
      "Epoch [2413], train_loss: 37100.59 with loss1: 503.11, loss2: 74.70 and loss3: 36522.78\n",
      "Epoch [2414], train_loss: 37099.51 with loss1: 506.30, loss2: 74.73 and loss3: 36518.48\n",
      "Epoch [2415], train_loss: 37094.32 with loss1: 505.52, loss2: 74.62 and loss3: 36514.19\n",
      "Epoch [2416], train_loss: 37091.25 with loss1: 506.25, loss2: 75.11 and loss3: 36509.89\n",
      "Epoch [2417], train_loss: 37088.98 with loss1: 508.46, loss2: 74.93 and loss3: 36505.59\n",
      "Epoch [2418], train_loss: 37087.22 with loss1: 510.95, loss2: 74.97 and loss3: 36501.30\n",
      "Epoch [2419], train_loss: 37081.65 with loss1: 510.14, loss2: 74.51 and loss3: 36497.00\n",
      "Epoch [2420], train_loss: 37078.31 with loss1: 510.98, loss2: 74.63 and loss3: 36492.70\n",
      "Epoch [2421], train_loss: 37073.88 with loss1: 511.24, loss2: 74.22 and loss3: 36488.41\n",
      "Epoch [2422], train_loss: 37071.85 with loss1: 513.02, loss2: 74.72 and loss3: 36484.11\n",
      "Epoch [2423], train_loss: 37067.29 with loss1: 512.77, loss2: 74.69 and loss3: 36479.83\n",
      "Epoch [2424], train_loss: 37066.31 with loss1: 516.24, loss2: 74.55 and loss3: 36475.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2425], train_loss: 37060.22 with loss1: 514.56, loss2: 74.42 and loss3: 36471.24\n",
      "Epoch [2426], train_loss: 37059.15 with loss1: 517.51, loss2: 74.72 and loss3: 36466.93\n",
      "Epoch [2427], train_loss: 37053.74 with loss1: 516.77, loss2: 74.32 and loss3: 36462.65\n",
      "Epoch [2428], train_loss: 37052.74 with loss1: 519.81, loss2: 74.59 and loss3: 36458.34\n",
      "Epoch [2429], train_loss: 37047.29 with loss1: 519.15, loss2: 74.08 and loss3: 36454.07\n",
      "Epoch [2430], train_loss: 37046.80 with loss1: 522.50, loss2: 74.55 and loss3: 36449.76\n",
      "Epoch [2431], train_loss: 37040.06 with loss1: 520.60, loss2: 73.97 and loss3: 36445.49\n",
      "Epoch [2432], train_loss: 37040.61 with loss1: 524.96, loss2: 74.48 and loss3: 36441.17\n",
      "Epoch [2433], train_loss: 37034.53 with loss1: 523.46, loss2: 74.16 and loss3: 36436.91\n",
      "Epoch [2434], train_loss: 37034.23 with loss1: 527.35, loss2: 74.30 and loss3: 36432.58\n",
      "Epoch [2435], train_loss: 37027.41 with loss1: 525.07, loss2: 74.02 and loss3: 36428.32\n",
      "Epoch [2436], train_loss: 37027.08 with loss1: 528.66, loss2: 74.42 and loss3: 36424.00\n",
      "Epoch [2437], train_loss: 37019.96 with loss1: 526.02, loss2: 74.20 and loss3: 36419.73\n",
      "Epoch [2438], train_loss: 37019.20 with loss1: 529.19, loss2: 74.59 and loss3: 36415.43\n",
      "Epoch [2439], train_loss: 37012.46 with loss1: 527.36, loss2: 73.95 and loss3: 36411.15\n",
      "Epoch [2440], train_loss: 37011.46 with loss1: 530.57, loss2: 74.05 and loss3: 36406.84\n",
      "Epoch [2441], train_loss: 37003.42 with loss1: 527.03, loss2: 73.83 and loss3: 36402.56\n",
      "Epoch [2442], train_loss: 37004.82 with loss1: 532.18, loss2: 74.38 and loss3: 36398.27\n",
      "Epoch [2443], train_loss: 36996.48 with loss1: 528.93, loss2: 73.58 and loss3: 36393.98\n",
      "Epoch [2444], train_loss: 36996.72 with loss1: 532.83, loss2: 74.21 and loss3: 36389.69\n",
      "Epoch [2445], train_loss: 36988.31 with loss1: 529.40, loss2: 73.51 and loss3: 36385.40\n",
      "Epoch [2446], train_loss: 36989.99 with loss1: 534.78, loss2: 74.09 and loss3: 36381.12\n",
      "Epoch [2447], train_loss: 36981.73 with loss1: 531.03, loss2: 73.87 and loss3: 36376.83\n",
      "Epoch [2448], train_loss: 36983.95 with loss1: 537.41, loss2: 74.00 and loss3: 36372.54\n",
      "Epoch [2449], train_loss: 36974.98 with loss1: 533.10, loss2: 73.62 and loss3: 36368.26\n",
      "Epoch [2450], train_loss: 36976.00 with loss1: 538.01, loss2: 74.02 and loss3: 36363.97\n",
      "Epoch [2451], train_loss: 36964.96 with loss1: 531.81, loss2: 73.47 and loss3: 36359.69\n",
      "Epoch [2452], train_loss: 36965.97 with loss1: 536.53, loss2: 74.05 and loss3: 36355.39\n",
      "Epoch [2453], train_loss: 36955.90 with loss1: 531.29, loss2: 73.49 and loss3: 36351.12\n",
      "Epoch [2454], train_loss: 36957.56 with loss1: 536.93, loss2: 73.82 and loss3: 36346.82\n",
      "Epoch [2455], train_loss: 36947.45 with loss1: 531.56, loss2: 73.35 and loss3: 36342.55\n",
      "Epoch [2456], train_loss: 36947.89 with loss1: 535.93, loss2: 73.71 and loss3: 36338.25\n",
      "Epoch [2457], train_loss: 36937.09 with loss1: 529.57, loss2: 73.55 and loss3: 36333.98\n",
      "Epoch [2458], train_loss: 36936.31 with loss1: 533.13, loss2: 73.50 and loss3: 36329.68\n",
      "Epoch [2459], train_loss: 36926.19 with loss1: 527.64, loss2: 73.15 and loss3: 36325.40\n",
      "Epoch [2460], train_loss: 36925.67 with loss1: 530.40, loss2: 74.16 and loss3: 36321.11\n",
      "Epoch [2461], train_loss: 36915.51 with loss1: 525.41, loss2: 73.27 and loss3: 36316.83\n",
      "Epoch [2462], train_loss: 36914.16 with loss1: 528.12, loss2: 73.50 and loss3: 36312.54\n",
      "Epoch [2463], train_loss: 36903.59 with loss1: 522.33, loss2: 73.00 and loss3: 36308.26\n",
      "Epoch [2464], train_loss: 36903.50 with loss1: 526.15, loss2: 73.38 and loss3: 36303.97\n",
      "Epoch [2465], train_loss: 36895.41 with loss1: 522.49, loss2: 73.24 and loss3: 36299.69\n",
      "Epoch [2466], train_loss: 36894.20 with loss1: 525.52, loss2: 73.27 and loss3: 36295.41\n",
      "Epoch [2467], train_loss: 36886.82 with loss1: 522.52, loss2: 73.18 and loss3: 36291.12\n",
      "Epoch [2468], train_loss: 36881.09 with loss1: 521.05, loss2: 73.21 and loss3: 36286.84\n",
      "Epoch [2469], train_loss: 36872.96 with loss1: 517.28, loss2: 73.13 and loss3: 36282.55\n",
      "Epoch [2470], train_loss: 36871.69 with loss1: 520.05, loss2: 73.37 and loss3: 36278.27\n",
      "Epoch [2471], train_loss: 36863.80 with loss1: 516.72, loss2: 73.09 and loss3: 36273.99\n",
      "Epoch [2472], train_loss: 36860.28 with loss1: 517.34, loss2: 73.25 and loss3: 36269.70\n",
      "Epoch [2473], train_loss: 36851.25 with loss1: 512.83, loss2: 72.99 and loss3: 36265.43\n",
      "Epoch [2474], train_loss: 36848.21 with loss1: 514.16, loss2: 72.92 and loss3: 36261.13\n",
      "Epoch [2475], train_loss: 36842.10 with loss1: 512.30, loss2: 72.95 and loss3: 36256.86\n",
      "Epoch [2476], train_loss: 36838.57 with loss1: 513.01, loss2: 73.00 and loss3: 36252.56\n",
      "Epoch [2477], train_loss: 36831.38 with loss1: 509.72, loss2: 73.37 and loss3: 36248.29\n",
      "Epoch [2478], train_loss: 36826.70 with loss1: 509.89, loss2: 72.81 and loss3: 36244.00\n",
      "Epoch [2479], train_loss: 36820.40 with loss1: 508.05, loss2: 72.63 and loss3: 36239.72\n",
      "Epoch [2480], train_loss: 36816.50 with loss1: 508.27, loss2: 72.79 and loss3: 36235.43\n",
      "Epoch [2481], train_loss: 36810.50 with loss1: 506.49, loss2: 72.85 and loss3: 36231.16\n",
      "Epoch [2482], train_loss: 36808.43 with loss1: 508.65, loss2: 72.91 and loss3: 36226.87\n",
      "Epoch [2483], train_loss: 36800.36 with loss1: 505.01, loss2: 72.75 and loss3: 36222.59\n",
      "Epoch [2484], train_loss: 36799.07 with loss1: 507.86, loss2: 72.91 and loss3: 36218.30\n",
      "Epoch [2485], train_loss: 36791.27 with loss1: 504.77, loss2: 72.48 and loss3: 36214.02\n",
      "Epoch [2486], train_loss: 36789.26 with loss1: 506.74, loss2: 72.77 and loss3: 36209.75\n",
      "Epoch [2487], train_loss: 36783.17 with loss1: 505.02, loss2: 72.68 and loss3: 36205.46\n",
      "Epoch [2488], train_loss: 36781.34 with loss1: 507.47, loss2: 72.69 and loss3: 36201.18\n",
      "Epoch [2489], train_loss: 36772.45 with loss1: 503.03, loss2: 72.52 and loss3: 36196.89\n",
      "Epoch [2490], train_loss: 36769.69 with loss1: 504.37, loss2: 72.69 and loss3: 36192.62\n",
      "Epoch [2491], train_loss: 36762.25 with loss1: 501.50, loss2: 72.41 and loss3: 36188.34\n",
      "Epoch [2492], train_loss: 36759.50 with loss1: 502.86, loss2: 72.59 and loss3: 36184.06\n",
      "Epoch [2493], train_loss: 36754.34 with loss1: 502.22, loss2: 72.35 and loss3: 36179.77\n",
      "Epoch [2494], train_loss: 36751.16 with loss1: 503.32, loss2: 72.34 and loss3: 36175.50\n",
      "Epoch [2495], train_loss: 36745.61 with loss1: 501.89, loss2: 72.50 and loss3: 36171.22\n",
      "Epoch [2496], train_loss: 36742.84 with loss1: 503.59, loss2: 72.30 and loss3: 36166.95\n",
      "Epoch [2497], train_loss: 36736.34 with loss1: 501.54, loss2: 72.14 and loss3: 36162.66\n",
      "Epoch [2498], train_loss: 36736.88 with loss1: 506.00, loss2: 72.49 and loss3: 36158.39\n",
      "Epoch [2499], train_loss: 36729.97 with loss1: 503.57, loss2: 72.30 and loss3: 36154.11\n",
      "Epoch [2500], train_loss: 36726.36 with loss1: 504.15, loss2: 72.38 and loss3: 36149.83\n",
      "Epoch [2501], train_loss: 36721.38 with loss1: 503.80, loss2: 72.02 and loss3: 36145.55\n",
      "Epoch [2502], train_loss: 36719.34 with loss1: 505.84, loss2: 72.23 and loss3: 36141.27\n",
      "Epoch [2503], train_loss: 36712.80 with loss1: 503.72, loss2: 72.08 and loss3: 36137.00\n",
      "Epoch [2504], train_loss: 36711.91 with loss1: 507.10, loss2: 72.10 and loss3: 36132.72\n",
      "Epoch [2505], train_loss: 36704.47 with loss1: 504.04, loss2: 71.99 and loss3: 36128.44\n",
      "Epoch [2506], train_loss: 36702.68 with loss1: 506.56, loss2: 71.95 and loss3: 36124.17\n",
      "Epoch [2507], train_loss: 36696.60 with loss1: 504.59, loss2: 72.13 and loss3: 36119.88\n",
      "Epoch [2508], train_loss: 36694.88 with loss1: 507.22, loss2: 72.04 and loss3: 36115.62\n",
      "Epoch [2509], train_loss: 36688.56 with loss1: 505.03, loss2: 72.20 and loss3: 36111.33\n",
      "Epoch [2510], train_loss: 36686.88 with loss1: 507.77, loss2: 72.05 and loss3: 36107.06\n",
      "Epoch [2511], train_loss: 36679.98 with loss1: 505.15, loss2: 72.04 and loss3: 36102.78\n",
      "Epoch [2512], train_loss: 36679.34 with loss1: 508.82, loss2: 72.00 and loss3: 36098.51\n",
      "Epoch [2513], train_loss: 36672.92 with loss1: 506.73, loss2: 71.96 and loss3: 36094.23\n",
      "Epoch [2514], train_loss: 36672.21 with loss1: 510.35, loss2: 71.90 and loss3: 36089.96\n",
      "Epoch [2515], train_loss: 36664.25 with loss1: 506.76, loss2: 71.82 and loss3: 36085.68\n",
      "Epoch [2516], train_loss: 36662.51 with loss1: 509.33, loss2: 71.76 and loss3: 36081.41\n",
      "Epoch [2517], train_loss: 36657.48 with loss1: 508.76, loss2: 71.59 and loss3: 36077.12\n",
      "Epoch [2518], train_loss: 36655.46 with loss1: 510.95, loss2: 71.64 and loss3: 36072.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2519], train_loss: 36647.90 with loss1: 507.40, loss2: 71.92 and loss3: 36068.57\n",
      "Epoch [2520], train_loss: 36647.11 with loss1: 510.97, loss2: 71.81 and loss3: 36064.33\n",
      "Epoch [2521], train_loss: 36636.37 with loss1: 504.69, loss2: 71.64 and loss3: 36060.04\n",
      "Epoch [2522], train_loss: 36634.81 with loss1: 506.91, loss2: 72.12 and loss3: 36055.79\n",
      "Epoch [2523], train_loss: 36626.59 with loss1: 503.37, loss2: 71.73 and loss3: 36051.49\n",
      "Epoch [2524], train_loss: 36626.04 with loss1: 507.43, loss2: 71.38 and loss3: 36047.23\n",
      "Epoch [2525], train_loss: 36617.95 with loss1: 503.29, loss2: 71.71 and loss3: 36042.95\n",
      "Epoch [2526], train_loss: 36616.02 with loss1: 505.66, loss2: 71.67 and loss3: 36038.69\n",
      "Epoch [2527], train_loss: 36607.56 with loss1: 501.55, loss2: 71.60 and loss3: 36034.41\n",
      "Epoch [2528], train_loss: 36605.41 with loss1: 503.83, loss2: 71.43 and loss3: 36030.15\n",
      "Epoch [2529], train_loss: 36597.14 with loss1: 499.94, loss2: 71.33 and loss3: 36025.87\n",
      "Epoch [2530], train_loss: 36595.05 with loss1: 501.97, loss2: 71.46 and loss3: 36021.62\n",
      "Epoch [2531], train_loss: 36588.32 with loss1: 499.60, loss2: 71.40 and loss3: 36017.32\n",
      "Epoch [2532], train_loss: 36585.08 with loss1: 500.34, loss2: 71.66 and loss3: 36013.08\n",
      "Epoch [2533], train_loss: 36580.23 with loss1: 499.96, loss2: 71.48 and loss3: 36008.79\n",
      "Epoch [2534], train_loss: 36577.01 with loss1: 501.19, loss2: 71.28 and loss3: 36004.54\n",
      "Epoch [2535], train_loss: 36569.17 with loss1: 497.53, loss2: 71.40 and loss3: 36000.24\n",
      "Epoch [2536], train_loss: 36565.76 with loss1: 498.39, loss2: 71.37 and loss3: 35996.00\n",
      "Epoch [2537], train_loss: 36559.19 with loss1: 496.07, loss2: 71.41 and loss3: 35991.71\n",
      "Epoch [2538], train_loss: 36556.64 with loss1: 498.00, loss2: 71.18 and loss3: 35987.46\n",
      "Epoch [2539], train_loss: 36549.22 with loss1: 494.82, loss2: 71.23 and loss3: 35983.17\n",
      "Epoch [2540], train_loss: 36547.61 with loss1: 497.39, loss2: 71.30 and loss3: 35978.93\n",
      "Epoch [2541], train_loss: 36540.86 with loss1: 494.97, loss2: 71.26 and loss3: 35974.64\n",
      "Epoch [2542], train_loss: 36539.16 with loss1: 497.83, loss2: 70.94 and loss3: 35970.39\n",
      "Epoch [2543], train_loss: 36532.52 with loss1: 495.52, loss2: 70.91 and loss3: 35966.10\n",
      "Epoch [2544], train_loss: 36530.04 with loss1: 497.13, loss2: 71.06 and loss3: 35961.85\n",
      "Epoch [2545], train_loss: 36524.56 with loss1: 495.98, loss2: 71.01 and loss3: 35957.57\n",
      "Epoch [2546], train_loss: 36522.31 with loss1: 498.08, loss2: 70.91 and loss3: 35953.31\n",
      "Epoch [2547], train_loss: 36518.12 with loss1: 498.09, loss2: 70.99 and loss3: 35949.04\n",
      "Epoch [2548], train_loss: 36515.07 with loss1: 499.54, loss2: 70.76 and loss3: 35944.78\n",
      "Epoch [2549], train_loss: 36507.08 with loss1: 495.73, loss2: 70.85 and loss3: 35940.50\n",
      "Epoch [2550], train_loss: 36505.22 with loss1: 498.11, loss2: 70.87 and loss3: 35936.25\n",
      "Epoch [2551], train_loss: 36500.20 with loss1: 497.37, loss2: 70.86 and loss3: 35931.97\n",
      "Epoch [2552], train_loss: 36497.69 with loss1: 499.36, loss2: 70.61 and loss3: 35927.71\n",
      "Epoch [2553], train_loss: 36492.12 with loss1: 497.76, loss2: 70.92 and loss3: 35923.44\n",
      "Epoch [2554], train_loss: 36493.11 with loss1: 503.21, loss2: 70.72 and loss3: 35919.18\n",
      "Epoch [2555], train_loss: 36486.38 with loss1: 500.46, loss2: 71.01 and loss3: 35914.91\n",
      "Epoch [2556], train_loss: 36485.35 with loss1: 503.87, loss2: 70.82 and loss3: 35910.66\n",
      "Epoch [2557], train_loss: 36480.45 with loss1: 503.22, loss2: 70.84 and loss3: 35906.38\n",
      "Epoch [2558], train_loss: 36480.41 with loss1: 507.74, loss2: 70.54 and loss3: 35902.12\n",
      "Epoch [2559], train_loss: 36475.42 with loss1: 506.77, loss2: 70.80 and loss3: 35897.85\n",
      "Epoch [2560], train_loss: 36476.15 with loss1: 511.94, loss2: 70.61 and loss3: 35893.60\n",
      "Epoch [2561], train_loss: 36471.25 with loss1: 511.33, loss2: 70.61 and loss3: 35889.32\n",
      "Epoch [2562], train_loss: 36472.60 with loss1: 516.90, loss2: 70.62 and loss3: 35885.07\n",
      "Epoch [2563], train_loss: 36465.69 with loss1: 513.97, loss2: 70.93 and loss3: 35880.79\n",
      "Epoch [2564], train_loss: 36468.15 with loss1: 521.43, loss2: 70.17 and loss3: 35876.55\n",
      "Epoch [2565], train_loss: 36461.95 with loss1: 519.10, loss2: 70.59 and loss3: 35872.27\n",
      "Epoch [2566], train_loss: 36462.66 with loss1: 524.30, loss2: 70.34 and loss3: 35868.02\n",
      "Epoch [2567], train_loss: 36456.27 with loss1: 521.84, loss2: 70.69 and loss3: 35863.73\n",
      "Epoch [2568], train_loss: 36458.39 with loss1: 528.53, loss2: 70.36 and loss3: 35859.50\n",
      "Epoch [2569], train_loss: 36451.00 with loss1: 525.42, loss2: 70.37 and loss3: 35855.21\n",
      "Epoch [2570], train_loss: 36452.25 with loss1: 531.07, loss2: 70.19 and loss3: 35850.98\n",
      "Epoch [2571], train_loss: 36444.84 with loss1: 527.45, loss2: 70.71 and loss3: 35846.68\n",
      "Epoch [2572], train_loss: 36447.00 with loss1: 534.18, loss2: 70.37 and loss3: 35842.46\n",
      "Epoch [2573], train_loss: 36439.45 with loss1: 530.66, loss2: 70.63 and loss3: 35838.16\n",
      "Epoch [2574], train_loss: 36439.16 with loss1: 535.03, loss2: 70.20 and loss3: 35833.94\n",
      "Epoch [2575], train_loss: 36430.93 with loss1: 530.76, loss2: 70.53 and loss3: 35829.64\n",
      "Epoch [2576], train_loss: 36429.98 with loss1: 534.22, loss2: 70.35 and loss3: 35825.41\n",
      "Epoch [2577], train_loss: 36419.79 with loss1: 528.43, loss2: 70.24 and loss3: 35821.12\n",
      "Epoch [2578], train_loss: 36420.11 with loss1: 532.90, loss2: 70.32 and loss3: 35816.88\n",
      "Epoch [2579], train_loss: 36408.66 with loss1: 525.76, loss2: 70.30 and loss3: 35812.60\n",
      "Epoch [2580], train_loss: 36406.25 with loss1: 527.90, loss2: 69.99 and loss3: 35808.36\n",
      "Epoch [2581], train_loss: 36397.20 with loss1: 522.84, loss2: 70.28 and loss3: 35804.08\n",
      "Epoch [2582], train_loss: 36396.58 with loss1: 526.82, loss2: 69.92 and loss3: 35799.84\n",
      "Epoch [2583], train_loss: 36386.76 with loss1: 520.89, loss2: 70.31 and loss3: 35795.56\n",
      "Epoch [2584], train_loss: 36383.97 with loss1: 522.65, loss2: 70.00 and loss3: 35791.32\n",
      "Epoch [2585], train_loss: 36372.88 with loss1: 515.72, loss2: 70.11 and loss3: 35787.05\n",
      "Epoch [2586], train_loss: 36369.61 with loss1: 516.62, loss2: 70.18 and loss3: 35782.80\n",
      "Epoch [2587], train_loss: 36360.18 with loss1: 511.33, loss2: 70.32 and loss3: 35778.53\n",
      "Epoch [2588], train_loss: 36356.43 with loss1: 512.26, loss2: 69.88 and loss3: 35774.29\n",
      "Epoch [2589], train_loss: 36347.54 with loss1: 507.58, loss2: 69.95 and loss3: 35770.02\n",
      "Epoch [2590], train_loss: 36342.42 with loss1: 506.65, loss2: 69.99 and loss3: 35765.78\n",
      "Epoch [2591], train_loss: 36334.36 with loss1: 502.83, loss2: 70.03 and loss3: 35761.50\n",
      "Epoch [2592], train_loss: 36330.52 with loss1: 503.65, loss2: 69.60 and loss3: 35757.27\n",
      "Epoch [2593], train_loss: 36321.89 with loss1: 499.16, loss2: 69.73 and loss3: 35753.00\n",
      "Epoch [2594], train_loss: 36319.84 with loss1: 501.04, loss2: 70.04 and loss3: 35748.76\n",
      "Epoch [2595], train_loss: 36310.91 with loss1: 496.62, loss2: 69.81 and loss3: 35744.49\n",
      "Epoch [2596], train_loss: 36308.48 with loss1: 498.60, loss2: 69.64 and loss3: 35740.24\n",
      "Epoch [2597], train_loss: 36301.43 with loss1: 495.70, loss2: 69.76 and loss3: 35735.98\n",
      "Epoch [2598], train_loss: 36296.72 with loss1: 495.25, loss2: 69.73 and loss3: 35731.74\n",
      "Epoch [2599], train_loss: 36291.41 with loss1: 494.48, loss2: 69.46 and loss3: 35727.47\n",
      "Epoch [2600], train_loss: 36287.08 with loss1: 494.16, loss2: 69.69 and loss3: 35723.23\n",
      "Epoch [2601], train_loss: 36280.66 with loss1: 492.17, loss2: 69.52 and loss3: 35718.96\n",
      "Epoch [2602], train_loss: 36279.57 with loss1: 495.10, loss2: 69.75 and loss3: 35714.72\n",
      "Epoch [2603], train_loss: 36271.13 with loss1: 491.15, loss2: 69.52 and loss3: 35710.46\n",
      "Epoch [2604], train_loss: 36269.35 with loss1: 493.47, loss2: 69.66 and loss3: 35706.22\n",
      "Epoch [2605], train_loss: 36262.60 with loss1: 491.26, loss2: 69.39 and loss3: 35701.96\n",
      "Epoch [2606], train_loss: 36261.46 with loss1: 494.10, loss2: 69.65 and loss3: 35697.71\n",
      "Epoch [2607], train_loss: 36255.14 with loss1: 492.23, loss2: 69.44 and loss3: 35693.46\n",
      "Epoch [2608], train_loss: 36252.49 with loss1: 493.74, loss2: 69.54 and loss3: 35689.21\n",
      "Epoch [2609], train_loss: 36245.06 with loss1: 490.70, loss2: 69.40 and loss3: 35684.96\n",
      "Epoch [2610], train_loss: 36241.24 with loss1: 491.16, loss2: 69.37 and loss3: 35680.71\n",
      "Epoch [2611], train_loss: 36237.85 with loss1: 491.93, loss2: 69.46 and loss3: 35676.45\n",
      "Epoch [2612], train_loss: 36233.89 with loss1: 492.11, loss2: 69.57 and loss3: 35672.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2613], train_loss: 36227.91 with loss1: 490.68, loss2: 69.28 and loss3: 35667.95\n",
      "Epoch [2614], train_loss: 36226.69 with loss1: 493.39, loss2: 69.59 and loss3: 35663.71\n",
      "Epoch [2615], train_loss: 36220.29 with loss1: 491.45, loss2: 69.38 and loss3: 35659.45\n",
      "Epoch [2616], train_loss: 36218.94 with loss1: 494.37, loss2: 69.36 and loss3: 35655.20\n",
      "Epoch [2617], train_loss: 36210.99 with loss1: 490.85, loss2: 69.18 and loss3: 35650.95\n",
      "Epoch [2618], train_loss: 36209.14 with loss1: 492.87, loss2: 69.57 and loss3: 35646.70\n",
      "Epoch [2619], train_loss: 36205.64 with loss1: 493.82, loss2: 69.37 and loss3: 35642.45\n",
      "Epoch [2620], train_loss: 36202.31 with loss1: 494.97, loss2: 69.13 and loss3: 35638.20\n",
      "Epoch [2621], train_loss: 36194.44 with loss1: 491.38, loss2: 69.10 and loss3: 35633.95\n",
      "Epoch [2622], train_loss: 36193.88 with loss1: 494.81, loss2: 69.35 and loss3: 35629.71\n",
      "Epoch [2623], train_loss: 36185.17 with loss1: 490.54, loss2: 69.18 and loss3: 35625.45\n",
      "Epoch [2624], train_loss: 36183.99 with loss1: 493.57, loss2: 69.21 and loss3: 35621.21\n",
      "Epoch [2625], train_loss: 36178.98 with loss1: 492.97, loss2: 69.06 and loss3: 35616.95\n",
      "Epoch [2626], train_loss: 36175.30 with loss1: 493.25, loss2: 69.34 and loss3: 35612.71\n",
      "Epoch [2627], train_loss: 36168.15 with loss1: 490.76, loss2: 68.94 and loss3: 35608.45\n",
      "Epoch [2628], train_loss: 36166.22 with loss1: 492.94, loss2: 69.07 and loss3: 35604.21\n",
      "Epoch [2629], train_loss: 36160.10 with loss1: 491.34, loss2: 68.79 and loss3: 35599.96\n",
      "Epoch [2630], train_loss: 36156.55 with loss1: 491.75, loss2: 69.09 and loss3: 35595.72\n",
      "Epoch [2631], train_loss: 36150.23 with loss1: 490.03, loss2: 68.73 and loss3: 35591.47\n",
      "Epoch [2632], train_loss: 36146.94 with loss1: 490.73, loss2: 68.98 and loss3: 35587.23\n",
      "Epoch [2633], train_loss: 36138.77 with loss1: 487.04, loss2: 68.75 and loss3: 35582.98\n",
      "Epoch [2634], train_loss: 36137.13 with loss1: 489.53, loss2: 68.87 and loss3: 35578.73\n",
      "Epoch [2635], train_loss: 36130.57 with loss1: 487.37, loss2: 68.71 and loss3: 35574.48\n",
      "Epoch [2636], train_loss: 36128.42 with loss1: 489.30, loss2: 68.88 and loss3: 35570.24\n",
      "Epoch [2637], train_loss: 36121.93 with loss1: 486.93, loss2: 69.01 and loss3: 35565.99\n",
      "Epoch [2638], train_loss: 36120.67 with loss1: 489.83, loss2: 69.09 and loss3: 35561.75\n",
      "Epoch [2639], train_loss: 36111.84 with loss1: 485.67, loss2: 68.67 and loss3: 35557.50\n",
      "Epoch [2640], train_loss: 36109.95 with loss1: 487.71, loss2: 68.98 and loss3: 35553.26\n",
      "Epoch [2641], train_loss: 36101.88 with loss1: 484.44, loss2: 68.43 and loss3: 35549.00\n",
      "Epoch [2642], train_loss: 36099.36 with loss1: 485.78, loss2: 68.81 and loss3: 35544.77\n",
      "Epoch [2643], train_loss: 36095.98 with loss1: 486.78, loss2: 68.69 and loss3: 35540.51\n",
      "Epoch [2644], train_loss: 36091.84 with loss1: 486.88, loss2: 68.68 and loss3: 35536.27\n",
      "Epoch [2645], train_loss: 36084.78 with loss1: 484.40, loss2: 68.36 and loss3: 35532.02\n",
      "Epoch [2646], train_loss: 36084.17 with loss1: 487.68, loss2: 68.71 and loss3: 35527.78\n",
      "Epoch [2647], train_loss: 36078.35 with loss1: 486.36, loss2: 68.46 and loss3: 35523.53\n",
      "Epoch [2648], train_loss: 36075.02 with loss1: 487.13, loss2: 68.60 and loss3: 35519.29\n",
      "Epoch [2649], train_loss: 36069.50 with loss1: 486.17, loss2: 68.29 and loss3: 35515.04\n",
      "Epoch [2650], train_loss: 36067.62 with loss1: 488.23, loss2: 68.58 and loss3: 35510.80\n",
      "Epoch [2651], train_loss: 36061.53 with loss1: 486.54, loss2: 68.44 and loss3: 35506.55\n",
      "Epoch [2652], train_loss: 36058.56 with loss1: 487.84, loss2: 68.39 and loss3: 35502.32\n",
      "Epoch [2653], train_loss: 36053.25 with loss1: 486.93, loss2: 68.25 and loss3: 35498.07\n",
      "Epoch [2654], train_loss: 36050.82 with loss1: 488.63, loss2: 68.37 and loss3: 35493.83\n",
      "Epoch [2655], train_loss: 36043.84 with loss1: 486.00, loss2: 68.24 and loss3: 35489.59\n",
      "Epoch [2656], train_loss: 36043.32 with loss1: 489.62, loss2: 68.36 and loss3: 35485.34\n",
      "Epoch [2657], train_loss: 36035.62 with loss1: 486.01, loss2: 68.50 and loss3: 35481.11\n",
      "Epoch [2658], train_loss: 36032.89 with loss1: 487.66, loss2: 68.37 and loss3: 35476.86\n",
      "Epoch [2659], train_loss: 36027.52 with loss1: 486.66, loss2: 68.24 and loss3: 35472.62\n",
      "Epoch [2660], train_loss: 36027.61 with loss1: 491.03, loss2: 68.20 and loss3: 35468.38\n",
      "Epoch [2661], train_loss: 36020.35 with loss1: 488.20, loss2: 68.00 and loss3: 35464.14\n",
      "Epoch [2662], train_loss: 36017.58 with loss1: 489.54, loss2: 68.14 and loss3: 35459.90\n",
      "Epoch [2663], train_loss: 36011.91 with loss1: 488.29, loss2: 67.96 and loss3: 35455.66\n",
      "Epoch [2664], train_loss: 36008.07 with loss1: 488.41, loss2: 68.24 and loss3: 35451.41\n",
      "Epoch [2665], train_loss: 36003.59 with loss1: 488.24, loss2: 68.17 and loss3: 35447.18\n",
      "Epoch [2666], train_loss: 36001.42 with loss1: 490.20, loss2: 68.28 and loss3: 35442.93\n",
      "Epoch [2667], train_loss: 35993.83 with loss1: 487.31, loss2: 67.82 and loss3: 35438.70\n",
      "Epoch [2668], train_loss: 35991.55 with loss1: 489.17, loss2: 67.92 and loss3: 35434.46\n",
      "Epoch [2669], train_loss: 35985.93 with loss1: 488.00, loss2: 67.72 and loss3: 35430.22\n",
      "Epoch [2670], train_loss: 35984.76 with loss1: 490.59, loss2: 68.19 and loss3: 35425.98\n",
      "Epoch [2671], train_loss: 35977.72 with loss1: 488.30, loss2: 67.68 and loss3: 35421.74\n",
      "Epoch [2672], train_loss: 35976.79 with loss1: 491.27, loss2: 68.01 and loss3: 35417.50\n",
      "Epoch [2673], train_loss: 35969.93 with loss1: 488.96, loss2: 67.70 and loss3: 35413.26\n",
      "Epoch [2674], train_loss: 35970.06 with loss1: 493.06, loss2: 67.97 and loss3: 35409.03\n",
      "Epoch [2675], train_loss: 35962.00 with loss1: 489.60, loss2: 67.61 and loss3: 35404.79\n",
      "Epoch [2676], train_loss: 35960.15 with loss1: 491.80, loss2: 67.79 and loss3: 35400.55\n",
      "Epoch [2677], train_loss: 35952.43 with loss1: 488.55, loss2: 67.57 and loss3: 35396.32\n",
      "Epoch [2678], train_loss: 35952.80 with loss1: 492.79, loss2: 67.93 and loss3: 35392.08\n",
      "Epoch [2679], train_loss: 35945.37 with loss1: 489.90, loss2: 67.62 and loss3: 35387.85\n",
      "Epoch [2680], train_loss: 35944.21 with loss1: 492.77, loss2: 67.84 and loss3: 35383.61\n",
      "Epoch [2681], train_loss: 35937.32 with loss1: 490.49, loss2: 67.46 and loss3: 35379.38\n",
      "Epoch [2682], train_loss: 35935.77 with loss1: 492.72, loss2: 67.91 and loss3: 35375.14\n",
      "Epoch [2683], train_loss: 35927.41 with loss1: 489.24, loss2: 67.27 and loss3: 35370.90\n",
      "Epoch [2684], train_loss: 35926.27 with loss1: 492.12, loss2: 67.48 and loss3: 35366.66\n",
      "Epoch [2685], train_loss: 35919.22 with loss1: 489.31, loss2: 67.48 and loss3: 35362.44\n",
      "Epoch [2686], train_loss: 35916.63 with loss1: 490.85, loss2: 67.59 and loss3: 35358.20\n",
      "Epoch [2687], train_loss: 35910.95 with loss1: 489.53, loss2: 67.45 and loss3: 35353.96\n",
      "Epoch [2688], train_loss: 35906.85 with loss1: 489.74, loss2: 67.37 and loss3: 35349.73\n",
      "Epoch [2689], train_loss: 35899.91 with loss1: 487.11, loss2: 67.30 and loss3: 35345.49\n",
      "Epoch [2690], train_loss: 35899.24 with loss1: 490.57, loss2: 67.41 and loss3: 35341.27\n",
      "Epoch [2691], train_loss: 35892.67 with loss1: 488.26, loss2: 67.39 and loss3: 35337.02\n",
      "Epoch [2692], train_loss: 35890.03 with loss1: 489.91, loss2: 67.31 and loss3: 35332.80\n",
      "Epoch [2693], train_loss: 35882.37 with loss1: 486.72, loss2: 67.10 and loss3: 35328.55\n",
      "Epoch [2694], train_loss: 35879.70 with loss1: 487.94, loss2: 67.42 and loss3: 35324.34\n",
      "Epoch [2695], train_loss: 35873.46 with loss1: 486.19, loss2: 67.19 and loss3: 35320.08\n",
      "Epoch [2696], train_loss: 35870.03 with loss1: 486.68, loss2: 67.47 and loss3: 35315.88\n",
      "Epoch [2697], train_loss: 35862.03 with loss1: 483.52, loss2: 66.90 and loss3: 35311.62\n",
      "Epoch [2698], train_loss: 35860.71 with loss1: 486.27, loss2: 67.04 and loss3: 35307.41\n",
      "Epoch [2699], train_loss: 35853.79 with loss1: 483.46, loss2: 67.17 and loss3: 35303.16\n",
      "Epoch [2700], train_loss: 35851.76 with loss1: 485.72, loss2: 67.10 and loss3: 35298.95\n",
      "Epoch [2701], train_loss: 35845.36 with loss1: 483.74, loss2: 66.94 and loss3: 35294.69\n",
      "Epoch [2702], train_loss: 35840.90 with loss1: 483.36, loss2: 67.07 and loss3: 35290.48\n",
      "Epoch [2703], train_loss: 35835.64 with loss1: 482.36, loss2: 67.05 and loss3: 35286.23\n",
      "Epoch [2704], train_loss: 35831.52 with loss1: 482.47, loss2: 67.04 and loss3: 35282.01\n",
      "Epoch [2705], train_loss: 35823.54 with loss1: 478.95, loss2: 66.81 and loss3: 35277.77\n",
      "Epoch [2706], train_loss: 35821.67 with loss1: 481.24, loss2: 66.88 and loss3: 35273.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2707], train_loss: 35814.12 with loss1: 478.06, loss2: 66.74 and loss3: 35269.31\n",
      "Epoch [2708], train_loss: 35812.58 with loss1: 480.73, loss2: 66.76 and loss3: 35265.10\n",
      "Epoch [2709], train_loss: 35804.32 with loss1: 476.73, loss2: 66.75 and loss3: 35260.85\n",
      "Epoch [2710], train_loss: 35802.72 with loss1: 479.23, loss2: 66.85 and loss3: 35256.64\n",
      "Epoch [2711], train_loss: 35796.31 with loss1: 477.18, loss2: 66.75 and loss3: 35252.39\n",
      "Epoch [2712], train_loss: 35791.52 with loss1: 476.33, loss2: 67.01 and loss3: 35248.18\n",
      "Epoch [2713], train_loss: 35785.35 with loss1: 474.86, loss2: 66.56 and loss3: 35243.93\n",
      "Epoch [2714], train_loss: 35782.23 with loss1: 475.99, loss2: 66.52 and loss3: 35239.72\n",
      "Epoch [2715], train_loss: 35776.00 with loss1: 473.79, loss2: 66.73 and loss3: 35235.47\n",
      "Epoch [2716], train_loss: 35772.90 with loss1: 474.85, loss2: 66.78 and loss3: 35231.27\n",
      "Epoch [2717], train_loss: 35766.57 with loss1: 472.91, loss2: 66.66 and loss3: 35227.01\n",
      "Epoch [2718], train_loss: 35763.66 with loss1: 474.10, loss2: 66.76 and loss3: 35222.81\n",
      "Epoch [2719], train_loss: 35757.96 with loss1: 472.82, loss2: 66.58 and loss3: 35218.55\n",
      "Epoch [2720], train_loss: 35755.30 with loss1: 474.17, loss2: 66.78 and loss3: 35214.35\n",
      "Epoch [2721], train_loss: 35748.85 with loss1: 472.05, loss2: 66.70 and loss3: 35210.10\n",
      "Epoch [2722], train_loss: 35745.83 with loss1: 473.30, loss2: 66.64 and loss3: 35205.89\n",
      "Epoch [2723], train_loss: 35737.85 with loss1: 469.89, loss2: 66.32 and loss3: 35201.64\n",
      "Epoch [2724], train_loss: 35736.60 with loss1: 472.76, loss2: 66.41 and loss3: 35197.44\n",
      "Epoch [2725], train_loss: 35730.76 with loss1: 471.17, loss2: 66.40 and loss3: 35193.18\n",
      "Epoch [2726], train_loss: 35727.70 with loss1: 472.20, loss2: 66.51 and loss3: 35188.98\n",
      "Epoch [2727], train_loss: 35720.26 with loss1: 469.25, loss2: 66.27 and loss3: 35184.73\n",
      "Epoch [2728], train_loss: 35717.50 with loss1: 470.44, loss2: 66.55 and loss3: 35180.52\n",
      "Epoch [2729], train_loss: 35709.89 with loss1: 467.40, loss2: 66.21 and loss3: 35176.28\n",
      "Epoch [2730], train_loss: 35708.34 with loss1: 469.88, loss2: 66.40 and loss3: 35172.07\n",
      "Epoch [2731], train_loss: 35701.18 with loss1: 467.00, loss2: 66.35 and loss3: 35167.83\n",
      "Epoch [2732], train_loss: 35699.34 with loss1: 469.26, loss2: 66.46 and loss3: 35163.61\n",
      "Epoch [2733], train_loss: 35693.79 with loss1: 468.10, loss2: 66.31 and loss3: 35159.38\n",
      "Epoch [2734], train_loss: 35690.27 with loss1: 468.84, loss2: 66.28 and loss3: 35155.16\n",
      "Epoch [2735], train_loss: 35684.03 with loss1: 466.89, loss2: 66.21 and loss3: 35150.93\n",
      "Epoch [2736], train_loss: 35682.61 with loss1: 469.41, loss2: 66.50 and loss3: 35146.71\n",
      "Epoch [2737], train_loss: 35676.02 with loss1: 467.21, loss2: 66.33 and loss3: 35142.48\n",
      "Epoch [2738], train_loss: 35672.67 with loss1: 468.26, loss2: 66.15 and loss3: 35138.26\n",
      "Epoch [2739], train_loss: 35669.84 with loss1: 469.63, loss2: 66.18 and loss3: 35134.04\n",
      "Epoch [2740], train_loss: 35666.52 with loss1: 470.58, loss2: 66.14 and loss3: 35129.81\n",
      "Epoch [2741], train_loss: 35660.71 with loss1: 469.17, loss2: 65.95 and loss3: 35125.59\n",
      "Epoch [2742], train_loss: 35658.69 with loss1: 471.27, loss2: 66.06 and loss3: 35121.36\n",
      "Epoch [2743], train_loss: 35650.86 with loss1: 467.61, loss2: 66.10 and loss3: 35117.14\n",
      "Epoch [2744], train_loss: 35648.82 with loss1: 470.01, loss2: 65.90 and loss3: 35112.91\n",
      "Epoch [2745], train_loss: 35645.29 with loss1: 470.58, loss2: 66.01 and loss3: 35108.70\n",
      "Epoch [2746], train_loss: 35640.75 with loss1: 470.42, loss2: 65.87 and loss3: 35104.46\n",
      "Epoch [2747], train_loss: 35634.57 with loss1: 468.42, loss2: 65.91 and loss3: 35100.25\n",
      "Epoch [2748], train_loss: 35632.44 with loss1: 470.47, loss2: 65.95 and loss3: 35096.02\n",
      "Epoch [2749], train_loss: 35628.25 with loss1: 470.53, loss2: 65.92 and loss3: 35091.80\n",
      "Epoch [2750], train_loss: 35623.48 with loss1: 470.27, loss2: 65.63 and loss3: 35087.57\n",
      "Epoch [2751], train_loss: 35619.75 with loss1: 470.44, loss2: 65.96 and loss3: 35083.36\n",
      "Epoch [2752], train_loss: 35619.52 with loss1: 474.62, loss2: 65.78 and loss3: 35079.13\n",
      "Epoch [2753], train_loss: 35612.91 with loss1: 472.20, loss2: 65.79 and loss3: 35074.92\n",
      "Epoch [2754], train_loss: 35610.65 with loss1: 474.30, loss2: 65.66 and loss3: 35070.69\n",
      "Epoch [2755], train_loss: 35606.64 with loss1: 474.52, loss2: 65.65 and loss3: 35066.48\n",
      "Epoch [2756], train_loss: 35604.58 with loss1: 476.62, loss2: 65.70 and loss3: 35062.26\n",
      "Epoch [2757], train_loss: 35599.66 with loss1: 476.09, loss2: 65.53 and loss3: 35058.04\n",
      "Epoch [2758], train_loss: 35598.63 with loss1: 479.05, loss2: 65.76 and loss3: 35053.82\n",
      "Epoch [2759], train_loss: 35593.83 with loss1: 478.65, loss2: 65.58 and loss3: 35049.60\n",
      "Epoch [2760], train_loss: 35591.92 with loss1: 480.67, loss2: 65.87 and loss3: 35045.38\n",
      "Epoch [2761], train_loss: 35586.30 with loss1: 479.53, loss2: 65.61 and loss3: 35041.16\n",
      "Epoch [2762], train_loss: 35584.16 with loss1: 481.72, loss2: 65.50 and loss3: 35036.94\n",
      "Epoch [2763], train_loss: 35577.34 with loss1: 479.00, loss2: 65.61 and loss3: 35032.73\n",
      "Epoch [2764], train_loss: 35577.75 with loss1: 483.69, loss2: 65.55 and loss3: 35028.51\n",
      "Epoch [2765], train_loss: 35569.90 with loss1: 479.84, loss2: 65.77 and loss3: 35024.30\n",
      "Epoch [2766], train_loss: 35569.22 with loss1: 483.58, loss2: 65.57 and loss3: 35020.07\n",
      "Epoch [2767], train_loss: 35561.54 with loss1: 480.33, loss2: 65.33 and loss3: 35015.87\n",
      "Epoch [2768], train_loss: 35559.88 with loss1: 482.78, loss2: 65.46 and loss3: 35011.64\n",
      "Epoch [2769], train_loss: 35551.83 with loss1: 478.92, loss2: 65.47 and loss3: 35007.44\n",
      "Epoch [2770], train_loss: 35548.20 with loss1: 479.49, loss2: 65.50 and loss3: 35003.21\n",
      "Epoch [2771], train_loss: 35541.22 with loss1: 476.82, loss2: 65.40 and loss3: 34999.00\n",
      "Epoch [2772], train_loss: 35539.42 with loss1: 479.11, loss2: 65.53 and loss3: 34994.77\n",
      "Epoch [2773], train_loss: 35529.98 with loss1: 474.15, loss2: 65.27 and loss3: 34990.57\n",
      "Epoch [2774], train_loss: 35529.15 with loss1: 477.35, loss2: 65.45 and loss3: 34986.34\n",
      "Epoch [2775], train_loss: 35519.66 with loss1: 472.28, loss2: 65.23 and loss3: 34982.14\n",
      "Epoch [2776], train_loss: 35515.94 with loss1: 472.90, loss2: 65.13 and loss3: 34977.91\n",
      "Epoch [2777], train_loss: 35509.93 with loss1: 471.06, loss2: 65.16 and loss3: 34973.71\n",
      "Epoch [2778], train_loss: 35504.83 with loss1: 470.20, loss2: 65.14 and loss3: 34969.49\n",
      "Epoch [2779], train_loss: 35498.27 with loss1: 468.02, loss2: 64.97 and loss3: 34965.28\n",
      "Epoch [2780], train_loss: 35495.61 with loss1: 469.46, loss2: 65.08 and loss3: 34961.06\n",
      "Epoch [2781], train_loss: 35488.32 with loss1: 466.33, loss2: 65.15 and loss3: 34956.84\n",
      "Epoch [2782], train_loss: 35483.90 with loss1: 466.29, loss2: 64.97 and loss3: 34952.64\n",
      "Epoch [2783], train_loss: 35477.31 with loss1: 463.76, loss2: 65.14 and loss3: 34948.41\n",
      "Epoch [2784], train_loss: 35474.84 with loss1: 465.61, loss2: 65.02 and loss3: 34944.21\n",
      "Epoch [2785], train_loss: 35468.34 with loss1: 463.52, loss2: 64.83 and loss3: 34939.98\n",
      "Epoch [2786], train_loss: 35464.44 with loss1: 463.25, loss2: 65.41 and loss3: 34935.78\n",
      "Epoch [2787], train_loss: 35457.89 with loss1: 461.44, loss2: 64.89 and loss3: 34931.56\n",
      "Epoch [2788], train_loss: 35455.73 with loss1: 463.44, loss2: 64.94 and loss3: 34927.36\n",
      "Epoch [2789], train_loss: 35449.52 with loss1: 461.59, loss2: 64.80 and loss3: 34923.13\n",
      "Epoch [2790], train_loss: 35445.64 with loss1: 461.83, loss2: 64.88 and loss3: 34918.93\n",
      "Epoch [2791], train_loss: 35440.71 with loss1: 461.12, loss2: 64.88 and loss3: 34914.71\n",
      "Epoch [2792], train_loss: 35436.65 with loss1: 461.08, loss2: 65.06 and loss3: 34910.50\n",
      "Epoch [2793], train_loss: 35431.32 with loss1: 460.19, loss2: 64.85 and loss3: 34906.29\n",
      "Epoch [2794], train_loss: 35428.28 with loss1: 461.33, loss2: 64.87 and loss3: 34902.08\n",
      "Epoch [2795], train_loss: 35421.70 with loss1: 459.11, loss2: 64.74 and loss3: 34897.86\n",
      "Epoch [2796], train_loss: 35420.51 with loss1: 461.97, loss2: 64.88 and loss3: 34893.66\n",
      "Epoch [2797], train_loss: 35411.61 with loss1: 457.58, loss2: 64.59 and loss3: 34889.45\n",
      "Epoch [2798], train_loss: 35409.11 with loss1: 459.20, loss2: 64.68 and loss3: 34885.23\n",
      "Epoch [2799], train_loss: 35404.43 with loss1: 458.86, loss2: 64.55 and loss3: 34881.02\n",
      "Epoch [2800], train_loss: 35399.79 with loss1: 458.17, loss2: 64.81 and loss3: 34876.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2801], train_loss: 35394.68 with loss1: 457.44, loss2: 64.63 and loss3: 34872.61\n",
      "Epoch [2802], train_loss: 35393.48 with loss1: 460.44, loss2: 64.66 and loss3: 34868.39\n",
      "Epoch [2803], train_loss: 35388.36 with loss1: 459.36, loss2: 64.81 and loss3: 34864.18\n",
      "Epoch [2804], train_loss: 35384.23 with loss1: 459.73, loss2: 64.53 and loss3: 34859.97\n",
      "Epoch [2805], train_loss: 35380.12 with loss1: 459.65, loss2: 64.72 and loss3: 34855.76\n",
      "Epoch [2806], train_loss: 35377.96 with loss1: 461.76, loss2: 64.65 and loss3: 34851.55\n",
      "Epoch [2807], train_loss: 35372.28 with loss1: 460.50, loss2: 64.44 and loss3: 34847.34\n",
      "Epoch [2808], train_loss: 35370.20 with loss1: 462.65, loss2: 64.40 and loss3: 34843.14\n",
      "Epoch [2809], train_loss: 35367.26 with loss1: 464.04, loss2: 64.31 and loss3: 34838.91\n",
      "Epoch [2810], train_loss: 35364.34 with loss1: 465.08, loss2: 64.53 and loss3: 34834.72\n",
      "Epoch [2811], train_loss: 35360.04 with loss1: 465.25, loss2: 64.28 and loss3: 34830.50\n",
      "Epoch [2812], train_loss: 35357.72 with loss1: 466.94, loss2: 64.48 and loss3: 34826.30\n",
      "Epoch [2813], train_loss: 35352.70 with loss1: 466.21, loss2: 64.41 and loss3: 34822.08\n",
      "Epoch [2814], train_loss: 35352.85 with loss1: 470.60, loss2: 64.36 and loss3: 34817.89\n",
      "Epoch [2815], train_loss: 35345.61 with loss1: 467.85, loss2: 64.09 and loss3: 34813.66\n",
      "Epoch [2816], train_loss: 35345.97 with loss1: 472.35, loss2: 64.16 and loss3: 34809.47\n",
      "Epoch [2817], train_loss: 35338.80 with loss1: 469.44, loss2: 64.11 and loss3: 34805.25\n",
      "Epoch [2818], train_loss: 35338.96 with loss1: 473.68, loss2: 64.23 and loss3: 34801.05\n",
      "Epoch [2819], train_loss: 35334.77 with loss1: 473.48, loss2: 64.46 and loss3: 34796.84\n",
      "Epoch [2820], train_loss: 35334.42 with loss1: 477.41, loss2: 64.38 and loss3: 34792.63\n",
      "Epoch [2821], train_loss: 35328.52 with loss1: 475.99, loss2: 64.11 and loss3: 34788.42\n",
      "Epoch [2822], train_loss: 35327.82 with loss1: 479.40, loss2: 64.21 and loss3: 34784.22\n",
      "Epoch [2823], train_loss: 35322.95 with loss1: 478.81, loss2: 64.13 and loss3: 34780.01\n",
      "Epoch [2824], train_loss: 35324.34 with loss1: 484.26, loss2: 64.27 and loss3: 34775.80\n",
      "Epoch [2825], train_loss: 35318.07 with loss1: 482.27, loss2: 64.20 and loss3: 34771.59\n",
      "Epoch [2826], train_loss: 35318.64 with loss1: 486.81, loss2: 64.45 and loss3: 34767.39\n",
      "Epoch [2827], train_loss: 35312.09 with loss1: 484.72, loss2: 64.19 and loss3: 34763.18\n",
      "Epoch [2828], train_loss: 35313.97 with loss1: 491.10, loss2: 63.90 and loss3: 34758.97\n",
      "Epoch [2829], train_loss: 35306.77 with loss1: 488.04, loss2: 63.96 and loss3: 34754.77\n",
      "Epoch [2830], train_loss: 35307.52 with loss1: 492.43, loss2: 64.52 and loss3: 34750.56\n",
      "Epoch [2831], train_loss: 35300.20 with loss1: 489.69, loss2: 64.15 and loss3: 34746.36\n",
      "Epoch [2832], train_loss: 35301.61 with loss1: 495.55, loss2: 63.91 and loss3: 34742.15\n",
      "Epoch [2833], train_loss: 35293.51 with loss1: 491.78, loss2: 63.77 and loss3: 34737.95\n",
      "Epoch [2834], train_loss: 35294.48 with loss1: 496.77, loss2: 63.96 and loss3: 34733.74\n",
      "Epoch [2835], train_loss: 35285.86 with loss1: 492.53, loss2: 63.79 and loss3: 34729.54\n",
      "Epoch [2836], train_loss: 35286.34 with loss1: 497.12, loss2: 63.88 and loss3: 34725.33\n",
      "Epoch [2837], train_loss: 35281.03 with loss1: 495.96, loss2: 63.93 and loss3: 34721.14\n",
      "Epoch [2838], train_loss: 35280.32 with loss1: 499.38, loss2: 64.02 and loss3: 34716.92\n",
      "Epoch [2839], train_loss: 35272.84 with loss1: 496.33, loss2: 63.77 and loss3: 34712.73\n",
      "Epoch [2840], train_loss: 35272.85 with loss1: 500.29, loss2: 64.05 and loss3: 34708.51\n",
      "Epoch [2841], train_loss: 35263.79 with loss1: 495.76, loss2: 63.70 and loss3: 34704.33\n",
      "Epoch [2842], train_loss: 35263.37 with loss1: 499.53, loss2: 63.73 and loss3: 34700.11\n",
      "Epoch [2843], train_loss: 35253.86 with loss1: 494.31, loss2: 63.62 and loss3: 34695.93\n",
      "Epoch [2844], train_loss: 35253.29 with loss1: 497.89, loss2: 63.70 and loss3: 34691.70\n",
      "Epoch [2845], train_loss: 35243.97 with loss1: 492.75, loss2: 63.70 and loss3: 34687.52\n",
      "Epoch [2846], train_loss: 35243.30 with loss1: 496.21, loss2: 63.78 and loss3: 34683.30\n",
      "Epoch [2847], train_loss: 35233.82 with loss1: 491.11, loss2: 63.60 and loss3: 34679.12\n",
      "Epoch [2848], train_loss: 35234.16 with loss1: 495.31, loss2: 63.94 and loss3: 34674.91\n",
      "Epoch [2849], train_loss: 35224.38 with loss1: 490.12, loss2: 63.55 and loss3: 34670.71\n",
      "Epoch [2850], train_loss: 35224.43 with loss1: 494.32, loss2: 63.60 and loss3: 34666.52\n",
      "Epoch [2851], train_loss: 35215.11 with loss1: 489.44, loss2: 63.36 and loss3: 34662.31\n",
      "Epoch [2852], train_loss: 35213.91 with loss1: 492.02, loss2: 63.77 and loss3: 34658.12\n",
      "Epoch [2853], train_loss: 35203.90 with loss1: 486.53, loss2: 63.45 and loss3: 34653.91\n",
      "Epoch [2854], train_loss: 35202.13 with loss1: 488.81, loss2: 63.61 and loss3: 34649.71\n",
      "Epoch [2855], train_loss: 35191.68 with loss1: 482.70, loss2: 63.46 and loss3: 34645.52\n",
      "Epoch [2856], train_loss: 35190.64 with loss1: 485.58, loss2: 63.75 and loss3: 34641.31\n",
      "Epoch [2857], train_loss: 35180.00 with loss1: 479.50, loss2: 63.39 and loss3: 34637.12\n",
      "Epoch [2858], train_loss: 35176.33 with loss1: 479.86, loss2: 63.55 and loss3: 34632.92\n",
      "Epoch [2859], train_loss: 35167.60 with loss1: 475.43, loss2: 63.45 and loss3: 34628.72\n",
      "Epoch [2860], train_loss: 35164.00 with loss1: 476.08, loss2: 63.40 and loss3: 34624.52\n",
      "Epoch [2861], train_loss: 35154.41 with loss1: 470.77, loss2: 63.31 and loss3: 34620.32\n",
      "Epoch [2862], train_loss: 35151.68 with loss1: 472.18, loss2: 63.37 and loss3: 34616.13\n",
      "Epoch [2863], train_loss: 35143.38 with loss1: 468.21, loss2: 63.25 and loss3: 34611.93\n",
      "Epoch [2864], train_loss: 35141.84 with loss1: 470.70, loss2: 63.41 and loss3: 34607.73\n",
      "Epoch [2865], train_loss: 35131.84 with loss1: 465.25, loss2: 63.04 and loss3: 34603.54\n",
      "Epoch [2866], train_loss: 35129.59 with loss1: 467.12, loss2: 63.13 and loss3: 34599.34\n",
      "Epoch [2867], train_loss: 35120.90 with loss1: 462.68, loss2: 63.08 and loss3: 34595.15\n",
      "Epoch [2868], train_loss: 35117.09 with loss1: 463.08, loss2: 63.06 and loss3: 34590.95\n",
      "Epoch [2869], train_loss: 35110.96 with loss1: 461.18, loss2: 63.03 and loss3: 34586.75\n",
      "Epoch [2870], train_loss: 35106.04 with loss1: 460.26, loss2: 63.22 and loss3: 34582.56\n",
      "Epoch [2871], train_loss: 35099.53 with loss1: 458.15, loss2: 63.02 and loss3: 34578.36\n",
      "Epoch [2872], train_loss: 35095.20 with loss1: 457.88, loss2: 63.15 and loss3: 34574.18\n",
      "Epoch [2873], train_loss: 35089.98 with loss1: 457.16, loss2: 62.85 and loss3: 34569.97\n",
      "Epoch [2874], train_loss: 35084.75 with loss1: 456.00, loss2: 62.97 and loss3: 34565.78\n",
      "Epoch [2875], train_loss: 35079.26 with loss1: 454.74, loss2: 62.94 and loss3: 34561.58\n",
      "Epoch [2876], train_loss: 35075.38 with loss1: 455.05, loss2: 62.94 and loss3: 34557.39\n",
      "Epoch [2877], train_loss: 35070.05 with loss1: 454.05, loss2: 62.80 and loss3: 34553.20\n",
      "Epoch [2878], train_loss: 35065.77 with loss1: 453.89, loss2: 62.87 and loss3: 34549.00\n",
      "Epoch [2879], train_loss: 35060.05 with loss1: 452.36, loss2: 62.88 and loss3: 34544.81\n",
      "Epoch [2880], train_loss: 35056.17 with loss1: 452.61, loss2: 62.95 and loss3: 34540.62\n",
      "Epoch [2881], train_loss: 35050.81 with loss1: 451.33, loss2: 63.06 and loss3: 34536.42\n",
      "Epoch [2882], train_loss: 35047.02 with loss1: 451.99, loss2: 62.80 and loss3: 34532.23\n",
      "Epoch [2883], train_loss: 35040.23 with loss1: 449.47, loss2: 62.72 and loss3: 34528.04\n",
      "Epoch [2884], train_loss: 35037.42 with loss1: 450.85, loss2: 62.72 and loss3: 34523.85\n",
      "Epoch [2885], train_loss: 35031.11 with loss1: 448.54, loss2: 62.93 and loss3: 34519.65\n",
      "Epoch [2886], train_loss: 35027.39 with loss1: 449.21, loss2: 62.72 and loss3: 34515.47\n",
      "Epoch [2887], train_loss: 35023.80 with loss1: 449.80, loss2: 62.74 and loss3: 34511.27\n",
      "Epoch [2888], train_loss: 35020.04 with loss1: 450.29, loss2: 62.65 and loss3: 34507.09\n",
      "Epoch [2889], train_loss: 35012.91 with loss1: 447.40, loss2: 62.62 and loss3: 34502.89\n",
      "Epoch [2890], train_loss: 35010.50 with loss1: 449.10, loss2: 62.69 and loss3: 34498.71\n",
      "Epoch [2891], train_loss: 35005.56 with loss1: 448.43, loss2: 62.63 and loss3: 34494.51\n",
      "Epoch [2892], train_loss: 35002.00 with loss1: 449.11, loss2: 62.55 and loss3: 34490.34\n",
      "Epoch [2893], train_loss: 34996.12 with loss1: 447.51, loss2: 62.48 and loss3: 34486.13\n",
      "Epoch [2894], train_loss: 34992.17 with loss1: 447.74, loss2: 62.48 and loss3: 34481.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2895], train_loss: 34986.86 with loss1: 446.69, loss2: 62.40 and loss3: 34477.77\n",
      "Epoch [2896], train_loss: 34984.72 with loss1: 448.61, loss2: 62.53 and loss3: 34473.58\n",
      "Epoch [2897], train_loss: 34977.99 with loss1: 446.12, loss2: 62.49 and loss3: 34469.39\n",
      "Epoch [2898], train_loss: 34974.96 with loss1: 447.27, loss2: 62.49 and loss3: 34465.20\n",
      "Epoch [2899], train_loss: 34970.91 with loss1: 447.41, loss2: 62.48 and loss3: 34461.02\n",
      "Epoch [2900], train_loss: 34966.75 with loss1: 447.57, loss2: 62.35 and loss3: 34456.83\n",
      "Epoch [2901], train_loss: 34962.21 with loss1: 447.24, loss2: 62.33 and loss3: 34452.65\n",
      "Epoch [2902], train_loss: 34958.61 with loss1: 447.71, loss2: 62.44 and loss3: 34448.46\n",
      "Epoch [2903], train_loss: 34952.41 with loss1: 445.99, loss2: 62.15 and loss3: 34444.27\n",
      "Epoch [2904], train_loss: 34949.78 with loss1: 447.37, loss2: 62.31 and loss3: 34440.09\n",
      "Epoch [2905], train_loss: 34945.21 with loss1: 446.86, loss2: 62.44 and loss3: 34435.91\n",
      "Epoch [2906], train_loss: 34942.94 with loss1: 448.59, loss2: 62.62 and loss3: 34431.72\n",
      "Epoch [2907], train_loss: 34937.44 with loss1: 447.66, loss2: 62.23 and loss3: 34427.54\n",
      "Epoch [2908], train_loss: 34933.17 with loss1: 447.71, loss2: 62.10 and loss3: 34423.35\n",
      "Epoch [2909], train_loss: 34929.96 with loss1: 448.49, loss2: 62.30 and loss3: 34419.18\n",
      "Epoch [2910], train_loss: 34925.89 with loss1: 448.82, loss2: 62.09 and loss3: 34414.98\n",
      "Epoch [2911], train_loss: 34920.92 with loss1: 448.06, loss2: 62.05 and loss3: 34410.81\n",
      "Epoch [2912], train_loss: 34917.88 with loss1: 449.18, loss2: 62.08 and loss3: 34406.62\n",
      "Epoch [2913], train_loss: 34913.11 with loss1: 448.62, loss2: 62.04 and loss3: 34402.45\n",
      "Epoch [2914], train_loss: 34911.54 with loss1: 450.91, loss2: 62.38 and loss3: 34398.25\n",
      "Epoch [2915], train_loss: 34905.84 with loss1: 449.72, loss2: 62.03 and loss3: 34394.08\n",
      "Epoch [2916], train_loss: 34903.12 with loss1: 451.10, loss2: 62.14 and loss3: 34389.89\n",
      "Epoch [2917], train_loss: 34898.88 with loss1: 451.13, loss2: 62.03 and loss3: 34385.72\n",
      "Epoch [2918], train_loss: 34895.22 with loss1: 451.82, loss2: 61.88 and loss3: 34381.53\n",
      "Epoch [2919], train_loss: 34890.89 with loss1: 451.53, loss2: 62.01 and loss3: 34377.35\n",
      "Epoch [2920], train_loss: 34888.25 with loss1: 453.13, loss2: 61.95 and loss3: 34373.17\n",
      "Epoch [2921], train_loss: 34882.10 with loss1: 451.16, loss2: 61.95 and loss3: 34368.99\n",
      "Epoch [2922], train_loss: 34880.77 with loss1: 454.06, loss2: 61.91 and loss3: 34364.80\n",
      "Epoch [2923], train_loss: 34875.26 with loss1: 452.84, loss2: 61.79 and loss3: 34360.63\n",
      "Epoch [2924], train_loss: 34872.76 with loss1: 454.31, loss2: 62.01 and loss3: 34356.45\n",
      "Epoch [2925], train_loss: 34867.27 with loss1: 453.11, loss2: 61.88 and loss3: 34352.27\n",
      "Epoch [2926], train_loss: 34865.09 with loss1: 455.12, loss2: 61.88 and loss3: 34348.09\n",
      "Epoch [2927], train_loss: 34858.37 with loss1: 452.68, loss2: 61.79 and loss3: 34343.91\n",
      "Epoch [2928], train_loss: 34858.30 with loss1: 456.84, loss2: 61.73 and loss3: 34339.73\n",
      "Epoch [2929], train_loss: 34851.20 with loss1: 453.80, loss2: 61.84 and loss3: 34335.56\n",
      "Epoch [2930], train_loss: 34847.33 with loss1: 454.41, loss2: 61.55 and loss3: 34331.38\n",
      "Epoch [2931], train_loss: 34843.46 with loss1: 454.40, loss2: 61.85 and loss3: 34327.21\n",
      "Epoch [2932], train_loss: 34840.10 with loss1: 455.42, loss2: 61.66 and loss3: 34323.02\n",
      "Epoch [2933], train_loss: 34833.87 with loss1: 453.40, loss2: 61.61 and loss3: 34318.86\n",
      "Epoch [2934], train_loss: 34831.80 with loss1: 455.30, loss2: 61.83 and loss3: 34314.67\n",
      "Epoch [2935], train_loss: 34825.97 with loss1: 453.76, loss2: 61.71 and loss3: 34310.51\n",
      "Epoch [2936], train_loss: 34822.88 with loss1: 454.85, loss2: 61.70 and loss3: 34306.32\n",
      "Epoch [2937], train_loss: 34817.62 with loss1: 453.86, loss2: 61.61 and loss3: 34302.16\n",
      "Epoch [2938], train_loss: 34816.94 with loss1: 457.39, loss2: 61.58 and loss3: 34297.97\n",
      "Epoch [2939], train_loss: 34810.92 with loss1: 455.44, loss2: 61.68 and loss3: 34293.80\n",
      "Epoch [2940], train_loss: 34809.22 with loss1: 458.18, loss2: 61.42 and loss3: 34289.62\n",
      "Epoch [2941], train_loss: 34802.87 with loss1: 455.80, loss2: 61.62 and loss3: 34285.46\n",
      "Epoch [2942], train_loss: 34800.34 with loss1: 457.61, loss2: 61.45 and loss3: 34281.27\n",
      "Epoch [2943], train_loss: 34794.36 with loss1: 455.72, loss2: 61.53 and loss3: 34277.11\n",
      "Epoch [2944], train_loss: 34791.64 with loss1: 457.33, loss2: 61.39 and loss3: 34272.93\n",
      "Epoch [2945], train_loss: 34786.35 with loss1: 456.20, loss2: 61.38 and loss3: 34268.77\n",
      "Epoch [2946], train_loss: 34783.20 with loss1: 457.38, loss2: 61.23 and loss3: 34264.59\n",
      "Epoch [2947], train_loss: 34777.91 with loss1: 455.91, loss2: 61.57 and loss3: 34260.43\n",
      "Epoch [2948], train_loss: 34774.71 with loss1: 457.15, loss2: 61.31 and loss3: 34256.25\n",
      "Epoch [2949], train_loss: 34768.95 with loss1: 455.54, loss2: 61.32 and loss3: 34252.09\n",
      "Epoch [2950], train_loss: 34765.88 with loss1: 456.87, loss2: 61.10 and loss3: 34247.91\n",
      "Epoch [2951], train_loss: 34759.20 with loss1: 454.09, loss2: 61.36 and loss3: 34243.75\n",
      "Epoch [2952], train_loss: 34757.22 with loss1: 456.22, loss2: 61.43 and loss3: 34239.57\n",
      "Epoch [2953], train_loss: 34749.46 with loss1: 452.80, loss2: 61.26 and loss3: 34235.41\n",
      "Epoch [2954], train_loss: 34747.61 with loss1: 455.28, loss2: 61.09 and loss3: 34231.24\n",
      "Epoch [2955], train_loss: 34742.39 with loss1: 454.28, loss2: 61.04 and loss3: 34227.07\n",
      "Epoch [2956], train_loss: 34740.32 with loss1: 456.33, loss2: 61.09 and loss3: 34222.91\n",
      "Epoch [2957], train_loss: 34733.82 with loss1: 453.77, loss2: 61.31 and loss3: 34218.74\n",
      "Epoch [2958], train_loss: 34731.59 with loss1: 455.84, loss2: 61.18 and loss3: 34214.57\n",
      "Epoch [2959], train_loss: 34724.70 with loss1: 453.03, loss2: 61.27 and loss3: 34210.41\n",
      "Epoch [2960], train_loss: 34722.10 with loss1: 454.73, loss2: 61.13 and loss3: 34206.23\n",
      "Epoch [2961], train_loss: 34715.97 with loss1: 452.82, loss2: 61.08 and loss3: 34202.07\n",
      "Epoch [2962], train_loss: 34714.67 with loss1: 455.62, loss2: 61.14 and loss3: 34197.90\n",
      "Epoch [2963], train_loss: 34708.04 with loss1: 453.41, loss2: 60.90 and loss3: 34193.73\n",
      "Epoch [2964], train_loss: 34705.03 with loss1: 454.68, loss2: 60.78 and loss3: 34189.57\n",
      "Epoch [2965], train_loss: 34700.39 with loss1: 454.11, loss2: 60.87 and loss3: 34185.40\n",
      "Epoch [2966], train_loss: 34699.17 with loss1: 456.82, loss2: 61.11 and loss3: 34181.24\n",
      "Epoch [2967], train_loss: 34691.45 with loss1: 453.41, loss2: 60.96 and loss3: 34177.07\n",
      "Epoch [2968], train_loss: 34689.47 with loss1: 455.56, loss2: 61.00 and loss3: 34172.91\n",
      "Epoch [2969], train_loss: 34682.94 with loss1: 453.19, loss2: 61.00 and loss3: 34168.75\n",
      "Epoch [2970], train_loss: 34681.39 with loss1: 456.03, loss2: 60.80 and loss3: 34164.57\n",
      "Epoch [2971], train_loss: 34675.24 with loss1: 454.07, loss2: 60.74 and loss3: 34160.42\n",
      "Epoch [2972], train_loss: 34672.10 with loss1: 454.92, loss2: 60.94 and loss3: 34156.24\n",
      "Epoch [2973], train_loss: 34667.27 with loss1: 454.26, loss2: 60.92 and loss3: 34152.09\n",
      "Epoch [2974], train_loss: 34663.95 with loss1: 455.31, loss2: 60.74 and loss3: 34147.91\n",
      "Epoch [2975], train_loss: 34657.70 with loss1: 452.93, loss2: 61.01 and loss3: 34143.76\n",
      "Epoch [2976], train_loss: 34654.82 with loss1: 454.55, loss2: 60.70 and loss3: 34139.57\n",
      "Epoch [2977], train_loss: 34649.31 with loss1: 453.27, loss2: 60.61 and loss3: 34135.43\n",
      "Epoch [2978], train_loss: 34645.72 with loss1: 453.70, loss2: 60.78 and loss3: 34131.25\n",
      "Epoch [2979], train_loss: 34639.91 with loss1: 452.00, loss2: 60.81 and loss3: 34127.10\n",
      "Epoch [2980], train_loss: 34636.07 with loss1: 452.36, loss2: 60.79 and loss3: 34122.92\n",
      "Epoch [2981], train_loss: 34630.96 with loss1: 451.34, loss2: 60.85 and loss3: 34118.77\n",
      "Epoch [2982], train_loss: 34627.82 with loss1: 452.70, loss2: 60.52 and loss3: 34114.60\n",
      "Epoch [2983], train_loss: 34622.70 with loss1: 451.74, loss2: 60.52 and loss3: 34110.44\n",
      "Epoch [2984], train_loss: 34619.53 with loss1: 452.53, loss2: 60.72 and loss3: 34106.28\n",
      "Epoch [2985], train_loss: 34614.24 with loss1: 451.39, loss2: 60.73 and loss3: 34102.11\n",
      "Epoch [2986], train_loss: 34610.97 with loss1: 452.57, loss2: 60.44 and loss3: 34097.95\n",
      "Epoch [2987], train_loss: 34605.13 with loss1: 450.93, loss2: 60.42 and loss3: 34093.79\n",
      "Epoch [2988], train_loss: 34603.94 with loss1: 453.83, loss2: 60.49 and loss3: 34089.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2989], train_loss: 34596.77 with loss1: 450.85, loss2: 60.45 and loss3: 34085.46\n",
      "Epoch [2990], train_loss: 34595.93 with loss1: 454.10, loss2: 60.52 and loss3: 34081.30\n",
      "Epoch [2991], train_loss: 34591.36 with loss1: 453.87, loss2: 60.35 and loss3: 34077.14\n",
      "Epoch [2992], train_loss: 34589.56 with loss1: 455.94, loss2: 60.64 and loss3: 34072.98\n",
      "Epoch [2993], train_loss: 34583.09 with loss1: 453.92, loss2: 60.36 and loss3: 34068.82\n",
      "Epoch [2994], train_loss: 34581.71 with loss1: 456.75, loss2: 60.31 and loss3: 34064.65\n",
      "Epoch [2995], train_loss: 34577.52 with loss1: 456.73, loss2: 60.29 and loss3: 34060.50\n",
      "Epoch [2996], train_loss: 34574.59 with loss1: 457.81, loss2: 60.45 and loss3: 34056.33\n",
      "Epoch [2997], train_loss: 34569.59 with loss1: 457.11, loss2: 60.32 and loss3: 34052.17\n",
      "Epoch [2998], train_loss: 34569.75 with loss1: 461.25, loss2: 60.49 and loss3: 34048.01\n",
      "Epoch [2999], train_loss: 34565.43 with loss1: 461.15, loss2: 60.42 and loss3: 34043.86\n",
      "Epoch [3000], train_loss: 34564.32 with loss1: 464.44, loss2: 60.19 and loss3: 34039.69\n",
      "Epoch [3001], train_loss: 34559.62 with loss1: 463.91, loss2: 60.17 and loss3: 34035.54\n",
      "Epoch [3002], train_loss: 34560.26 with loss1: 468.58, loss2: 60.31 and loss3: 34031.37\n",
      "Epoch [3003], train_loss: 34554.88 with loss1: 467.51, loss2: 60.15 and loss3: 34027.22\n",
      "Epoch [3004], train_loss: 34557.33 with loss1: 474.10, loss2: 60.18 and loss3: 34023.05\n",
      "Epoch [3005], train_loss: 34550.32 with loss1: 471.30, loss2: 60.12 and loss3: 34018.90\n",
      "Epoch [3006], train_loss: 34551.77 with loss1: 476.69, loss2: 60.33 and loss3: 34014.74\n",
      "Epoch [3007], train_loss: 34544.52 with loss1: 474.04, loss2: 59.89 and loss3: 34010.59\n",
      "Epoch [3008], train_loss: 34547.59 with loss1: 481.05, loss2: 60.11 and loss3: 34006.43\n",
      "Epoch [3009], train_loss: 34541.45 with loss1: 479.35, loss2: 59.83 and loss3: 34002.27\n",
      "Epoch [3010], train_loss: 34543.68 with loss1: 485.29, loss2: 60.27 and loss3: 33998.12\n",
      "Epoch [3011], train_loss: 34538.77 with loss1: 484.63, loss2: 60.18 and loss3: 33993.96\n",
      "Epoch [3012], train_loss: 34541.92 with loss1: 491.77, loss2: 60.35 and loss3: 33989.81\n",
      "Epoch [3013], train_loss: 34530.81 with loss1: 485.43, loss2: 59.73 and loss3: 33985.65\n",
      "Epoch [3014], train_loss: 34534.44 with loss1: 492.85, loss2: 60.09 and loss3: 33981.50\n",
      "Epoch [3015], train_loss: 34525.50 with loss1: 488.37, loss2: 59.79 and loss3: 33977.34\n",
      "Epoch [3016], train_loss: 34527.66 with loss1: 494.38, loss2: 60.10 and loss3: 33973.19\n",
      "Epoch [3017], train_loss: 34517.88 with loss1: 488.88, loss2: 59.96 and loss3: 33969.03\n",
      "Epoch [3018], train_loss: 34519.08 with loss1: 494.04, loss2: 60.15 and loss3: 33964.88\n",
      "Epoch [3019], train_loss: 34507.65 with loss1: 487.18, loss2: 59.74 and loss3: 33960.73\n",
      "Epoch [3020], train_loss: 34509.69 with loss1: 492.87, loss2: 60.24 and loss3: 33956.58\n",
      "Epoch [3021], train_loss: 34498.02 with loss1: 485.68, loss2: 59.91 and loss3: 33952.43\n",
      "Epoch [3022], train_loss: 34495.59 with loss1: 487.45, loss2: 59.87 and loss3: 33948.27\n",
      "Epoch [3023], train_loss: 34484.30 with loss1: 480.59, loss2: 59.58 and loss3: 33944.12\n",
      "Epoch [3024], train_loss: 34479.73 with loss1: 479.82, loss2: 59.94 and loss3: 33939.97\n",
      "Epoch [3025], train_loss: 34469.72 with loss1: 474.34, loss2: 59.55 and loss3: 33935.82\n",
      "Epoch [3026], train_loss: 34467.30 with loss1: 475.64, loss2: 59.99 and loss3: 33931.67\n",
      "Epoch [3027], train_loss: 34454.49 with loss1: 467.34, loss2: 59.62 and loss3: 33927.53\n",
      "Epoch [3028], train_loss: 34451.37 with loss1: 467.95, loss2: 60.05 and loss3: 33923.37\n",
      "Epoch [3029], train_loss: 34441.15 with loss1: 462.18, loss2: 59.73 and loss3: 33919.23\n",
      "Epoch [3030], train_loss: 34437.17 with loss1: 462.18, loss2: 59.91 and loss3: 33915.07\n",
      "Epoch [3031], train_loss: 34426.87 with loss1: 456.33, loss2: 59.60 and loss3: 33910.93\n",
      "Epoch [3032], train_loss: 34421.70 with loss1: 454.93, loss2: 59.99 and loss3: 33906.78\n",
      "Epoch [3033], train_loss: 34412.43 with loss1: 450.25, loss2: 59.54 and loss3: 33902.64\n",
      "Epoch [3034], train_loss: 34408.90 with loss1: 450.65, loss2: 59.77 and loss3: 33898.48\n",
      "Epoch [3035], train_loss: 34398.96 with loss1: 445.05, loss2: 59.57 and loss3: 33894.34\n",
      "Epoch [3036], train_loss: 34395.22 with loss1: 445.42, loss2: 59.61 and loss3: 33890.19\n",
      "Epoch [3037], train_loss: 34386.92 with loss1: 441.56, loss2: 59.30 and loss3: 33886.05\n",
      "Epoch [3038], train_loss: 34381.79 with loss1: 440.33, loss2: 59.56 and loss3: 33881.90\n",
      "Epoch [3039], train_loss: 34375.80 with loss1: 438.80, loss2: 59.24 and loss3: 33877.75\n",
      "Epoch [3040], train_loss: 34371.72 with loss1: 438.55, loss2: 59.56 and loss3: 33873.61\n",
      "Epoch [3041], train_loss: 34365.48 with loss1: 436.60, loss2: 59.42 and loss3: 33869.46\n",
      "Epoch [3042], train_loss: 34359.66 with loss1: 434.62, loss2: 59.72 and loss3: 33865.32\n",
      "Epoch [3043], train_loss: 34356.32 with loss1: 435.91, loss2: 59.24 and loss3: 33861.18\n",
      "Epoch [3044], train_loss: 34348.50 with loss1: 432.09, loss2: 59.38 and loss3: 33857.03\n",
      "Epoch [3045], train_loss: 34345.20 with loss1: 432.87, loss2: 59.44 and loss3: 33852.89\n",
      "Epoch [3046], train_loss: 34340.02 with loss1: 431.93, loss2: 59.35 and loss3: 33848.75\n",
      "Epoch [3047], train_loss: 34333.51 with loss1: 429.43, loss2: 59.49 and loss3: 33844.59\n",
      "Epoch [3048], train_loss: 34328.34 with loss1: 428.72, loss2: 59.16 and loss3: 33840.46\n",
      "Epoch [3049], train_loss: 34321.71 with loss1: 426.45, loss2: 58.95 and loss3: 33836.31\n",
      "Epoch [3050], train_loss: 34319.27 with loss1: 427.58, loss2: 59.50 and loss3: 33832.18\n",
      "Epoch [3051], train_loss: 34312.88 with loss1: 425.85, loss2: 59.01 and loss3: 33828.02\n",
      "Epoch [3052], train_loss: 34309.71 with loss1: 426.52, loss2: 59.29 and loss3: 33823.90\n",
      "Epoch [3053], train_loss: 34306.23 with loss1: 427.55, loss2: 58.94 and loss3: 33819.74\n",
      "Epoch [3054], train_loss: 34300.41 with loss1: 425.53, loss2: 59.28 and loss3: 33815.61\n",
      "Epoch [3055], train_loss: 34294.82 with loss1: 424.27, loss2: 59.07 and loss3: 33811.47\n",
      "Epoch [3056], train_loss: 34291.38 with loss1: 424.96, loss2: 59.09 and loss3: 33807.32\n",
      "Epoch [3057], train_loss: 34285.70 with loss1: 423.54, loss2: 58.97 and loss3: 33803.19\n",
      "Epoch [3058], train_loss: 34281.35 with loss1: 423.27, loss2: 59.04 and loss3: 33799.04\n",
      "Epoch [3059], train_loss: 34276.00 with loss1: 422.05, loss2: 59.03 and loss3: 33794.91\n",
      "Epoch [3060], train_loss: 34272.11 with loss1: 422.16, loss2: 59.18 and loss3: 33790.77\n",
      "Epoch [3061], train_loss: 34268.07 with loss1: 422.68, loss2: 58.76 and loss3: 33786.64\n",
      "Epoch [3062], train_loss: 34263.83 with loss1: 422.08, loss2: 59.25 and loss3: 33782.49\n",
      "Epoch [3063], train_loss: 34259.81 with loss1: 422.61, loss2: 58.85 and loss3: 33778.36\n",
      "Epoch [3064], train_loss: 34256.54 with loss1: 423.24, loss2: 59.09 and loss3: 33774.21\n",
      "Epoch [3065], train_loss: 34250.36 with loss1: 421.46, loss2: 58.81 and loss3: 33770.09\n",
      "Epoch [3066], train_loss: 34246.76 with loss1: 422.09, loss2: 58.73 and loss3: 33765.94\n",
      "Epoch [3067], train_loss: 34240.65 with loss1: 420.10, loss2: 58.73 and loss3: 33761.81\n",
      "Epoch [3068], train_loss: 34238.60 with loss1: 422.19, loss2: 58.74 and loss3: 33757.66\n",
      "Epoch [3069], train_loss: 34234.45 with loss1: 422.19, loss2: 58.72 and loss3: 33753.54\n",
      "Epoch [3070], train_loss: 34230.41 with loss1: 422.25, loss2: 58.77 and loss3: 33749.39\n",
      "Epoch [3071], train_loss: 34226.46 with loss1: 422.26, loss2: 58.94 and loss3: 33745.27\n",
      "Epoch [3072], train_loss: 34221.31 with loss1: 421.46, loss2: 58.73 and loss3: 33741.12\n",
      "Epoch [3073], train_loss: 34217.21 with loss1: 421.60, loss2: 58.62 and loss3: 33736.99\n",
      "Epoch [3074], train_loss: 34216.06 with loss1: 424.47, loss2: 58.74 and loss3: 33732.85\n",
      "Epoch [3075], train_loss: 34209.21 with loss1: 421.83, loss2: 58.65 and loss3: 33728.73\n",
      "Epoch [3076], train_loss: 34206.34 with loss1: 423.15, loss2: 58.61 and loss3: 33724.58\n",
      "Epoch [3077], train_loss: 34203.23 with loss1: 424.31, loss2: 58.47 and loss3: 33720.46\n",
      "Epoch [3078], train_loss: 34198.84 with loss1: 423.96, loss2: 58.56 and loss3: 33716.31\n",
      "Epoch [3079], train_loss: 34193.09 with loss1: 422.42, loss2: 58.47 and loss3: 33712.20\n",
      "Epoch [3080], train_loss: 34192.53 with loss1: 425.89, loss2: 58.60 and loss3: 33708.05\n",
      "Epoch [3081], train_loss: 34188.22 with loss1: 425.89, loss2: 58.41 and loss3: 33703.93\n",
      "Epoch [3082], train_loss: 34184.09 with loss1: 425.68, loss2: 58.63 and loss3: 33699.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3083], train_loss: 34180.12 with loss1: 426.08, loss2: 58.37 and loss3: 33695.66\n",
      "Epoch [3084], train_loss: 34176.72 with loss1: 426.53, loss2: 58.67 and loss3: 33691.52\n",
      "Epoch [3085], train_loss: 34171.31 with loss1: 425.51, loss2: 58.40 and loss3: 33687.39\n",
      "Epoch [3086], train_loss: 34169.32 with loss1: 427.61, loss2: 58.45 and loss3: 33683.26\n",
      "Epoch [3087], train_loss: 34166.80 with loss1: 429.24, loss2: 58.43 and loss3: 33679.12\n",
      "Epoch [3088], train_loss: 34162.34 with loss1: 428.90, loss2: 58.43 and loss3: 33675.00\n",
      "Epoch [3089], train_loss: 34157.28 with loss1: 428.10, loss2: 58.32 and loss3: 33670.86\n",
      "Epoch [3090], train_loss: 34156.04 with loss1: 430.99, loss2: 58.30 and loss3: 33666.74\n",
      "Epoch [3091], train_loss: 34151.78 with loss1: 430.82, loss2: 58.35 and loss3: 33662.60\n",
      "Epoch [3092], train_loss: 34148.11 with loss1: 431.38, loss2: 58.25 and loss3: 33658.48\n",
      "Epoch [3093], train_loss: 34144.15 with loss1: 431.39, loss2: 58.42 and loss3: 33654.34\n",
      "Epoch [3094], train_loss: 34142.03 with loss1: 433.43, loss2: 58.38 and loss3: 33650.22\n",
      "Epoch [3095], train_loss: 34137.26 with loss1: 432.79, loss2: 58.38 and loss3: 33646.08\n",
      "Epoch [3096], train_loss: 34134.48 with loss1: 434.16, loss2: 58.36 and loss3: 33641.96\n",
      "Epoch [3097], train_loss: 34130.65 with loss1: 434.65, loss2: 58.17 and loss3: 33637.83\n",
      "Epoch [3098], train_loss: 34129.16 with loss1: 437.36, loss2: 58.10 and loss3: 33633.70\n",
      "Epoch [3099], train_loss: 34124.30 with loss1: 436.37, loss2: 58.36 and loss3: 33629.57\n",
      "Epoch [3100], train_loss: 34122.67 with loss1: 439.09, loss2: 58.15 and loss3: 33625.44\n",
      "Epoch [3101], train_loss: 34116.55 with loss1: 437.11, loss2: 58.13 and loss3: 33621.31\n",
      "Epoch [3102], train_loss: 34116.23 with loss1: 440.86, loss2: 58.19 and loss3: 33617.18\n",
      "Epoch [3103], train_loss: 34111.00 with loss1: 439.97, loss2: 57.98 and loss3: 33613.05\n",
      "Epoch [3104], train_loss: 34111.14 with loss1: 444.15, loss2: 58.07 and loss3: 33608.93\n",
      "Epoch [3105], train_loss: 34107.55 with loss1: 444.65, loss2: 58.10 and loss3: 33604.80\n",
      "Epoch [3106], train_loss: 34105.20 with loss1: 446.40, loss2: 58.13 and loss3: 33600.67\n",
      "Epoch [3107], train_loss: 34100.33 with loss1: 445.72, loss2: 58.07 and loss3: 33596.54\n",
      "Epoch [3108], train_loss: 34099.18 with loss1: 448.67, loss2: 58.09 and loss3: 33592.42\n",
      "Epoch [3109], train_loss: 34095.46 with loss1: 449.22, loss2: 57.95 and loss3: 33588.29\n",
      "Epoch [3110], train_loss: 34093.91 with loss1: 451.51, loss2: 58.24 and loss3: 33584.16\n",
      "Epoch [3111], train_loss: 34088.95 with loss1: 451.00, loss2: 57.92 and loss3: 33580.03\n",
      "Epoch [3112], train_loss: 34089.50 with loss1: 455.61, loss2: 57.98 and loss3: 33575.91\n",
      "Epoch [3113], train_loss: 34082.47 with loss1: 452.95, loss2: 57.74 and loss3: 33571.77\n",
      "Epoch [3114], train_loss: 34081.47 with loss1: 456.05, loss2: 57.76 and loss3: 33567.66\n",
      "Epoch [3115], train_loss: 34075.91 with loss1: 454.58, loss2: 57.81 and loss3: 33563.52\n",
      "Epoch [3116], train_loss: 34077.28 with loss1: 460.08, loss2: 57.79 and loss3: 33559.41\n",
      "Epoch [3117], train_loss: 34070.11 with loss1: 456.90, loss2: 57.94 and loss3: 33555.27\n",
      "Epoch [3118], train_loss: 34070.03 with loss1: 461.06, loss2: 57.82 and loss3: 33551.16\n",
      "Epoch [3119], train_loss: 34064.09 with loss1: 459.09, loss2: 57.97 and loss3: 33547.02\n",
      "Epoch [3120], train_loss: 34064.00 with loss1: 463.02, loss2: 58.07 and loss3: 33542.91\n",
      "Epoch [3121], train_loss: 34057.73 with loss1: 461.08, loss2: 57.88 and loss3: 33538.77\n",
      "Epoch [3122], train_loss: 34059.12 with loss1: 466.62, loss2: 57.84 and loss3: 33534.66\n",
      "Epoch [3123], train_loss: 34050.94 with loss1: 462.71, loss2: 57.70 and loss3: 33530.52\n",
      "Epoch [3124], train_loss: 34052.39 with loss1: 468.30, loss2: 57.68 and loss3: 33526.41\n",
      "Epoch [3125], train_loss: 34043.88 with loss1: 463.51, loss2: 58.09 and loss3: 33522.28\n",
      "Epoch [3126], train_loss: 34042.33 with loss1: 466.57, loss2: 57.58 and loss3: 33518.17\n",
      "Epoch [3127], train_loss: 34033.64 with loss1: 461.89, loss2: 57.71 and loss3: 33514.04\n",
      "Epoch [3128], train_loss: 34035.59 with loss1: 467.94, loss2: 57.72 and loss3: 33509.93\n",
      "Epoch [3129], train_loss: 34024.82 with loss1: 461.27, loss2: 57.76 and loss3: 33505.80\n",
      "Epoch [3130], train_loss: 34023.59 with loss1: 464.20, loss2: 57.70 and loss3: 33501.68\n",
      "Epoch [3131], train_loss: 34015.23 with loss1: 459.93, loss2: 57.75 and loss3: 33497.55\n",
      "Epoch [3132], train_loss: 34014.03 with loss1: 463.08, loss2: 57.51 and loss3: 33493.44\n",
      "Epoch [3133], train_loss: 34002.08 with loss1: 455.22, loss2: 57.55 and loss3: 33489.32\n",
      "Epoch [3134], train_loss: 33999.38 with loss1: 456.60, loss2: 57.58 and loss3: 33485.20\n",
      "Epoch [3135], train_loss: 33988.65 with loss1: 449.97, loss2: 57.60 and loss3: 33481.08\n",
      "Epoch [3136], train_loss: 33986.63 with loss1: 452.06, loss2: 57.62 and loss3: 33476.96\n",
      "Epoch [3137], train_loss: 33976.64 with loss1: 446.32, loss2: 57.48 and loss3: 33472.84\n",
      "Epoch [3138], train_loss: 33973.28 with loss1: 447.05, loss2: 57.50 and loss3: 33468.72\n",
      "Epoch [3139], train_loss: 33964.91 with loss1: 442.70, loss2: 57.60 and loss3: 33464.60\n",
      "Epoch [3140], train_loss: 33961.04 with loss1: 443.03, loss2: 57.53 and loss3: 33460.48\n",
      "Epoch [3141], train_loss: 33952.27 with loss1: 438.61, loss2: 57.29 and loss3: 33456.36\n",
      "Epoch [3142], train_loss: 33948.95 with loss1: 439.24, loss2: 57.46 and loss3: 33452.25\n",
      "Epoch [3143], train_loss: 33940.21 with loss1: 434.75, loss2: 57.32 and loss3: 33448.13\n",
      "Epoch [3144], train_loss: 33936.72 with loss1: 435.53, loss2: 57.17 and loss3: 33444.02\n",
      "Epoch [3145], train_loss: 33930.04 with loss1: 432.67, loss2: 57.47 and loss3: 33439.90\n",
      "Epoch [3146], train_loss: 33926.10 with loss1: 432.96, loss2: 57.36 and loss3: 33435.79\n",
      "Epoch [3147], train_loss: 33918.71 with loss1: 429.70, loss2: 57.34 and loss3: 33431.67\n",
      "Epoch [3148], train_loss: 33914.28 with loss1: 429.41, loss2: 57.32 and loss3: 33427.55\n",
      "Epoch [3149], train_loss: 33908.36 with loss1: 427.56, loss2: 57.36 and loss3: 33423.44\n",
      "Epoch [3150], train_loss: 33903.73 with loss1: 427.33, loss2: 57.08 and loss3: 33419.33\n",
      "Epoch [3151], train_loss: 33897.56 with loss1: 424.95, loss2: 57.40 and loss3: 33415.21\n",
      "Epoch [3152], train_loss: 33894.19 with loss1: 425.90, loss2: 57.20 and loss3: 33411.10\n",
      "Epoch [3153], train_loss: 33888.75 with loss1: 424.62, loss2: 57.15 and loss3: 33406.98\n",
      "Epoch [3154], train_loss: 33884.21 with loss1: 424.22, loss2: 57.12 and loss3: 33402.87\n",
      "Epoch [3155], train_loss: 33879.06 with loss1: 423.23, loss2: 57.08 and loss3: 33398.75\n",
      "Epoch [3156], train_loss: 33875.22 with loss1: 423.56, loss2: 57.02 and loss3: 33394.64\n",
      "Epoch [3157], train_loss: 33868.88 with loss1: 421.30, loss2: 57.05 and loss3: 33390.53\n",
      "Epoch [3158], train_loss: 33864.86 with loss1: 421.22, loss2: 57.22 and loss3: 33386.41\n",
      "Epoch [3159], train_loss: 33862.44 with loss1: 422.91, loss2: 57.22 and loss3: 33382.31\n",
      "Epoch [3160], train_loss: 33857.18 with loss1: 421.98, loss2: 57.01 and loss3: 33378.19\n",
      "Epoch [3161], train_loss: 33852.73 with loss1: 421.60, loss2: 57.05 and loss3: 33374.09\n",
      "Epoch [3162], train_loss: 33850.43 with loss1: 423.51, loss2: 56.95 and loss3: 33369.96\n",
      "Epoch [3163], train_loss: 33843.79 with loss1: 420.84, loss2: 57.08 and loss3: 33365.87\n",
      "Epoch [3164], train_loss: 33841.87 with loss1: 423.12, loss2: 57.00 and loss3: 33361.74\n",
      "Epoch [3165], train_loss: 33837.92 with loss1: 423.39, loss2: 56.88 and loss3: 33357.65\n",
      "Epoch [3166], train_loss: 33833.63 with loss1: 422.91, loss2: 57.19 and loss3: 33353.53\n",
      "Epoch [3167], train_loss: 33828.32 with loss1: 422.08, loss2: 56.83 and loss3: 33349.42\n",
      "Epoch [3168], train_loss: 33824.94 with loss1: 422.77, loss2: 56.86 and loss3: 33345.31\n",
      "Epoch [3169], train_loss: 33822.22 with loss1: 424.11, loss2: 56.90 and loss3: 33341.21\n",
      "Epoch [3170], train_loss: 33819.60 with loss1: 425.55, loss2: 56.96 and loss3: 33337.09\n",
      "Epoch [3171], train_loss: 33815.13 with loss1: 425.18, loss2: 56.96 and loss3: 33332.99\n",
      "Epoch [3172], train_loss: 33810.90 with loss1: 425.01, loss2: 57.01 and loss3: 33328.88\n",
      "Epoch [3173], train_loss: 33807.59 with loss1: 426.23, loss2: 56.59 and loss3: 33324.77\n",
      "Epoch [3174], train_loss: 33806.83 with loss1: 429.19, loss2: 56.98 and loss3: 33320.67\n",
      "Epoch [3175], train_loss: 33802.91 with loss1: 429.60, loss2: 56.75 and loss3: 33316.56\n",
      "Epoch [3176], train_loss: 33800.12 with loss1: 431.03, loss2: 56.64 and loss3: 33312.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3177], train_loss: 33795.52 with loss1: 430.48, loss2: 56.68 and loss3: 33308.36\n",
      "Epoch [3178], train_loss: 33795.10 with loss1: 434.07, loss2: 56.79 and loss3: 33304.24\n",
      "Epoch [3179], train_loss: 33790.82 with loss1: 433.85, loss2: 56.83 and loss3: 33300.14\n",
      "Epoch [3180], train_loss: 33789.00 with loss1: 436.05, loss2: 56.91 and loss3: 33296.04\n",
      "Epoch [3181], train_loss: 33784.83 with loss1: 436.46, loss2: 56.44 and loss3: 33291.93\n",
      "Epoch [3182], train_loss: 33782.75 with loss1: 438.44, loss2: 56.49 and loss3: 33287.82\n",
      "Epoch [3183], train_loss: 33779.27 with loss1: 438.93, loss2: 56.61 and loss3: 33283.73\n",
      "Epoch [3184], train_loss: 33777.93 with loss1: 441.79, loss2: 56.53 and loss3: 33279.61\n",
      "Epoch [3185], train_loss: 33774.49 with loss1: 442.41, loss2: 56.56 and loss3: 33275.52\n",
      "Epoch [3186], train_loss: 33773.27 with loss1: 445.26, loss2: 56.60 and loss3: 33271.41\n",
      "Epoch [3187], train_loss: 33769.07 with loss1: 445.30, loss2: 56.46 and loss3: 33267.31\n",
      "Epoch [3188], train_loss: 33768.77 with loss1: 449.06, loss2: 56.51 and loss3: 33263.20\n",
      "Epoch [3189], train_loss: 33765.49 with loss1: 449.90, loss2: 56.49 and loss3: 33259.10\n",
      "Epoch [3190], train_loss: 33764.92 with loss1: 453.37, loss2: 56.55 and loss3: 33255.00\n",
      "Epoch [3191], train_loss: 33759.96 with loss1: 452.59, loss2: 56.47 and loss3: 33250.90\n",
      "Epoch [3192], train_loss: 33758.79 with loss1: 455.38, loss2: 56.63 and loss3: 33246.78\n",
      "Epoch [3193], train_loss: 33752.37 with loss1: 453.32, loss2: 56.35 and loss3: 33242.70\n",
      "Epoch [3194], train_loss: 33752.93 with loss1: 457.98, loss2: 56.38 and loss3: 33238.58\n",
      "Epoch [3195], train_loss: 33746.02 with loss1: 455.30, loss2: 56.22 and loss3: 33234.49\n",
      "Epoch [3196], train_loss: 33744.78 with loss1: 458.00, loss2: 56.41 and loss3: 33230.37\n",
      "Epoch [3197], train_loss: 33737.87 with loss1: 455.40, loss2: 56.17 and loss3: 33226.29\n",
      "Epoch [3198], train_loss: 33738.16 with loss1: 459.38, loss2: 56.61 and loss3: 33222.17\n",
      "Epoch [3199], train_loss: 33730.24 with loss1: 455.87, loss2: 56.29 and loss3: 33218.09\n",
      "Epoch [3200], train_loss: 33729.94 with loss1: 459.49, loss2: 56.48 and loss3: 33213.96\n",
      "Epoch [3201], train_loss: 33723.80 with loss1: 457.72, loss2: 56.19 and loss3: 33209.89\n",
      "Epoch [3202], train_loss: 33722.64 with loss1: 460.58, loss2: 56.30 and loss3: 33205.76\n",
      "Epoch [3203], train_loss: 33715.95 with loss1: 457.94, loss2: 56.32 and loss3: 33201.69\n",
      "Epoch [3204], train_loss: 33714.73 with loss1: 460.92, loss2: 56.24 and loss3: 33197.56\n",
      "Epoch [3205], train_loss: 33705.84 with loss1: 456.29, loss2: 56.06 and loss3: 33193.49\n",
      "Epoch [3206], train_loss: 33704.83 with loss1: 459.22, loss2: 56.25 and loss3: 33189.37\n",
      "Epoch [3207], train_loss: 33698.07 with loss1: 456.80, loss2: 55.97 and loss3: 33185.30\n",
      "Epoch [3208], train_loss: 33698.00 with loss1: 460.42, loss2: 56.41 and loss3: 33181.17\n",
      "Epoch [3209], train_loss: 33690.46 with loss1: 457.12, loss2: 56.24 and loss3: 33177.10\n",
      "Epoch [3210], train_loss: 33688.12 with loss1: 458.95, loss2: 56.19 and loss3: 33172.98\n",
      "Epoch [3211], train_loss: 33681.05 with loss1: 455.95, loss2: 56.20 and loss3: 33168.90\n",
      "Epoch [3212], train_loss: 33678.36 with loss1: 457.44, loss2: 56.13 and loss3: 33164.79\n",
      "Epoch [3213], train_loss: 33671.34 with loss1: 454.52, loss2: 56.12 and loss3: 33160.71\n",
      "Epoch [3214], train_loss: 33669.15 with loss1: 456.42, loss2: 56.12 and loss3: 33156.61\n",
      "Epoch [3215], train_loss: 33660.16 with loss1: 451.59, loss2: 56.05 and loss3: 33152.52\n",
      "Epoch [3216], train_loss: 33658.53 with loss1: 454.16, loss2: 55.96 and loss3: 33148.42\n",
      "Epoch [3217], train_loss: 33650.84 with loss1: 450.63, loss2: 55.88 and loss3: 33144.33\n",
      "Epoch [3218], train_loss: 33648.38 with loss1: 451.90, loss2: 56.24 and loss3: 33140.23\n",
      "Epoch [3219], train_loss: 33638.85 with loss1: 446.77, loss2: 55.93 and loss3: 33136.14\n",
      "Epoch [3220], train_loss: 33636.39 with loss1: 448.25, loss2: 56.10 and loss3: 33132.05\n",
      "Epoch [3221], train_loss: 33628.25 with loss1: 444.12, loss2: 56.17 and loss3: 33127.95\n",
      "Epoch [3222], train_loss: 33625.52 with loss1: 445.50, loss2: 56.16 and loss3: 33123.87\n",
      "Epoch [3223], train_loss: 33615.81 with loss1: 440.13, loss2: 55.91 and loss3: 33119.77\n",
      "Epoch [3224], train_loss: 33614.46 with loss1: 442.77, loss2: 55.99 and loss3: 33115.69\n",
      "Epoch [3225], train_loss: 33605.62 with loss1: 438.24, loss2: 55.80 and loss3: 33111.59\n",
      "Epoch [3226], train_loss: 33602.97 with loss1: 439.46, loss2: 56.01 and loss3: 33107.50\n",
      "Epoch [3227], train_loss: 33596.39 with loss1: 437.14, loss2: 55.84 and loss3: 33103.41\n",
      "Epoch [3228], train_loss: 33592.47 with loss1: 437.17, loss2: 55.98 and loss3: 33099.33\n",
      "Epoch [3229], train_loss: 33583.50 with loss1: 432.56, loss2: 55.72 and loss3: 33095.23\n",
      "Epoch [3230], train_loss: 33581.84 with loss1: 434.79, loss2: 55.91 and loss3: 33091.14\n",
      "Epoch [3231], train_loss: 33573.25 with loss1: 430.28, loss2: 55.92 and loss3: 33087.05\n",
      "Epoch [3232], train_loss: 33571.73 with loss1: 433.13, loss2: 55.64 and loss3: 33082.96\n",
      "Epoch [3233], train_loss: 33563.51 with loss1: 428.89, loss2: 55.75 and loss3: 33078.87\n",
      "Epoch [3234], train_loss: 33559.09 with loss1: 428.25, loss2: 56.07 and loss3: 33074.78\n",
      "Epoch [3235], train_loss: 33551.77 with loss1: 425.26, loss2: 55.81 and loss3: 33070.70\n",
      "Epoch [3236], train_loss: 33548.36 with loss1: 425.89, loss2: 55.87 and loss3: 33066.60\n",
      "Epoch [3237], train_loss: 33541.23 with loss1: 422.79, loss2: 55.92 and loss3: 33062.52\n",
      "Epoch [3238], train_loss: 33538.27 with loss1: 424.04, loss2: 55.80 and loss3: 33058.43\n",
      "Epoch [3239], train_loss: 33531.82 with loss1: 421.84, loss2: 55.63 and loss3: 33054.35\n",
      "Epoch [3240], train_loss: 33527.76 with loss1: 421.79, loss2: 55.72 and loss3: 33050.26\n",
      "Epoch [3241], train_loss: 33522.00 with loss1: 420.22, loss2: 55.60 and loss3: 33046.18\n",
      "Epoch [3242], train_loss: 33517.90 with loss1: 420.14, loss2: 55.68 and loss3: 33042.09\n",
      "Epoch [3243], train_loss: 33511.45 with loss1: 417.68, loss2: 55.76 and loss3: 33038.00\n",
      "Epoch [3244], train_loss: 33507.80 with loss1: 418.35, loss2: 55.54 and loss3: 33033.91\n",
      "Epoch [3245], train_loss: 33501.51 with loss1: 416.19, loss2: 55.49 and loss3: 33029.83\n",
      "Epoch [3246], train_loss: 33498.84 with loss1: 417.44, loss2: 55.65 and loss3: 33025.75\n",
      "Epoch [3247], train_loss: 33492.18 with loss1: 415.10, loss2: 55.42 and loss3: 33021.66\n",
      "Epoch [3248], train_loss: 33488.41 with loss1: 415.33, loss2: 55.50 and loss3: 33017.57\n",
      "Epoch [3249], train_loss: 33483.47 with loss1: 414.49, loss2: 55.49 and loss3: 33013.50\n",
      "Epoch [3250], train_loss: 33478.77 with loss1: 413.83, loss2: 55.54 and loss3: 33009.41\n",
      "Epoch [3251], train_loss: 33473.19 with loss1: 412.58, loss2: 55.28 and loss3: 33005.33\n",
      "Epoch [3252], train_loss: 33470.27 with loss1: 413.54, loss2: 55.49 and loss3: 33001.24\n",
      "Epoch [3253], train_loss: 33465.16 with loss1: 412.65, loss2: 55.34 and loss3: 32997.16\n",
      "Epoch [3254], train_loss: 33461.80 with loss1: 413.42, loss2: 55.31 and loss3: 32993.07\n",
      "Epoch [3255], train_loss: 33457.07 with loss1: 412.75, loss2: 55.33 and loss3: 32989.00\n",
      "Epoch [3256], train_loss: 33453.03 with loss1: 412.80, loss2: 55.32 and loss3: 32984.91\n",
      "Epoch [3257], train_loss: 33448.02 with loss1: 412.04, loss2: 55.15 and loss3: 32980.83\n",
      "Epoch [3258], train_loss: 33444.26 with loss1: 412.12, loss2: 55.38 and loss3: 32976.75\n",
      "Epoch [3259], train_loss: 33439.10 with loss1: 411.40, loss2: 55.04 and loss3: 32972.66\n",
      "Epoch [3260], train_loss: 33435.07 with loss1: 411.31, loss2: 55.17 and loss3: 32968.59\n",
      "Epoch [3261], train_loss: 33430.99 with loss1: 411.21, loss2: 55.27 and loss3: 32964.51\n",
      "Epoch [3262], train_loss: 33426.32 with loss1: 410.42, loss2: 55.46 and loss3: 32960.43\n",
      "Epoch [3263], train_loss: 33421.84 with loss1: 410.42, loss2: 55.07 and loss3: 32956.35\n",
      "Epoch [3264], train_loss: 33417.70 with loss1: 410.01, loss2: 55.41 and loss3: 32952.27\n",
      "Epoch [3265], train_loss: 33412.74 with loss1: 409.36, loss2: 55.19 and loss3: 32948.20\n",
      "Epoch [3266], train_loss: 33409.12 with loss1: 409.96, loss2: 55.05 and loss3: 32944.11\n",
      "Epoch [3267], train_loss: 33405.96 with loss1: 410.86, loss2: 55.06 and loss3: 32940.04\n",
      "Epoch [3268], train_loss: 33401.68 with loss1: 410.64, loss2: 55.09 and loss3: 32935.95\n",
      "Epoch [3269], train_loss: 33396.56 with loss1: 409.86, loss2: 54.81 and loss3: 32931.88\n",
      "Epoch [3270], train_loss: 33395.57 with loss1: 412.67, loss2: 55.10 and loss3: 32927.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3271], train_loss: 33388.74 with loss1: 410.02, loss2: 54.99 and loss3: 32923.73\n",
      "Epoch [3272], train_loss: 33385.40 with loss1: 410.76, loss2: 55.00 and loss3: 32919.64\n",
      "Epoch [3273], train_loss: 33381.47 with loss1: 410.88, loss2: 55.01 and loss3: 32915.58\n",
      "Epoch [3274], train_loss: 33378.79 with loss1: 412.02, loss2: 55.28 and loss3: 32911.48\n",
      "Epoch [3275], train_loss: 33372.88 with loss1: 410.43, loss2: 55.02 and loss3: 32907.42\n",
      "Epoch [3276], train_loss: 33369.93 with loss1: 411.57, loss2: 55.02 and loss3: 32903.33\n",
      "Epoch [3277], train_loss: 33364.76 with loss1: 410.54, loss2: 54.95 and loss3: 32899.27\n",
      "Epoch [3278], train_loss: 33361.72 with loss1: 411.60, loss2: 54.94 and loss3: 32895.18\n",
      "Epoch [3279], train_loss: 33357.17 with loss1: 411.32, loss2: 54.74 and loss3: 32891.11\n",
      "Epoch [3280], train_loss: 33355.20 with loss1: 413.23, loss2: 54.95 and loss3: 32887.02\n",
      "Epoch [3281], train_loss: 33350.96 with loss1: 413.04, loss2: 54.98 and loss3: 32882.95\n",
      "Epoch [3282], train_loss: 33347.86 with loss1: 414.18, loss2: 54.81 and loss3: 32878.87\n",
      "Epoch [3283], train_loss: 33343.78 with loss1: 414.22, loss2: 54.75 and loss3: 32874.80\n",
      "Epoch [3284], train_loss: 33340.97 with loss1: 415.49, loss2: 54.76 and loss3: 32870.72\n",
      "Epoch [3285], train_loss: 33336.69 with loss1: 415.42, loss2: 54.62 and loss3: 32866.65\n",
      "Epoch [3286], train_loss: 33331.71 with loss1: 414.51, loss2: 54.64 and loss3: 32862.57\n",
      "Epoch [3287], train_loss: 33328.62 with loss1: 415.43, loss2: 54.70 and loss3: 32858.50\n",
      "Epoch [3288], train_loss: 33326.62 with loss1: 417.33, loss2: 54.88 and loss3: 32854.42\n",
      "Epoch [3289], train_loss: 33321.97 with loss1: 417.01, loss2: 54.62 and loss3: 32850.35\n",
      "Epoch [3290], train_loss: 33321.11 with loss1: 420.03, loss2: 54.81 and loss3: 32846.27\n",
      "Epoch [3291], train_loss: 33315.24 with loss1: 418.48, loss2: 54.56 and loss3: 32842.20\n",
      "Epoch [3292], train_loss: 33313.33 with loss1: 420.53, loss2: 54.69 and loss3: 32838.11\n",
      "Epoch [3293], train_loss: 33309.38 with loss1: 420.84, loss2: 54.48 and loss3: 32834.05\n",
      "Epoch [3294], train_loss: 33307.48 with loss1: 422.86, loss2: 54.66 and loss3: 32829.96\n",
      "Epoch [3295], train_loss: 33301.85 with loss1: 421.56, loss2: 54.38 and loss3: 32825.91\n",
      "Epoch [3296], train_loss: 33301.32 with loss1: 424.89, loss2: 54.61 and loss3: 32821.82\n",
      "Epoch [3297], train_loss: 33296.76 with loss1: 424.51, loss2: 54.49 and loss3: 32817.77\n",
      "Epoch [3298], train_loss: 33296.24 with loss1: 428.08, loss2: 54.49 and loss3: 32813.67\n",
      "Epoch [3299], train_loss: 33291.04 with loss1: 427.10, loss2: 54.32 and loss3: 32809.62\n",
      "Epoch [3300], train_loss: 33289.57 with loss1: 429.47, loss2: 54.58 and loss3: 32805.52\n",
      "Epoch [3301], train_loss: 33285.18 with loss1: 429.33, loss2: 54.38 and loss3: 32801.48\n",
      "Epoch [3302], train_loss: 33285.62 with loss1: 433.46, loss2: 54.77 and loss3: 32797.38\n",
      "Epoch [3303], train_loss: 33278.55 with loss1: 430.82, loss2: 54.40 and loss3: 32793.33\n",
      "Epoch [3304], train_loss: 33279.66 with loss1: 435.90, loss2: 54.52 and loss3: 32789.24\n",
      "Epoch [3305], train_loss: 33272.05 with loss1: 432.43, loss2: 54.43 and loss3: 32785.19\n",
      "Epoch [3306], train_loss: 33272.41 with loss1: 436.68, loss2: 54.63 and loss3: 32781.10\n",
      "Epoch [3307], train_loss: 33264.85 with loss1: 433.39, loss2: 54.42 and loss3: 32777.04\n",
      "Epoch [3308], train_loss: 33264.44 with loss1: 436.94, loss2: 54.54 and loss3: 32772.96\n",
      "Epoch [3309], train_loss: 33259.05 with loss1: 435.79, loss2: 54.35 and loss3: 32768.90\n",
      "Epoch [3310], train_loss: 33258.45 with loss1: 439.22, loss2: 54.41 and loss3: 32764.82\n",
      "Epoch [3311], train_loss: 33251.10 with loss1: 436.19, loss2: 54.14 and loss3: 32760.77\n",
      "Epoch [3312], train_loss: 33249.34 with loss1: 438.48, loss2: 54.18 and loss3: 32756.68\n",
      "Epoch [3313], train_loss: 33244.08 with loss1: 437.35, loss2: 54.09 and loss3: 32752.63\n",
      "Epoch [3314], train_loss: 33242.33 with loss1: 439.36, loss2: 54.43 and loss3: 32748.54\n",
      "Epoch [3315], train_loss: 33235.72 with loss1: 437.14, loss2: 54.09 and loss3: 32744.49\n",
      "Epoch [3316], train_loss: 33234.00 with loss1: 439.31, loss2: 54.28 and loss3: 32740.41\n",
      "Epoch [3317], train_loss: 33227.08 with loss1: 436.49, loss2: 54.23 and loss3: 32736.36\n",
      "Epoch [3318], train_loss: 33225.83 with loss1: 439.28, loss2: 54.27 and loss3: 32732.28\n",
      "Epoch [3319], train_loss: 33218.11 with loss1: 435.78, loss2: 54.10 and loss3: 32728.22\n",
      "Epoch [3320], train_loss: 33216.98 with loss1: 438.69, loss2: 54.14 and loss3: 32724.15\n",
      "Epoch [3321], train_loss: 33208.20 with loss1: 434.20, loss2: 53.90 and loss3: 32720.09\n",
      "Epoch [3322], train_loss: 33208.02 with loss1: 437.77, loss2: 54.23 and loss3: 32716.03\n",
      "Epoch [3323], train_loss: 33200.68 with loss1: 434.80, loss2: 53.92 and loss3: 32711.96\n",
      "Epoch [3324], train_loss: 33199.07 with loss1: 437.02, loss2: 54.15 and loss3: 32707.90\n",
      "Epoch [3325], train_loss: 33192.56 with loss1: 434.96, loss2: 53.77 and loss3: 32703.83\n",
      "Epoch [3326], train_loss: 33191.21 with loss1: 437.33, loss2: 54.11 and loss3: 32699.77\n",
      "Epoch [3327], train_loss: 33183.04 with loss1: 433.52, loss2: 53.82 and loss3: 32695.71\n",
      "Epoch [3328], train_loss: 33181.01 with loss1: 435.25, loss2: 54.11 and loss3: 32691.65\n",
      "Epoch [3329], train_loss: 33172.89 with loss1: 431.35, loss2: 53.96 and loss3: 32687.58\n",
      "Epoch [3330], train_loss: 33170.52 with loss1: 432.97, loss2: 54.03 and loss3: 32683.52\n",
      "Epoch [3331], train_loss: 33163.68 with loss1: 430.19, loss2: 54.03 and loss3: 32679.46\n",
      "Epoch [3332], train_loss: 33160.68 with loss1: 431.32, loss2: 53.96 and loss3: 32675.39\n",
      "Epoch [3333], train_loss: 33154.04 with loss1: 428.87, loss2: 53.84 and loss3: 32671.34\n",
      "Epoch [3334], train_loss: 33152.36 with loss1: 431.07, loss2: 54.02 and loss3: 32667.26\n",
      "Epoch [3335], train_loss: 33144.06 with loss1: 427.10, loss2: 53.74 and loss3: 32663.21\n",
      "Epoch [3336], train_loss: 33141.03 with loss1: 427.83, loss2: 54.06 and loss3: 32659.14\n",
      "Epoch [3337], train_loss: 33134.00 with loss1: 425.21, loss2: 53.69 and loss3: 32655.09\n",
      "Epoch [3338], train_loss: 33131.71 with loss1: 426.55, loss2: 54.14 and loss3: 32651.02\n",
      "Epoch [3339], train_loss: 33125.79 with loss1: 425.16, loss2: 53.66 and loss3: 32646.97\n",
      "Epoch [3340], train_loss: 33121.37 with loss1: 424.58, loss2: 53.89 and loss3: 32642.89\n",
      "Epoch [3341], train_loss: 33114.13 with loss1: 421.52, loss2: 53.76 and loss3: 32638.85\n",
      "Epoch [3342], train_loss: 33111.73 with loss1: 423.22, loss2: 53.74 and loss3: 32634.77\n",
      "Epoch [3343], train_loss: 33104.53 with loss1: 420.21, loss2: 53.59 and loss3: 32630.73\n",
      "Epoch [3344], train_loss: 33101.37 with loss1: 420.95, loss2: 53.77 and loss3: 32626.65\n",
      "Epoch [3345], train_loss: 33093.04 with loss1: 416.98, loss2: 53.45 and loss3: 32622.61\n",
      "Epoch [3346], train_loss: 33090.57 with loss1: 418.28, loss2: 53.75 and loss3: 32618.54\n",
      "Epoch [3347], train_loss: 33084.54 with loss1: 416.28, loss2: 53.76 and loss3: 32614.49\n",
      "Epoch [3348], train_loss: 33080.62 with loss1: 416.37, loss2: 53.83 and loss3: 32610.42\n",
      "Epoch [3349], train_loss: 33075.48 with loss1: 415.68, loss2: 53.43 and loss3: 32606.37\n",
      "Epoch [3350], train_loss: 33072.88 with loss1: 416.75, loss2: 53.81 and loss3: 32602.31\n",
      "Epoch [3351], train_loss: 33065.11 with loss1: 413.45, loss2: 53.41 and loss3: 32598.25\n",
      "Epoch [3352], train_loss: 33063.35 with loss1: 415.53, loss2: 53.61 and loss3: 32594.20\n",
      "Epoch [3353], train_loss: 33057.75 with loss1: 414.07, loss2: 53.55 and loss3: 32590.14\n",
      "Epoch [3354], train_loss: 33052.37 with loss1: 412.54, loss2: 53.74 and loss3: 32586.09\n",
      "Epoch [3355], train_loss: 33049.15 with loss1: 413.73, loss2: 53.40 and loss3: 32582.03\n",
      "Epoch [3356], train_loss: 33044.02 with loss1: 412.54, loss2: 53.50 and loss3: 32577.98\n",
      "Epoch [3357], train_loss: 33038.71 with loss1: 411.48, loss2: 53.31 and loss3: 32573.92\n",
      "Epoch [3358], train_loss: 33035.79 with loss1: 412.50, loss2: 53.42 and loss3: 32569.86\n",
      "Epoch [3359], train_loss: 33030.89 with loss1: 411.79, loss2: 53.28 and loss3: 32565.82\n",
      "Epoch [3360], train_loss: 33026.86 with loss1: 411.48, loss2: 53.61 and loss3: 32561.76\n",
      "Epoch [3361], train_loss: 33021.23 with loss1: 410.19, loss2: 53.34 and loss3: 32557.71\n",
      "Epoch [3362], train_loss: 33018.11 with loss1: 411.12, loss2: 53.34 and loss3: 32553.65\n",
      "Epoch [3363], train_loss: 33013.09 with loss1: 410.27, loss2: 53.22 and loss3: 32549.60\n",
      "Epoch [3364], train_loss: 33011.50 with loss1: 412.58, loss2: 53.37 and loss3: 32545.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3365], train_loss: 33004.15 with loss1: 409.51, loss2: 53.14 and loss3: 32541.50\n",
      "Epoch [3366], train_loss: 33000.91 with loss1: 410.06, loss2: 53.42 and loss3: 32537.44\n",
      "Epoch [3367], train_loss: 32996.02 with loss1: 409.41, loss2: 53.22 and loss3: 32533.39\n",
      "Epoch [3368], train_loss: 32993.51 with loss1: 410.86, loss2: 53.30 and loss3: 32529.35\n",
      "Epoch [3369], train_loss: 32988.76 with loss1: 410.38, loss2: 53.09 and loss3: 32525.29\n",
      "Epoch [3370], train_loss: 32985.94 with loss1: 411.46, loss2: 53.24 and loss3: 32521.24\n",
      "Epoch [3371], train_loss: 32981.07 with loss1: 410.56, loss2: 53.32 and loss3: 32517.19\n",
      "Epoch [3372], train_loss: 32978.89 with loss1: 412.59, loss2: 53.16 and loss3: 32513.14\n",
      "Epoch [3373], train_loss: 32973.69 with loss1: 411.63, loss2: 52.98 and loss3: 32509.08\n",
      "Epoch [3374], train_loss: 32970.37 with loss1: 412.15, loss2: 53.18 and loss3: 32505.04\n",
      "Epoch [3375], train_loss: 32966.29 with loss1: 412.25, loss2: 53.05 and loss3: 32500.98\n",
      "Epoch [3376], train_loss: 32963.73 with loss1: 413.50, loss2: 53.30 and loss3: 32496.94\n",
      "Epoch [3377], train_loss: 32957.64 with loss1: 411.74, loss2: 53.01 and loss3: 32492.89\n",
      "Epoch [3378], train_loss: 32955.68 with loss1: 413.73, loss2: 53.10 and loss3: 32488.84\n",
      "Epoch [3379], train_loss: 32950.85 with loss1: 413.04, loss2: 53.02 and loss3: 32484.79\n",
      "Epoch [3380], train_loss: 32949.12 with loss1: 415.09, loss2: 53.28 and loss3: 32480.75\n",
      "Epoch [3381], train_loss: 32944.52 with loss1: 414.88, loss2: 52.94 and loss3: 32476.70\n",
      "Epoch [3382], train_loss: 32941.93 with loss1: 416.20, loss2: 53.07 and loss3: 32472.65\n",
      "Epoch [3383], train_loss: 32937.33 with loss1: 415.81, loss2: 52.92 and loss3: 32468.61\n",
      "Epoch [3384], train_loss: 32934.97 with loss1: 417.41, loss2: 53.00 and loss3: 32464.56\n",
      "Epoch [3385], train_loss: 32930.59 with loss1: 417.06, loss2: 53.01 and loss3: 32460.52\n",
      "Epoch [3386], train_loss: 32928.68 with loss1: 419.28, loss2: 52.93 and loss3: 32456.46\n",
      "Epoch [3387], train_loss: 32923.71 with loss1: 418.33, loss2: 52.96 and loss3: 32452.42\n",
      "Epoch [3388], train_loss: 32921.45 with loss1: 419.96, loss2: 53.11 and loss3: 32448.38\n",
      "Epoch [3389], train_loss: 32916.44 with loss1: 419.10, loss2: 53.01 and loss3: 32444.33\n",
      "Epoch [3390], train_loss: 32913.22 with loss1: 419.96, loss2: 52.97 and loss3: 32440.29\n",
      "Epoch [3391], train_loss: 32906.98 with loss1: 417.90, loss2: 52.84 and loss3: 32436.23\n",
      "Epoch [3392], train_loss: 32904.59 with loss1: 419.54, loss2: 52.85 and loss3: 32432.20\n",
      "Epoch [3393], train_loss: 32898.29 with loss1: 417.03, loss2: 53.11 and loss3: 32428.14\n",
      "Epoch [3394], train_loss: 32895.75 with loss1: 418.79, loss2: 52.86 and loss3: 32424.11\n",
      "Epoch [3395], train_loss: 32889.43 with loss1: 416.57, loss2: 52.80 and loss3: 32420.06\n",
      "Epoch [3396], train_loss: 32885.16 with loss1: 416.37, loss2: 52.77 and loss3: 32416.02\n",
      "Epoch [3397], train_loss: 32880.27 with loss1: 415.43, loss2: 52.87 and loss3: 32411.97\n",
      "Epoch [3398], train_loss: 32876.21 with loss1: 415.55, loss2: 52.74 and loss3: 32407.93\n",
      "Epoch [3399], train_loss: 32870.44 with loss1: 413.73, loss2: 52.82 and loss3: 32403.89\n",
      "Epoch [3400], train_loss: 32866.85 with loss1: 414.21, loss2: 52.80 and loss3: 32399.84\n",
      "Epoch [3401], train_loss: 32860.40 with loss1: 411.91, loss2: 52.69 and loss3: 32395.80\n",
      "Epoch [3402], train_loss: 32857.89 with loss1: 413.35, loss2: 52.79 and loss3: 32391.76\n",
      "Epoch [3403], train_loss: 32851.19 with loss1: 410.75, loss2: 52.72 and loss3: 32387.71\n",
      "Epoch [3404], train_loss: 32847.12 with loss1: 410.58, loss2: 52.87 and loss3: 32383.67\n",
      "Epoch [3405], train_loss: 32841.92 with loss1: 409.44, loss2: 52.85 and loss3: 32379.63\n",
      "Epoch [3406], train_loss: 32837.14 with loss1: 409.06, loss2: 52.49 and loss3: 32375.59\n",
      "Epoch [3407], train_loss: 32830.70 with loss1: 406.48, loss2: 52.67 and loss3: 32371.54\n",
      "Epoch [3408], train_loss: 32829.10 with loss1: 408.96, loss2: 52.64 and loss3: 32367.50\n",
      "Epoch [3409], train_loss: 32823.04 with loss1: 407.01, loss2: 52.57 and loss3: 32363.46\n",
      "Epoch [3410], train_loss: 32819.35 with loss1: 407.40, loss2: 52.53 and loss3: 32359.42\n",
      "Epoch [3411], train_loss: 32814.12 with loss1: 406.30, loss2: 52.45 and loss3: 32355.37\n",
      "Epoch [3412], train_loss: 32811.22 with loss1: 407.44, loss2: 52.44 and loss3: 32351.34\n",
      "Epoch [3413], train_loss: 32804.18 with loss1: 404.23, loss2: 52.66 and loss3: 32347.29\n",
      "Epoch [3414], train_loss: 32801.75 with loss1: 405.95, loss2: 52.53 and loss3: 32343.26\n",
      "Epoch [3415], train_loss: 32796.06 with loss1: 404.22, loss2: 52.63 and loss3: 32339.21\n",
      "Epoch [3416], train_loss: 32793.20 with loss1: 405.67, loss2: 52.36 and loss3: 32335.18\n",
      "Epoch [3417], train_loss: 32787.77 with loss1: 404.31, loss2: 52.33 and loss3: 32331.12\n",
      "Epoch [3418], train_loss: 32784.50 with loss1: 405.10, loss2: 52.30 and loss3: 32327.10\n",
      "Epoch [3419], train_loss: 32778.90 with loss1: 403.52, loss2: 52.33 and loss3: 32323.04\n",
      "Epoch [3420], train_loss: 32776.50 with loss1: 404.99, loss2: 52.50 and loss3: 32319.02\n",
      "Epoch [3421], train_loss: 32771.32 with loss1: 403.95, loss2: 52.41 and loss3: 32314.96\n",
      "Epoch [3422], train_loss: 32768.34 with loss1: 405.05, loss2: 52.36 and loss3: 32310.93\n",
      "Epoch [3423], train_loss: 32763.29 with loss1: 404.07, loss2: 52.34 and loss3: 32306.88\n",
      "Epoch [3424], train_loss: 32759.66 with loss1: 404.59, loss2: 52.22 and loss3: 32302.85\n",
      "Epoch [3425], train_loss: 32756.96 with loss1: 406.00, loss2: 52.15 and loss3: 32298.80\n",
      "Epoch [3426], train_loss: 32752.22 with loss1: 405.12, loss2: 52.33 and loss3: 32294.77\n",
      "Epoch [3427], train_loss: 32748.81 with loss1: 405.66, loss2: 52.42 and loss3: 32290.72\n",
      "Epoch [3428], train_loss: 32746.59 with loss1: 407.75, loss2: 52.15 and loss3: 32286.69\n",
      "Epoch [3429], train_loss: 32740.46 with loss1: 405.51, loss2: 52.30 and loss3: 32282.64\n",
      "Epoch [3430], train_loss: 32737.98 with loss1: 407.20, loss2: 52.17 and loss3: 32278.61\n",
      "Epoch [3431], train_loss: 32733.10 with loss1: 406.22, loss2: 52.31 and loss3: 32274.57\n",
      "Epoch [3432], train_loss: 32733.53 with loss1: 410.78, loss2: 52.22 and loss3: 32270.54\n",
      "Epoch [3433], train_loss: 32728.84 with loss1: 410.35, loss2: 51.99 and loss3: 32266.49\n",
      "Epoch [3434], train_loss: 32727.50 with loss1: 412.72, loss2: 52.32 and loss3: 32262.46\n",
      "Epoch [3435], train_loss: 32722.57 with loss1: 412.02, loss2: 52.14 and loss3: 32258.42\n",
      "Epoch [3436], train_loss: 32720.10 with loss1: 413.67, loss2: 52.05 and loss3: 32254.38\n",
      "Epoch [3437], train_loss: 32715.16 with loss1: 412.61, loss2: 52.20 and loss3: 32250.35\n",
      "Epoch [3438], train_loss: 32714.60 with loss1: 416.06, loss2: 52.24 and loss3: 32246.31\n",
      "Epoch [3439], train_loss: 32709.34 with loss1: 415.04, loss2: 52.03 and loss3: 32242.27\n",
      "Epoch [3440], train_loss: 32707.78 with loss1: 417.56, loss2: 51.98 and loss3: 32238.24\n",
      "Epoch [3441], train_loss: 32702.50 with loss1: 416.21, loss2: 52.10 and loss3: 32234.20\n",
      "Epoch [3442], train_loss: 32703.79 with loss1: 421.63, loss2: 51.99 and loss3: 32230.17\n",
      "Epoch [3443], train_loss: 32697.74 with loss1: 419.71, loss2: 51.91 and loss3: 32226.12\n",
      "Epoch [3444], train_loss: 32697.54 with loss1: 423.56, loss2: 51.88 and loss3: 32222.10\n",
      "Epoch [3445], train_loss: 32693.66 with loss1: 423.55, loss2: 52.06 and loss3: 32218.05\n",
      "Epoch [3446], train_loss: 32694.20 with loss1: 428.22, loss2: 51.95 and loss3: 32214.03\n",
      "Epoch [3447], train_loss: 32688.32 with loss1: 426.31, loss2: 52.04 and loss3: 32209.97\n",
      "Epoch [3448], train_loss: 32687.12 with loss1: 429.15, loss2: 52.01 and loss3: 32205.96\n",
      "Epoch [3449], train_loss: 32683.18 with loss1: 429.40, loss2: 51.88 and loss3: 32201.90\n",
      "Epoch [3450], train_loss: 32682.56 with loss1: 432.86, loss2: 51.80 and loss3: 32197.89\n",
      "Epoch [3451], train_loss: 32677.24 with loss1: 431.48, loss2: 51.92 and loss3: 32193.84\n",
      "Epoch [3452], train_loss: 32677.26 with loss1: 435.73, loss2: 51.71 and loss3: 32189.82\n",
      "Epoch [3453], train_loss: 32672.84 with loss1: 435.12, loss2: 51.94 and loss3: 32185.77\n",
      "Epoch [3454], train_loss: 32672.59 with loss1: 439.19, loss2: 51.66 and loss3: 32181.74\n",
      "Epoch [3455], train_loss: 32666.18 with loss1: 436.71, loss2: 51.76 and loss3: 32177.71\n",
      "Epoch [3456], train_loss: 32666.59 with loss1: 441.12, loss2: 51.79 and loss3: 32173.68\n",
      "Epoch [3457], train_loss: 32660.20 with loss1: 438.97, loss2: 51.59 and loss3: 32169.64\n",
      "Epoch [3458], train_loss: 32658.00 with loss1: 440.58, loss2: 51.81 and loss3: 32165.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3459], train_loss: 32650.81 with loss1: 437.24, loss2: 52.01 and loss3: 32161.57\n",
      "Epoch [3460], train_loss: 32651.22 with loss1: 442.07, loss2: 51.61 and loss3: 32157.54\n",
      "Epoch [3461], train_loss: 32642.75 with loss1: 437.45, loss2: 51.80 and loss3: 32153.50\n",
      "Epoch [3462], train_loss: 32642.49 with loss1: 441.36, loss2: 51.66 and loss3: 32149.47\n",
      "Epoch [3463], train_loss: 32635.10 with loss1: 437.96, loss2: 51.70 and loss3: 32145.44\n",
      "Epoch [3464], train_loss: 32634.45 with loss1: 441.35, loss2: 51.70 and loss3: 32141.40\n",
      "Epoch [3465], train_loss: 32627.17 with loss1: 438.09, loss2: 51.70 and loss3: 32137.38\n",
      "Epoch [3466], train_loss: 32624.47 with loss1: 439.53, loss2: 51.61 and loss3: 32133.34\n",
      "Epoch [3467], train_loss: 32617.13 with loss1: 436.16, loss2: 51.66 and loss3: 32129.31\n",
      "Epoch [3468], train_loss: 32615.69 with loss1: 438.74, loss2: 51.68 and loss3: 32125.28\n",
      "Epoch [3469], train_loss: 32607.73 with loss1: 434.95, loss2: 51.52 and loss3: 32121.25\n",
      "Epoch [3470], train_loss: 32605.38 with loss1: 436.59, loss2: 51.58 and loss3: 32117.21\n",
      "Epoch [3471], train_loss: 32596.92 with loss1: 432.16, loss2: 51.56 and loss3: 32113.20\n",
      "Epoch [3472], train_loss: 32596.88 with loss1: 436.37, loss2: 51.37 and loss3: 32109.15\n",
      "Epoch [3473], train_loss: 32588.28 with loss1: 431.21, loss2: 51.93 and loss3: 32105.14\n",
      "Epoch [3474], train_loss: 32584.34 with loss1: 431.68, loss2: 51.57 and loss3: 32101.09\n",
      "Epoch [3475], train_loss: 32576.40 with loss1: 427.84, loss2: 51.48 and loss3: 32097.08\n",
      "Epoch [3476], train_loss: 32574.45 with loss1: 429.89, loss2: 51.53 and loss3: 32093.04\n",
      "Epoch [3477], train_loss: 32565.60 with loss1: 424.91, loss2: 51.67 and loss3: 32089.02\n",
      "Epoch [3478], train_loss: 32561.30 with loss1: 424.89, loss2: 51.42 and loss3: 32084.98\n",
      "Epoch [3479], train_loss: 32555.40 with loss1: 422.96, loss2: 51.48 and loss3: 32080.96\n",
      "Epoch [3480], train_loss: 32552.81 with loss1: 424.59, loss2: 51.30 and loss3: 32076.93\n",
      "Epoch [3481], train_loss: 32544.85 with loss1: 420.53, loss2: 51.42 and loss3: 32072.90\n",
      "Epoch [3482], train_loss: 32542.18 with loss1: 421.90, loss2: 51.40 and loss3: 32068.88\n",
      "Epoch [3483], train_loss: 32535.87 with loss1: 419.49, loss2: 51.53 and loss3: 32064.85\n",
      "Epoch [3484], train_loss: 32532.23 with loss1: 420.16, loss2: 51.25 and loss3: 32060.82\n",
      "Epoch [3485], train_loss: 32525.17 with loss1: 417.15, loss2: 51.23 and loss3: 32056.79\n",
      "Epoch [3486], train_loss: 32522.60 with loss1: 418.51, loss2: 51.33 and loss3: 32052.76\n",
      "Epoch [3487], train_loss: 32517.02 with loss1: 417.03, loss2: 51.25 and loss3: 32048.73\n",
      "Epoch [3488], train_loss: 32513.92 with loss1: 417.91, loss2: 51.30 and loss3: 32044.71\n",
      "Epoch [3489], train_loss: 32507.18 with loss1: 415.24, loss2: 51.25 and loss3: 32040.68\n",
      "Epoch [3490], train_loss: 32505.31 with loss1: 417.41, loss2: 51.25 and loss3: 32036.65\n",
      "Epoch [3491], train_loss: 32500.17 with loss1: 416.15, loss2: 51.39 and loss3: 32032.63\n",
      "Epoch [3492], train_loss: 32496.24 with loss1: 416.43, loss2: 51.20 and loss3: 32028.60\n",
      "Epoch [3493], train_loss: 32489.68 with loss1: 413.95, loss2: 51.16 and loss3: 32024.57\n",
      "Epoch [3494], train_loss: 32485.55 with loss1: 413.77, loss2: 51.22 and loss3: 32020.55\n",
      "Epoch [3495], train_loss: 32481.52 with loss1: 413.97, loss2: 51.02 and loss3: 32016.52\n",
      "Epoch [3496], train_loss: 32477.34 with loss1: 413.61, loss2: 51.23 and loss3: 32012.50\n",
      "Epoch [3497], train_loss: 32471.38 with loss1: 411.67, loss2: 51.24 and loss3: 32008.47\n",
      "Epoch [3498], train_loss: 32469.16 with loss1: 413.44, loss2: 51.26 and loss3: 32004.45\n",
      "Epoch [3499], train_loss: 32462.48 with loss1: 410.80, loss2: 51.26 and loss3: 32000.41\n",
      "Epoch [3500], train_loss: 32459.17 with loss1: 411.54, loss2: 51.23 and loss3: 31996.40\n",
      "Epoch [3501], train_loss: 32453.49 with loss1: 410.00, loss2: 51.13 and loss3: 31992.37\n",
      "Epoch [3502], train_loss: 32450.21 with loss1: 410.77, loss2: 51.10 and loss3: 31988.34\n",
      "Epoch [3503], train_loss: 32444.96 with loss1: 409.67, loss2: 50.97 and loss3: 31984.32\n",
      "Epoch [3504], train_loss: 32441.02 with loss1: 409.75, loss2: 50.98 and loss3: 31980.29\n",
      "Epoch [3505], train_loss: 32435.54 with loss1: 408.26, loss2: 51.01 and loss3: 31976.27\n",
      "Epoch [3506], train_loss: 32432.56 with loss1: 409.27, loss2: 51.05 and loss3: 31972.24\n",
      "Epoch [3507], train_loss: 32427.40 with loss1: 408.29, loss2: 50.89 and loss3: 31968.22\n",
      "Epoch [3508], train_loss: 32424.75 with loss1: 409.60, loss2: 50.95 and loss3: 31964.20\n",
      "Epoch [3509], train_loss: 32419.39 with loss1: 408.18, loss2: 51.04 and loss3: 31960.17\n",
      "Epoch [3510], train_loss: 32415.88 with loss1: 408.55, loss2: 51.19 and loss3: 31956.14\n",
      "Epoch [3511], train_loss: 32409.85 with loss1: 406.83, loss2: 50.90 and loss3: 31952.12\n",
      "Epoch [3512], train_loss: 32409.46 with loss1: 410.37, loss2: 51.01 and loss3: 31948.09\n",
      "Epoch [3513], train_loss: 32401.46 with loss1: 406.51, loss2: 50.87 and loss3: 31944.07\n",
      "Epoch [3514], train_loss: 32397.38 with loss1: 406.47, loss2: 50.87 and loss3: 31940.04\n",
      "Epoch [3515], train_loss: 32393.05 with loss1: 406.27, loss2: 50.76 and loss3: 31936.03\n",
      "Epoch [3516], train_loss: 32389.78 with loss1: 406.88, loss2: 50.90 and loss3: 31932.00\n",
      "Epoch [3517], train_loss: 32383.83 with loss1: 405.17, loss2: 50.69 and loss3: 31927.98\n",
      "Epoch [3518], train_loss: 32381.90 with loss1: 407.19, loss2: 50.76 and loss3: 31923.95\n",
      "Epoch [3519], train_loss: 32375.31 with loss1: 404.63, loss2: 50.75 and loss3: 31919.93\n",
      "Epoch [3520], train_loss: 32371.69 with loss1: 404.92, loss2: 50.85 and loss3: 31915.91\n",
      "Epoch [3521], train_loss: 32365.98 with loss1: 403.38, loss2: 50.71 and loss3: 31911.88\n",
      "Epoch [3522], train_loss: 32361.97 with loss1: 403.37, loss2: 50.74 and loss3: 31907.87\n",
      "Epoch [3523], train_loss: 32357.63 with loss1: 402.89, loss2: 50.90 and loss3: 31903.84\n",
      "Epoch [3524], train_loss: 32354.52 with loss1: 404.00, loss2: 50.69 and loss3: 31899.83\n",
      "Epoch [3525], train_loss: 32348.24 with loss1: 401.74, loss2: 50.70 and loss3: 31895.80\n",
      "Epoch [3526], train_loss: 32346.44 with loss1: 403.88, loss2: 50.77 and loss3: 31891.79\n",
      "Epoch [3527], train_loss: 32339.46 with loss1: 401.23, loss2: 50.47 and loss3: 31887.76\n",
      "Epoch [3528], train_loss: 32336.97 with loss1: 402.68, loss2: 50.53 and loss3: 31883.75\n",
      "Epoch [3529], train_loss: 32331.86 with loss1: 401.60, loss2: 50.53 and loss3: 31879.72\n",
      "Epoch [3530], train_loss: 32327.81 with loss1: 401.54, loss2: 50.56 and loss3: 31875.71\n",
      "Epoch [3531], train_loss: 32323.75 with loss1: 401.66, loss2: 50.41 and loss3: 31871.69\n",
      "Epoch [3532], train_loss: 32320.33 with loss1: 402.12, loss2: 50.54 and loss3: 31867.68\n",
      "Epoch [3533], train_loss: 32315.35 with loss1: 401.20, loss2: 50.50 and loss3: 31863.65\n",
      "Epoch [3534], train_loss: 32312.61 with loss1: 402.25, loss2: 50.72 and loss3: 31859.64\n",
      "Epoch [3535], train_loss: 32308.56 with loss1: 402.59, loss2: 50.36 and loss3: 31855.61\n",
      "Epoch [3536], train_loss: 32303.73 with loss1: 401.58, loss2: 50.55 and loss3: 31851.60\n",
      "Epoch [3537], train_loss: 32298.18 with loss1: 400.33, loss2: 50.28 and loss3: 31847.57\n",
      "Epoch [3538], train_loss: 32295.15 with loss1: 400.98, loss2: 50.61 and loss3: 31843.56\n",
      "Epoch [3539], train_loss: 32290.70 with loss1: 400.81, loss2: 50.35 and loss3: 31839.54\n",
      "Epoch [3540], train_loss: 32287.92 with loss1: 401.88, loss2: 50.51 and loss3: 31835.53\n",
      "Epoch [3541], train_loss: 32282.20 with loss1: 400.29, loss2: 50.40 and loss3: 31831.51\n",
      "Epoch [3542], train_loss: 32278.69 with loss1: 400.93, loss2: 50.26 and loss3: 31827.50\n",
      "Epoch [3543], train_loss: 32275.38 with loss1: 401.65, loss2: 50.26 and loss3: 31823.48\n",
      "Epoch [3544], train_loss: 32270.49 with loss1: 400.51, loss2: 50.52 and loss3: 31819.46\n",
      "Epoch [3545], train_loss: 32266.27 with loss1: 400.53, loss2: 50.29 and loss3: 31815.44\n",
      "Epoch [3546], train_loss: 32263.19 with loss1: 401.45, loss2: 50.32 and loss3: 31811.43\n",
      "Epoch [3547], train_loss: 32259.30 with loss1: 401.51, loss2: 50.38 and loss3: 31807.41\n",
      "Epoch [3548], train_loss: 32254.23 with loss1: 400.49, loss2: 50.35 and loss3: 31803.40\n",
      "Epoch [3549], train_loss: 32251.18 with loss1: 401.61, loss2: 50.18 and loss3: 31799.39\n",
      "Epoch [3550], train_loss: 32247.99 with loss1: 402.25, loss2: 50.37 and loss3: 31795.37\n",
      "Epoch [3551], train_loss: 32243.76 with loss1: 402.23, loss2: 50.17 and loss3: 31791.36\n",
      "Epoch [3552], train_loss: 32240.20 with loss1: 402.48, loss2: 50.37 and loss3: 31787.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3553], train_loss: 32236.04 with loss1: 402.59, loss2: 50.12 and loss3: 31783.32\n",
      "Epoch [3554], train_loss: 32231.90 with loss1: 402.33, loss2: 50.25 and loss3: 31779.31\n",
      "Epoch [3555], train_loss: 32228.13 with loss1: 402.55, loss2: 50.28 and loss3: 31775.30\n",
      "Epoch [3556], train_loss: 32223.28 with loss1: 401.72, loss2: 50.26 and loss3: 31771.29\n",
      "Epoch [3557], train_loss: 32219.14 with loss1: 401.78, loss2: 50.09 and loss3: 31767.27\n",
      "Epoch [3558], train_loss: 32216.26 with loss1: 402.79, loss2: 50.20 and loss3: 31763.27\n",
      "Epoch [3559], train_loss: 32211.50 with loss1: 402.22, loss2: 50.05 and loss3: 31759.24\n",
      "Epoch [3560], train_loss: 32208.55 with loss1: 403.17, loss2: 50.13 and loss3: 31755.25\n",
      "Epoch [3561], train_loss: 32202.83 with loss1: 401.69, loss2: 49.93 and loss3: 31751.21\n",
      "Epoch [3562], train_loss: 32202.11 with loss1: 404.74, loss2: 50.14 and loss3: 31747.23\n",
      "Epoch [3563], train_loss: 32194.84 with loss1: 401.78, loss2: 49.88 and loss3: 31743.19\n",
      "Epoch [3564], train_loss: 32193.55 with loss1: 404.41, loss2: 49.94 and loss3: 31739.20\n",
      "Epoch [3565], train_loss: 32188.80 with loss1: 403.67, loss2: 49.96 and loss3: 31735.16\n",
      "Epoch [3566], train_loss: 32185.28 with loss1: 404.09, loss2: 50.01 and loss3: 31731.18\n",
      "Epoch [3567], train_loss: 32181.19 with loss1: 403.99, loss2: 50.05 and loss3: 31727.14\n",
      "Epoch [3568], train_loss: 32178.29 with loss1: 404.91, loss2: 50.22 and loss3: 31723.16\n",
      "Epoch [3569], train_loss: 32173.99 with loss1: 404.76, loss2: 50.11 and loss3: 31719.12\n",
      "Epoch [3570], train_loss: 32170.92 with loss1: 405.78, loss2: 50.01 and loss3: 31715.13\n",
      "Epoch [3571], train_loss: 32165.99 with loss1: 405.06, loss2: 49.82 and loss3: 31711.11\n",
      "Epoch [3572], train_loss: 32161.79 with loss1: 404.71, loss2: 49.97 and loss3: 31707.11\n",
      "Epoch [3573], train_loss: 32158.09 with loss1: 405.31, loss2: 49.68 and loss3: 31703.10\n",
      "Epoch [3574], train_loss: 32156.00 with loss1: 406.93, loss2: 49.98 and loss3: 31699.09\n",
      "Epoch [3575], train_loss: 32149.58 with loss1: 404.72, loss2: 49.78 and loss3: 31695.08\n",
      "Epoch [3576], train_loss: 32146.82 with loss1: 405.86, loss2: 49.89 and loss3: 31691.07\n",
      "Epoch [3577], train_loss: 32140.74 with loss1: 403.88, loss2: 49.78 and loss3: 31687.07\n",
      "Epoch [3578], train_loss: 32139.20 with loss1: 406.01, loss2: 50.14 and loss3: 31683.06\n",
      "Epoch [3579], train_loss: 32133.51 with loss1: 404.67, loss2: 49.79 and loss3: 31679.05\n",
      "Epoch [3580], train_loss: 32131.95 with loss1: 407.09, loss2: 49.81 and loss3: 31675.04\n",
      "Epoch [3581], train_loss: 32126.41 with loss1: 405.72, loss2: 49.65 and loss3: 31671.04\n",
      "Epoch [3582], train_loss: 32124.25 with loss1: 407.38, loss2: 49.84 and loss3: 31667.03\n",
      "Epoch [3583], train_loss: 32117.52 with loss1: 404.91, loss2: 49.58 and loss3: 31663.02\n",
      "Epoch [3584], train_loss: 32115.91 with loss1: 407.04, loss2: 49.85 and loss3: 31659.02\n",
      "Epoch [3585], train_loss: 32109.87 with loss1: 405.34, loss2: 49.53 and loss3: 31655.01\n",
      "Epoch [3586], train_loss: 32108.29 with loss1: 407.58, loss2: 49.70 and loss3: 31651.01\n",
      "Epoch [3587], train_loss: 32101.45 with loss1: 404.74, loss2: 49.72 and loss3: 31647.00\n",
      "Epoch [3588], train_loss: 32099.00 with loss1: 406.41, loss2: 49.60 and loss3: 31643.00\n",
      "Epoch [3589], train_loss: 32092.78 with loss1: 404.22, loss2: 49.57 and loss3: 31638.99\n",
      "Epoch [3590], train_loss: 32090.74 with loss1: 405.91, loss2: 49.85 and loss3: 31634.98\n",
      "Epoch [3591], train_loss: 32084.59 with loss1: 403.99, loss2: 49.63 and loss3: 31630.98\n",
      "Epoch [3592], train_loss: 32082.78 with loss1: 406.17, loss2: 49.64 and loss3: 31626.97\n",
      "Epoch [3593], train_loss: 32075.07 with loss1: 402.66, loss2: 49.44 and loss3: 31622.97\n",
      "Epoch [3594], train_loss: 32072.80 with loss1: 404.26, loss2: 49.57 and loss3: 31618.96\n",
      "Epoch [3595], train_loss: 32068.79 with loss1: 404.36, loss2: 49.46 and loss3: 31614.96\n",
      "Epoch [3596], train_loss: 32064.76 with loss1: 404.28, loss2: 49.53 and loss3: 31610.95\n",
      "Epoch [3597], train_loss: 32059.98 with loss1: 403.44, loss2: 49.59 and loss3: 31606.95\n",
      "Epoch [3598], train_loss: 32057.93 with loss1: 405.50, loss2: 49.49 and loss3: 31602.95\n",
      "Epoch [3599], train_loss: 32052.06 with loss1: 403.75, loss2: 49.36 and loss3: 31598.95\n",
      "Epoch [3600], train_loss: 32049.27 with loss1: 404.57, loss2: 49.76 and loss3: 31594.94\n",
      "Epoch [3601], train_loss: 32043.08 with loss1: 402.92, loss2: 49.22 and loss3: 31590.95\n",
      "Epoch [3602], train_loss: 32041.10 with loss1: 404.79, loss2: 49.37 and loss3: 31586.94\n",
      "Epoch [3603], train_loss: 32035.45 with loss1: 403.09, loss2: 49.42 and loss3: 31582.94\n",
      "Epoch [3604], train_loss: 32033.50 with loss1: 405.17, loss2: 49.40 and loss3: 31578.93\n",
      "Epoch [3605], train_loss: 32026.86 with loss1: 402.65, loss2: 49.27 and loss3: 31574.94\n",
      "Epoch [3606], train_loss: 32024.91 with loss1: 404.58, loss2: 49.40 and loss3: 31570.93\n",
      "Epoch [3607], train_loss: 32018.23 with loss1: 402.08, loss2: 49.21 and loss3: 31566.94\n",
      "Epoch [3608], train_loss: 32015.35 with loss1: 403.00, loss2: 49.41 and loss3: 31562.93\n",
      "Epoch [3609], train_loss: 32009.69 with loss1: 401.55, loss2: 49.20 and loss3: 31558.95\n",
      "Epoch [3610], train_loss: 32007.12 with loss1: 402.77, loss2: 49.41 and loss3: 31554.94\n",
      "Epoch [3611], train_loss: 32001.60 with loss1: 401.53, loss2: 49.13 and loss3: 31550.95\n",
      "Epoch [3612], train_loss: 31999.34 with loss1: 403.08, loss2: 49.32 and loss3: 31546.95\n",
      "Epoch [3613], train_loss: 31993.96 with loss1: 401.72, loss2: 49.29 and loss3: 31542.95\n",
      "Epoch [3614], train_loss: 31991.61 with loss1: 403.45, loss2: 49.21 and loss3: 31538.95\n",
      "Epoch [3615], train_loss: 31985.31 with loss1: 401.09, loss2: 49.25 and loss3: 31534.96\n",
      "Epoch [3616], train_loss: 31982.19 with loss1: 402.13, loss2: 49.11 and loss3: 31530.96\n",
      "Epoch [3617], train_loss: 31976.59 with loss1: 400.61, loss2: 49.00 and loss3: 31526.97\n",
      "Epoch [3618], train_loss: 31975.48 with loss1: 403.25, loss2: 49.27 and loss3: 31522.96\n",
      "Epoch [3619], train_loss: 31969.07 with loss1: 401.09, loss2: 49.00 and loss3: 31518.98\n",
      "Epoch [3620], train_loss: 31965.59 with loss1: 401.54, loss2: 49.08 and loss3: 31514.98\n",
      "Epoch [3621], train_loss: 31959.16 with loss1: 398.86, loss2: 49.31 and loss3: 31510.99\n",
      "Epoch [3622], train_loss: 31956.74 with loss1: 400.64, loss2: 49.11 and loss3: 31506.99\n",
      "Epoch [3623], train_loss: 31951.04 with loss1: 399.10, loss2: 48.94 and loss3: 31502.99\n",
      "Epoch [3624], train_loss: 31948.07 with loss1: 399.94, loss2: 49.13 and loss3: 31499.00\n",
      "Epoch [3625], train_loss: 31941.49 with loss1: 397.56, loss2: 48.92 and loss3: 31495.01\n",
      "Epoch [3626], train_loss: 31938.75 with loss1: 398.75, loss2: 48.99 and loss3: 31491.01\n",
      "Epoch [3627], train_loss: 31934.11 with loss1: 398.13, loss2: 48.97 and loss3: 31487.02\n",
      "Epoch [3628], train_loss: 31930.64 with loss1: 398.68, loss2: 48.94 and loss3: 31483.02\n",
      "Epoch [3629], train_loss: 31925.05 with loss1: 397.27, loss2: 48.75 and loss3: 31479.03\n",
      "Epoch [3630], train_loss: 31921.95 with loss1: 397.96, loss2: 48.96 and loss3: 31475.03\n",
      "Epoch [3631], train_loss: 31916.29 with loss1: 396.21, loss2: 49.04 and loss3: 31471.05\n",
      "Epoch [3632], train_loss: 31913.45 with loss1: 397.45, loss2: 48.95 and loss3: 31467.05\n",
      "Epoch [3633], train_loss: 31909.54 with loss1: 397.68, loss2: 48.80 and loss3: 31463.06\n",
      "Epoch [3634], train_loss: 31906.93 with loss1: 398.92, loss2: 48.94 and loss3: 31459.06\n",
      "Epoch [3635], train_loss: 31901.54 with loss1: 397.69, loss2: 48.78 and loss3: 31455.08\n",
      "Epoch [3636], train_loss: 31897.74 with loss1: 397.81, loss2: 48.86 and loss3: 31451.08\n",
      "Epoch [3637], train_loss: 31894.11 with loss1: 398.22, loss2: 48.80 and loss3: 31447.09\n",
      "Epoch [3638], train_loss: 31889.99 with loss1: 398.23, loss2: 48.67 and loss3: 31443.09\n",
      "Epoch [3639], train_loss: 31885.22 with loss1: 397.40, loss2: 48.71 and loss3: 31439.12\n",
      "Epoch [3640], train_loss: 31881.43 with loss1: 397.62, loss2: 48.69 and loss3: 31435.11\n",
      "Epoch [3641], train_loss: 31875.87 with loss1: 395.93, loss2: 48.80 and loss3: 31431.14\n",
      "Epoch [3642], train_loss: 31873.57 with loss1: 397.68, loss2: 48.76 and loss3: 31427.13\n",
      "Epoch [3643], train_loss: 31866.78 with loss1: 395.01, loss2: 48.61 and loss3: 31423.16\n",
      "Epoch [3644], train_loss: 31865.62 with loss1: 397.78, loss2: 48.68 and loss3: 31419.16\n",
      "Epoch [3645], train_loss: 31858.99 with loss1: 395.25, loss2: 48.55 and loss3: 31415.19\n",
      "Epoch [3646], train_loss: 31856.23 with loss1: 396.43, loss2: 48.62 and loss3: 31411.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3647], train_loss: 31850.93 with loss1: 395.07, loss2: 48.65 and loss3: 31407.21\n",
      "Epoch [3648], train_loss: 31847.58 with loss1: 395.84, loss2: 48.53 and loss3: 31403.21\n",
      "Epoch [3649], train_loss: 31841.95 with loss1: 394.21, loss2: 48.50 and loss3: 31399.24\n",
      "Epoch [3650], train_loss: 31839.90 with loss1: 396.02, loss2: 48.63 and loss3: 31395.25\n",
      "Epoch [3651], train_loss: 31835.12 with loss1: 395.32, loss2: 48.53 and loss3: 31391.27\n",
      "Epoch [3652], train_loss: 31831.60 with loss1: 395.65, loss2: 48.67 and loss3: 31387.28\n",
      "Epoch [3653], train_loss: 31825.61 with loss1: 393.76, loss2: 48.55 and loss3: 31383.30\n",
      "Epoch [3654], train_loss: 31823.04 with loss1: 395.21, loss2: 48.52 and loss3: 31379.31\n",
      "Epoch [3655], train_loss: 31818.27 with loss1: 394.27, loss2: 48.66 and loss3: 31375.34\n",
      "Epoch [3656], train_loss: 31814.70 with loss1: 394.80, loss2: 48.56 and loss3: 31371.34\n",
      "Epoch [3657], train_loss: 31810.36 with loss1: 394.44, loss2: 48.55 and loss3: 31367.37\n",
      "Epoch [3658], train_loss: 31808.59 with loss1: 396.69, loss2: 48.52 and loss3: 31363.38\n",
      "Epoch [3659], train_loss: 31802.68 with loss1: 394.86, loss2: 48.43 and loss3: 31359.40\n",
      "Epoch [3660], train_loss: 31800.63 with loss1: 396.67, loss2: 48.55 and loss3: 31355.41\n",
      "Epoch [3661], train_loss: 31794.32 with loss1: 394.32, loss2: 48.57 and loss3: 31351.43\n",
      "Epoch [3662], train_loss: 31793.24 with loss1: 397.15, loss2: 48.64 and loss3: 31347.45\n",
      "Epoch [3663], train_loss: 31787.42 with loss1: 395.54, loss2: 48.42 and loss3: 31343.46\n",
      "Epoch [3664], train_loss: 31786.29 with loss1: 398.43, loss2: 48.38 and loss3: 31339.48\n",
      "Epoch [3665], train_loss: 31780.61 with loss1: 396.53, loss2: 48.58 and loss3: 31335.50\n",
      "Epoch [3666], train_loss: 31778.86 with loss1: 398.93, loss2: 48.42 and loss3: 31331.51\n",
      "Epoch [3667], train_loss: 31773.35 with loss1: 397.49, loss2: 48.33 and loss3: 31327.53\n",
      "Epoch [3668], train_loss: 31769.39 with loss1: 397.52, loss2: 48.33 and loss3: 31323.54\n",
      "Epoch [3669], train_loss: 31763.88 with loss1: 396.04, loss2: 48.28 and loss3: 31319.57\n",
      "Epoch [3670], train_loss: 31761.68 with loss1: 397.82, loss2: 48.28 and loss3: 31315.58\n",
      "Epoch [3671], train_loss: 31755.94 with loss1: 395.99, loss2: 48.35 and loss3: 31311.60\n",
      "Epoch [3672], train_loss: 31753.08 with loss1: 397.14, loss2: 48.31 and loss3: 31307.62\n",
      "Epoch [3673], train_loss: 31747.52 with loss1: 395.64, loss2: 48.24 and loss3: 31303.64\n",
      "Epoch [3674], train_loss: 31745.12 with loss1: 397.33, loss2: 48.13 and loss3: 31299.66\n",
      "Epoch [3675], train_loss: 31740.53 with loss1: 396.61, loss2: 48.24 and loss3: 31295.67\n",
      "Epoch [3676], train_loss: 31735.84 with loss1: 396.02, loss2: 48.12 and loss3: 31291.70\n",
      "Epoch [3677], train_loss: 31731.71 with loss1: 395.84, loss2: 48.15 and loss3: 31287.71\n",
      "Epoch [3678], train_loss: 31727.54 with loss1: 395.76, loss2: 48.04 and loss3: 31283.73\n",
      "Epoch [3679], train_loss: 31721.40 with loss1: 393.47, loss2: 48.17 and loss3: 31279.76\n",
      "Epoch [3680], train_loss: 31718.83 with loss1: 394.76, loss2: 48.29 and loss3: 31275.78\n",
      "Epoch [3681], train_loss: 31712.38 with loss1: 392.54, loss2: 48.05 and loss3: 31271.80\n",
      "Epoch [3682], train_loss: 31709.51 with loss1: 393.65, loss2: 48.05 and loss3: 31267.82\n",
      "Epoch [3683], train_loss: 31703.20 with loss1: 391.25, loss2: 48.11 and loss3: 31263.85\n",
      "Epoch [3684], train_loss: 31699.80 with loss1: 391.74, loss2: 48.20 and loss3: 31259.86\n",
      "Epoch [3685], train_loss: 31693.47 with loss1: 389.49, loss2: 48.09 and loss3: 31255.89\n",
      "Epoch [3686], train_loss: 31691.52 with loss1: 391.51, loss2: 48.11 and loss3: 31251.91\n",
      "Epoch [3687], train_loss: 31684.75 with loss1: 388.73, loss2: 48.09 and loss3: 31247.93\n",
      "Epoch [3688], train_loss: 31680.75 with loss1: 388.88, loss2: 47.91 and loss3: 31243.96\n",
      "Epoch [3689], train_loss: 31676.12 with loss1: 388.11, loss2: 48.03 and loss3: 31239.98\n",
      "Epoch [3690], train_loss: 31672.02 with loss1: 388.13, loss2: 47.89 and loss3: 31236.00\n",
      "Epoch [3691], train_loss: 31666.54 with loss1: 386.52, loss2: 47.99 and loss3: 31232.03\n",
      "Epoch [3692], train_loss: 31663.19 with loss1: 387.21, loss2: 47.93 and loss3: 31228.05\n",
      "Epoch [3693], train_loss: 31657.27 with loss1: 385.25, loss2: 47.94 and loss3: 31224.08\n",
      "Epoch [3694], train_loss: 31654.26 with loss1: 386.23, loss2: 47.93 and loss3: 31220.10\n",
      "Epoch [3695], train_loss: 31649.22 with loss1: 385.14, loss2: 47.95 and loss3: 31216.13\n",
      "Epoch [3696], train_loss: 31645.84 with loss1: 385.75, loss2: 47.95 and loss3: 31212.15\n",
      "Epoch [3697], train_loss: 31639.90 with loss1: 383.71, loss2: 48.00 and loss3: 31208.18\n",
      "Epoch [3698], train_loss: 31636.19 with loss1: 384.08, loss2: 47.91 and loss3: 31204.20\n",
      "Epoch [3699], train_loss: 31631.47 with loss1: 383.24, loss2: 47.99 and loss3: 31200.24\n",
      "Epoch [3700], train_loss: 31627.31 with loss1: 383.25, loss2: 47.81 and loss3: 31196.25\n",
      "Epoch [3701], train_loss: 31622.37 with loss1: 382.30, loss2: 47.77 and loss3: 31192.30\n",
      "Epoch [3702], train_loss: 31618.90 with loss1: 382.74, loss2: 47.86 and loss3: 31188.30\n",
      "Epoch [3703], train_loss: 31615.01 with loss1: 382.70, loss2: 47.96 and loss3: 31184.36\n",
      "Epoch [3704], train_loss: 31609.82 with loss1: 381.60, loss2: 47.86 and loss3: 31180.36\n",
      "Epoch [3705], train_loss: 31605.78 with loss1: 381.46, loss2: 47.90 and loss3: 31176.41\n",
      "Epoch [3706], train_loss: 31601.62 with loss1: 381.56, loss2: 47.64 and loss3: 31172.42\n",
      "Epoch [3707], train_loss: 31596.77 with loss1: 380.48, loss2: 47.82 and loss3: 31168.47\n",
      "Epoch [3708], train_loss: 31594.85 with loss1: 382.56, loss2: 47.81 and loss3: 31164.48\n",
      "Epoch [3709], train_loss: 31587.95 with loss1: 379.71, loss2: 47.71 and loss3: 31160.53\n",
      "Epoch [3710], train_loss: 31584.70 with loss1: 380.49, loss2: 47.66 and loss3: 31156.55\n",
      "Epoch [3711], train_loss: 31579.70 with loss1: 379.42, loss2: 47.68 and loss3: 31152.60\n",
      "Epoch [3712], train_loss: 31576.08 with loss1: 379.81, loss2: 47.67 and loss3: 31148.61\n",
      "Epoch [3713], train_loss: 31571.59 with loss1: 379.39, loss2: 47.54 and loss3: 31144.66\n",
      "Epoch [3714], train_loss: 31569.30 with loss1: 380.87, loss2: 47.75 and loss3: 31140.68\n",
      "Epoch [3715], train_loss: 31564.35 with loss1: 379.91, loss2: 47.71 and loss3: 31136.73\n",
      "Epoch [3716], train_loss: 31561.00 with loss1: 380.67, loss2: 47.59 and loss3: 31132.74\n",
      "Epoch [3717], train_loss: 31556.50 with loss1: 380.22, loss2: 47.48 and loss3: 31128.80\n",
      "Epoch [3718], train_loss: 31553.15 with loss1: 380.80, loss2: 47.54 and loss3: 31124.81\n",
      "Epoch [3719], train_loss: 31549.01 with loss1: 380.51, loss2: 47.63 and loss3: 31120.87\n",
      "Epoch [3720], train_loss: 31547.19 with loss1: 382.58, loss2: 47.73 and loss3: 31116.88\n",
      "Epoch [3721], train_loss: 31541.37 with loss1: 380.99, loss2: 47.45 and loss3: 31112.94\n",
      "Epoch [3722], train_loss: 31538.09 with loss1: 381.57, loss2: 47.57 and loss3: 31108.95\n",
      "Epoch [3723], train_loss: 31533.38 with loss1: 380.86, loss2: 47.52 and loss3: 31105.00\n",
      "Epoch [3724], train_loss: 31531.41 with loss1: 382.91, loss2: 47.47 and loss3: 31101.03\n",
      "Epoch [3725], train_loss: 31525.76 with loss1: 381.03, loss2: 47.65 and loss3: 31097.07\n",
      "Epoch [3726], train_loss: 31523.24 with loss1: 382.62, loss2: 47.52 and loss3: 31093.10\n",
      "Epoch [3727], train_loss: 31518.95 with loss1: 382.42, loss2: 47.38 and loss3: 31089.14\n",
      "Epoch [3728], train_loss: 31516.53 with loss1: 383.99, loss2: 47.37 and loss3: 31085.17\n",
      "Epoch [3729], train_loss: 31511.19 with loss1: 382.46, loss2: 47.52 and loss3: 31081.21\n",
      "Epoch [3730], train_loss: 31508.49 with loss1: 383.98, loss2: 47.26 and loss3: 31077.24\n",
      "Epoch [3731], train_loss: 31504.80 with loss1: 384.12, loss2: 47.39 and loss3: 31073.29\n",
      "Epoch [3732], train_loss: 31501.92 with loss1: 385.26, loss2: 47.35 and loss3: 31069.31\n",
      "Epoch [3733], train_loss: 31497.58 with loss1: 384.84, loss2: 47.37 and loss3: 31065.37\n",
      "Epoch [3734], train_loss: 31494.30 with loss1: 385.63, loss2: 47.30 and loss3: 31061.38\n",
      "Epoch [3735], train_loss: 31489.67 with loss1: 384.97, loss2: 47.25 and loss3: 31057.45\n",
      "Epoch [3736], train_loss: 31487.23 with loss1: 386.57, loss2: 47.20 and loss3: 31053.46\n",
      "Epoch [3737], train_loss: 31483.27 with loss1: 386.59, loss2: 47.16 and loss3: 31049.52\n",
      "Epoch [3738], train_loss: 31480.69 with loss1: 387.92, loss2: 47.23 and loss3: 31045.54\n",
      "Epoch [3739], train_loss: 31475.90 with loss1: 387.00, loss2: 47.30 and loss3: 31041.59\n",
      "Epoch [3740], train_loss: 31474.48 with loss1: 389.57, loss2: 47.29 and loss3: 31037.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3741], train_loss: 31470.41 with loss1: 389.50, loss2: 47.24 and loss3: 31033.67\n",
      "Epoch [3742], train_loss: 31467.99 with loss1: 391.12, loss2: 47.17 and loss3: 31029.70\n",
      "Epoch [3743], train_loss: 31461.85 with loss1: 389.00, loss2: 47.11 and loss3: 31025.74\n",
      "Epoch [3744], train_loss: 31459.67 with loss1: 390.73, loss2: 47.15 and loss3: 31021.79\n",
      "Epoch [3745], train_loss: 31456.50 with loss1: 391.49, loss2: 47.18 and loss3: 31017.82\n",
      "Epoch [3746], train_loss: 31453.65 with loss1: 392.56, loss2: 47.22 and loss3: 31013.87\n",
      "Epoch [3747], train_loss: 31448.29 with loss1: 391.26, loss2: 47.12 and loss3: 31009.91\n",
      "Epoch [3748], train_loss: 31446.98 with loss1: 393.93, loss2: 47.11 and loss3: 31005.95\n",
      "Epoch [3749], train_loss: 31442.46 with loss1: 393.30, loss2: 47.17 and loss3: 31001.99\n",
      "Epoch [3750], train_loss: 31441.04 with loss1: 395.79, loss2: 47.22 and loss3: 30998.03\n",
      "Epoch [3751], train_loss: 31435.20 with loss1: 393.82, loss2: 47.30 and loss3: 30994.08\n",
      "Epoch [3752], train_loss: 31433.57 with loss1: 396.49, loss2: 46.96 and loss3: 30990.12\n",
      "Epoch [3753], train_loss: 31427.67 with loss1: 394.49, loss2: 47.01 and loss3: 30986.17\n",
      "Epoch [3754], train_loss: 31427.15 with loss1: 397.92, loss2: 47.02 and loss3: 30982.21\n",
      "Epoch [3755], train_loss: 31421.84 with loss1: 396.59, loss2: 46.99 and loss3: 30978.26\n",
      "Epoch [3756], train_loss: 31420.29 with loss1: 399.08, loss2: 46.90 and loss3: 30974.30\n",
      "Epoch [3757], train_loss: 31416.12 with loss1: 398.82, loss2: 46.94 and loss3: 30970.36\n",
      "Epoch [3758], train_loss: 31413.70 with loss1: 400.41, loss2: 46.90 and loss3: 30966.39\n",
      "Epoch [3759], train_loss: 31409.62 with loss1: 400.24, loss2: 46.92 and loss3: 30962.46\n",
      "Epoch [3760], train_loss: 31408.56 with loss1: 403.09, loss2: 46.98 and loss3: 30958.49\n",
      "Epoch [3761], train_loss: 31402.64 with loss1: 401.17, loss2: 46.91 and loss3: 30954.55\n",
      "Epoch [3762], train_loss: 31401.10 with loss1: 403.63, loss2: 46.88 and loss3: 30950.58\n",
      "Epoch [3763], train_loss: 31395.46 with loss1: 401.94, loss2: 46.87 and loss3: 30946.64\n",
      "Epoch [3764], train_loss: 31394.96 with loss1: 405.33, loss2: 46.95 and loss3: 30942.68\n",
      "Epoch [3765], train_loss: 31388.44 with loss1: 402.95, loss2: 46.75 and loss3: 30938.74\n",
      "Epoch [3766], train_loss: 31388.42 with loss1: 406.85, loss2: 46.80 and loss3: 30934.78\n",
      "Epoch [3767], train_loss: 31383.04 with loss1: 405.48, loss2: 46.72 and loss3: 30930.84\n",
      "Epoch [3768], train_loss: 31382.31 with loss1: 408.52, loss2: 46.91 and loss3: 30926.88\n",
      "Epoch [3769], train_loss: 31376.21 with loss1: 406.48, loss2: 46.80 and loss3: 30922.93\n",
      "Epoch [3770], train_loss: 31377.04 with loss1: 410.86, loss2: 47.20 and loss3: 30918.98\n",
      "Epoch [3771], train_loss: 31369.40 with loss1: 407.61, loss2: 46.77 and loss3: 30915.03\n",
      "Epoch [3772], train_loss: 31369.38 with loss1: 411.48, loss2: 46.83 and loss3: 30911.07\n",
      "Epoch [3773], train_loss: 31363.41 with loss1: 409.57, loss2: 46.71 and loss3: 30907.12\n",
      "Epoch [3774], train_loss: 31363.61 with loss1: 413.66, loss2: 46.78 and loss3: 30903.18\n",
      "Epoch [3775], train_loss: 31356.89 with loss1: 411.02, loss2: 46.65 and loss3: 30899.22\n",
      "Epoch [3776], train_loss: 31357.73 with loss1: 415.61, loss2: 46.85 and loss3: 30895.28\n",
      "Epoch [3777], train_loss: 31350.45 with loss1: 412.55, loss2: 46.59 and loss3: 30891.32\n",
      "Epoch [3778], train_loss: 31352.13 with loss1: 417.98, loss2: 46.77 and loss3: 30887.38\n",
      "Epoch [3779], train_loss: 31345.03 with loss1: 414.97, loss2: 46.63 and loss3: 30883.42\n",
      "Epoch [3780], train_loss: 31345.90 with loss1: 419.77, loss2: 46.64 and loss3: 30879.49\n",
      "Epoch [3781], train_loss: 31338.89 with loss1: 416.80, loss2: 46.57 and loss3: 30875.53\n",
      "Epoch [3782], train_loss: 31339.88 with loss1: 421.51, loss2: 46.78 and loss3: 30871.59\n",
      "Epoch [3783], train_loss: 31332.58 with loss1: 418.32, loss2: 46.63 and loss3: 30867.63\n",
      "Epoch [3784], train_loss: 31332.60 with loss1: 422.36, loss2: 46.55 and loss3: 30863.69\n",
      "Epoch [3785], train_loss: 31325.85 with loss1: 419.41, loss2: 46.69 and loss3: 30859.74\n",
      "Epoch [3786], train_loss: 31325.19 with loss1: 422.71, loss2: 46.68 and loss3: 30855.80\n",
      "Epoch [3787], train_loss: 31318.27 with loss1: 419.83, loss2: 46.60 and loss3: 30851.85\n",
      "Epoch [3788], train_loss: 31317.54 with loss1: 423.04, loss2: 46.59 and loss3: 30847.91\n",
      "Epoch [3789], train_loss: 31312.08 with loss1: 421.57, loss2: 46.55 and loss3: 30843.96\n",
      "Epoch [3790], train_loss: 31310.98 with loss1: 424.23, loss2: 46.74 and loss3: 30840.01\n",
      "Epoch [3791], train_loss: 31303.09 with loss1: 420.40, loss2: 46.63 and loss3: 30836.06\n",
      "Epoch [3792], train_loss: 31301.52 with loss1: 422.84, loss2: 46.55 and loss3: 30832.12\n",
      "Epoch [3793], train_loss: 31293.49 with loss1: 418.84, loss2: 46.49 and loss3: 30828.16\n",
      "Epoch [3794], train_loss: 31294.36 with loss1: 423.56, loss2: 46.57 and loss3: 30824.23\n",
      "Epoch [3795], train_loss: 31284.97 with loss1: 418.14, loss2: 46.56 and loss3: 30820.28\n",
      "Epoch [3796], train_loss: 31284.01 with loss1: 421.24, loss2: 46.43 and loss3: 30816.34\n",
      "Epoch [3797], train_loss: 31274.30 with loss1: 415.52, loss2: 46.40 and loss3: 30812.39\n",
      "Epoch [3798], train_loss: 31273.61 with loss1: 418.68, loss2: 46.48 and loss3: 30808.45\n",
      "Epoch [3799], train_loss: 31264.71 with loss1: 413.78, loss2: 46.43 and loss3: 30804.50\n",
      "Epoch [3800], train_loss: 31262.14 with loss1: 415.01, loss2: 46.56 and loss3: 30800.56\n",
      "Epoch [3801], train_loss: 31254.27 with loss1: 411.10, loss2: 46.56 and loss3: 30796.62\n",
      "Epoch [3802], train_loss: 31251.11 with loss1: 412.03, loss2: 46.40 and loss3: 30792.68\n",
      "Epoch [3803], train_loss: 31242.99 with loss1: 408.00, loss2: 46.27 and loss3: 30788.73\n",
      "Epoch [3804], train_loss: 31239.20 with loss1: 408.04, loss2: 46.38 and loss3: 30784.79\n",
      "Epoch [3805], train_loss: 31231.90 with loss1: 404.66, loss2: 46.39 and loss3: 30780.85\n",
      "Epoch [3806], train_loss: 31228.60 with loss1: 405.20, loss2: 46.50 and loss3: 30776.90\n",
      "Epoch [3807], train_loss: 31221.22 with loss1: 401.96, loss2: 46.29 and loss3: 30772.97\n",
      "Epoch [3808], train_loss: 31216.47 with loss1: 401.22, loss2: 46.23 and loss3: 30769.02\n",
      "Epoch [3809], train_loss: 31208.34 with loss1: 396.89, loss2: 46.36 and loss3: 30765.08\n",
      "Epoch [3810], train_loss: 31204.61 with loss1: 397.11, loss2: 46.36 and loss3: 30761.14\n",
      "Epoch [3811], train_loss: 31197.36 with loss1: 393.90, loss2: 46.26 and loss3: 30757.20\n",
      "Epoch [3812], train_loss: 31194.71 with loss1: 395.16, loss2: 46.29 and loss3: 30753.26\n",
      "Epoch [3813], train_loss: 31187.09 with loss1: 391.48, loss2: 46.29 and loss3: 30749.32\n",
      "Epoch [3814], train_loss: 31182.30 with loss1: 390.68, loss2: 46.24 and loss3: 30745.38\n",
      "Epoch [3815], train_loss: 31176.83 with loss1: 389.16, loss2: 46.22 and loss3: 30741.45\n",
      "Epoch [3816], train_loss: 31170.89 with loss1: 387.22, loss2: 46.18 and loss3: 30737.49\n",
      "Epoch [3817], train_loss: 31165.12 with loss1: 385.27, loss2: 46.29 and loss3: 30733.56\n",
      "Epoch [3818], train_loss: 31161.69 with loss1: 385.91, loss2: 46.16 and loss3: 30729.61\n",
      "Epoch [3819], train_loss: 31154.96 with loss1: 383.05, loss2: 46.22 and loss3: 30725.69\n",
      "Epoch [3820], train_loss: 31150.73 with loss1: 382.84, loss2: 46.16 and loss3: 30721.74\n",
      "Epoch [3821], train_loss: 31144.76 with loss1: 380.81, loss2: 46.13 and loss3: 30717.81\n",
      "Epoch [3822], train_loss: 31140.44 with loss1: 380.49, loss2: 46.09 and loss3: 30713.86\n",
      "Epoch [3823], train_loss: 31135.05 with loss1: 379.00, loss2: 46.12 and loss3: 30709.93\n",
      "Epoch [3824], train_loss: 31131.75 with loss1: 379.72, loss2: 46.04 and loss3: 30705.99\n",
      "Epoch [3825], train_loss: 31125.98 with loss1: 377.90, loss2: 46.03 and loss3: 30702.05\n",
      "Epoch [3826], train_loss: 31122.94 with loss1: 378.80, loss2: 46.02 and loss3: 30698.11\n",
      "Epoch [3827], train_loss: 31117.94 with loss1: 377.78, loss2: 45.97 and loss3: 30694.18\n",
      "Epoch [3828], train_loss: 31112.99 with loss1: 376.67, loss2: 46.08 and loss3: 30690.24\n",
      "Epoch [3829], train_loss: 31108.16 with loss1: 375.86, loss2: 45.99 and loss3: 30686.31\n",
      "Epoch [3830], train_loss: 31105.13 with loss1: 376.82, loss2: 45.94 and loss3: 30682.37\n",
      "Epoch [3831], train_loss: 31100.11 with loss1: 375.70, loss2: 45.97 and loss3: 30678.44\n",
      "Epoch [3832], train_loss: 31097.13 with loss1: 376.54, loss2: 46.08 and loss3: 30674.51\n",
      "Epoch [3833], train_loss: 31091.54 with loss1: 375.10, loss2: 45.87 and loss3: 30670.57\n",
      "Epoch [3834], train_loss: 31088.26 with loss1: 375.80, loss2: 45.81 and loss3: 30666.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3835], train_loss: 31082.71 with loss1: 373.92, loss2: 46.09 and loss3: 30662.70\n",
      "Epoch [3836], train_loss: 31080.37 with loss1: 375.67, loss2: 45.92 and loss3: 30658.78\n",
      "Epoch [3837], train_loss: 31075.26 with loss1: 374.41, loss2: 46.01 and loss3: 30654.84\n",
      "Epoch [3838], train_loss: 31070.80 with loss1: 373.96, loss2: 45.92 and loss3: 30650.92\n",
      "Epoch [3839], train_loss: 31067.39 with loss1: 374.44, loss2: 45.98 and loss3: 30646.98\n",
      "Epoch [3840], train_loss: 31062.80 with loss1: 373.94, loss2: 45.82 and loss3: 30643.05\n",
      "Epoch [3841], train_loss: 31059.95 with loss1: 375.01, loss2: 45.83 and loss3: 30639.12\n",
      "Epoch [3842], train_loss: 31056.57 with loss1: 375.37, loss2: 46.00 and loss3: 30635.19\n",
      "Epoch [3843], train_loss: 31051.38 with loss1: 374.36, loss2: 45.77 and loss3: 30631.25\n",
      "Epoch [3844], train_loss: 31047.73 with loss1: 374.54, loss2: 45.85 and loss3: 30627.33\n",
      "Epoch [3845], train_loss: 31043.07 with loss1: 373.91, loss2: 45.77 and loss3: 30623.39\n",
      "Epoch [3846], train_loss: 31039.60 with loss1: 374.16, loss2: 45.96 and loss3: 30619.48\n",
      "Epoch [3847], train_loss: 31035.01 with loss1: 373.59, loss2: 45.89 and loss3: 30615.54\n",
      "Epoch [3848], train_loss: 31031.75 with loss1: 374.41, loss2: 45.73 and loss3: 30611.62\n",
      "Epoch [3849], train_loss: 31027.82 with loss1: 374.36, loss2: 45.78 and loss3: 30607.68\n",
      "Epoch [3850], train_loss: 31024.75 with loss1: 375.31, loss2: 45.68 and loss3: 30603.76\n",
      "Epoch [3851], train_loss: 31019.76 with loss1: 374.19, loss2: 45.75 and loss3: 30599.82\n",
      "Epoch [3852], train_loss: 31017.26 with loss1: 375.43, loss2: 45.92 and loss3: 30595.91\n",
      "Epoch [3853], train_loss: 31011.71 with loss1: 374.11, loss2: 45.64 and loss3: 30591.96\n",
      "Epoch [3854], train_loss: 31010.32 with loss1: 376.56, loss2: 45.71 and loss3: 30588.05\n",
      "Epoch [3855], train_loss: 31006.02 with loss1: 376.20, loss2: 45.71 and loss3: 30584.10\n",
      "Epoch [3856], train_loss: 31001.90 with loss1: 376.18, loss2: 45.53 and loss3: 30580.20\n",
      "Epoch [3857], train_loss: 30996.35 with loss1: 374.58, loss2: 45.53 and loss3: 30576.25\n",
      "Epoch [3858], train_loss: 30993.41 with loss1: 375.54, loss2: 45.53 and loss3: 30572.34\n",
      "Epoch [3859], train_loss: 30989.82 with loss1: 375.87, loss2: 45.55 and loss3: 30568.39\n",
      "Epoch [3860], train_loss: 30986.63 with loss1: 376.67, loss2: 45.47 and loss3: 30564.48\n",
      "Epoch [3861], train_loss: 30981.34 with loss1: 375.30, loss2: 45.50 and loss3: 30560.54\n",
      "Epoch [3862], train_loss: 30979.04 with loss1: 376.91, loss2: 45.50 and loss3: 30556.62\n",
      "Epoch [3863], train_loss: 30974.70 with loss1: 376.38, loss2: 45.63 and loss3: 30552.69\n",
      "Epoch [3864], train_loss: 30971.40 with loss1: 377.13, loss2: 45.50 and loss3: 30548.77\n",
      "Epoch [3865], train_loss: 30967.35 with loss1: 376.99, loss2: 45.52 and loss3: 30544.84\n",
      "Epoch [3866], train_loss: 30962.70 with loss1: 376.38, loss2: 45.40 and loss3: 30540.91\n",
      "Epoch [3867], train_loss: 30959.24 with loss1: 376.90, loss2: 45.35 and loss3: 30536.99\n",
      "Epoch [3868], train_loss: 30955.24 with loss1: 376.76, loss2: 45.42 and loss3: 30533.06\n",
      "Epoch [3869], train_loss: 30951.47 with loss1: 376.90, loss2: 45.43 and loss3: 30529.14\n",
      "Epoch [3870], train_loss: 30949.72 with loss1: 379.08, loss2: 45.43 and loss3: 30525.21\n",
      "Epoch [3871], train_loss: 30944.24 with loss1: 377.52, loss2: 45.43 and loss3: 30521.29\n",
      "Epoch [3872], train_loss: 30941.34 with loss1: 378.72, loss2: 45.26 and loss3: 30517.36\n",
      "Epoch [3873], train_loss: 30937.61 with loss1: 378.75, loss2: 45.42 and loss3: 30513.44\n",
      "Epoch [3874], train_loss: 30934.20 with loss1: 379.18, loss2: 45.49 and loss3: 30509.52\n",
      "Epoch [3875], train_loss: 30929.31 with loss1: 378.30, loss2: 45.42 and loss3: 30505.59\n",
      "Epoch [3876], train_loss: 30925.73 with loss1: 378.79, loss2: 45.27 and loss3: 30501.67\n",
      "Epoch [3877], train_loss: 30920.96 with loss1: 377.79, loss2: 45.43 and loss3: 30497.75\n",
      "Epoch [3878], train_loss: 30917.84 with loss1: 378.71, loss2: 45.30 and loss3: 30493.83\n",
      "Epoch [3879], train_loss: 30912.94 with loss1: 377.73, loss2: 45.31 and loss3: 30489.90\n",
      "Epoch [3880], train_loss: 30909.67 with loss1: 378.39, loss2: 45.30 and loss3: 30485.99\n",
      "Epoch [3881], train_loss: 30905.47 with loss1: 378.07, loss2: 45.34 and loss3: 30482.06\n",
      "Epoch [3882], train_loss: 30901.32 with loss1: 377.94, loss2: 45.23 and loss3: 30478.15\n",
      "Epoch [3883], train_loss: 30896.69 with loss1: 377.22, loss2: 45.25 and loss3: 30474.22\n",
      "Epoch [3884], train_loss: 30893.12 with loss1: 377.70, loss2: 45.11 and loss3: 30470.31\n",
      "Epoch [3885], train_loss: 30889.54 with loss1: 377.72, loss2: 45.44 and loss3: 30466.38\n",
      "Epoch [3886], train_loss: 30885.50 with loss1: 377.85, loss2: 45.18 and loss3: 30462.47\n",
      "Epoch [3887], train_loss: 30880.64 with loss1: 376.80, loss2: 45.29 and loss3: 30458.55\n",
      "Epoch [3888], train_loss: 30877.48 with loss1: 377.70, loss2: 45.14 and loss3: 30454.64\n",
      "Epoch [3889], train_loss: 30873.58 with loss1: 377.67, loss2: 45.20 and loss3: 30450.71\n",
      "Epoch [3890], train_loss: 30871.21 with loss1: 379.29, loss2: 45.12 and loss3: 30446.80\n",
      "Epoch [3891], train_loss: 30866.46 with loss1: 378.30, loss2: 45.28 and loss3: 30442.88\n",
      "Epoch [3892], train_loss: 30863.65 with loss1: 379.50, loss2: 45.18 and loss3: 30438.97\n",
      "Epoch [3893], train_loss: 30858.13 with loss1: 377.83, loss2: 45.25 and loss3: 30435.05\n",
      "Epoch [3894], train_loss: 30854.27 with loss1: 378.12, loss2: 45.01 and loss3: 30431.14\n",
      "Epoch [3895], train_loss: 30851.85 with loss1: 379.45, loss2: 45.19 and loss3: 30427.21\n",
      "Epoch [3896], train_loss: 30848.72 with loss1: 380.23, loss2: 45.18 and loss3: 30423.31\n",
      "Epoch [3897], train_loss: 30842.95 with loss1: 378.57, loss2: 45.00 and loss3: 30419.38\n",
      "Epoch [3898], train_loss: 30842.01 with loss1: 381.52, loss2: 45.00 and loss3: 30415.48\n",
      "Epoch [3899], train_loss: 30835.45 with loss1: 378.88, loss2: 45.02 and loss3: 30411.55\n",
      "Epoch [3900], train_loss: 30833.44 with loss1: 380.80, loss2: 44.98 and loss3: 30407.66\n",
      "Epoch [3901], train_loss: 30829.76 with loss1: 381.03, loss2: 45.00 and loss3: 30403.73\n",
      "Epoch [3902], train_loss: 30826.73 with loss1: 381.90, loss2: 45.00 and loss3: 30399.82\n",
      "Epoch [3903], train_loss: 30822.71 with loss1: 381.68, loss2: 45.13 and loss3: 30395.90\n",
      "Epoch [3904], train_loss: 30819.31 with loss1: 382.41, loss2: 44.91 and loss3: 30392.00\n",
      "Epoch [3905], train_loss: 30815.05 with loss1: 382.05, loss2: 44.93 and loss3: 30388.07\n",
      "Epoch [3906], train_loss: 30812.51 with loss1: 383.40, loss2: 44.94 and loss3: 30384.17\n",
      "Epoch [3907], train_loss: 30807.77 with loss1: 382.65, loss2: 44.87 and loss3: 30380.25\n",
      "Epoch [3908], train_loss: 30806.25 with loss1: 385.03, loss2: 44.87 and loss3: 30376.34\n",
      "Epoch [3909], train_loss: 30801.67 with loss1: 384.35, loss2: 44.90 and loss3: 30372.42\n",
      "Epoch [3910], train_loss: 30800.18 with loss1: 386.76, loss2: 44.91 and loss3: 30368.52\n",
      "Epoch [3911], train_loss: 30795.73 with loss1: 386.27, loss2: 44.86 and loss3: 30364.60\n",
      "Epoch [3912], train_loss: 30793.64 with loss1: 388.12, loss2: 44.82 and loss3: 30360.69\n",
      "Epoch [3913], train_loss: 30788.33 with loss1: 386.74, loss2: 44.81 and loss3: 30356.78\n",
      "Epoch [3914], train_loss: 30786.75 with loss1: 389.09, loss2: 44.79 and loss3: 30352.88\n",
      "Epoch [3915], train_loss: 30782.50 with loss1: 388.70, loss2: 44.85 and loss3: 30348.96\n",
      "Epoch [3916], train_loss: 30781.35 with loss1: 391.51, loss2: 44.78 and loss3: 30345.05\n",
      "Epoch [3917], train_loss: 30776.58 with loss1: 390.59, loss2: 44.85 and loss3: 30341.13\n",
      "Epoch [3918], train_loss: 30776.00 with loss1: 393.92, loss2: 44.84 and loss3: 30337.24\n",
      "Epoch [3919], train_loss: 30769.66 with loss1: 391.58, loss2: 44.76 and loss3: 30333.31\n",
      "Epoch [3920], train_loss: 30769.40 with loss1: 395.10, loss2: 44.88 and loss3: 30329.42\n",
      "Epoch [3921], train_loss: 30763.58 with loss1: 393.46, loss2: 44.63 and loss3: 30325.49\n",
      "Epoch [3922], train_loss: 30762.98 with loss1: 396.66, loss2: 44.73 and loss3: 30321.59\n",
      "Epoch [3923], train_loss: 30757.43 with loss1: 395.09, loss2: 44.67 and loss3: 30317.67\n",
      "Epoch [3924], train_loss: 30755.96 with loss1: 397.49, loss2: 44.70 and loss3: 30313.77\n",
      "Epoch [3925], train_loss: 30750.95 with loss1: 396.52, loss2: 44.58 and loss3: 30309.85\n",
      "Epoch [3926], train_loss: 30750.02 with loss1: 399.27, loss2: 44.80 and loss3: 30305.95\n",
      "Epoch [3927], train_loss: 30743.21 with loss1: 396.49, loss2: 44.68 and loss3: 30302.04\n",
      "Epoch [3928], train_loss: 30742.29 with loss1: 399.48, loss2: 44.68 and loss3: 30298.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3929], train_loss: 30735.37 with loss1: 396.56, loss2: 44.59 and loss3: 30294.22\n",
      "Epoch [3930], train_loss: 30737.20 with loss1: 402.16, loss2: 44.72 and loss3: 30290.31\n",
      "Epoch [3931], train_loss: 30730.72 with loss1: 399.78, loss2: 44.53 and loss3: 30286.41\n",
      "Epoch [3932], train_loss: 30729.22 with loss1: 402.12, loss2: 44.61 and loss3: 30282.49\n",
      "Epoch [3933], train_loss: 30722.84 with loss1: 399.73, loss2: 44.51 and loss3: 30278.59\n",
      "Epoch [3934], train_loss: 30721.54 with loss1: 402.26, loss2: 44.61 and loss3: 30274.67\n",
      "Epoch [3935], train_loss: 30714.48 with loss1: 399.37, loss2: 44.33 and loss3: 30270.78\n",
      "Epoch [3936], train_loss: 30714.38 with loss1: 402.88, loss2: 44.65 and loss3: 30266.86\n",
      "Epoch [3937], train_loss: 30706.16 with loss1: 398.72, loss2: 44.47 and loss3: 30262.96\n",
      "Epoch [3938], train_loss: 30706.64 with loss1: 403.00, loss2: 44.59 and loss3: 30259.05\n",
      "Epoch [3939], train_loss: 30698.70 with loss1: 399.24, loss2: 44.32 and loss3: 30255.14\n",
      "Epoch [3940], train_loss: 30697.22 with loss1: 401.41, loss2: 44.57 and loss3: 30251.23\n",
      "Epoch [3941], train_loss: 30690.86 with loss1: 398.94, loss2: 44.60 and loss3: 30247.32\n",
      "Epoch [3942], train_loss: 30689.30 with loss1: 401.32, loss2: 44.56 and loss3: 30243.42\n",
      "Epoch [3943], train_loss: 30680.80 with loss1: 396.90, loss2: 44.40 and loss3: 30239.51\n",
      "Epoch [3944], train_loss: 30679.25 with loss1: 399.12, loss2: 44.53 and loss3: 30235.61\n",
      "Epoch [3945], train_loss: 30671.40 with loss1: 395.41, loss2: 44.30 and loss3: 30231.69\n",
      "Epoch [3946], train_loss: 30671.96 with loss1: 399.68, loss2: 44.49 and loss3: 30227.80\n",
      "Epoch [3947], train_loss: 30662.88 with loss1: 394.68, loss2: 44.34 and loss3: 30223.87\n",
      "Epoch [3948], train_loss: 30660.64 with loss1: 396.21, loss2: 44.44 and loss3: 30219.99\n",
      "Epoch [3949], train_loss: 30652.98 with loss1: 392.58, loss2: 44.34 and loss3: 30216.06\n",
      "Epoch [3950], train_loss: 30649.86 with loss1: 393.35, loss2: 44.33 and loss3: 30212.18\n",
      "Epoch [3951], train_loss: 30643.03 with loss1: 390.39, loss2: 44.39 and loss3: 30208.25\n",
      "Epoch [3952], train_loss: 30640.86 with loss1: 391.90, loss2: 44.59 and loss3: 30204.37\n",
      "Epoch [3953], train_loss: 30634.43 with loss1: 389.82, loss2: 44.16 and loss3: 30200.45\n",
      "Epoch [3954], train_loss: 30631.32 with loss1: 390.40, loss2: 44.36 and loss3: 30196.56\n",
      "Epoch [3955], train_loss: 30623.65 with loss1: 386.89, loss2: 44.12 and loss3: 30192.64\n",
      "Epoch [3956], train_loss: 30621.54 with loss1: 388.51, loss2: 44.28 and loss3: 30188.75\n",
      "Epoch [3957], train_loss: 30614.26 with loss1: 385.21, loss2: 44.22 and loss3: 30184.83\n",
      "Epoch [3958], train_loss: 30610.46 with loss1: 385.19, loss2: 44.33 and loss3: 30180.94\n",
      "Epoch [3959], train_loss: 30605.60 with loss1: 384.29, loss2: 44.28 and loss3: 30177.03\n",
      "Epoch [3960], train_loss: 30602.38 with loss1: 385.01, loss2: 44.24 and loss3: 30173.13\n",
      "Epoch [3961], train_loss: 30595.95 with loss1: 382.67, loss2: 44.05 and loss3: 30169.23\n",
      "Epoch [3962], train_loss: 30592.16 with loss1: 382.59, loss2: 44.25 and loss3: 30165.32\n",
      "Epoch [3963], train_loss: 30586.13 with loss1: 380.54, loss2: 44.17 and loss3: 30161.42\n",
      "Epoch [3964], train_loss: 30581.86 with loss1: 380.23, loss2: 44.12 and loss3: 30157.52\n",
      "Epoch [3965], train_loss: 30575.89 with loss1: 378.05, loss2: 44.23 and loss3: 30153.61\n",
      "Epoch [3966], train_loss: 30572.64 with loss1: 378.76, loss2: 44.18 and loss3: 30149.71\n",
      "Epoch [3967], train_loss: 30567.26 with loss1: 377.36, loss2: 44.10 and loss3: 30145.80\n",
      "Epoch [3968], train_loss: 30563.32 with loss1: 377.30, loss2: 44.11 and loss3: 30141.91\n",
      "Epoch [3969], train_loss: 30558.01 with loss1: 375.87, loss2: 44.14 and loss3: 30138.00\n",
      "Epoch [3970], train_loss: 30553.39 with loss1: 375.14, loss2: 44.13 and loss3: 30134.12\n",
      "Epoch [3971], train_loss: 30547.80 with loss1: 373.65, loss2: 43.95 and loss3: 30130.20\n",
      "Epoch [3972], train_loss: 30543.96 with loss1: 373.47, loss2: 44.18 and loss3: 30126.31\n",
      "Epoch [3973], train_loss: 30538.11 with loss1: 371.84, loss2: 43.87 and loss3: 30122.40\n",
      "Epoch [3974], train_loss: 30534.81 with loss1: 372.11, loss2: 44.19 and loss3: 30118.52\n",
      "Epoch [3975], train_loss: 30530.09 with loss1: 371.45, loss2: 44.04 and loss3: 30114.60\n",
      "Epoch [3976], train_loss: 30525.64 with loss1: 370.93, loss2: 44.00 and loss3: 30110.71\n",
      "Epoch [3977], train_loss: 30519.95 with loss1: 369.20, loss2: 43.95 and loss3: 30106.80\n",
      "Epoch [3978], train_loss: 30516.75 with loss1: 369.78, loss2: 44.06 and loss3: 30102.91\n",
      "Epoch [3979], train_loss: 30511.69 with loss1: 368.80, loss2: 43.89 and loss3: 30099.00\n",
      "Epoch [3980], train_loss: 30507.48 with loss1: 368.47, loss2: 43.89 and loss3: 30095.11\n",
      "Epoch [3981], train_loss: 30501.70 with loss1: 366.66, loss2: 43.84 and loss3: 30091.20\n",
      "Epoch [3982], train_loss: 30499.07 with loss1: 367.68, loss2: 44.07 and loss3: 30087.31\n",
      "Epoch [3983], train_loss: 30493.89 with loss1: 366.67, loss2: 43.81 and loss3: 30083.40\n",
      "Epoch [3984], train_loss: 30491.59 with loss1: 368.27, loss2: 43.81 and loss3: 30079.51\n",
      "Epoch [3985], train_loss: 30485.97 with loss1: 366.65, loss2: 43.72 and loss3: 30075.61\n",
      "Epoch [3986], train_loss: 30482.33 with loss1: 366.68, loss2: 43.93 and loss3: 30071.71\n",
      "Epoch [3987], train_loss: 30477.71 with loss1: 366.12, loss2: 43.78 and loss3: 30067.82\n",
      "Epoch [3988], train_loss: 30474.62 with loss1: 366.72, loss2: 43.98 and loss3: 30063.92\n",
      "Epoch [3989], train_loss: 30469.78 with loss1: 366.03, loss2: 43.72 and loss3: 30060.02\n",
      "Epoch [3990], train_loss: 30467.76 with loss1: 367.76, loss2: 43.88 and loss3: 30056.13\n",
      "Epoch [3991], train_loss: 30461.86 with loss1: 365.90, loss2: 43.72 and loss3: 30052.23\n",
      "Epoch [3992], train_loss: 30458.30 with loss1: 366.21, loss2: 43.75 and loss3: 30048.34\n",
      "Epoch [3993], train_loss: 30453.88 with loss1: 365.65, loss2: 43.79 and loss3: 30044.44\n",
      "Epoch [3994], train_loss: 30448.79 with loss1: 364.51, loss2: 43.73 and loss3: 30040.54\n",
      "Epoch [3995], train_loss: 30444.95 with loss1: 364.63, loss2: 43.67 and loss3: 30036.65\n",
      "Epoch [3996], train_loss: 30440.88 with loss1: 364.41, loss2: 43.72 and loss3: 30032.75\n",
      "Epoch [3997], train_loss: 30437.01 with loss1: 364.43, loss2: 43.71 and loss3: 30028.86\n",
      "Epoch [3998], train_loss: 30432.52 with loss1: 363.83, loss2: 43.72 and loss3: 30024.97\n",
      "Epoch [3999], train_loss: 30428.51 with loss1: 363.90, loss2: 43.54 and loss3: 30021.07\n",
      "Epoch [4000], train_loss: 30425.34 with loss1: 364.53, loss2: 43.63 and loss3: 30017.18\n",
      "Epoch [4001], train_loss: 30420.03 with loss1: 363.18, loss2: 43.55 and loss3: 30013.29\n",
      "Epoch [4002], train_loss: 30417.98 with loss1: 364.94, loss2: 43.65 and loss3: 30009.39\n",
      "Epoch [4003], train_loss: 30414.06 with loss1: 365.03, loss2: 43.53 and loss3: 30005.51\n",
      "Epoch [4004], train_loss: 30410.68 with loss1: 365.41, loss2: 43.65 and loss3: 30001.61\n",
      "Epoch [4005], train_loss: 30404.23 with loss1: 363.09, loss2: 43.41 and loss3: 29997.72\n",
      "Epoch [4006], train_loss: 30402.19 with loss1: 364.64, loss2: 43.72 and loss3: 29993.83\n",
      "Epoch [4007], train_loss: 30396.92 with loss1: 363.54, loss2: 43.43 and loss3: 29989.94\n",
      "Epoch [4008], train_loss: 30394.33 with loss1: 364.75, loss2: 43.54 and loss3: 29986.04\n",
      "Epoch [4009], train_loss: 30391.02 with loss1: 365.38, loss2: 43.48 and loss3: 29982.16\n",
      "Epoch [4010], train_loss: 30387.45 with loss1: 365.64, loss2: 43.55 and loss3: 29978.25\n",
      "Epoch [4011], train_loss: 30382.66 with loss1: 364.89, loss2: 43.38 and loss3: 29974.38\n",
      "Epoch [4012], train_loss: 30379.43 with loss1: 365.51, loss2: 43.45 and loss3: 29970.47\n",
      "Epoch [4013], train_loss: 30375.68 with loss1: 365.67, loss2: 43.42 and loss3: 29966.59\n",
      "Epoch [4014], train_loss: 30372.26 with loss1: 365.91, loss2: 43.67 and loss3: 29962.69\n",
      "Epoch [4015], train_loss: 30367.23 with loss1: 364.91, loss2: 43.51 and loss3: 29958.81\n",
      "Epoch [4016], train_loss: 30365.87 with loss1: 367.48, loss2: 43.48 and loss3: 29954.91\n",
      "Epoch [4017], train_loss: 30360.94 with loss1: 366.61, loss2: 43.29 and loss3: 29951.03\n",
      "Epoch [4018], train_loss: 30357.60 with loss1: 366.93, loss2: 43.53 and loss3: 29947.13\n",
      "Epoch [4019], train_loss: 30354.01 with loss1: 367.31, loss2: 43.45 and loss3: 29943.25\n",
      "Epoch [4020], train_loss: 30350.92 with loss1: 368.27, loss2: 43.29 and loss3: 29939.36\n",
      "Epoch [4021], train_loss: 30347.00 with loss1: 368.23, loss2: 43.30 and loss3: 29935.47\n",
      "Epoch [4022], train_loss: 30343.95 with loss1: 369.05, loss2: 43.32 and loss3: 29931.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4023], train_loss: 30340.20 with loss1: 369.40, loss2: 43.11 and loss3: 29927.69\n",
      "Epoch [4024], train_loss: 30337.44 with loss1: 370.37, loss2: 43.26 and loss3: 29923.81\n",
      "Epoch [4025], train_loss: 30333.97 with loss1: 370.80, loss2: 43.25 and loss3: 29919.92\n",
      "Epoch [4026], train_loss: 30331.25 with loss1: 371.76, loss2: 43.47 and loss3: 29916.03\n",
      "Epoch [4027], train_loss: 30326.95 with loss1: 371.52, loss2: 43.28 and loss3: 29912.15\n",
      "Epoch [4028], train_loss: 30323.67 with loss1: 372.16, loss2: 43.26 and loss3: 29908.25\n",
      "Epoch [4029], train_loss: 30319.48 with loss1: 371.90, loss2: 43.21 and loss3: 29904.38\n",
      "Epoch [4030], train_loss: 30316.55 with loss1: 372.78, loss2: 43.30 and loss3: 29900.47\n",
      "Epoch [4031], train_loss: 30312.29 with loss1: 372.58, loss2: 43.11 and loss3: 29896.60\n",
      "Epoch [4032], train_loss: 30309.40 with loss1: 373.54, loss2: 43.16 and loss3: 29892.70\n",
      "Epoch [4033], train_loss: 30305.61 with loss1: 373.59, loss2: 43.19 and loss3: 29888.83\n",
      "Epoch [4034], train_loss: 30303.59 with loss1: 375.24, loss2: 43.42 and loss3: 29884.93\n",
      "Epoch [4035], train_loss: 30298.04 with loss1: 373.94, loss2: 43.05 and loss3: 29881.06\n",
      "Epoch [4036], train_loss: 30296.17 with loss1: 375.72, loss2: 43.28 and loss3: 29877.16\n",
      "Epoch [4037], train_loss: 30292.19 with loss1: 375.91, loss2: 42.98 and loss3: 29873.29\n",
      "Epoch [4038], train_loss: 30289.02 with loss1: 376.48, loss2: 43.15 and loss3: 29869.39\n",
      "Epoch [4039], train_loss: 30284.39 with loss1: 375.84, loss2: 43.03 and loss3: 29865.53\n",
      "Epoch [4040], train_loss: 30282.95 with loss1: 378.10, loss2: 43.22 and loss3: 29861.63\n",
      "Epoch [4041], train_loss: 30277.35 with loss1: 376.55, loss2: 43.04 and loss3: 29857.76\n",
      "Epoch [4042], train_loss: 30275.96 with loss1: 379.08, loss2: 43.02 and loss3: 29853.87\n",
      "Epoch [4043], train_loss: 30271.34 with loss1: 378.33, loss2: 43.01 and loss3: 29850.00\n",
      "Epoch [4044], train_loss: 30269.21 with loss1: 380.10, loss2: 43.01 and loss3: 29846.10\n",
      "Epoch [4045], train_loss: 30264.81 with loss1: 379.62, loss2: 42.96 and loss3: 29842.24\n",
      "Epoch [4046], train_loss: 30262.34 with loss1: 381.07, loss2: 42.93 and loss3: 29838.34\n",
      "Epoch [4047], train_loss: 30257.70 with loss1: 380.33, loss2: 42.90 and loss3: 29834.47\n",
      "Epoch [4048], train_loss: 30256.32 with loss1: 382.78, loss2: 42.96 and loss3: 29830.59\n",
      "Epoch [4049], train_loss: 30251.08 with loss1: 381.43, loss2: 42.94 and loss3: 29826.71\n",
      "Epoch [4050], train_loss: 30250.73 with loss1: 384.86, loss2: 43.03 and loss3: 29822.83\n",
      "Epoch [4051], train_loss: 30245.08 with loss1: 383.08, loss2: 43.04 and loss3: 29818.96\n",
      "Epoch [4052], train_loss: 30243.51 with loss1: 385.49, loss2: 42.96 and loss3: 29815.06\n",
      "Epoch [4053], train_loss: 30236.86 with loss1: 382.80, loss2: 42.86 and loss3: 29811.20\n",
      "Epoch [4054], train_loss: 30235.91 with loss1: 385.69, loss2: 42.91 and loss3: 29807.31\n",
      "Epoch [4055], train_loss: 30230.94 with loss1: 384.57, loss2: 42.93 and loss3: 29803.44\n",
      "Epoch [4056], train_loss: 30229.26 with loss1: 386.69, loss2: 43.02 and loss3: 29799.55\n",
      "Epoch [4057], train_loss: 30222.63 with loss1: 384.12, loss2: 42.83 and loss3: 29795.69\n",
      "Epoch [4058], train_loss: 30221.16 with loss1: 386.39, loss2: 42.97 and loss3: 29791.80\n",
      "Epoch [4059], train_loss: 30215.36 with loss1: 384.42, loss2: 43.01 and loss3: 29787.93\n",
      "Epoch [4060], train_loss: 30213.05 with loss1: 386.08, loss2: 42.92 and loss3: 29784.04\n",
      "Epoch [4061], train_loss: 30207.76 with loss1: 384.60, loss2: 42.98 and loss3: 29780.18\n",
      "Epoch [4062], train_loss: 30207.39 with loss1: 388.35, loss2: 42.76 and loss3: 29776.29\n",
      "Epoch [4063], train_loss: 30200.73 with loss1: 385.44, loss2: 42.86 and loss3: 29772.42\n",
      "Epoch [4064], train_loss: 30200.19 with loss1: 388.62, loss2: 43.02 and loss3: 29768.54\n",
      "Epoch [4065], train_loss: 30193.50 with loss1: 386.01, loss2: 42.82 and loss3: 29764.67\n",
      "Epoch [4066], train_loss: 30192.73 with loss1: 389.03, loss2: 42.92 and loss3: 29760.79\n",
      "Epoch [4067], train_loss: 30184.90 with loss1: 385.24, loss2: 42.75 and loss3: 29756.91\n",
      "Epoch [4068], train_loss: 30183.20 with loss1: 387.37, loss2: 42.79 and loss3: 29753.04\n",
      "Epoch [4069], train_loss: 30175.68 with loss1: 383.81, loss2: 42.71 and loss3: 29749.16\n",
      "Epoch [4070], train_loss: 30174.55 with loss1: 386.60, loss2: 42.67 and loss3: 29745.29\n",
      "Epoch [4071], train_loss: 30166.76 with loss1: 382.46, loss2: 42.88 and loss3: 29741.42\n",
      "Epoch [4072], train_loss: 30164.87 with loss1: 384.53, loss2: 42.81 and loss3: 29737.54\n",
      "Epoch [4073], train_loss: 30158.85 with loss1: 382.33, loss2: 42.85 and loss3: 29733.67\n",
      "Epoch [4074], train_loss: 30156.87 with loss1: 384.29, loss2: 42.79 and loss3: 29729.78\n",
      "Epoch [4075], train_loss: 30148.59 with loss1: 379.94, loss2: 42.73 and loss3: 29725.93\n",
      "Epoch [4076], train_loss: 30145.06 with loss1: 380.43, loss2: 42.60 and loss3: 29722.03\n",
      "Epoch [4077], train_loss: 30137.73 with loss1: 377.00, loss2: 42.56 and loss3: 29718.17\n",
      "Epoch [4078], train_loss: 30134.80 with loss1: 377.83, loss2: 42.67 and loss3: 29714.29\n",
      "Epoch [4079], train_loss: 30128.31 with loss1: 375.17, loss2: 42.71 and loss3: 29710.43\n",
      "Epoch [4080], train_loss: 30125.11 with loss1: 375.83, loss2: 42.73 and loss3: 29706.54\n",
      "Epoch [4081], train_loss: 30117.29 with loss1: 372.02, loss2: 42.60 and loss3: 29702.68\n",
      "Epoch [4082], train_loss: 30113.29 with loss1: 371.94, loss2: 42.55 and loss3: 29698.80\n",
      "Epoch [4083], train_loss: 30106.87 with loss1: 369.27, loss2: 42.66 and loss3: 29694.94\n",
      "Epoch [4084], train_loss: 30102.93 with loss1: 369.25, loss2: 42.63 and loss3: 29691.05\n",
      "Epoch [4085], train_loss: 30097.81 with loss1: 368.03, loss2: 42.58 and loss3: 29687.20\n",
      "Epoch [4086], train_loss: 30092.58 with loss1: 366.73, loss2: 42.54 and loss3: 29683.30\n",
      "Epoch [4087], train_loss: 30087.35 with loss1: 365.45, loss2: 42.45 and loss3: 29679.45\n",
      "Epoch [4088], train_loss: 30083.17 with loss1: 365.16, loss2: 42.45 and loss3: 29675.57\n",
      "Epoch [4089], train_loss: 30077.44 with loss1: 363.22, loss2: 42.51 and loss3: 29671.71\n",
      "Epoch [4090], train_loss: 30073.58 with loss1: 363.23, loss2: 42.52 and loss3: 29667.83\n",
      "Epoch [4091], train_loss: 30067.78 with loss1: 361.32, loss2: 42.50 and loss3: 29663.97\n",
      "Epoch [4092], train_loss: 30063.68 with loss1: 361.17, loss2: 42.42 and loss3: 29660.09\n",
      "Epoch [4093], train_loss: 30059.36 with loss1: 360.64, loss2: 42.49 and loss3: 29656.23\n",
      "Epoch [4094], train_loss: 30054.49 with loss1: 359.81, loss2: 42.34 and loss3: 29652.35\n",
      "Epoch [4095], train_loss: 30050.01 with loss1: 359.07, loss2: 42.46 and loss3: 29648.49\n",
      "Epoch [4096], train_loss: 30046.87 with loss1: 359.79, loss2: 42.46 and loss3: 29644.61\n",
      "Epoch [4097], train_loss: 30041.45 with loss1: 358.30, loss2: 42.40 and loss3: 29640.74\n",
      "Epoch [4098], train_loss: 30038.36 with loss1: 359.07, loss2: 42.41 and loss3: 29636.88\n",
      "Epoch [4099], train_loss: 30032.64 with loss1: 357.29, loss2: 42.34 and loss3: 29633.00\n",
      "Epoch [4100], train_loss: 30029.21 with loss1: 357.73, loss2: 42.34 and loss3: 29629.14\n",
      "Epoch [4101], train_loss: 30023.95 with loss1: 356.30, loss2: 42.39 and loss3: 29625.26\n",
      "Epoch [4102], train_loss: 30021.90 with loss1: 358.09, loss2: 42.40 and loss3: 29621.40\n",
      "Epoch [4103], train_loss: 30015.98 with loss1: 356.18, loss2: 42.27 and loss3: 29617.53\n",
      "Epoch [4104], train_loss: 30013.29 with loss1: 357.30, loss2: 42.32 and loss3: 29613.67\n",
      "Epoch [4105], train_loss: 30008.44 with loss1: 356.30, loss2: 42.34 and loss3: 29609.80\n",
      "Epoch [4106], train_loss: 30004.51 with loss1: 356.21, loss2: 42.37 and loss3: 29605.94\n",
      "Epoch [4107], train_loss: 30000.90 with loss1: 356.68, loss2: 42.15 and loss3: 29602.07\n",
      "Epoch [4108], train_loss: 29996.43 with loss1: 355.97, loss2: 42.24 and loss3: 29598.21\n",
      "Epoch [4109], train_loss: 29991.25 with loss1: 354.65, loss2: 42.26 and loss3: 29594.34\n",
      "Epoch [4110], train_loss: 29987.79 with loss1: 354.97, loss2: 42.33 and loss3: 29590.49\n",
      "Epoch [4111], train_loss: 29983.90 with loss1: 355.09, loss2: 42.20 and loss3: 29586.62\n",
      "Epoch [4112], train_loss: 29980.41 with loss1: 355.44, loss2: 42.20 and loss3: 29582.76\n",
      "Epoch [4113], train_loss: 29975.37 with loss1: 354.31, loss2: 42.17 and loss3: 29578.89\n",
      "Epoch [4114], train_loss: 29972.84 with loss1: 355.51, loss2: 42.29 and loss3: 29575.04\n",
      "Epoch [4115], train_loss: 29968.73 with loss1: 355.48, loss2: 42.08 and loss3: 29571.17\n",
      "Epoch [4116], train_loss: 29964.66 with loss1: 355.24, loss2: 42.12 and loss3: 29567.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4117], train_loss: 29960.01 with loss1: 354.43, loss2: 42.12 and loss3: 29563.45\n",
      "Epoch [4118], train_loss: 29957.32 with loss1: 355.44, loss2: 42.29 and loss3: 29559.59\n",
      "Epoch [4119], train_loss: 29952.49 with loss1: 354.59, loss2: 42.16 and loss3: 29555.73\n",
      "Epoch [4120], train_loss: 29948.97 with loss1: 354.92, loss2: 42.18 and loss3: 29551.87\n",
      "Epoch [4121], train_loss: 29945.49 with loss1: 355.29, loss2: 42.18 and loss3: 29548.02\n",
      "Epoch [4122], train_loss: 29940.85 with loss1: 354.72, loss2: 41.98 and loss3: 29544.15\n",
      "Epoch [4123], train_loss: 29937.15 with loss1: 354.91, loss2: 41.94 and loss3: 29540.30\n",
      "Epoch [4124], train_loss: 29934.09 with loss1: 355.51, loss2: 42.14 and loss3: 29536.43\n",
      "Epoch [4125], train_loss: 29930.07 with loss1: 355.53, loss2: 41.95 and loss3: 29532.59\n",
      "Epoch [4126], train_loss: 29926.74 with loss1: 356.02, loss2: 42.01 and loss3: 29528.71\n",
      "Epoch [4127], train_loss: 29923.97 with loss1: 357.03, loss2: 42.08 and loss3: 29524.87\n",
      "Epoch [4128], train_loss: 29920.22 with loss1: 357.31, loss2: 41.92 and loss3: 29521.00\n",
      "Epoch [4129], train_loss: 29914.94 with loss1: 355.90, loss2: 41.89 and loss3: 29517.15\n",
      "Epoch [4130], train_loss: 29912.72 with loss1: 357.50, loss2: 41.94 and loss3: 29513.29\n",
      "Epoch [4131], train_loss: 29909.02 with loss1: 357.65, loss2: 41.94 and loss3: 29509.43\n",
      "Epoch [4132], train_loss: 29906.98 with loss1: 359.34, loss2: 42.07 and loss3: 29505.58\n",
      "Epoch [4133], train_loss: 29902.39 with loss1: 358.67, loss2: 42.00 and loss3: 29501.71\n",
      "Epoch [4134], train_loss: 29899.65 with loss1: 359.95, loss2: 41.84 and loss3: 29497.86\n",
      "Epoch [4135], train_loss: 29896.67 with loss1: 360.57, loss2: 42.11 and loss3: 29493.99\n",
      "Epoch [4136], train_loss: 29894.55 with loss1: 362.54, loss2: 41.86 and loss3: 29490.15\n",
      "Epoch [4137], train_loss: 29891.81 with loss1: 363.60, loss2: 41.94 and loss3: 29486.28\n",
      "Epoch [4138], train_loss: 29888.70 with loss1: 364.37, loss2: 41.89 and loss3: 29482.44\n",
      "Epoch [4139], train_loss: 29884.18 with loss1: 363.74, loss2: 41.88 and loss3: 29478.57\n",
      "Epoch [4140], train_loss: 29882.22 with loss1: 365.68, loss2: 41.82 and loss3: 29474.72\n",
      "Epoch [4141], train_loss: 29878.54 with loss1: 365.82, loss2: 41.85 and loss3: 29470.86\n",
      "Epoch [4142], train_loss: 29877.60 with loss1: 368.62, loss2: 41.97 and loss3: 29467.01\n",
      "Epoch [4143], train_loss: 29871.32 with loss1: 366.35, loss2: 41.83 and loss3: 29463.14\n",
      "Epoch [4144], train_loss: 29871.55 with loss1: 370.37, loss2: 41.89 and loss3: 29459.29\n",
      "Epoch [4145], train_loss: 29866.83 with loss1: 369.67, loss2: 41.72 and loss3: 29455.43\n",
      "Epoch [4146], train_loss: 29864.98 with loss1: 371.70, loss2: 41.70 and loss3: 29451.58\n",
      "Epoch [4147], train_loss: 29861.68 with loss1: 372.12, loss2: 41.83 and loss3: 29447.72\n",
      "Epoch [4148], train_loss: 29860.91 with loss1: 375.33, loss2: 41.71 and loss3: 29443.87\n",
      "Epoch [4149], train_loss: 29857.30 with loss1: 375.53, loss2: 41.75 and loss3: 29440.01\n",
      "Epoch [4150], train_loss: 29856.87 with loss1: 378.94, loss2: 41.77 and loss3: 29436.16\n",
      "Epoch [4151], train_loss: 29852.55 with loss1: 378.47, loss2: 41.77 and loss3: 29432.30\n",
      "Epoch [4152], train_loss: 29852.78 with loss1: 382.72, loss2: 41.61 and loss3: 29428.45\n",
      "Epoch [4153], train_loss: 29848.47 with loss1: 382.14, loss2: 41.74 and loss3: 29424.59\n",
      "Epoch [4154], train_loss: 29848.03 with loss1: 385.62, loss2: 41.67 and loss3: 29420.74\n",
      "Epoch [4155], train_loss: 29843.25 with loss1: 384.65, loss2: 41.71 and loss3: 29416.88\n",
      "Epoch [4156], train_loss: 29843.91 with loss1: 389.11, loss2: 41.76 and loss3: 29413.04\n",
      "Epoch [4157], train_loss: 29839.61 with loss1: 388.76, loss2: 41.68 and loss3: 29409.18\n",
      "Epoch [4158], train_loss: 29839.15 with loss1: 392.21, loss2: 41.61 and loss3: 29405.33\n",
      "Epoch [4159], train_loss: 29834.22 with loss1: 391.06, loss2: 41.69 and loss3: 29401.47\n",
      "Epoch [4160], train_loss: 29834.47 with loss1: 395.27, loss2: 41.58 and loss3: 29397.62\n",
      "Epoch [4161], train_loss: 29828.91 with loss1: 393.47, loss2: 41.68 and loss3: 29393.77\n",
      "Epoch [4162], train_loss: 29829.06 with loss1: 397.43, loss2: 41.71 and loss3: 29389.92\n",
      "Epoch [4163], train_loss: 29824.52 with loss1: 396.83, loss2: 41.62 and loss3: 29386.07\n",
      "Epoch [4164], train_loss: 29823.40 with loss1: 399.61, loss2: 41.57 and loss3: 29382.21\n",
      "Epoch [4165], train_loss: 29817.42 with loss1: 397.46, loss2: 41.60 and loss3: 29378.36\n",
      "Epoch [4166], train_loss: 29819.20 with loss1: 403.02, loss2: 41.67 and loss3: 29374.51\n",
      "Epoch [4167], train_loss: 29812.71 with loss1: 400.48, loss2: 41.57 and loss3: 29370.66\n",
      "Epoch [4168], train_loss: 29812.47 with loss1: 404.02, loss2: 41.64 and loss3: 29366.81\n",
      "Epoch [4169], train_loss: 29806.26 with loss1: 401.72, loss2: 41.58 and loss3: 29362.96\n",
      "Epoch [4170], train_loss: 29806.03 with loss1: 405.32, loss2: 41.60 and loss3: 29359.11\n",
      "Epoch [4171], train_loss: 29798.70 with loss1: 401.95, loss2: 41.49 and loss3: 29355.26\n",
      "Epoch [4172], train_loss: 29797.50 with loss1: 404.45, loss2: 41.64 and loss3: 29351.41\n",
      "Epoch [4173], train_loss: 29789.92 with loss1: 400.87, loss2: 41.49 and loss3: 29347.57\n",
      "Epoch [4174], train_loss: 29788.18 with loss1: 402.89, loss2: 41.57 and loss3: 29343.71\n",
      "Epoch [4175], train_loss: 29780.76 with loss1: 399.25, loss2: 41.64 and loss3: 29339.87\n",
      "Epoch [4176], train_loss: 29778.92 with loss1: 401.40, loss2: 41.51 and loss3: 29336.02\n",
      "Epoch [4177], train_loss: 29772.59 with loss1: 398.97, loss2: 41.45 and loss3: 29332.17\n",
      "Epoch [4178], train_loss: 29770.00 with loss1: 400.25, loss2: 41.42 and loss3: 29328.32\n",
      "Epoch [4179], train_loss: 29761.02 with loss1: 395.07, loss2: 41.48 and loss3: 29324.47\n",
      "Epoch [4180], train_loss: 29759.43 with loss1: 397.31, loss2: 41.50 and loss3: 29320.62\n",
      "Epoch [4181], train_loss: 29750.72 with loss1: 392.57, loss2: 41.38 and loss3: 29316.77\n",
      "Epoch [4182], train_loss: 29748.25 with loss1: 393.81, loss2: 41.51 and loss3: 29312.93\n",
      "Epoch [4183], train_loss: 29740.87 with loss1: 390.48, loss2: 41.32 and loss3: 29309.08\n",
      "Epoch [4184], train_loss: 29737.60 with loss1: 390.98, loss2: 41.38 and loss3: 29305.24\n",
      "Epoch [4185], train_loss: 29731.18 with loss1: 388.42, loss2: 41.37 and loss3: 29301.39\n",
      "Epoch [4186], train_loss: 29728.52 with loss1: 389.55, loss2: 41.43 and loss3: 29297.55\n",
      "Epoch [4187], train_loss: 29721.71 with loss1: 386.61, loss2: 41.40 and loss3: 29293.70\n",
      "Epoch [4188], train_loss: 29717.81 with loss1: 386.50, loss2: 41.45 and loss3: 29289.86\n",
      "Epoch [4189], train_loss: 29710.24 with loss1: 382.86, loss2: 41.38 and loss3: 29286.00\n",
      "Epoch [4190], train_loss: 29708.55 with loss1: 384.96, loss2: 41.42 and loss3: 29282.17\n",
      "Epoch [4191], train_loss: 29700.01 with loss1: 380.45, loss2: 41.25 and loss3: 29278.31\n",
      "Epoch [4192], train_loss: 29698.10 with loss1: 382.23, loss2: 41.38 and loss3: 29274.48\n",
      "Epoch [4193], train_loss: 29690.63 with loss1: 378.66, loss2: 41.34 and loss3: 29270.62\n",
      "Epoch [4194], train_loss: 29688.10 with loss1: 379.90, loss2: 41.40 and loss3: 29266.80\n",
      "Epoch [4195], train_loss: 29680.84 with loss1: 376.66, loss2: 41.24 and loss3: 29262.93\n",
      "Epoch [4196], train_loss: 29676.87 with loss1: 376.44, loss2: 41.33 and loss3: 29259.11\n",
      "Epoch [4197], train_loss: 29671.19 with loss1: 374.78, loss2: 41.16 and loss3: 29255.25\n",
      "Epoch [4198], train_loss: 29668.36 with loss1: 375.56, loss2: 41.38 and loss3: 29251.43\n",
      "Epoch [4199], train_loss: 29661.77 with loss1: 372.99, loss2: 41.21 and loss3: 29247.57\n",
      "Epoch [4200], train_loss: 29659.47 with loss1: 374.43, loss2: 41.30 and loss3: 29243.74\n",
      "Epoch [4201], train_loss: 29652.28 with loss1: 371.09, loss2: 41.30 and loss3: 29239.89\n",
      "Epoch [4202], train_loss: 29648.09 with loss1: 370.78, loss2: 41.24 and loss3: 29236.06\n",
      "Epoch [4203], train_loss: 29641.84 with loss1: 368.42, loss2: 41.20 and loss3: 29232.21\n",
      "Epoch [4204], train_loss: 29639.06 with loss1: 369.42, loss2: 41.26 and loss3: 29228.39\n",
      "Epoch [4205], train_loss: 29633.41 with loss1: 367.74, loss2: 41.14 and loss3: 29224.54\n",
      "Epoch [4206], train_loss: 29629.19 with loss1: 367.33, loss2: 41.14 and loss3: 29220.71\n",
      "Epoch [4207], train_loss: 29624.40 with loss1: 366.43, loss2: 41.11 and loss3: 29216.86\n",
      "Epoch [4208], train_loss: 29620.38 with loss1: 365.98, loss2: 41.37 and loss3: 29213.04\n",
      "Epoch [4209], train_loss: 29614.16 with loss1: 363.94, loss2: 41.03 and loss3: 29209.19\n",
      "Epoch [4210], train_loss: 29611.36 with loss1: 364.94, loss2: 41.07 and loss3: 29205.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4211], train_loss: 29605.74 with loss1: 363.06, loss2: 41.16 and loss3: 29201.52\n",
      "Epoch [4212], train_loss: 29602.17 with loss1: 363.38, loss2: 41.10 and loss3: 29197.69\n",
      "Epoch [4213], train_loss: 29596.44 with loss1: 361.62, loss2: 40.97 and loss3: 29193.85\n",
      "Epoch [4214], train_loss: 29592.81 with loss1: 361.64, loss2: 41.15 and loss3: 29190.02\n",
      "Epoch [4215], train_loss: 29588.11 with loss1: 360.96, loss2: 40.98 and loss3: 29186.17\n",
      "Epoch [4216], train_loss: 29584.40 with loss1: 361.03, loss2: 41.02 and loss3: 29182.35\n",
      "Epoch [4217], train_loss: 29578.57 with loss1: 359.12, loss2: 40.95 and loss3: 29178.50\n",
      "Epoch [4218], train_loss: 29575.86 with loss1: 360.07, loss2: 41.11 and loss3: 29174.68\n",
      "Epoch [4219], train_loss: 29570.13 with loss1: 358.35, loss2: 40.95 and loss3: 29170.83\n",
      "Epoch [4220], train_loss: 29566.41 with loss1: 358.30, loss2: 41.09 and loss3: 29167.01\n",
      "Epoch [4221], train_loss: 29561.47 with loss1: 357.34, loss2: 40.97 and loss3: 29163.16\n",
      "Epoch [4222], train_loss: 29557.49 with loss1: 357.20, loss2: 40.95 and loss3: 29159.34\n",
      "Epoch [4223], train_loss: 29552.14 with loss1: 355.74, loss2: 40.92 and loss3: 29155.49\n",
      "Epoch [4224], train_loss: 29549.76 with loss1: 357.23, loss2: 40.86 and loss3: 29151.67\n",
      "Epoch [4225], train_loss: 29543.67 with loss1: 355.09, loss2: 40.74 and loss3: 29147.83\n",
      "Epoch [4226], train_loss: 29540.88 with loss1: 355.98, loss2: 40.91 and loss3: 29144.00\n",
      "Epoch [4227], train_loss: 29535.41 with loss1: 354.47, loss2: 40.78 and loss3: 29140.17\n",
      "Epoch [4228], train_loss: 29532.01 with loss1: 354.84, loss2: 40.85 and loss3: 29136.33\n",
      "Epoch [4229], train_loss: 29526.76 with loss1: 353.47, loss2: 40.79 and loss3: 29132.50\n",
      "Epoch [4230], train_loss: 29524.26 with loss1: 354.79, loss2: 40.81 and loss3: 29128.66\n",
      "Epoch [4231], train_loss: 29519.37 with loss1: 353.79, loss2: 40.75 and loss3: 29124.83\n",
      "Epoch [4232], train_loss: 29515.84 with loss1: 354.01, loss2: 40.84 and loss3: 29121.00\n",
      "Epoch [4233], train_loss: 29511.87 with loss1: 353.88, loss2: 40.83 and loss3: 29117.16\n",
      "Epoch [4234], train_loss: 29507.80 with loss1: 353.72, loss2: 40.74 and loss3: 29113.34\n",
      "Epoch [4235], train_loss: 29503.37 with loss1: 353.12, loss2: 40.76 and loss3: 29109.50\n",
      "Epoch [4236], train_loss: 29500.21 with loss1: 353.66, loss2: 40.87 and loss3: 29105.68\n",
      "Epoch [4237], train_loss: 29496.13 with loss1: 353.51, loss2: 40.79 and loss3: 29101.84\n",
      "Epoch [4238], train_loss: 29491.72 with loss1: 352.88, loss2: 40.82 and loss3: 29098.02\n",
      "Epoch [4239], train_loss: 29488.01 with loss1: 353.18, loss2: 40.66 and loss3: 29094.18\n",
      "Epoch [4240], train_loss: 29484.78 with loss1: 353.66, loss2: 40.76 and loss3: 29090.36\n",
      "Epoch [4241], train_loss: 29480.34 with loss1: 353.05, loss2: 40.78 and loss3: 29086.51\n",
      "Epoch [4242], train_loss: 29476.04 with loss1: 352.69, loss2: 40.65 and loss3: 29082.70\n",
      "Epoch [4243], train_loss: 29472.08 with loss1: 352.63, loss2: 40.60 and loss3: 29078.85\n",
      "Epoch [4244], train_loss: 29468.34 with loss1: 352.59, loss2: 40.71 and loss3: 29075.04\n",
      "Epoch [4245], train_loss: 29464.37 with loss1: 352.70, loss2: 40.48 and loss3: 29071.19\n",
      "Epoch [4246], train_loss: 29461.57 with loss1: 353.46, loss2: 40.73 and loss3: 29067.38\n",
      "Epoch [4247], train_loss: 29457.02 with loss1: 352.95, loss2: 40.53 and loss3: 29063.54\n",
      "Epoch [4248], train_loss: 29454.33 with loss1: 353.85, loss2: 40.76 and loss3: 29059.71\n",
      "Epoch [4249], train_loss: 29450.31 with loss1: 353.74, loss2: 40.69 and loss3: 29055.88\n",
      "Epoch [4250], train_loss: 29447.30 with loss1: 354.64, loss2: 40.61 and loss3: 29052.05\n",
      "Epoch [4251], train_loss: 29442.49 with loss1: 353.76, loss2: 40.51 and loss3: 29048.22\n",
      "Epoch [4252], train_loss: 29440.23 with loss1: 355.09, loss2: 40.74 and loss3: 29044.40\n",
      "Epoch [4253], train_loss: 29436.03 with loss1: 354.88, loss2: 40.59 and loss3: 29040.56\n",
      "Epoch [4254], train_loss: 29433.60 with loss1: 356.35, loss2: 40.51 and loss3: 29036.74\n",
      "Epoch [4255], train_loss: 29428.55 with loss1: 355.13, loss2: 40.52 and loss3: 29032.91\n",
      "Epoch [4256], train_loss: 29425.79 with loss1: 356.13, loss2: 40.58 and loss3: 29029.09\n",
      "Epoch [4257], train_loss: 29421.73 with loss1: 355.96, loss2: 40.52 and loss3: 29025.25\n",
      "Epoch [4258], train_loss: 29419.54 with loss1: 357.49, loss2: 40.63 and loss3: 29021.43\n",
      "Epoch [4259], train_loss: 29415.13 with loss1: 357.01, loss2: 40.53 and loss3: 29017.59\n",
      "Epoch [4260], train_loss: 29412.43 with loss1: 358.16, loss2: 40.49 and loss3: 29013.78\n",
      "Epoch [4261], train_loss: 29408.07 with loss1: 357.79, loss2: 40.34 and loss3: 29009.94\n",
      "Epoch [4262], train_loss: 29407.44 with loss1: 360.73, loss2: 40.58 and loss3: 29006.12\n",
      "Epoch [4263], train_loss: 29402.83 with loss1: 360.20, loss2: 40.34 and loss3: 29002.29\n",
      "Epoch [4264], train_loss: 29398.94 with loss1: 360.01, loss2: 40.46 and loss3: 28998.47\n",
      "Epoch [4265], train_loss: 29395.10 with loss1: 360.21, loss2: 40.25 and loss3: 28994.64\n",
      "Epoch [4266], train_loss: 29392.59 with loss1: 361.42, loss2: 40.36 and loss3: 28990.82\n",
      "Epoch [4267], train_loss: 29387.55 with loss1: 360.20, loss2: 40.37 and loss3: 28986.99\n",
      "Epoch [4268], train_loss: 29386.49 with loss1: 362.89, loss2: 40.44 and loss3: 28983.16\n",
      "Epoch [4269], train_loss: 29381.85 with loss1: 362.14, loss2: 40.36 and loss3: 28979.34\n",
      "Epoch [4270], train_loss: 29379.88 with loss1: 363.80, loss2: 40.57 and loss3: 28975.51\n",
      "Epoch [4271], train_loss: 29374.55 with loss1: 362.53, loss2: 40.33 and loss3: 28971.70\n",
      "Epoch [4272], train_loss: 29372.61 with loss1: 364.26, loss2: 40.49 and loss3: 28967.87\n",
      "Epoch [4273], train_loss: 29368.15 with loss1: 363.69, loss2: 40.41 and loss3: 28964.05\n",
      "Epoch [4274], train_loss: 29366.42 with loss1: 365.82, loss2: 40.37 and loss3: 28960.22\n",
      "Epoch [4275], train_loss: 29361.39 with loss1: 364.80, loss2: 40.20 and loss3: 28956.40\n",
      "Epoch [4276], train_loss: 29361.16 with loss1: 368.27, loss2: 40.31 and loss3: 28952.58\n",
      "Epoch [4277], train_loss: 29354.35 with loss1: 365.40, loss2: 40.19 and loss3: 28948.75\n",
      "Epoch [4278], train_loss: 29352.09 with loss1: 366.89, loss2: 40.27 and loss3: 28944.94\n",
      "Epoch [4279], train_loss: 29345.91 with loss1: 364.71, loss2: 40.10 and loss3: 28941.11\n",
      "Epoch [4280], train_loss: 29346.20 with loss1: 368.70, loss2: 40.21 and loss3: 28937.29\n",
      "Epoch [4281], train_loss: 29339.52 with loss1: 365.82, loss2: 40.23 and loss3: 28933.47\n",
      "Epoch [4282], train_loss: 29336.98 with loss1: 367.05, loss2: 40.28 and loss3: 28929.65\n",
      "Epoch [4283], train_loss: 29332.36 with loss1: 366.28, loss2: 40.26 and loss3: 28925.83\n",
      "Epoch [4284], train_loss: 29329.55 with loss1: 367.23, loss2: 40.32 and loss3: 28922.00\n",
      "Epoch [4285], train_loss: 29324.42 with loss1: 366.08, loss2: 40.16 and loss3: 28918.19\n",
      "Epoch [4286], train_loss: 29322.58 with loss1: 368.07, loss2: 40.15 and loss3: 28914.36\n",
      "Epoch [4287], train_loss: 29316.15 with loss1: 365.53, loss2: 40.08 and loss3: 28910.54\n",
      "Epoch [4288], train_loss: 29314.25 with loss1: 367.43, loss2: 40.10 and loss3: 28906.72\n",
      "Epoch [4289], train_loss: 29308.45 with loss1: 365.41, loss2: 40.14 and loss3: 28902.90\n",
      "Epoch [4290], train_loss: 29305.61 with loss1: 366.27, loss2: 40.25 and loss3: 28899.09\n",
      "Epoch [4291], train_loss: 29300.78 with loss1: 365.43, loss2: 40.10 and loss3: 28895.25\n",
      "Epoch [4292], train_loss: 29298.20 with loss1: 366.72, loss2: 40.03 and loss3: 28891.45\n",
      "Epoch [4293], train_loss: 29291.90 with loss1: 364.34, loss2: 39.95 and loss3: 28887.62\n",
      "Epoch [4294], train_loss: 29289.18 with loss1: 365.26, loss2: 40.10 and loss3: 28883.81\n",
      "Epoch [4295], train_loss: 29283.37 with loss1: 363.32, loss2: 40.08 and loss3: 28879.98\n",
      "Epoch [4296], train_loss: 29281.70 with loss1: 365.43, loss2: 40.09 and loss3: 28876.17\n",
      "Epoch [4297], train_loss: 29276.07 with loss1: 363.63, loss2: 40.10 and loss3: 28872.34\n",
      "Epoch [4298], train_loss: 29273.31 with loss1: 364.80, loss2: 39.98 and loss3: 28868.53\n",
      "Epoch [4299], train_loss: 29267.43 with loss1: 362.68, loss2: 40.04 and loss3: 28864.71\n",
      "Epoch [4300], train_loss: 29264.02 with loss1: 363.18, loss2: 39.94 and loss3: 28860.89\n",
      "Epoch [4301], train_loss: 29258.74 with loss1: 361.66, loss2: 40.01 and loss3: 28857.07\n",
      "Epoch [4302], train_loss: 29257.75 with loss1: 364.44, loss2: 40.05 and loss3: 28853.26\n",
      "Epoch [4303], train_loss: 29250.96 with loss1: 361.67, loss2: 39.85 and loss3: 28849.44\n",
      "Epoch [4304], train_loss: 29247.95 with loss1: 362.41, loss2: 39.91 and loss3: 28845.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4305], train_loss: 29242.19 with loss1: 360.57, loss2: 39.81 and loss3: 28841.81\n",
      "Epoch [4306], train_loss: 29240.23 with loss1: 362.31, loss2: 39.93 and loss3: 28838.00\n",
      "Epoch [4307], train_loss: 29235.10 with loss1: 360.99, loss2: 39.93 and loss3: 28834.18\n",
      "Epoch [4308], train_loss: 29231.20 with loss1: 360.97, loss2: 39.86 and loss3: 28830.37\n",
      "Epoch [4309], train_loss: 29227.08 with loss1: 360.69, loss2: 39.83 and loss3: 28826.55\n",
      "Epoch [4310], train_loss: 29223.71 with loss1: 361.11, loss2: 39.86 and loss3: 28822.74\n",
      "Epoch [4311], train_loss: 29218.19 with loss1: 359.38, loss2: 39.88 and loss3: 28818.93\n",
      "Epoch [4312], train_loss: 29214.80 with loss1: 359.85, loss2: 39.84 and loss3: 28815.11\n",
      "Epoch [4313], train_loss: 29210.37 with loss1: 359.22, loss2: 39.85 and loss3: 28811.30\n",
      "Epoch [4314], train_loss: 29206.83 with loss1: 359.50, loss2: 39.84 and loss3: 28807.49\n",
      "Epoch [4315], train_loss: 29202.42 with loss1: 358.99, loss2: 39.76 and loss3: 28803.68\n",
      "Epoch [4316], train_loss: 29198.88 with loss1: 359.23, loss2: 39.79 and loss3: 28799.86\n",
      "Epoch [4317], train_loss: 29193.67 with loss1: 357.94, loss2: 39.67 and loss3: 28796.05\n",
      "Epoch [4318], train_loss: 29189.81 with loss1: 357.82, loss2: 39.75 and loss3: 28792.24\n",
      "Epoch [4319], train_loss: 29185.64 with loss1: 357.48, loss2: 39.72 and loss3: 28788.43\n",
      "Epoch [4320], train_loss: 29183.01 with loss1: 358.67, loss2: 39.73 and loss3: 28784.62\n",
      "Epoch [4321], train_loss: 29177.46 with loss1: 356.97, loss2: 39.68 and loss3: 28780.81\n",
      "Epoch [4322], train_loss: 29175.57 with loss1: 358.81, loss2: 39.77 and loss3: 28777.00\n",
      "Epoch [4323], train_loss: 29170.12 with loss1: 357.12, loss2: 39.81 and loss3: 28773.19\n",
      "Epoch [4324], train_loss: 29166.63 with loss1: 357.60, loss2: 39.65 and loss3: 28769.38\n",
      "Epoch [4325], train_loss: 29161.90 with loss1: 356.70, loss2: 39.63 and loss3: 28765.57\n",
      "Epoch [4326], train_loss: 29158.60 with loss1: 357.23, loss2: 39.61 and loss3: 28761.76\n",
      "Epoch [4327], train_loss: 29153.46 with loss1: 355.93, loss2: 39.57 and loss3: 28757.96\n",
      "Epoch [4328], train_loss: 29150.48 with loss1: 356.65, loss2: 39.69 and loss3: 28754.14\n",
      "Epoch [4329], train_loss: 29146.58 with loss1: 356.44, loss2: 39.79 and loss3: 28750.35\n",
      "Epoch [4330], train_loss: 29142.85 with loss1: 356.70, loss2: 39.63 and loss3: 28746.52\n",
      "Epoch [4331], train_loss: 29137.29 with loss1: 354.96, loss2: 39.59 and loss3: 28742.74\n",
      "Epoch [4332], train_loss: 29134.06 with loss1: 355.55, loss2: 39.60 and loss3: 28738.91\n",
      "Epoch [4333], train_loss: 29129.54 with loss1: 354.75, loss2: 39.66 and loss3: 28735.13\n",
      "Epoch [4334], train_loss: 29126.56 with loss1: 355.68, loss2: 39.58 and loss3: 28731.29\n",
      "Epoch [4335], train_loss: 29120.92 with loss1: 353.92, loss2: 39.49 and loss3: 28727.52\n",
      "Epoch [4336], train_loss: 29118.52 with loss1: 355.27, loss2: 39.57 and loss3: 28723.69\n",
      "Epoch [4337], train_loss: 29113.72 with loss1: 354.33, loss2: 39.50 and loss3: 28719.90\n",
      "Epoch [4338], train_loss: 29111.77 with loss1: 356.05, loss2: 39.63 and loss3: 28716.08\n",
      "Epoch [4339], train_loss: 29107.24 with loss1: 355.56, loss2: 39.40 and loss3: 28712.29\n",
      "Epoch [4340], train_loss: 29103.97 with loss1: 355.88, loss2: 39.61 and loss3: 28708.48\n",
      "Epoch [4341], train_loss: 29099.63 with loss1: 355.41, loss2: 39.55 and loss3: 28704.67\n",
      "Epoch [4342], train_loss: 29096.62 with loss1: 356.25, loss2: 39.49 and loss3: 28700.88\n",
      "Epoch [4343], train_loss: 29092.48 with loss1: 355.94, loss2: 39.48 and loss3: 28697.05\n",
      "Epoch [4344], train_loss: 29089.18 with loss1: 356.39, loss2: 39.53 and loss3: 28693.27\n",
      "Epoch [4345], train_loss: 29084.09 with loss1: 355.20, loss2: 39.45 and loss3: 28689.45\n",
      "Epoch [4346], train_loss: 29081.01 with loss1: 355.91, loss2: 39.44 and loss3: 28685.66\n",
      "Epoch [4347], train_loss: 29076.93 with loss1: 355.74, loss2: 39.35 and loss3: 28681.84\n",
      "Epoch [4348], train_loss: 29074.20 with loss1: 356.69, loss2: 39.46 and loss3: 28678.05\n",
      "Epoch [4349], train_loss: 29069.27 with loss1: 355.75, loss2: 39.28 and loss3: 28674.24\n",
      "Epoch [4350], train_loss: 29066.51 with loss1: 356.70, loss2: 39.36 and loss3: 28670.44\n",
      "Epoch [4351], train_loss: 29061.80 with loss1: 355.83, loss2: 39.33 and loss3: 28666.64\n",
      "Epoch [4352], train_loss: 29059.82 with loss1: 357.60, loss2: 39.38 and loss3: 28662.84\n",
      "Epoch [4353], train_loss: 29054.24 with loss1: 355.89, loss2: 39.32 and loss3: 28659.03\n",
      "Epoch [4354], train_loss: 29052.93 with loss1: 358.36, loss2: 39.34 and loss3: 28655.23\n",
      "Epoch [4355], train_loss: 29047.33 with loss1: 356.49, loss2: 39.41 and loss3: 28651.43\n",
      "Epoch [4356], train_loss: 29044.96 with loss1: 358.01, loss2: 39.33 and loss3: 28647.62\n",
      "Epoch [4357], train_loss: 29039.90 with loss1: 356.65, loss2: 39.41 and loss3: 28643.84\n",
      "Epoch [4358], train_loss: 29037.75 with loss1: 358.49, loss2: 39.24 and loss3: 28640.02\n",
      "Epoch [4359], train_loss: 29032.46 with loss1: 356.97, loss2: 39.25 and loss3: 28636.24\n",
      "Epoch [4360], train_loss: 29030.41 with loss1: 358.76, loss2: 39.23 and loss3: 28632.42\n",
      "Epoch [4361], train_loss: 29025.89 with loss1: 358.07, loss2: 39.18 and loss3: 28628.64\n",
      "Epoch [4362], train_loss: 29023.31 with loss1: 359.23, loss2: 39.25 and loss3: 28624.82\n",
      "Epoch [4363], train_loss: 29018.76 with loss1: 358.36, loss2: 39.36 and loss3: 28621.04\n",
      "Epoch [4364], train_loss: 29016.17 with loss1: 359.78, loss2: 39.17 and loss3: 28617.23\n",
      "Epoch [4365], train_loss: 29011.29 with loss1: 358.63, loss2: 39.21 and loss3: 28613.45\n",
      "Epoch [4366], train_loss: 29009.12 with loss1: 360.32, loss2: 39.16 and loss3: 28609.63\n",
      "Epoch [4367], train_loss: 29003.84 with loss1: 358.74, loss2: 39.25 and loss3: 28605.86\n",
      "Epoch [4368], train_loss: 29001.62 with loss1: 360.42, loss2: 39.16 and loss3: 28602.04\n",
      "Epoch [4369], train_loss: 28995.99 with loss1: 358.67, loss2: 39.06 and loss3: 28598.27\n",
      "Epoch [4370], train_loss: 28994.20 with loss1: 360.49, loss2: 39.26 and loss3: 28594.45\n",
      "Epoch [4371], train_loss: 28989.27 with loss1: 359.38, loss2: 39.21 and loss3: 28590.68\n",
      "Epoch [4372], train_loss: 28986.81 with loss1: 360.75, loss2: 39.19 and loss3: 28586.86\n",
      "Epoch [4373], train_loss: 28981.91 with loss1: 359.63, loss2: 39.19 and loss3: 28583.09\n",
      "Epoch [4374], train_loss: 28978.86 with loss1: 360.37, loss2: 39.21 and loss3: 28579.27\n",
      "Epoch [4375], train_loss: 28974.79 with loss1: 360.11, loss2: 39.17 and loss3: 28575.51\n",
      "Epoch [4376], train_loss: 28970.95 with loss1: 360.07, loss2: 39.18 and loss3: 28571.70\n",
      "Epoch [4377], train_loss: 28964.26 with loss1: 357.28, loss2: 39.05 and loss3: 28567.92\n",
      "Epoch [4378], train_loss: 28961.00 with loss1: 357.78, loss2: 39.10 and loss3: 28564.11\n",
      "Epoch [4379], train_loss: 28954.15 with loss1: 354.76, loss2: 39.05 and loss3: 28560.34\n",
      "Epoch [4380], train_loss: 28951.84 with loss1: 356.19, loss2: 39.13 and loss3: 28556.53\n",
      "Epoch [4381], train_loss: 28947.00 with loss1: 355.13, loss2: 39.11 and loss3: 28552.76\n",
      "Epoch [4382], train_loss: 28943.25 with loss1: 355.28, loss2: 39.03 and loss3: 28548.94\n",
      "Epoch [4383], train_loss: 28937.46 with loss1: 353.31, loss2: 38.97 and loss3: 28545.18\n",
      "Epoch [4384], train_loss: 28934.01 with loss1: 353.62, loss2: 39.02 and loss3: 28541.36\n",
      "Epoch [4385], train_loss: 28929.49 with loss1: 352.87, loss2: 39.03 and loss3: 28537.60\n",
      "Epoch [4386], train_loss: 28925.93 with loss1: 353.01, loss2: 39.14 and loss3: 28533.78\n",
      "Epoch [4387], train_loss: 28920.16 with loss1: 351.06, loss2: 39.08 and loss3: 28530.02\n",
      "Epoch [4388], train_loss: 28916.56 with loss1: 351.39, loss2: 38.97 and loss3: 28526.20\n",
      "Epoch [4389], train_loss: 28911.61 with loss1: 350.10, loss2: 39.07 and loss3: 28522.44\n",
      "Epoch [4390], train_loss: 28908.46 with loss1: 351.01, loss2: 38.83 and loss3: 28518.62\n",
      "Epoch [4391], train_loss: 28902.18 with loss1: 348.35, loss2: 38.97 and loss3: 28514.86\n",
      "Epoch [4392], train_loss: 28898.76 with loss1: 348.71, loss2: 39.00 and loss3: 28511.04\n",
      "Epoch [4393], train_loss: 28893.88 with loss1: 347.62, loss2: 38.98 and loss3: 28507.29\n",
      "Epoch [4394], train_loss: 28891.07 with loss1: 348.65, loss2: 38.95 and loss3: 28503.47\n",
      "Epoch [4395], train_loss: 28886.28 with loss1: 347.64, loss2: 38.94 and loss3: 28499.71\n",
      "Epoch [4396], train_loss: 28882.06 with loss1: 347.30, loss2: 38.85 and loss3: 28495.90\n",
      "Epoch [4397], train_loss: 28877.70 with loss1: 346.46, loss2: 39.11 and loss3: 28492.13\n",
      "Epoch [4398], train_loss: 28875.16 with loss1: 347.95, loss2: 38.87 and loss3: 28488.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4399], train_loss: 28868.84 with loss1: 345.38, loss2: 38.91 and loss3: 28484.56\n",
      "Epoch [4400], train_loss: 28864.93 with loss1: 345.33, loss2: 38.84 and loss3: 28480.76\n",
      "Epoch [4401], train_loss: 28859.70 with loss1: 343.92, loss2: 38.80 and loss3: 28476.98\n",
      "Epoch [4402], train_loss: 28856.57 with loss1: 344.59, loss2: 38.78 and loss3: 28473.19\n",
      "Epoch [4403], train_loss: 28852.91 with loss1: 344.69, loss2: 38.81 and loss3: 28469.41\n",
      "Epoch [4404], train_loss: 28849.78 with loss1: 345.30, loss2: 38.86 and loss3: 28465.62\n",
      "Epoch [4405], train_loss: 28843.91 with loss1: 343.24, loss2: 38.83 and loss3: 28461.84\n",
      "Epoch [4406], train_loss: 28840.31 with loss1: 343.45, loss2: 38.80 and loss3: 28458.05\n",
      "Epoch [4407], train_loss: 28836.70 with loss1: 343.72, loss2: 38.70 and loss3: 28454.27\n",
      "Epoch [4408], train_loss: 28832.88 with loss1: 343.69, loss2: 38.70 and loss3: 28450.49\n",
      "Epoch [4409], train_loss: 28827.74 with loss1: 342.36, loss2: 38.68 and loss3: 28446.70\n",
      "Epoch [4410], train_loss: 28823.77 with loss1: 342.15, loss2: 38.70 and loss3: 28442.92\n",
      "Epoch [4411], train_loss: 28819.40 with loss1: 341.60, loss2: 38.68 and loss3: 28439.13\n",
      "Epoch [4412], train_loss: 28818.04 with loss1: 343.94, loss2: 38.74 and loss3: 28435.36\n",
      "Epoch [4413], train_loss: 28812.64 with loss1: 342.33, loss2: 38.75 and loss3: 28431.57\n",
      "Epoch [4414], train_loss: 28809.58 with loss1: 343.02, loss2: 38.77 and loss3: 28427.79\n",
      "Epoch [4415], train_loss: 28804.80 with loss1: 342.16, loss2: 38.64 and loss3: 28424.01\n",
      "Epoch [4416], train_loss: 28801.58 with loss1: 342.65, loss2: 38.70 and loss3: 28420.23\n",
      "Epoch [4417], train_loss: 28796.94 with loss1: 341.87, loss2: 38.62 and loss3: 28416.45\n",
      "Epoch [4418], train_loss: 28793.33 with loss1: 342.08, loss2: 38.58 and loss3: 28412.66\n",
      "Epoch [4419], train_loss: 28789.14 with loss1: 341.76, loss2: 38.50 and loss3: 28408.89\n",
      "Epoch [4420], train_loss: 28787.48 with loss1: 343.76, loss2: 38.62 and loss3: 28405.10\n",
      "Epoch [4421], train_loss: 28781.60 with loss1: 341.54, loss2: 38.73 and loss3: 28401.33\n",
      "Epoch [4422], train_loss: 28780.16 with loss1: 343.97, loss2: 38.66 and loss3: 28397.53\n",
      "Epoch [4423], train_loss: 28774.86 with loss1: 342.56, loss2: 38.53 and loss3: 28393.77\n",
      "Epoch [4424], train_loss: 28772.47 with loss1: 343.92, loss2: 38.58 and loss3: 28389.97\n",
      "Epoch [4425], train_loss: 28767.33 with loss1: 342.56, loss2: 38.58 and loss3: 28386.20\n",
      "Epoch [4426], train_loss: 28764.67 with loss1: 343.70, loss2: 38.56 and loss3: 28382.41\n",
      "Epoch [4427], train_loss: 28761.43 with loss1: 344.30, loss2: 38.49 and loss3: 28378.64\n",
      "Epoch [4428], train_loss: 28757.84 with loss1: 344.35, loss2: 38.63 and loss3: 28374.85\n",
      "Epoch [4429], train_loss: 28753.89 with loss1: 344.32, loss2: 38.49 and loss3: 28371.08\n",
      "Epoch [4430], train_loss: 28752.06 with loss1: 346.27, loss2: 38.49 and loss3: 28367.30\n",
      "Epoch [4431], train_loss: 28747.69 with loss1: 345.84, loss2: 38.32 and loss3: 28363.53\n",
      "Epoch [4432], train_loss: 28744.76 with loss1: 346.46, loss2: 38.56 and loss3: 28359.74\n",
      "Epoch [4433], train_loss: 28741.04 with loss1: 346.54, loss2: 38.52 and loss3: 28355.98\n",
      "Epoch [4434], train_loss: 28737.57 with loss1: 346.86, loss2: 38.52 and loss3: 28352.18\n",
      "Epoch [4435], train_loss: 28733.80 with loss1: 346.93, loss2: 38.44 and loss3: 28348.43\n",
      "Epoch [4436], train_loss: 28731.73 with loss1: 348.57, loss2: 38.53 and loss3: 28344.63\n",
      "Epoch [4437], train_loss: 28727.98 with loss1: 348.64, loss2: 38.47 and loss3: 28340.87\n",
      "Epoch [4438], train_loss: 28725.19 with loss1: 349.74, loss2: 38.37 and loss3: 28337.07\n",
      "Epoch [4439], train_loss: 28722.74 with loss1: 350.92, loss2: 38.50 and loss3: 28333.32\n",
      "Epoch [4440], train_loss: 28721.13 with loss1: 353.13, loss2: 38.47 and loss3: 28329.53\n",
      "Epoch [4441], train_loss: 28717.09 with loss1: 352.97, loss2: 38.35 and loss3: 28325.78\n",
      "Epoch [4442], train_loss: 28715.70 with loss1: 355.37, loss2: 38.35 and loss3: 28321.98\n",
      "Epoch [4443], train_loss: 28711.90 with loss1: 355.33, loss2: 38.34 and loss3: 28318.23\n",
      "Epoch [4444], train_loss: 28709.93 with loss1: 357.06, loss2: 38.44 and loss3: 28314.43\n",
      "Epoch [4445], train_loss: 28705.58 with loss1: 356.65, loss2: 38.25 and loss3: 28310.69\n",
      "Epoch [4446], train_loss: 28704.79 with loss1: 359.52, loss2: 38.39 and loss3: 28306.88\n",
      "Epoch [4447], train_loss: 28701.28 with loss1: 359.87, loss2: 38.27 and loss3: 28303.14\n",
      "Epoch [4448], train_loss: 28699.36 with loss1: 361.69, loss2: 38.33 and loss3: 28299.34\n",
      "Epoch [4449], train_loss: 28696.41 with loss1: 362.55, loss2: 38.27 and loss3: 28295.59\n",
      "Epoch [4450], train_loss: 28693.49 with loss1: 363.27, loss2: 38.43 and loss3: 28291.80\n",
      "Epoch [4451], train_loss: 28688.75 with loss1: 362.43, loss2: 38.28 and loss3: 28288.05\n",
      "Epoch [4452], train_loss: 28688.29 with loss1: 365.71, loss2: 38.32 and loss3: 28284.26\n",
      "Epoch [4453], train_loss: 28684.12 with loss1: 365.47, loss2: 38.15 and loss3: 28280.51\n",
      "Epoch [4454], train_loss: 28683.57 with loss1: 368.59, loss2: 38.26 and loss3: 28276.72\n",
      "Epoch [4455], train_loss: 28679.28 with loss1: 368.18, loss2: 38.14 and loss3: 28272.96\n",
      "Epoch [4456], train_loss: 28678.59 with loss1: 371.08, loss2: 38.33 and loss3: 28269.18\n",
      "Epoch [4457], train_loss: 28674.36 with loss1: 370.66, loss2: 38.29 and loss3: 28265.41\n",
      "Epoch [4458], train_loss: 28673.94 with loss1: 373.95, loss2: 38.35 and loss3: 28261.64\n",
      "Epoch [4459], train_loss: 28668.09 with loss1: 372.00, loss2: 38.23 and loss3: 28257.87\n",
      "Epoch [4460], train_loss: 28667.75 with loss1: 375.34, loss2: 38.31 and loss3: 28254.10\n",
      "Epoch [4461], train_loss: 28661.93 with loss1: 373.48, loss2: 38.11 and loss3: 28250.33\n",
      "Epoch [4462], train_loss: 28662.30 with loss1: 377.52, loss2: 38.22 and loss3: 28246.56\n",
      "Epoch [4463], train_loss: 28656.77 with loss1: 375.86, loss2: 38.12 and loss3: 28242.79\n",
      "Epoch [4464], train_loss: 28656.62 with loss1: 379.33, loss2: 38.27 and loss3: 28239.03\n",
      "Epoch [4465], train_loss: 28650.67 with loss1: 377.28, loss2: 38.14 and loss3: 28235.26\n",
      "Epoch [4466], train_loss: 28650.78 with loss1: 381.04, loss2: 38.25 and loss3: 28231.49\n",
      "Epoch [4467], train_loss: 28644.39 with loss1: 378.57, loss2: 38.10 and loss3: 28227.72\n",
      "Epoch [4468], train_loss: 28645.00 with loss1: 382.91, loss2: 38.13 and loss3: 28223.96\n",
      "Epoch [4469], train_loss: 28637.66 with loss1: 379.40, loss2: 38.08 and loss3: 28220.18\n",
      "Epoch [4470], train_loss: 28637.93 with loss1: 383.30, loss2: 38.21 and loss3: 28216.42\n",
      "Epoch [4471], train_loss: 28631.74 with loss1: 381.07, loss2: 38.02 and loss3: 28212.65\n",
      "Epoch [4472], train_loss: 28631.75 with loss1: 384.75, loss2: 38.12 and loss3: 28208.88\n",
      "Epoch [4473], train_loss: 28625.09 with loss1: 381.99, loss2: 37.98 and loss3: 28205.11\n",
      "Epoch [4474], train_loss: 28624.55 with loss1: 384.99, loss2: 38.22 and loss3: 28201.34\n",
      "Epoch [4475], train_loss: 28617.73 with loss1: 382.08, loss2: 38.07 and loss3: 28197.58\n",
      "Epoch [4476], train_loss: 28616.94 with loss1: 385.03, loss2: 38.10 and loss3: 28193.80\n",
      "Epoch [4477], train_loss: 28610.08 with loss1: 381.96, loss2: 38.06 and loss3: 28190.05\n",
      "Epoch [4478], train_loss: 28608.80 with loss1: 384.18, loss2: 38.34 and loss3: 28186.27\n",
      "Epoch [4479], train_loss: 28601.06 with loss1: 380.56, loss2: 37.99 and loss3: 28182.52\n",
      "Epoch [4480], train_loss: 28599.64 with loss1: 382.78, loss2: 38.11 and loss3: 28178.74\n",
      "Epoch [4481], train_loss: 28592.69 with loss1: 379.67, loss2: 38.04 and loss3: 28174.98\n",
      "Epoch [4482], train_loss: 28590.87 with loss1: 381.65, loss2: 38.01 and loss3: 28171.21\n",
      "Epoch [4483], train_loss: 28583.81 with loss1: 378.39, loss2: 37.97 and loss3: 28167.45\n",
      "Epoch [4484], train_loss: 28581.29 with loss1: 379.66, loss2: 37.96 and loss3: 28163.68\n",
      "Epoch [4485], train_loss: 28574.54 with loss1: 376.67, loss2: 37.95 and loss3: 28159.92\n",
      "Epoch [4486], train_loss: 28573.16 with loss1: 378.96, loss2: 38.04 and loss3: 28156.15\n",
      "Epoch [4487], train_loss: 28565.17 with loss1: 374.81, loss2: 37.97 and loss3: 28152.38\n",
      "Epoch [4488], train_loss: 28562.62 with loss1: 376.00, loss2: 38.01 and loss3: 28148.62\n",
      "Epoch [4489], train_loss: 28554.96 with loss1: 372.15, loss2: 37.95 and loss3: 28144.85\n",
      "Epoch [4490], train_loss: 28551.68 with loss1: 372.47, loss2: 38.11 and loss3: 28141.10\n",
      "Epoch [4491], train_loss: 28545.13 with loss1: 369.65, loss2: 38.16 and loss3: 28137.32\n",
      "Epoch [4492], train_loss: 28541.64 with loss1: 370.17, loss2: 37.90 and loss3: 28133.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4493], train_loss: 28535.10 with loss1: 367.19, loss2: 38.12 and loss3: 28129.80\n",
      "Epoch [4494], train_loss: 28531.12 with loss1: 367.13, loss2: 37.94 and loss3: 28126.04\n",
      "Epoch [4495], train_loss: 28524.30 with loss1: 364.16, loss2: 37.86 and loss3: 28122.28\n",
      "Epoch [4496], train_loss: 28521.38 with loss1: 365.05, loss2: 37.82 and loss3: 28118.51\n",
      "Epoch [4497], train_loss: 28516.09 with loss1: 363.32, loss2: 38.01 and loss3: 28114.76\n",
      "Epoch [4498], train_loss: 28511.46 with loss1: 362.55, loss2: 37.93 and loss3: 28110.99\n",
      "Epoch [4499], train_loss: 28505.90 with loss1: 360.83, loss2: 37.83 and loss3: 28107.24\n",
      "Epoch [4500], train_loss: 28501.84 with loss1: 360.63, loss2: 37.74 and loss3: 28103.47\n",
      "Epoch [4501], train_loss: 28495.67 with loss1: 358.18, loss2: 37.77 and loss3: 28099.72\n",
      "Epoch [4502], train_loss: 28493.35 with loss1: 359.53, loss2: 37.87 and loss3: 28095.95\n",
      "Epoch [4503], train_loss: 28486.15 with loss1: 356.05, loss2: 37.91 and loss3: 28092.19\n",
      "Epoch [4504], train_loss: 28483.77 with loss1: 357.47, loss2: 37.87 and loss3: 28088.43\n",
      "Epoch [4505], train_loss: 28476.67 with loss1: 354.22, loss2: 37.79 and loss3: 28084.67\n",
      "Epoch [4506], train_loss: 28473.21 with loss1: 354.50, loss2: 37.80 and loss3: 28080.91\n",
      "Epoch [4507], train_loss: 28468.01 with loss1: 353.03, loss2: 37.83 and loss3: 28077.15\n",
      "Epoch [4508], train_loss: 28464.45 with loss1: 353.39, loss2: 37.67 and loss3: 28073.39\n",
      "Epoch [4509], train_loss: 28460.00 with loss1: 352.61, loss2: 37.76 and loss3: 28069.63\n",
      "Epoch [4510], train_loss: 28456.56 with loss1: 353.00, loss2: 37.69 and loss3: 28065.87\n",
      "Epoch [4511], train_loss: 28450.90 with loss1: 351.06, loss2: 37.72 and loss3: 28062.11\n",
      "Epoch [4512], train_loss: 28447.12 with loss1: 351.11, loss2: 37.65 and loss3: 28058.36\n",
      "Epoch [4513], train_loss: 28441.53 with loss1: 349.07, loss2: 37.86 and loss3: 28054.59\n",
      "Epoch [4514], train_loss: 28436.99 with loss1: 348.51, loss2: 37.64 and loss3: 28050.84\n",
      "Epoch [4515], train_loss: 28432.88 with loss1: 348.21, loss2: 37.59 and loss3: 28047.07\n",
      "Epoch [4516], train_loss: 28429.39 with loss1: 348.42, loss2: 37.65 and loss3: 28043.32\n",
      "Epoch [4517], train_loss: 28424.64 with loss1: 347.37, loss2: 37.72 and loss3: 28039.55\n",
      "Epoch [4518], train_loss: 28422.20 with loss1: 348.46, loss2: 37.94 and loss3: 28035.80\n",
      "Epoch [4519], train_loss: 28415.58 with loss1: 345.82, loss2: 37.73 and loss3: 28032.04\n",
      "Epoch [4520], train_loss: 28411.74 with loss1: 345.88, loss2: 37.57 and loss3: 28028.29\n",
      "Epoch [4521], train_loss: 28407.04 with loss1: 344.78, loss2: 37.74 and loss3: 28024.52\n",
      "Epoch [4522], train_loss: 28403.84 with loss1: 345.51, loss2: 37.56 and loss3: 28020.77\n",
      "Epoch [4523], train_loss: 28398.86 with loss1: 344.33, loss2: 37.52 and loss3: 28017.01\n",
      "Epoch [4524], train_loss: 28395.05 with loss1: 344.31, loss2: 37.49 and loss3: 28013.25\n",
      "Epoch [4525], train_loss: 28391.75 with loss1: 344.65, loss2: 37.60 and loss3: 28009.50\n",
      "Epoch [4526], train_loss: 28388.22 with loss1: 344.82, loss2: 37.66 and loss3: 28005.74\n",
      "Epoch [4527], train_loss: 28382.61 with loss1: 343.13, loss2: 37.48 and loss3: 28001.99\n",
      "Epoch [4528], train_loss: 28379.22 with loss1: 343.51, loss2: 37.48 and loss3: 27998.23\n",
      "Epoch [4529], train_loss: 28374.49 with loss1: 342.47, loss2: 37.55 and loss3: 27994.47\n",
      "Epoch [4530], train_loss: 28371.25 with loss1: 343.04, loss2: 37.48 and loss3: 27990.72\n",
      "Epoch [4531], train_loss: 28366.45 with loss1: 341.98, loss2: 37.50 and loss3: 27986.97\n",
      "Epoch [4532], train_loss: 28363.75 with loss1: 343.08, loss2: 37.46 and loss3: 27983.21\n",
      "Epoch [4533], train_loss: 28359.69 with loss1: 342.83, loss2: 37.40 and loss3: 27979.46\n",
      "Epoch [4534], train_loss: 28355.48 with loss1: 342.43, loss2: 37.35 and loss3: 27975.70\n",
      "Epoch [4535], train_loss: 28351.10 with loss1: 341.75, loss2: 37.41 and loss3: 27971.95\n",
      "Epoch [4536], train_loss: 28348.10 with loss1: 342.47, loss2: 37.45 and loss3: 27968.19\n",
      "Epoch [4537], train_loss: 28342.96 with loss1: 341.17, loss2: 37.35 and loss3: 27964.44\n",
      "Epoch [4538], train_loss: 28340.30 with loss1: 342.18, loss2: 37.43 and loss3: 27960.68\n",
      "Epoch [4539], train_loss: 28334.37 with loss1: 340.07, loss2: 37.38 and loss3: 27956.93\n",
      "Epoch [4540], train_loss: 28331.21 with loss1: 340.65, loss2: 37.39 and loss3: 27953.18\n",
      "Epoch [4541], train_loss: 28327.06 with loss1: 340.24, loss2: 37.40 and loss3: 27949.42\n",
      "Epoch [4542], train_loss: 28323.13 with loss1: 340.10, loss2: 37.36 and loss3: 27945.67\n",
      "Epoch [4543], train_loss: 28319.21 with loss1: 339.92, loss2: 37.38 and loss3: 27941.91\n",
      "Epoch [4544], train_loss: 28315.05 with loss1: 339.57, loss2: 37.32 and loss3: 27938.16\n",
      "Epoch [4545], train_loss: 28311.05 with loss1: 339.37, loss2: 37.27 and loss3: 27934.41\n",
      "Epoch [4546], train_loss: 28307.77 with loss1: 339.74, loss2: 37.38 and loss3: 27930.66\n",
      "Epoch [4547], train_loss: 28303.18 with loss1: 339.02, loss2: 37.25 and loss3: 27926.90\n",
      "Epoch [4548], train_loss: 28299.44 with loss1: 339.03, loss2: 37.26 and loss3: 27923.15\n",
      "Epoch [4549], train_loss: 28296.89 with loss1: 340.15, loss2: 37.34 and loss3: 27919.40\n",
      "Epoch [4550], train_loss: 28292.58 with loss1: 339.66, loss2: 37.27 and loss3: 27915.65\n",
      "Epoch [4551], train_loss: 28289.44 with loss1: 340.17, loss2: 37.37 and loss3: 27911.90\n",
      "Epoch [4552], train_loss: 28285.43 with loss1: 340.05, loss2: 37.24 and loss3: 27908.14\n",
      "Epoch [4553], train_loss: 28280.92 with loss1: 339.31, loss2: 37.21 and loss3: 27904.40\n",
      "Epoch [4554], train_loss: 28278.30 with loss1: 340.42, loss2: 37.23 and loss3: 27900.65\n",
      "Epoch [4555], train_loss: 28273.04 with loss1: 338.93, loss2: 37.21 and loss3: 27896.90\n",
      "Epoch [4556], train_loss: 28270.36 with loss1: 339.98, loss2: 37.23 and loss3: 27893.14\n",
      "Epoch [4557], train_loss: 28265.01 with loss1: 338.43, loss2: 37.17 and loss3: 27889.41\n",
      "Epoch [4558], train_loss: 28262.41 with loss1: 339.55, loss2: 37.22 and loss3: 27885.64\n",
      "Epoch [4559], train_loss: 28258.15 with loss1: 339.10, loss2: 37.13 and loss3: 27881.91\n",
      "Epoch [4560], train_loss: 28255.62 with loss1: 340.29, loss2: 37.18 and loss3: 27878.15\n",
      "Epoch [4561], train_loss: 28251.25 with loss1: 339.64, loss2: 37.19 and loss3: 27874.42\n",
      "Epoch [4562], train_loss: 28247.95 with loss1: 340.23, loss2: 37.06 and loss3: 27870.66\n",
      "Epoch [4563], train_loss: 28243.83 with loss1: 339.81, loss2: 37.10 and loss3: 27866.92\n",
      "Epoch [4564], train_loss: 28240.41 with loss1: 340.20, loss2: 37.05 and loss3: 27863.16\n",
      "Epoch [4565], train_loss: 28237.62 with loss1: 340.98, loss2: 37.21 and loss3: 27859.43\n",
      "Epoch [4566], train_loss: 28233.92 with loss1: 341.20, loss2: 37.05 and loss3: 27855.67\n",
      "Epoch [4567], train_loss: 28229.87 with loss1: 340.76, loss2: 37.17 and loss3: 27851.94\n",
      "Epoch [4568], train_loss: 28226.87 with loss1: 341.67, loss2: 37.03 and loss3: 27848.18\n",
      "Epoch [4569], train_loss: 28222.85 with loss1: 341.26, loss2: 37.14 and loss3: 27844.45\n",
      "Epoch [4570], train_loss: 28220.28 with loss1: 342.55, loss2: 37.04 and loss3: 27840.68\n",
      "Epoch [4571], train_loss: 28216.03 with loss1: 342.04, loss2: 37.03 and loss3: 27836.95\n",
      "Epoch [4572], train_loss: 28213.07 with loss1: 342.86, loss2: 37.02 and loss3: 27833.20\n",
      "Epoch [4573], train_loss: 28209.19 with loss1: 342.67, loss2: 37.05 and loss3: 27829.46\n",
      "Epoch [4574], train_loss: 28206.27 with loss1: 343.60, loss2: 36.96 and loss3: 27825.71\n",
      "Epoch [4575], train_loss: 28202.10 with loss1: 343.16, loss2: 36.97 and loss3: 27821.96\n",
      "Epoch [4576], train_loss: 28199.26 with loss1: 344.09, loss2: 36.94 and loss3: 27818.22\n",
      "Epoch [4577], train_loss: 28194.38 with loss1: 343.02, loss2: 36.89 and loss3: 27814.47\n",
      "Epoch [4578], train_loss: 28192.96 with loss1: 345.37, loss2: 36.84 and loss3: 27810.74\n",
      "Epoch [4579], train_loss: 28189.37 with loss1: 345.31, loss2: 37.07 and loss3: 27806.98\n",
      "Epoch [4580], train_loss: 28186.60 with loss1: 346.42, loss2: 36.93 and loss3: 27803.25\n",
      "Epoch [4581], train_loss: 28183.33 with loss1: 346.87, loss2: 36.96 and loss3: 27799.49\n",
      "Epoch [4582], train_loss: 28179.82 with loss1: 347.20, loss2: 36.85 and loss3: 27795.77\n",
      "Epoch [4583], train_loss: 28176.00 with loss1: 346.99, loss2: 37.01 and loss3: 27792.00\n",
      "Epoch [4584], train_loss: 28172.79 with loss1: 347.64, loss2: 36.87 and loss3: 27788.28\n",
      "Epoch [4585], train_loss: 28169.47 with loss1: 348.05, loss2: 36.91 and loss3: 27784.52\n",
      "Epoch [4586], train_loss: 28167.53 with loss1: 349.91, loss2: 36.83 and loss3: 27780.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4587], train_loss: 28161.52 with loss1: 347.62, loss2: 36.87 and loss3: 27777.04\n",
      "Epoch [4588], train_loss: 28160.31 with loss1: 350.18, loss2: 36.83 and loss3: 27773.30\n",
      "Epoch [4589], train_loss: 28155.61 with loss1: 349.08, loss2: 36.97 and loss3: 27769.55\n",
      "Epoch [4590], train_loss: 28154.10 with loss1: 351.33, loss2: 36.96 and loss3: 27765.81\n",
      "Epoch [4591], train_loss: 28148.21 with loss1: 349.23, loss2: 36.90 and loss3: 27762.07\n",
      "Epoch [4592], train_loss: 28146.88 with loss1: 351.74, loss2: 36.80 and loss3: 27758.33\n",
      "Epoch [4593], train_loss: 28142.22 with loss1: 350.76, loss2: 36.87 and loss3: 27754.59\n",
      "Epoch [4594], train_loss: 28140.81 with loss1: 353.12, loss2: 36.84 and loss3: 27750.85\n",
      "Epoch [4595], train_loss: 28134.79 with loss1: 350.90, loss2: 36.78 and loss3: 27747.11\n",
      "Epoch [4596], train_loss: 28133.56 with loss1: 353.45, loss2: 36.74 and loss3: 27743.37\n",
      "Epoch [4597], train_loss: 28128.64 with loss1: 352.23, loss2: 36.79 and loss3: 27739.62\n",
      "Epoch [4598], train_loss: 28127.56 with loss1: 354.92, loss2: 36.74 and loss3: 27735.89\n",
      "Epoch [4599], train_loss: 28121.36 with loss1: 352.43, loss2: 36.80 and loss3: 27732.13\n",
      "Epoch [4600], train_loss: 28120.53 with loss1: 355.31, loss2: 36.81 and loss3: 27728.42\n",
      "Epoch [4601], train_loss: 28114.45 with loss1: 353.11, loss2: 36.69 and loss3: 27724.66\n",
      "Epoch [4602], train_loss: 28113.78 with loss1: 356.10, loss2: 36.74 and loss3: 27720.94\n",
      "Epoch [4603], train_loss: 28108.40 with loss1: 354.55, loss2: 36.67 and loss3: 27717.18\n",
      "Epoch [4604], train_loss: 28106.40 with loss1: 356.25, loss2: 36.69 and loss3: 27713.46\n",
      "Epoch [4605], train_loss: 28101.41 with loss1: 354.94, loss2: 36.77 and loss3: 27709.70\n",
      "Epoch [4606], train_loss: 28099.43 with loss1: 356.71, loss2: 36.74 and loss3: 27705.98\n",
      "Epoch [4607], train_loss: 28095.32 with loss1: 356.36, loss2: 36.74 and loss3: 27702.22\n",
      "Epoch [4608], train_loss: 28092.33 with loss1: 357.05, loss2: 36.78 and loss3: 27698.51\n",
      "Epoch [4609], train_loss: 28087.17 with loss1: 355.65, loss2: 36.78 and loss3: 27694.74\n",
      "Epoch [4610], train_loss: 28084.51 with loss1: 356.89, loss2: 36.59 and loss3: 27691.04\n",
      "Epoch [4611], train_loss: 28078.82 with loss1: 354.92, loss2: 36.63 and loss3: 27687.27\n",
      "Epoch [4612], train_loss: 28076.61 with loss1: 356.40, loss2: 36.65 and loss3: 27683.56\n",
      "Epoch [4613], train_loss: 28071.04 with loss1: 354.49, loss2: 36.75 and loss3: 27679.80\n",
      "Epoch [4614], train_loss: 28069.41 with loss1: 356.67, loss2: 36.65 and loss3: 27676.09\n",
      "Epoch [4615], train_loss: 28063.36 with loss1: 354.49, loss2: 36.53 and loss3: 27672.34\n",
      "Epoch [4616], train_loss: 28061.08 with loss1: 355.89, loss2: 36.56 and loss3: 27668.62\n",
      "Epoch [4617], train_loss: 28055.59 with loss1: 354.05, loss2: 36.66 and loss3: 27664.87\n",
      "Epoch [4618], train_loss: 28052.52 with loss1: 354.81, loss2: 36.56 and loss3: 27661.16\n",
      "Epoch [4619], train_loss: 28045.95 with loss1: 351.98, loss2: 36.56 and loss3: 27657.41\n",
      "Epoch [4620], train_loss: 28044.42 with loss1: 354.04, loss2: 36.69 and loss3: 27653.69\n",
      "Epoch [4621], train_loss: 28038.38 with loss1: 351.93, loss2: 36.49 and loss3: 27649.96\n",
      "Epoch [4622], train_loss: 28035.78 with loss1: 353.08, loss2: 36.48 and loss3: 27646.22\n",
      "Epoch [4623], train_loss: 28030.45 with loss1: 351.47, loss2: 36.48 and loss3: 27642.50\n",
      "Epoch [4624], train_loss: 28026.88 with loss1: 351.53, loss2: 36.58 and loss3: 27638.76\n",
      "Epoch [4625], train_loss: 28022.06 with loss1: 350.51, loss2: 36.51 and loss3: 27635.04\n",
      "Epoch [4626], train_loss: 28019.41 with loss1: 351.60, loss2: 36.51 and loss3: 27631.30\n",
      "Epoch [4627], train_loss: 28013.11 with loss1: 349.06, loss2: 36.47 and loss3: 27627.58\n",
      "Epoch [4628], train_loss: 28011.22 with loss1: 350.94, loss2: 36.44 and loss3: 27623.84\n",
      "Epoch [4629], train_loss: 28005.62 with loss1: 349.07, loss2: 36.44 and loss3: 27620.12\n",
      "Epoch [4630], train_loss: 28002.16 with loss1: 349.43, loss2: 36.34 and loss3: 27616.39\n",
      "Epoch [4631], train_loss: 27998.68 with loss1: 349.42, loss2: 36.60 and loss3: 27612.66\n",
      "Epoch [4632], train_loss: 27994.94 with loss1: 349.59, loss2: 36.42 and loss3: 27608.93\n",
      "Epoch [4633], train_loss: 27990.94 with loss1: 349.34, loss2: 36.40 and loss3: 27605.20\n",
      "Epoch [4634], train_loss: 27986.93 with loss1: 349.02, loss2: 36.43 and loss3: 27601.48\n",
      "Epoch [4635], train_loss: 27981.46 with loss1: 347.46, loss2: 36.25 and loss3: 27597.74\n",
      "Epoch [4636], train_loss: 27979.62 with loss1: 349.17, loss2: 36.42 and loss3: 27594.02\n",
      "Epoch [4637], train_loss: 27974.33 with loss1: 347.80, loss2: 36.25 and loss3: 27590.29\n",
      "Epoch [4638], train_loss: 27970.16 with loss1: 347.21, loss2: 36.38 and loss3: 27586.57\n",
      "Epoch [4639], train_loss: 27965.53 with loss1: 346.26, loss2: 36.43 and loss3: 27582.84\n",
      "Epoch [4640], train_loss: 27962.36 with loss1: 346.85, loss2: 36.39 and loss3: 27579.11\n",
      "Epoch [4641], train_loss: 27957.34 with loss1: 345.78, loss2: 36.18 and loss3: 27575.38\n",
      "Epoch [4642], train_loss: 27953.95 with loss1: 345.99, loss2: 36.30 and loss3: 27571.66\n",
      "Epoch [4643], train_loss: 27949.10 with loss1: 344.97, loss2: 36.20 and loss3: 27567.93\n",
      "Epoch [4644], train_loss: 27946.83 with loss1: 346.37, loss2: 36.23 and loss3: 27564.22\n",
      "Epoch [4645], train_loss: 27942.05 with loss1: 345.25, loss2: 36.32 and loss3: 27560.48\n",
      "Epoch [4646], train_loss: 27938.85 with loss1: 345.83, loss2: 36.25 and loss3: 27556.77\n",
      "Epoch [4647], train_loss: 27934.67 with loss1: 345.43, loss2: 36.22 and loss3: 27553.03\n",
      "Epoch [4648], train_loss: 27930.97 with loss1: 345.34, loss2: 36.31 and loss3: 27549.32\n",
      "Epoch [4649], train_loss: 27926.82 with loss1: 344.92, loss2: 36.31 and loss3: 27545.59\n",
      "Epoch [4650], train_loss: 27924.21 with loss1: 346.16, loss2: 36.18 and loss3: 27541.87\n",
      "Epoch [4651], train_loss: 27919.23 with loss1: 344.90, loss2: 36.19 and loss3: 27538.14\n",
      "Epoch [4652], train_loss: 27916.32 with loss1: 345.71, loss2: 36.19 and loss3: 27534.42\n",
      "Epoch [4653], train_loss: 27911.47 with loss1: 344.59, loss2: 36.19 and loss3: 27530.70\n",
      "Epoch [4654], train_loss: 27907.88 with loss1: 344.77, loss2: 36.14 and loss3: 27526.97\n",
      "Epoch [4655], train_loss: 27902.34 with loss1: 343.04, loss2: 36.05 and loss3: 27523.25\n",
      "Epoch [4656], train_loss: 27901.34 with loss1: 345.63, loss2: 36.18 and loss3: 27519.53\n",
      "Epoch [4657], train_loss: 27896.55 with loss1: 344.56, loss2: 36.18 and loss3: 27515.80\n",
      "Epoch [4658], train_loss: 27893.44 with loss1: 345.14, loss2: 36.21 and loss3: 27512.09\n",
      "Epoch [4659], train_loss: 27887.61 with loss1: 343.21, loss2: 36.04 and loss3: 27508.36\n",
      "Epoch [4660], train_loss: 27885.12 with loss1: 344.25, loss2: 36.22 and loss3: 27504.65\n",
      "Epoch [4661], train_loss: 27880.79 with loss1: 343.79, loss2: 36.08 and loss3: 27500.92\n",
      "Epoch [4662], train_loss: 27878.01 with loss1: 344.54, loss2: 36.26 and loss3: 27497.21\n",
      "Epoch [4663], train_loss: 27872.78 with loss1: 343.41, loss2: 35.89 and loss3: 27493.48\n",
      "Epoch [4664], train_loss: 27869.54 with loss1: 343.72, loss2: 36.05 and loss3: 27489.76\n",
      "Epoch [4665], train_loss: 27865.33 with loss1: 343.33, loss2: 35.96 and loss3: 27486.04\n",
      "Epoch [4666], train_loss: 27862.75 with loss1: 344.38, loss2: 36.06 and loss3: 27482.32\n",
      "Epoch [4667], train_loss: 27856.96 with loss1: 342.33, loss2: 36.03 and loss3: 27478.60\n",
      "Epoch [4668], train_loss: 27854.33 with loss1: 343.38, loss2: 36.06 and loss3: 27474.89\n",
      "Epoch [4669], train_loss: 27849.39 with loss1: 342.31, loss2: 35.91 and loss3: 27471.16\n",
      "Epoch [4670], train_loss: 27846.46 with loss1: 342.92, loss2: 36.10 and loss3: 27467.45\n",
      "Epoch [4671], train_loss: 27841.89 with loss1: 342.20, loss2: 35.96 and loss3: 27463.73\n",
      "Epoch [4672], train_loss: 27839.11 with loss1: 343.05, loss2: 36.05 and loss3: 27460.01\n",
      "Epoch [4673], train_loss: 27834.28 with loss1: 342.03, loss2: 35.96 and loss3: 27456.29\n",
      "Epoch [4674], train_loss: 27831.41 with loss1: 342.89, loss2: 35.95 and loss3: 27452.58\n",
      "Epoch [4675], train_loss: 27828.47 with loss1: 343.67, loss2: 35.95 and loss3: 27448.85\n",
      "Epoch [4676], train_loss: 27824.19 with loss1: 343.07, loss2: 35.98 and loss3: 27445.14\n",
      "Epoch [4677], train_loss: 27819.12 with loss1: 341.80, loss2: 35.90 and loss3: 27441.42\n",
      "Epoch [4678], train_loss: 27817.28 with loss1: 343.64, loss2: 35.94 and loss3: 27437.71\n",
      "Epoch [4679], train_loss: 27810.85 with loss1: 341.00, loss2: 35.86 and loss3: 27433.99\n",
      "Epoch [4680], train_loss: 27808.82 with loss1: 342.57, loss2: 35.97 and loss3: 27430.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4681], train_loss: 27803.36 with loss1: 340.96, loss2: 35.84 and loss3: 27426.56\n",
      "Epoch [4682], train_loss: 27800.66 with loss1: 341.90, loss2: 35.91 and loss3: 27422.85\n",
      "Epoch [4683], train_loss: 27795.00 with loss1: 340.11, loss2: 35.75 and loss3: 27419.14\n",
      "Epoch [4684], train_loss: 27793.13 with loss1: 341.78, loss2: 35.93 and loss3: 27415.42\n",
      "Epoch [4685], train_loss: 27790.02 with loss1: 342.48, loss2: 35.83 and loss3: 27411.71\n",
      "Epoch [4686], train_loss: 27785.70 with loss1: 341.85, loss2: 35.86 and loss3: 27407.99\n",
      "Epoch [4687], train_loss: 27780.85 with loss1: 340.75, loss2: 35.82 and loss3: 27404.28\n",
      "Epoch [4688], train_loss: 27778.34 with loss1: 341.93, loss2: 35.84 and loss3: 27400.57\n",
      "Epoch [4689], train_loss: 27775.17 with loss1: 342.41, loss2: 35.90 and loss3: 27396.86\n",
      "Epoch [4690], train_loss: 27772.61 with loss1: 343.66, loss2: 35.82 and loss3: 27393.14\n",
      "Epoch [4691], train_loss: 27767.97 with loss1: 342.81, loss2: 35.73 and loss3: 27389.43\n",
      "Epoch [4692], train_loss: 27765.49 with loss1: 344.04, loss2: 35.73 and loss3: 27385.72\n",
      "Epoch [4693], train_loss: 27760.47 with loss1: 342.75, loss2: 35.72 and loss3: 27382.00\n",
      "Epoch [4694], train_loss: 27758.60 with loss1: 344.45, loss2: 35.84 and loss3: 27378.30\n",
      "Epoch [4695], train_loss: 27753.09 with loss1: 342.74, loss2: 35.78 and loss3: 27374.57\n",
      "Epoch [4696], train_loss: 27751.79 with loss1: 345.12, loss2: 35.79 and loss3: 27370.88\n",
      "Epoch [4697], train_loss: 27747.27 with loss1: 344.43, loss2: 35.68 and loss3: 27367.15\n",
      "Epoch [4698], train_loss: 27744.27 with loss1: 345.10, loss2: 35.71 and loss3: 27363.46\n",
      "Epoch [4699], train_loss: 27741.01 with loss1: 345.60, loss2: 35.69 and loss3: 27359.73\n",
      "Epoch [4700], train_loss: 27738.16 with loss1: 346.36, loss2: 35.77 and loss3: 27356.04\n",
      "Epoch [4701], train_loss: 27734.72 with loss1: 346.71, loss2: 35.71 and loss3: 27352.31\n",
      "Epoch [4702], train_loss: 27731.48 with loss1: 347.10, loss2: 35.76 and loss3: 27348.62\n",
      "Epoch [4703], train_loss: 27727.52 with loss1: 346.84, loss2: 35.79 and loss3: 27344.89\n",
      "Epoch [4704], train_loss: 27726.09 with loss1: 349.29, loss2: 35.59 and loss3: 27341.20\n",
      "Epoch [4705], train_loss: 27721.01 with loss1: 347.86, loss2: 35.68 and loss3: 27337.46\n",
      "Epoch [4706], train_loss: 27718.57 with loss1: 349.09, loss2: 35.70 and loss3: 27333.78\n",
      "Epoch [4707], train_loss: 27712.46 with loss1: 346.68, loss2: 35.73 and loss3: 27330.05\n",
      "Epoch [4708], train_loss: 27710.27 with loss1: 348.30, loss2: 35.61 and loss3: 27326.37\n",
      "Epoch [4709], train_loss: 27704.83 with loss1: 346.57, loss2: 35.63 and loss3: 27322.64\n",
      "Epoch [4710], train_loss: 27702.57 with loss1: 347.94, loss2: 35.68 and loss3: 27318.95\n",
      "Epoch [4711], train_loss: 27697.24 with loss1: 346.47, loss2: 35.55 and loss3: 27315.23\n",
      "Epoch [4712], train_loss: 27694.71 with loss1: 347.59, loss2: 35.58 and loss3: 27311.53\n",
      "Epoch [4713], train_loss: 27690.55 with loss1: 347.11, loss2: 35.63 and loss3: 27307.82\n",
      "Epoch [4714], train_loss: 27687.23 with loss1: 347.59, loss2: 35.53 and loss3: 27304.11\n",
      "Epoch [4715], train_loss: 27682.56 with loss1: 346.43, loss2: 35.72 and loss3: 27300.41\n",
      "Epoch [4716], train_loss: 27679.75 with loss1: 347.49, loss2: 35.57 and loss3: 27296.69\n",
      "Epoch [4717], train_loss: 27673.97 with loss1: 345.31, loss2: 35.65 and loss3: 27293.00\n",
      "Epoch [4718], train_loss: 27671.20 with loss1: 346.32, loss2: 35.61 and loss3: 27289.28\n",
      "Epoch [4719], train_loss: 27665.79 with loss1: 344.62, loss2: 35.58 and loss3: 27285.59\n",
      "Epoch [4720], train_loss: 27662.77 with loss1: 345.39, loss2: 35.51 and loss3: 27281.87\n",
      "Epoch [4721], train_loss: 27658.30 with loss1: 344.53, loss2: 35.58 and loss3: 27278.19\n",
      "Epoch [4722], train_loss: 27655.03 with loss1: 345.01, loss2: 35.56 and loss3: 27274.45\n",
      "Epoch [4723], train_loss: 27649.84 with loss1: 343.53, loss2: 35.53 and loss3: 27270.77\n",
      "Epoch [4724], train_loss: 27646.63 with loss1: 344.09, loss2: 35.50 and loss3: 27267.04\n",
      "Epoch [4725], train_loss: 27640.89 with loss1: 341.91, loss2: 35.62 and loss3: 27263.36\n",
      "Epoch [4726], train_loss: 27637.54 with loss1: 342.39, loss2: 35.52 and loss3: 27259.63\n",
      "Epoch [4727], train_loss: 27633.36 with loss1: 341.84, loss2: 35.56 and loss3: 27255.95\n",
      "Epoch [4728], train_loss: 27627.90 with loss1: 340.05, loss2: 35.64 and loss3: 27252.22\n",
      "Epoch [4729], train_loss: 27622.27 with loss1: 338.25, loss2: 35.47 and loss3: 27248.54\n",
      "Epoch [4730], train_loss: 27618.95 with loss1: 338.63, loss2: 35.51 and loss3: 27244.82\n",
      "Epoch [4731], train_loss: 27612.96 with loss1: 336.32, loss2: 35.51 and loss3: 27241.13\n",
      "Epoch [4732], train_loss: 27610.10 with loss1: 337.26, loss2: 35.42 and loss3: 27237.41\n",
      "Epoch [4733], train_loss: 27605.87 with loss1: 336.66, loss2: 35.48 and loss3: 27233.73\n",
      "Epoch [4734], train_loss: 27602.24 with loss1: 336.79, loss2: 35.46 and loss3: 27230.00\n",
      "Epoch [4735], train_loss: 27595.94 with loss1: 334.17, loss2: 35.44 and loss3: 27226.32\n",
      "Epoch [4736], train_loss: 27592.92 with loss1: 334.83, loss2: 35.50 and loss3: 27222.59\n",
      "Epoch [4737], train_loss: 27587.40 with loss1: 333.06, loss2: 35.43 and loss3: 27218.92\n",
      "Epoch [4738], train_loss: 27584.02 with loss1: 333.43, loss2: 35.39 and loss3: 27215.19\n",
      "Epoch [4739], train_loss: 27579.00 with loss1: 332.09, loss2: 35.40 and loss3: 27211.51\n",
      "Epoch [4740], train_loss: 27574.62 with loss1: 331.39, loss2: 35.44 and loss3: 27207.80\n",
      "Epoch [4741], train_loss: 27570.78 with loss1: 331.33, loss2: 35.35 and loss3: 27204.10\n",
      "Epoch [4742], train_loss: 27567.34 with loss1: 331.64, loss2: 35.29 and loss3: 27200.40\n",
      "Epoch [4743], train_loss: 27561.99 with loss1: 329.89, loss2: 35.40 and loss3: 27196.70\n",
      "Epoch [4744], train_loss: 27559.22 with loss1: 330.94, loss2: 35.27 and loss3: 27193.00\n",
      "Epoch [4745], train_loss: 27553.60 with loss1: 328.91, loss2: 35.40 and loss3: 27189.29\n",
      "Epoch [4746], train_loss: 27549.96 with loss1: 329.08, loss2: 35.28 and loss3: 27185.60\n",
      "Epoch [4747], train_loss: 27545.04 with loss1: 327.89, loss2: 35.25 and loss3: 27181.89\n",
      "Epoch [4748], train_loss: 27541.34 with loss1: 327.77, loss2: 35.36 and loss3: 27178.20\n",
      "Epoch [4749], train_loss: 27536.40 with loss1: 326.68, loss2: 35.23 and loss3: 27174.49\n",
      "Epoch [4750], train_loss: 27533.30 with loss1: 327.33, loss2: 35.17 and loss3: 27170.80\n",
      "Epoch [4751], train_loss: 27530.04 with loss1: 327.64, loss2: 35.30 and loss3: 27167.09\n",
      "Epoch [4752], train_loss: 27525.53 with loss1: 326.94, loss2: 35.19 and loss3: 27163.40\n",
      "Epoch [4753], train_loss: 27521.78 with loss1: 326.90, loss2: 35.18 and loss3: 27159.70\n",
      "Epoch [4754], train_loss: 27517.83 with loss1: 326.65, loss2: 35.18 and loss3: 27156.00\n",
      "Epoch [4755], train_loss: 27515.51 with loss1: 328.02, loss2: 35.18 and loss3: 27152.30\n",
      "Epoch [4756], train_loss: 27511.08 with loss1: 327.25, loss2: 35.24 and loss3: 27148.60\n",
      "Epoch [4757], train_loss: 27507.06 with loss1: 326.91, loss2: 35.24 and loss3: 27144.91\n",
      "Epoch [4758], train_loss: 27502.97 with loss1: 326.66, loss2: 35.10 and loss3: 27141.20\n",
      "Epoch [4759], train_loss: 27498.57 with loss1: 325.83, loss2: 35.22 and loss3: 27137.52\n",
      "Epoch [4760], train_loss: 27495.05 with loss1: 326.02, loss2: 35.22 and loss3: 27133.80\n",
      "Epoch [4761], train_loss: 27491.13 with loss1: 325.84, loss2: 35.16 and loss3: 27130.13\n",
      "Epoch [4762], train_loss: 27487.76 with loss1: 326.21, loss2: 35.15 and loss3: 27126.40\n",
      "Epoch [4763], train_loss: 27483.89 with loss1: 326.06, loss2: 35.09 and loss3: 27122.74\n",
      "Epoch [4764], train_loss: 27480.43 with loss1: 326.29, loss2: 35.13 and loss3: 27119.01\n",
      "Epoch [4765], train_loss: 27476.89 with loss1: 326.46, loss2: 35.08 and loss3: 27115.35\n",
      "Epoch [4766], train_loss: 27473.65 with loss1: 326.90, loss2: 35.13 and loss3: 27111.62\n",
      "Epoch [4767], train_loss: 27469.73 with loss1: 326.74, loss2: 35.03 and loss3: 27107.96\n",
      "Epoch [4768], train_loss: 27466.81 with loss1: 327.45, loss2: 35.13 and loss3: 27104.23\n",
      "Epoch [4769], train_loss: 27462.50 with loss1: 326.84, loss2: 35.10 and loss3: 27100.56\n",
      "Epoch [4770], train_loss: 27459.03 with loss1: 327.15, loss2: 35.03 and loss3: 27096.84\n",
      "Epoch [4771], train_loss: 27455.35 with loss1: 327.17, loss2: 35.01 and loss3: 27093.17\n",
      "Epoch [4772], train_loss: 27452.76 with loss1: 328.27, loss2: 35.03 and loss3: 27089.45\n",
      "Epoch [4773], train_loss: 27448.79 with loss1: 327.90, loss2: 35.10 and loss3: 27085.79\n",
      "Epoch [4774], train_loss: 27446.80 with loss1: 329.59, loss2: 35.15 and loss3: 27082.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4775], train_loss: 27442.17 with loss1: 328.86, loss2: 34.90 and loss3: 27078.40\n",
      "Epoch [4776], train_loss: 27440.02 with loss1: 330.34, loss2: 35.00 and loss3: 27074.68\n",
      "Epoch [4777], train_loss: 27436.58 with loss1: 330.40, loss2: 35.15 and loss3: 27071.02\n",
      "Epoch [4778], train_loss: 27433.69 with loss1: 331.42, loss2: 34.97 and loss3: 27067.29\n",
      "Epoch [4779], train_loss: 27430.00 with loss1: 331.39, loss2: 34.97 and loss3: 27063.63\n",
      "Epoch [4780], train_loss: 27427.72 with loss1: 332.89, loss2: 34.92 and loss3: 27059.91\n",
      "Epoch [4781], train_loss: 27423.56 with loss1: 332.42, loss2: 34.89 and loss3: 27056.25\n",
      "Epoch [4782], train_loss: 27421.25 with loss1: 333.77, loss2: 34.95 and loss3: 27052.53\n",
      "Epoch [4783], train_loss: 27417.61 with loss1: 333.84, loss2: 34.90 and loss3: 27048.87\n",
      "Epoch [4784], train_loss: 27415.91 with loss1: 335.83, loss2: 34.93 and loss3: 27045.15\n",
      "Epoch [4785], train_loss: 27412.01 with loss1: 335.56, loss2: 34.97 and loss3: 27041.49\n",
      "Epoch [4786], train_loss: 27410.59 with loss1: 337.94, loss2: 34.88 and loss3: 27037.77\n",
      "Epoch [4787], train_loss: 27407.21 with loss1: 338.21, loss2: 34.90 and loss3: 27034.11\n",
      "Epoch [4788], train_loss: 27406.04 with loss1: 340.67, loss2: 34.98 and loss3: 27030.39\n",
      "Epoch [4789], train_loss: 27401.74 with loss1: 340.20, loss2: 34.81 and loss3: 27026.73\n",
      "Epoch [4790], train_loss: 27400.33 with loss1: 342.45, loss2: 34.87 and loss3: 27023.02\n",
      "Epoch [4791], train_loss: 27396.42 with loss1: 342.21, loss2: 34.86 and loss3: 27019.36\n",
      "Epoch [4792], train_loss: 27396.39 with loss1: 345.96, loss2: 34.79 and loss3: 27015.64\n",
      "Epoch [4793], train_loss: 27392.80 with loss1: 345.84, loss2: 34.97 and loss3: 27011.98\n",
      "Epoch [4794], train_loss: 27391.27 with loss1: 348.04, loss2: 34.96 and loss3: 27008.27\n",
      "Epoch [4795], train_loss: 27387.79 with loss1: 348.32, loss2: 34.86 and loss3: 27004.61\n",
      "Epoch [4796], train_loss: 27387.47 with loss1: 351.78, loss2: 34.80 and loss3: 27000.89\n",
      "Epoch [4797], train_loss: 27382.82 with loss1: 350.77, loss2: 34.80 and loss3: 26997.24\n",
      "Epoch [4798], train_loss: 27382.57 with loss1: 354.17, loss2: 34.88 and loss3: 26993.52\n",
      "Epoch [4799], train_loss: 27378.11 with loss1: 353.42, loss2: 34.82 and loss3: 26989.87\n",
      "Epoch [4800], train_loss: 27378.02 with loss1: 357.13, loss2: 34.73 and loss3: 26986.15\n",
      "Epoch [4801], train_loss: 27373.39 with loss1: 356.08, loss2: 34.80 and loss3: 26982.50\n",
      "Epoch [4802], train_loss: 27372.93 with loss1: 359.43, loss2: 34.72 and loss3: 26978.79\n",
      "Epoch [4803], train_loss: 27367.33 with loss1: 357.48, loss2: 34.72 and loss3: 26975.13\n",
      "Epoch [4804], train_loss: 27367.37 with loss1: 361.21, loss2: 34.74 and loss3: 26971.42\n",
      "Epoch [4805], train_loss: 27362.12 with loss1: 359.52, loss2: 34.84 and loss3: 26967.76\n",
      "Epoch [4806], train_loss: 27361.96 with loss1: 363.13, loss2: 34.78 and loss3: 26964.05\n",
      "Epoch [4807], train_loss: 27356.13 with loss1: 361.02, loss2: 34.72 and loss3: 26960.39\n",
      "Epoch [4808], train_loss: 27356.21 with loss1: 364.82, loss2: 34.71 and loss3: 26956.68\n",
      "Epoch [4809], train_loss: 27349.46 with loss1: 361.60, loss2: 34.83 and loss3: 26953.03\n",
      "Epoch [4810], train_loss: 27347.65 with loss1: 363.59, loss2: 34.75 and loss3: 26949.32\n",
      "Epoch [4811], train_loss: 27341.58 with loss1: 361.32, loss2: 34.60 and loss3: 26945.66\n",
      "Epoch [4812], train_loss: 27341.82 with loss1: 365.19, loss2: 34.67 and loss3: 26941.95\n",
      "Epoch [4813], train_loss: 27335.14 with loss1: 362.25, loss2: 34.60 and loss3: 26938.29\n",
      "Epoch [4814], train_loss: 27333.02 with loss1: 363.80, loss2: 34.63 and loss3: 26934.59\n",
      "Epoch [4815], train_loss: 27326.57 with loss1: 361.00, loss2: 34.64 and loss3: 26930.93\n",
      "Epoch [4816], train_loss: 27325.36 with loss1: 363.50, loss2: 34.64 and loss3: 26927.23\n",
      "Epoch [4817], train_loss: 27319.74 with loss1: 361.55, loss2: 34.62 and loss3: 26923.57\n",
      "Epoch [4818], train_loss: 27317.64 with loss1: 363.19, loss2: 34.59 and loss3: 26919.86\n",
      "Epoch [4819], train_loss: 27309.79 with loss1: 358.99, loss2: 34.60 and loss3: 26916.21\n",
      "Epoch [4820], train_loss: 27308.23 with loss1: 361.01, loss2: 34.72 and loss3: 26912.50\n",
      "Epoch [4821], train_loss: 27301.35 with loss1: 357.78, loss2: 34.73 and loss3: 26908.85\n",
      "Epoch [4822], train_loss: 27298.39 with loss1: 358.74, loss2: 34.51 and loss3: 26905.14\n",
      "Epoch [4823], train_loss: 27291.94 with loss1: 355.83, loss2: 34.63 and loss3: 26901.48\n",
      "Epoch [4824], train_loss: 27289.01 with loss1: 356.48, loss2: 34.75 and loss3: 26897.78\n",
      "Epoch [4825], train_loss: 27282.30 with loss1: 353.68, loss2: 34.49 and loss3: 26894.12\n",
      "Epoch [4826], train_loss: 27280.09 with loss1: 355.10, loss2: 34.57 and loss3: 26890.43\n",
      "Epoch [4827], train_loss: 27274.31 with loss1: 352.97, loss2: 34.57 and loss3: 26886.77\n",
      "Epoch [4828], train_loss: 27270.89 with loss1: 353.31, loss2: 34.52 and loss3: 26883.07\n",
      "Epoch [4829], train_loss: 27265.75 with loss1: 351.84, loss2: 34.50 and loss3: 26879.41\n",
      "Epoch [4830], train_loss: 27263.66 with loss1: 353.41, loss2: 34.55 and loss3: 26875.71\n",
      "Epoch [4831], train_loss: 27256.49 with loss1: 349.93, loss2: 34.51 and loss3: 26872.05\n",
      "Epoch [4832], train_loss: 27253.74 with loss1: 350.94, loss2: 34.45 and loss3: 26868.35\n",
      "Epoch [4833], train_loss: 27247.78 with loss1: 348.62, loss2: 34.47 and loss3: 26864.69\n",
      "Epoch [4834], train_loss: 27245.14 with loss1: 349.66, loss2: 34.49 and loss3: 26861.00\n",
      "Epoch [4835], train_loss: 27238.88 with loss1: 347.14, loss2: 34.42 and loss3: 26857.33\n",
      "Epoch [4836], train_loss: 27237.19 with loss1: 349.08, loss2: 34.48 and loss3: 26853.64\n",
      "Epoch [4837], train_loss: 27229.67 with loss1: 345.28, loss2: 34.42 and loss3: 26849.97\n",
      "Epoch [4838], train_loss: 27228.06 with loss1: 347.35, loss2: 34.42 and loss3: 26846.28\n",
      "Epoch [4839], train_loss: 27221.95 with loss1: 344.87, loss2: 34.46 and loss3: 26842.61\n",
      "Epoch [4840], train_loss: 27219.27 with loss1: 345.84, loss2: 34.51 and loss3: 26838.93\n",
      "Epoch [4841], train_loss: 27213.36 with loss1: 343.72, loss2: 34.39 and loss3: 26835.25\n",
      "Epoch [4842], train_loss: 27210.29 with loss1: 344.29, loss2: 34.43 and loss3: 26831.57\n",
      "Epoch [4843], train_loss: 27204.93 with loss1: 342.68, loss2: 34.36 and loss3: 26827.90\n",
      "Epoch [4844], train_loss: 27202.40 with loss1: 343.78, loss2: 34.40 and loss3: 26824.21\n",
      "Epoch [4845], train_loss: 27197.76 with loss1: 342.78, loss2: 34.44 and loss3: 26820.54\n",
      "Epoch [4846], train_loss: 27193.76 with loss1: 342.51, loss2: 34.39 and loss3: 26816.86\n",
      "Epoch [4847], train_loss: 27189.08 with loss1: 341.55, loss2: 34.34 and loss3: 26813.19\n",
      "Epoch [4848], train_loss: 27185.73 with loss1: 341.81, loss2: 34.42 and loss3: 26809.50\n",
      "Epoch [4849], train_loss: 27180.32 with loss1: 340.20, loss2: 34.28 and loss3: 26805.84\n",
      "Epoch [4850], train_loss: 27177.41 with loss1: 340.91, loss2: 34.34 and loss3: 26802.16\n",
      "Epoch [4851], train_loss: 27171.58 with loss1: 338.86, loss2: 34.24 and loss3: 26798.48\n",
      "Epoch [4852], train_loss: 27168.72 with loss1: 339.52, loss2: 34.38 and loss3: 26794.81\n",
      "Epoch [4853], train_loss: 27163.38 with loss1: 337.97, loss2: 34.28 and loss3: 26791.13\n",
      "Epoch [4854], train_loss: 27160.17 with loss1: 338.29, loss2: 34.41 and loss3: 26787.47\n",
      "Epoch [4855], train_loss: 27155.43 with loss1: 337.41, loss2: 34.24 and loss3: 26783.78\n",
      "Epoch [4856], train_loss: 27150.77 with loss1: 336.36, loss2: 34.29 and loss3: 26780.12\n",
      "Epoch [4857], train_loss: 27145.08 with loss1: 334.39, loss2: 34.25 and loss3: 26776.44\n",
      "Epoch [4858], train_loss: 27143.34 with loss1: 336.25, loss2: 34.32 and loss3: 26772.77\n",
      "Epoch [4859], train_loss: 27136.50 with loss1: 333.13, loss2: 34.27 and loss3: 26769.10\n",
      "Epoch [4860], train_loss: 27134.34 with loss1: 334.66, loss2: 34.25 and loss3: 26765.43\n",
      "Epoch [4861], train_loss: 27128.54 with loss1: 332.67, loss2: 34.12 and loss3: 26761.76\n",
      "Epoch [4862], train_loss: 27125.50 with loss1: 333.16, loss2: 34.26 and loss3: 26758.08\n",
      "Epoch [4863], train_loss: 27120.81 with loss1: 332.22, loss2: 34.17 and loss3: 26754.41\n",
      "Epoch [4864], train_loss: 27117.61 with loss1: 332.57, loss2: 34.31 and loss3: 26750.74\n",
      "Epoch [4865], train_loss: 27111.76 with loss1: 330.62, loss2: 34.07 and loss3: 26747.07\n",
      "Epoch [4866], train_loss: 27108.76 with loss1: 330.97, loss2: 34.39 and loss3: 26743.40\n",
      "Epoch [4867], train_loss: 27104.47 with loss1: 330.61, loss2: 34.13 and loss3: 26739.73\n",
      "Epoch [4868], train_loss: 27100.41 with loss1: 330.20, loss2: 34.15 and loss3: 26736.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4869], train_loss: 27095.30 with loss1: 328.82, loss2: 34.10 and loss3: 26732.38\n",
      "Epoch [4870], train_loss: 27092.01 with loss1: 329.02, loss2: 34.27 and loss3: 26728.72\n",
      "Epoch [4871], train_loss: 27087.86 with loss1: 328.72, loss2: 34.10 and loss3: 26725.04\n",
      "Epoch [4872], train_loss: 27083.19 with loss1: 327.65, loss2: 34.16 and loss3: 26721.38\n",
      "Epoch [4873], train_loss: 27078.75 with loss1: 326.96, loss2: 34.09 and loss3: 26717.70\n",
      "Epoch [4874], train_loss: 27075.90 with loss1: 327.85, loss2: 34.01 and loss3: 26714.04\n",
      "Epoch [4875], train_loss: 27071.45 with loss1: 327.05, loss2: 34.04 and loss3: 26710.36\n",
      "Epoch [4876], train_loss: 27068.33 with loss1: 327.51, loss2: 34.13 and loss3: 26706.70\n",
      "Epoch [4877], train_loss: 27063.75 with loss1: 326.75, loss2: 33.98 and loss3: 26703.02\n",
      "Epoch [4878], train_loss: 27060.46 with loss1: 327.02, loss2: 34.07 and loss3: 26699.36\n",
      "Epoch [4879], train_loss: 27055.99 with loss1: 326.44, loss2: 33.87 and loss3: 26695.68\n",
      "Epoch [4880], train_loss: 27052.87 with loss1: 326.90, loss2: 33.94 and loss3: 26692.03\n",
      "Epoch [4881], train_loss: 27047.94 with loss1: 325.62, loss2: 33.98 and loss3: 26688.34\n",
      "Epoch [4882], train_loss: 27045.31 with loss1: 326.58, loss2: 34.04 and loss3: 26684.69\n",
      "Epoch [4883], train_loss: 27039.76 with loss1: 324.82, loss2: 33.93 and loss3: 26681.01\n",
      "Epoch [4884], train_loss: 27037.53 with loss1: 326.20, loss2: 33.98 and loss3: 26677.35\n",
      "Epoch [4885], train_loss: 27032.85 with loss1: 325.22, loss2: 33.94 and loss3: 26673.68\n",
      "Epoch [4886], train_loss: 27028.45 with loss1: 324.46, loss2: 33.98 and loss3: 26670.01\n",
      "Epoch [4887], train_loss: 27025.01 with loss1: 324.80, loss2: 33.87 and loss3: 26666.34\n",
      "Epoch [4888], train_loss: 27022.37 with loss1: 325.57, loss2: 34.12 and loss3: 26662.68\n",
      "Epoch [4889], train_loss: 27017.76 with loss1: 324.83, loss2: 33.92 and loss3: 26659.01\n",
      "Epoch [4890], train_loss: 27013.92 with loss1: 324.60, loss2: 33.97 and loss3: 26655.35\n",
      "Epoch [4891], train_loss: 27009.42 with loss1: 323.93, loss2: 33.82 and loss3: 26651.67\n",
      "Epoch [4892], train_loss: 27006.37 with loss1: 324.43, loss2: 33.93 and loss3: 26648.01\n",
      "Epoch [4893], train_loss: 27002.26 with loss1: 324.01, loss2: 33.91 and loss3: 26644.34\n",
      "Epoch [4894], train_loss: 26999.53 with loss1: 325.03, loss2: 33.82 and loss3: 26640.68\n",
      "Epoch [4895], train_loss: 26994.89 with loss1: 324.10, loss2: 33.78 and loss3: 26637.01\n",
      "Epoch [4896], train_loss: 26992.03 with loss1: 324.81, loss2: 33.88 and loss3: 26633.34\n",
      "Epoch [4897], train_loss: 26987.71 with loss1: 324.16, loss2: 33.86 and loss3: 26629.68\n",
      "Epoch [4898], train_loss: 26985.03 with loss1: 325.16, loss2: 33.86 and loss3: 26626.01\n",
      "Epoch [4899], train_loss: 26981.70 with loss1: 325.37, loss2: 33.97 and loss3: 26622.35\n",
      "Epoch [4900], train_loss: 26978.08 with loss1: 325.52, loss2: 33.88 and loss3: 26618.68\n",
      "Epoch [4901], train_loss: 26973.06 with loss1: 324.23, loss2: 33.81 and loss3: 26615.02\n",
      "Epoch [4902], train_loss: 26970.47 with loss1: 325.29, loss2: 33.83 and loss3: 26611.34\n",
      "Epoch [4903], train_loss: 26966.22 with loss1: 324.77, loss2: 33.76 and loss3: 26607.69\n",
      "Epoch [4904], train_loss: 26963.55 with loss1: 325.61, loss2: 33.93 and loss3: 26604.01\n",
      "Epoch [4905], train_loss: 26959.56 with loss1: 325.45, loss2: 33.74 and loss3: 26600.36\n",
      "Epoch [4906], train_loss: 26955.84 with loss1: 325.31, loss2: 33.84 and loss3: 26596.68\n",
      "Epoch [4907], train_loss: 26952.23 with loss1: 325.51, loss2: 33.68 and loss3: 26593.04\n",
      "Epoch [4908], train_loss: 26950.00 with loss1: 326.88, loss2: 33.77 and loss3: 26589.36\n",
      "Epoch [4909], train_loss: 26945.55 with loss1: 326.02, loss2: 33.80 and loss3: 26585.72\n",
      "Epoch [4910], train_loss: 26942.34 with loss1: 326.46, loss2: 33.85 and loss3: 26582.03\n",
      "Epoch [4911], train_loss: 26939.10 with loss1: 326.98, loss2: 33.73 and loss3: 26578.39\n",
      "Epoch [4912], train_loss: 26936.40 with loss1: 327.89, loss2: 33.81 and loss3: 26574.70\n",
      "Epoch [4913], train_loss: 26931.48 with loss1: 326.81, loss2: 33.60 and loss3: 26571.07\n",
      "Epoch [4914], train_loss: 26928.69 with loss1: 327.54, loss2: 33.77 and loss3: 26567.38\n",
      "Epoch [4915], train_loss: 26924.89 with loss1: 327.46, loss2: 33.69 and loss3: 26563.75\n",
      "Epoch [4916], train_loss: 26923.24 with loss1: 329.42, loss2: 33.76 and loss3: 26560.06\n",
      "Epoch [4917], train_loss: 26918.55 with loss1: 328.48, loss2: 33.64 and loss3: 26556.43\n",
      "Epoch [4918], train_loss: 26916.48 with loss1: 330.09, loss2: 33.65 and loss3: 26552.74\n",
      "Epoch [4919], train_loss: 26911.61 with loss1: 328.78, loss2: 33.73 and loss3: 26549.10\n",
      "Epoch [4920], train_loss: 26909.68 with loss1: 330.64, loss2: 33.62 and loss3: 26545.42\n",
      "Epoch [4921], train_loss: 26905.87 with loss1: 330.55, loss2: 33.54 and loss3: 26541.79\n",
      "Epoch [4922], train_loss: 26903.53 with loss1: 331.77, loss2: 33.66 and loss3: 26538.10\n",
      "Epoch [4923], train_loss: 26898.96 with loss1: 330.95, loss2: 33.54 and loss3: 26534.46\n",
      "Epoch [4924], train_loss: 26896.76 with loss1: 332.36, loss2: 33.62 and loss3: 26530.78\n",
      "Epoch [4925], train_loss: 26893.24 with loss1: 332.55, loss2: 33.54 and loss3: 26527.15\n",
      "Epoch [4926], train_loss: 26891.14 with loss1: 333.93, loss2: 33.74 and loss3: 26523.47\n",
      "Epoch [4927], train_loss: 26885.90 with loss1: 332.58, loss2: 33.50 and loss3: 26519.83\n",
      "Epoch [4928], train_loss: 26884.68 with loss1: 334.99, loss2: 33.53 and loss3: 26516.15\n",
      "Epoch [4929], train_loss: 26879.71 with loss1: 333.71, loss2: 33.49 and loss3: 26512.51\n",
      "Epoch [4930], train_loss: 26878.28 with loss1: 335.79, loss2: 33.66 and loss3: 26508.83\n",
      "Epoch [4931], train_loss: 26872.98 with loss1: 334.39, loss2: 33.41 and loss3: 26505.19\n",
      "Epoch [4932], train_loss: 26871.67 with loss1: 336.56, loss2: 33.59 and loss3: 26501.52\n",
      "Epoch [4933], train_loss: 26867.23 with loss1: 335.91, loss2: 33.45 and loss3: 26497.88\n",
      "Epoch [4934], train_loss: 26865.65 with loss1: 337.76, loss2: 33.69 and loss3: 26494.20\n",
      "Epoch [4935], train_loss: 26860.70 with loss1: 336.72, loss2: 33.42 and loss3: 26490.56\n",
      "Epoch [4936], train_loss: 26858.84 with loss1: 338.45, loss2: 33.51 and loss3: 26486.88\n",
      "Epoch [4937], train_loss: 26855.23 with loss1: 338.36, loss2: 33.62 and loss3: 26483.25\n",
      "Epoch [4938], train_loss: 26853.10 with loss1: 340.01, loss2: 33.53 and loss3: 26479.57\n",
      "Epoch [4939], train_loss: 26849.63 with loss1: 340.32, loss2: 33.37 and loss3: 26475.94\n",
      "Epoch [4940], train_loss: 26846.61 with loss1: 340.83, loss2: 33.54 and loss3: 26472.25\n",
      "Epoch [4941], train_loss: 26841.51 with loss1: 339.47, loss2: 33.41 and loss3: 26468.63\n",
      "Epoch [4942], train_loss: 26839.75 with loss1: 341.31, loss2: 33.50 and loss3: 26464.94\n",
      "Epoch [4943], train_loss: 26835.73 with loss1: 341.04, loss2: 33.37 and loss3: 26461.31\n",
      "Epoch [4944], train_loss: 26833.47 with loss1: 342.36, loss2: 33.48 and loss3: 26457.63\n",
      "Epoch [4945], train_loss: 26828.64 with loss1: 341.22, loss2: 33.42 and loss3: 26454.00\n",
      "Epoch [4946], train_loss: 26826.76 with loss1: 342.99, loss2: 33.46 and loss3: 26450.32\n",
      "Epoch [4947], train_loss: 26821.92 with loss1: 341.88, loss2: 33.36 and loss3: 26446.69\n",
      "Epoch [4948], train_loss: 26818.71 with loss1: 342.32, loss2: 33.39 and loss3: 26443.01\n",
      "Epoch [4949], train_loss: 26813.50 with loss1: 340.66, loss2: 33.46 and loss3: 26439.38\n",
      "Epoch [4950], train_loss: 26811.59 with loss1: 342.58, loss2: 33.32 and loss3: 26435.70\n",
      "Epoch [4951], train_loss: 26805.97 with loss1: 340.54, loss2: 33.36 and loss3: 26432.07\n",
      "Epoch [4952], train_loss: 26804.74 with loss1: 342.91, loss2: 33.45 and loss3: 26428.39\n",
      "Epoch [4953], train_loss: 26798.89 with loss1: 340.83, loss2: 33.30 and loss3: 26424.76\n",
      "Epoch [4954], train_loss: 26796.79 with loss1: 342.42, loss2: 33.29 and loss3: 26421.08\n",
      "Epoch [4955], train_loss: 26791.65 with loss1: 340.88, loss2: 33.31 and loss3: 26417.46\n",
      "Epoch [4956], train_loss: 26788.78 with loss1: 341.73, loss2: 33.28 and loss3: 26413.77\n",
      "Epoch [4957], train_loss: 26784.71 with loss1: 341.26, loss2: 33.30 and loss3: 26410.15\n",
      "Epoch [4958], train_loss: 26782.00 with loss1: 342.31, loss2: 33.23 and loss3: 26406.46\n",
      "Epoch [4959], train_loss: 26776.16 with loss1: 340.02, loss2: 33.29 and loss3: 26402.85\n",
      "Epoch [4960], train_loss: 26773.91 with loss1: 341.49, loss2: 33.26 and loss3: 26399.16\n",
      "Epoch [4961], train_loss: 26768.28 with loss1: 339.57, loss2: 33.16 and loss3: 26395.54\n",
      "Epoch [4962], train_loss: 26766.07 with loss1: 340.89, loss2: 33.33 and loss3: 26391.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4963], train_loss: 26760.85 with loss1: 339.42, loss2: 33.20 and loss3: 26388.24\n",
      "Epoch [4964], train_loss: 26758.16 with loss1: 340.40, loss2: 33.21 and loss3: 26384.55\n",
      "Epoch [4965], train_loss: 26754.11 with loss1: 339.96, loss2: 33.21 and loss3: 26380.94\n",
      "Epoch [4966], train_loss: 26750.60 with loss1: 340.05, loss2: 33.30 and loss3: 26377.24\n",
      "Epoch [4967], train_loss: 26745.49 with loss1: 338.65, loss2: 33.22 and loss3: 26373.63\n",
      "Epoch [4968], train_loss: 26742.74 with loss1: 339.54, loss2: 33.25 and loss3: 26369.95\n",
      "Epoch [4969], train_loss: 26737.14 with loss1: 337.63, loss2: 33.17 and loss3: 26366.34\n",
      "Epoch [4970], train_loss: 26733.57 with loss1: 337.73, loss2: 33.20 and loss3: 26362.64\n",
      "Epoch [4971], train_loss: 26729.06 with loss1: 336.83, loss2: 33.19 and loss3: 26359.04\n",
      "Epoch [4972], train_loss: 26726.26 with loss1: 337.71, loss2: 33.20 and loss3: 26355.34\n",
      "Epoch [4973], train_loss: 26721.16 with loss1: 336.25, loss2: 33.17 and loss3: 26351.74\n",
      "Epoch [4974], train_loss: 26719.11 with loss1: 337.96, loss2: 33.10 and loss3: 26348.05\n",
      "Epoch [4975], train_loss: 26713.43 with loss1: 335.91, loss2: 33.08 and loss3: 26344.44\n",
      "Epoch [4976], train_loss: 26710.87 with loss1: 337.00, loss2: 33.10 and loss3: 26340.76\n",
      "Epoch [4977], train_loss: 26704.90 with loss1: 334.64, loss2: 33.12 and loss3: 26337.14\n",
      "Epoch [4978], train_loss: 26702.26 with loss1: 335.65, loss2: 33.14 and loss3: 26333.47\n",
      "Epoch [4979], train_loss: 26696.67 with loss1: 333.76, loss2: 33.08 and loss3: 26329.84\n",
      "Epoch [4980], train_loss: 26694.36 with loss1: 335.09, loss2: 33.09 and loss3: 26326.18\n",
      "Epoch [4981], train_loss: 26688.46 with loss1: 332.84, loss2: 33.06 and loss3: 26322.55\n",
      "Epoch [4982], train_loss: 26685.74 with loss1: 333.68, loss2: 33.17 and loss3: 26318.89\n",
      "Epoch [4983], train_loss: 26680.81 with loss1: 332.56, loss2: 32.99 and loss3: 26315.26\n",
      "Epoch [4984], train_loss: 26677.37 with loss1: 332.72, loss2: 33.05 and loss3: 26311.60\n",
      "Epoch [4985], train_loss: 26672.55 with loss1: 331.52, loss2: 33.06 and loss3: 26307.97\n",
      "Epoch [4986], train_loss: 26670.22 with loss1: 332.78, loss2: 33.13 and loss3: 26304.31\n",
      "Epoch [4987], train_loss: 26664.47 with loss1: 330.76, loss2: 33.03 and loss3: 26300.68\n",
      "Epoch [4988], train_loss: 26662.16 with loss1: 332.07, loss2: 33.07 and loss3: 26297.02\n",
      "Epoch [4989], train_loss: 26657.78 with loss1: 331.44, loss2: 32.95 and loss3: 26293.39\n",
      "Epoch [4990], train_loss: 26655.00 with loss1: 332.10, loss2: 33.16 and loss3: 26289.73\n",
      "Epoch [4991], train_loss: 26650.17 with loss1: 331.14, loss2: 32.93 and loss3: 26286.10\n",
      "Epoch [4992], train_loss: 26648.05 with loss1: 332.58, loss2: 33.03 and loss3: 26282.45\n",
      "Epoch [4993], train_loss: 26642.35 with loss1: 330.59, loss2: 32.96 and loss3: 26278.81\n",
      "Epoch [4994], train_loss: 26639.26 with loss1: 331.14, loss2: 32.96 and loss3: 26275.16\n",
      "Epoch [4995], train_loss: 26635.87 with loss1: 331.34, loss2: 33.01 and loss3: 26271.52\n",
      "Epoch [4996], train_loss: 26631.67 with loss1: 330.80, loss2: 33.01 and loss3: 26267.87\n",
      "Epoch [4997], train_loss: 26627.36 with loss1: 330.19, loss2: 32.93 and loss3: 26264.23\n",
      "Epoch [4998], train_loss: 26624.42 with loss1: 330.85, loss2: 32.99 and loss3: 26260.58\n",
      "Epoch [4999], train_loss: 26619.85 with loss1: 329.92, loss2: 32.98 and loss3: 26256.95\n"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1., lamb=10.\n",
    "# model version 3.5\n",
    "# with loss2 and loss3\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history = fit(epochs=5000, lr=5e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b35a688b",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'F_matrices' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/40307579/ipykernel_27870/968042214.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# with loss2 and loss3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# dataset: 4 images (IMCL not included)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/local/40307579/ipykernel_27870/2604192377.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, lr, h0, model, train_loader, val_loader, opt_func)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mF_matrices\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwhich_ones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'F_matrices' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# enc_out_dim=100, latent_dim=50, sqr_sig_x=1., sqr_sig_h=1., lamb=10.\n",
    "# model version 3.5\n",
    "# with loss2 and loss3\n",
    "# dataset: 4 images (IMCL not included)\n",
    "train_loss_history = fit(epochs=5000, lr=5e-6, h0=h0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "54a75d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, x, h0, which_ones):\n",
    "    model.eval()\n",
    "    x = x.unsqueeze(0)\n",
    "    x = to_device(x, device)\n",
    "    _, mu_history, _, _ = model(x, h0, which_ones)\n",
    "    return mu_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c663edb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 91, 109, 91, 177])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bb4f777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 91, 109, 91, 177])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "32e43604",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_history = evaluate(model, imgs_data[0], h0, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a5718edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mu_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3b0785dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 91, 109, 91])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_history[60].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4900401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tocompare = 60 # the timestamp on which we are to do comparison\n",
    "\n",
    "def save_image(mu_history, name, which_one, t=t_tocompare):\n",
    "    maxim = torch.Tensor.cpu(max_value[which_one]).detach().numpy()\n",
    "    minim = torch.Tensor.cpu(min_value[which_one]).detach().numpy()\n",
    "    one_mu = torch.Tensor.cpu(mu_history[t]).detach().numpy()\n",
    "    one_mu = one_mu[0,:,:,:,:]\n",
    "    one_mu = (one_mu * .5 + .5) * (maxim - minim) + minim\n",
    "    img_new = nib.Nifti1Image(one_mu, np.eye(4))\n",
    "    nib.save(img_new, os.path.join('Generated', name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5234cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(file_position, data_dir = './Generated'):\n",
    "    return os.listdir(data_dir)[file_position]\n",
    "\n",
    "\n",
    "def show_slices(slices):\n",
    "   \"\"\" Function to display row of image slices \"\"\"\n",
    "   fig, axes = plt.subplots(1, len(slices))\n",
    "   for i, slice in enumerate(slices):\n",
    "       axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")\n",
    "\n",
    "\n",
    "def get_plot(file_name, data_dir = './Generated', plot_name = 'Slices', x1=40, x2=40, x3=40, t=t_tocompare):\n",
    "    img = nib.load(os.path.join(data_dir, file_name))\n",
    "    img_data = img.get_fdata()\n",
    "    if data_dir == './Dataset':\n",
    "        show_slices([img_data[x1,:,:,t], img_data[:,x2,:,t], img_data[:,:,x3,t]])\n",
    "        plt.suptitle(plot_name)\n",
    "    else:\n",
    "        show_slices([img_data[0,x1,:,:], img_data[0,:,x2,:], img_data[0,:,:,x3]])\n",
    "        plt.suptitle(plot_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d20433f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(mu_history=mu_history, name='newmodel_temporal_lam10_img0_5000epochs.nii.gz', which_one=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aca19426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAADjCAYAAACcnE9mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAB4mklEQVR4nO29eWyl13ke/tz98m7cd45m0Yx2ya4t2bJrBG4c2a5iyFANyLLbWoFbKwhaNIltpCoMF0ER2DJaAW3gtE1Su1bc1qr9R6o0dRbDsNraSa0oqIqOVFmypNFohhwul3fjXXjX3x/8PYfPd/jdhRxyOFc5D0CQvMv3ne8s73nf511OoNPpdODg4ODgMHQIHncDHBwcHBwOBifAHRwcHIYUToA7ODg4DCmcAHdwcHAYUjgB7uDg4DCkcALcwcHBYUjhBLjDWwbf+MY38L73vc/8n0ql8Nprrx1jixwcjhZOgDsMHX74wx/ive99L0ZHRzExMYG//tf/Ov7iL/5iz+e2trZw5syZY2ihg8O1Qfi4G+DgsB8Ui0V85CMfwb/5N/8GDz30EOr1Ov7n//yfiMVix900B4drDqeBOwwVXn75ZQDAJz7xCYRCIYyMjOCDH/wg7rrrrj2fDQQC+OlPfwoAqFar+NznPoeTJ09idHQU73vf+1CtVgEA/+t//S+8973vxdjYGN72trfhmWeeMdf4xje+gTNnziCdTuP06dP4j//xPx79Qzo4DAingTsMFW666SaEQiE88sgjePjhh3HvvfdifHy87/c+//nP44UXXsCf/dmfYW5uDj/+8Y8RDAZx+fJl/PzP/zy++c1v4sMf/jC+//3v42Mf+xheeuklJBIJ/KN/9I/wF3/xF7j55puxsrKCzc3Na/CUDg6DwWngDkOFTCaDH/7whwgEAvjMZz6D6elpPPDAA1hdXe36nXa7ja9//ev4V//qX2FxcRGhUAjvfe97EYvF8B/+w3/A/fffj/vvvx/BYBD33Xcf7r77bnz3u98FAASDQZw/fx7VahXz8/O4/fbbr9WjOjj0hRPgDkOHW2+9Fd/4xjdw6dIlnD9/HsvLy/iVX/mVrp/f2NhArVbDjTfeuOe9N954A9/5zncwNjZmfn74wx9iZWUFyWQS//k//2f823/7bzE/P4+f//mfx0svvXSET+bgsD84Ae4w1LjlllvwC7/wCzh//nzXz0xNTSEej+PVV1/d896JEyfwd//u30U+nzc/5XIZjz32GADgQx/6EL73ve9hZWUFt9xyCz7zmc8c2bM4OOwXToA7DBVeeuklPPHEE7h06RIA4M0338S3vvUt3HvvvV2/EwwG8elPfxqf/exnsby8jFarhT//8z/H9vY2/s7f+Tv4r//1v+JP/uRP0Gq1UKvV8Mwzz+DSpUtYXV3FH/zBH6BcLiMWiyGVSiEUCl2rR3Vw6AsnwB2GCul0Gj/+8Y/x7ne/G8lkEvfeey/uuOMOPPHEEz2/9y/+xb/AnXfeiXvuuQcTExP4x//4H6PdbuPEiRN4+umn8aUvfQnT09M4ceIE/vk//+dot9tot9t44oknsLCwgImJCfz3//7f8a//9b++Rk/q4NAfAXegg4ODg8NwwmngDg4ODkMKJ8AdHBwchhROgDs4ODgMKZwAd3BwcBhSOAHu4ODgMKRwAtzBwcFhSOEEuIODg8OQwglwBwcHhyGFE+AODg4OQwonwB0cHByGFE6AOzg4OAwpnAB3cHBwGFI4Ae7g4OAwpHAC3MHBwWFI4QS4g4ODw5DCCXAHBweHIYUT4A4ODg5DCifAHRwcHIYUToA7ODg4DCmcAHdwcHAYUjgB7uDg4DCkcALcwcHBYUjhBLiDg4PDkMIJcAcHB4chhRPgDg4ODkMKJ8AdHBwchhROgDs4ODgMKZwAd3BwcBhSOAHu4ODgMKRwAtzBwcFhSOEEuIODg8OQwglwBwcHhyGFE+AODg4OQwonwB0cHByGFE6AOzg4OAwpnAB3cHBwGFI4Ae7g4OAwpHAC3MHBwWFI4QS4g4ODw5DCCXAHBweHIYUT4A4ODg5DCifAHRwcHIYUToA7ODg4DCmcAHdwcHAYUjgB7uDg4DCkcALcwcHBYUjhBLiDg4PDkMIJcAcHB4chhRPgDg4ODkMKJ8AdHBwchhROgDs4ODgMKZwAd3BwcBhSOAHu4ODgMKRwAtzBwcFhSOEEuIODg8OQwglwBwcHhyGFE+AODg4OQwonwB0cHByGFE6AOzg4OAwpnAB3cHBwGFI4Ae7g4OAwpHAC3MHBwWFI4QS4g4ODw5DCCXAHBweHIYUT4A4ODg5DCifAHRwcHIYUToA7ODg4DCnCx90ABweH6xOhUAidTmfgzwcCAQDo+Z1AIIBOp2M+O8j1er0eCATMNXlfv/f5Gbudfm1tt9ue57A/06v9eh8bfq/b19LPBAIBhEIhhMNhJJNJbGxs7Pm+E+AODg6HChXSfsKOAlU/S8EVDAb3fBbwCjp+htfh/+1223wuGAyi0+kgGAwiEAggEomY+6hAb7VaHqHZ6XTQaDQ8beom6LV99rP12sRsIe3XdwSF96lTp3yv5QS4g4ODL6iJDgo/TdXvdaC7dq3arwp6fc0W1HyPP8FgEO1223wuFAoZQc33ea9Wq4V2u+25Lttga+KDPPPVfs7+zvb2NlqtVtfPOAHu4OCwb9h0Sbf/+ZofTaCasq0Zq3ZNgcz/+ZlgMIhwOIxweEeMUdDxM81m01AQ8XjcCGsK8GaziUajYb5nC/Rms2n+ttuu6PfMflbIQagpPzgB7uDg4AubS/Z7bxABrt9RAU3hqsKZvHsgEEA4HEY0GkU6nTb/dzodRCIRAEAqlUIkEkE8Hkc0GjUCvdlsolareYRzKpVCMBg0n2u1WqjX66hWqyiVSqjX66hUKmg0Gmg2m2i1WqhUKkaI89mo2esz6v82JcTv2f/zt1ImvaiabnAC3MHBYV/oJtj9/laBrpQINetQKOR5LRQKmc9TOKfTaY/mHYlEEA6HMT4+7hHg4XAYwWAQzWbTCN96vY5Wq4XR0VGjiYdCITSbTWxvb6NarSIUCmF7exvtdhvRaBSVSsVsGAA81Ao1fz+Kxxbg/agX/X4/x283OAHu4DCE+PSnP40//MM/xMzMDM6fPw8A2NzcxMc//nFcuHABp06dwre//W2Mj48DAL785S/ja1/7GkKhEH7zN38TH/rQhw6lHbbGbTvh9HWb/giFQh6BHQwGEYlEEAqFEIlEEIvFMDIygsnJSYTDYaOdJ5NJjIyMYHp62lxHqRQKcHLI1WoVyWQSsVgMqVTK0CPU1FOpFMrlsnF05nI51Go1tFoto9G3222PNm4/m/623++HfkK813UCnYOw6w4ODseK//E//gdSqRQ+9alPGQH+a7/2a5iYmMBjjz2Gxx9/HLlcDl/5ylfw4osv4hOf+ASeffZZLC8v4+d+7ufw8ssvG+HZDXa0h0I1TVs4q6NQo0uobfP9SCSCaDSKZrNp6A8K2lgshkQigXQ6jcXFRSPMO50OJicnMT4+jvHxccOP8/vArgAPBoMolUool8sIh8MYGRlBJpMxn2m322g0Gsjlctja2sLa2ho6nQ5WV1dRLpexsrJihDwpl1qt5qFMbM0c8EbDsA90o7MdtXxf6Rnt51gshttvvx3PPffcnjFyGriDwxDiZ37mZ3DhwgXPa08//TSeeeYZAMAjjzyC97///fjKV76Cp59+Gg8//DBisRhOnz6Ns2fP4tlnn8V73vOege83iDbp52ykQOL/tsClc5Ea9NTUFJLJJBKJBMbHxzE5OYkTJ04YygQAZmdnkU6nMTIysuee5LcbjQYCgQDy+Tzq9To6nY7ZGIBd3rperyOXy6FarWJychKtVguxWAzFYhHVahW1Wg3AjsDn99QZqtEtAPb83c0x28tXsB84Ae7g8BbB6uoq5ufnAQDz8/NYW1sDAFy+fBn33nuv+dzS0hIuX77se43f+Z3fwe/8zu8AgCeOup+Zb4f18Xt8TzXVTqdjtOVkMgkASCQSiMfjOHHiBEZHRzEzM4MzZ85gfn4eS0tLaDabmJiYQDQaxeTkJIC9vLof6JBsNBqetvCn0WigXC5je3vbODNHRkaQzWaRy+VQLBYNXw7sCvBms4lAIOAR4NSgbadmL2ew9l839ArndALc4aqhGsVfRfRL3LgW92cbpqam9mTs7ScO+9FHH8Wjjz4KYP/javPdg3w3EokgEAggHo8jkUhgdHQUY2NjmJycxNzcHGZnZzExMYFms4lEImGiSAYFaSKb3iDo+IzFYggEAtje3kYymUStVkMikUCj0UCtVjPPxs3A5vl7talfss7VzB0nwB2uGn+VhTdwfT3/4uIiAGBlZQUzMzMAdjTuN99803zm0qVLWFhY2Nd1+2UP2hEkfvSCCrp4PG407kgkgtnZWWQyGdx4442YmprC4uIiTp48iUQiYXhrv3jqftBokm7PFYvF0Ol0kEql0G63USqVMD4+jlarhUKhgGw2i3w+b/j0er2OQqFg6JdWq4VWq4VAIIB6ve65t/L+TCrS2HLbShkkKkXhBLiDw1sEwWAQ2WwWAPDkk0/iox/9KADggQcewCc/+Ul89rOfxfLyMl555RW8613v2te1/aJL9DUKK20L4OWBGXVCoZlOp7GwsIBIJIKpqSlMTU3hxIkTmJqawvj4uBHyes/DhvLy4XAYrVYL8/PzGBkZQbPZRLlcxvr6OjY3N5HL5ZDP51GpVAAAtVoNW1tbHmev9oFubKRBmBXKvuFrNrWj6KXdOwHu8FcK/cxZP/TSPvW1wzKLDwJqd8ViEefOncMNN9yA73znOwCA22+/HQ899BBuu+02hMNh/NZv/VbfCBRek/DjYTV+OxwOGwehZjaq85KRIACwsLCAEydO4D3veQ/C4TBOnTqFqakpLC0tIZVKXV1n7BPcWDRBCADe9ra3odPpIJvNolAoYHV11fz82Z/9GTY2NlAoFDwOTo0R5zNzgwB2o1BoofjRL3Z9ll5wAtzhuoR67oH+AtHvc/Y1/JIuuglwO5KgV9vsehz29wdp/9WCguCmm27yDTf7whe+gC984QtHdn/SAhRQ2r8aldJoNDwp8BT8/BlkY7lWoDBmO6PRKCKRCILBIBqNhqFLumVd0rGpDl0Ae/rFxn4sDSfAHQ4VvSbfIEJMha3ftexr6Ofs2GT7c3bWny4sO8uO0MgJXXhcmIxnBrAnIkF/q9ncT+u3n6dfH9jfO2zYPDZ/238zdA/AnvDBUCiEaDSKVCqFVCqFW2+9FXNzczh58iSSySQWFxcxMjJi+vJ6QSAQQCKRMFTIyMgIkskkLl++jEQigWq1inq9bsIVOTdCoZCZH5xTjFCp1+t7olkAmPBEO1mo17g6Ae5wqDhImNSgmnE3wWzHHfslUVCIqEDRELlms7lHSAO7CR96DWBnsTEDkE4wFeBcnGpe+1kD3eiZbuFn10qjt+E3rn4p5d0sIG6OqVTKxHdPT09jamoKqVQK8XgcsVhsXxEm1wrcfJLJpHmWG2+8EZFIBKurq0YbZxYpBTiwu4HZwtrPWusGx4E7XBP04ocVg9AhtvCys9f8Pk8Nmya8ao22oFfhbRdR0nar1q4OL/1+Lw1VNX7bQdUtqsL+rr6mWv0gfX1YsJ/PrhIIwLMJqtAJBHbqmiQSCUxPT2NsbAy33347FhYWcPLkSZMmb1MN1wvYPhbWSqfT6HQ6uOGGG9BqtZDNZhGPx1EqlUwsuWrS/N1oNDwWne1X6Dau100c+PU4OH/VYJu41+Je+/m8/aPXUW5VoZyqZsnpglCHEv9Xoa/XVOHMhajaN5NDgN04Y7/Nwn4OauZ+m4w+p22F2P93o2GuxZh2a6fCFvZqqSSTSWQyGYyNjWFsbAyJROKatPlqoBs9MT4+jlAohLm5OQSDQWxubqLZbKJarZqNTDd7hZ9leVA4DfyvGI7C9La1Xb/3/dphC2gVCBpTrO+pQFNtjz/8LDVqTbPWeGD9nn6WwoYaIblKWxvWe2n77Ofl50iv6Gf8NHcAnk3F3hhsDVzH86joh27WgP3M9vixv+PxODKZjInvnpqawvT0tEmNPy74+VMGQTAYRDweR7vdxokTJzA2NmbqrWxtbZkKiOr/IBWnAt0OH/RrU7/16gS4w1VDJ5/folAhNAh9okkh6mxkwoTfZLdPXGH0gN5XhSY/w2uTfx0bG9sTbUAuu9FoeE5IsTlvvbZSAp3OTglU1ZCVW2cb/TYrPif7RTW7a82Ds23aRnsjBXYpB4YVkveem5vDjTfeiNnZWczPzx+rALcpDD9LohuCwaBxbN5www2o1+vIZrMIh8NYX1/3jC3njQpw1kDXzVgd3fvBdS3A92OC+zlP/K6j5my37++nXcexiK53+PU34I3A8PuMasX8nNIjFKQs7anf77cAlRahhqhHaRF2aVLValXwq7DW47tsqsNuA+ClgmwNXIWhfU27j+3vH6UDcJDNl30cjUYRjUYRj8cRDAYxOjqKiYkJzM/P4/Tp05iamsLo6Oix0ye2P2LQ/uMzAsDExARarRbm5ubQarUwPj5uxqpare4p7sW5p/4DCu5BrVfFsQnwQYSz38JUTrOX6dFNQNiv2+fg2fexv2/fw+/+NkfZbdPwe14/DNMmoRytPW5+dIGOMTVXFunX0L5AIGDqZvD69OorhdJut00taZ7CUq/XPaVGGfoXDocRCOykP8fjcaRSKdx4442eNgA7C7FQKKBcLgMArly5YkzlcDiMWq2GarVqwugYjdBoNMxrtqbqt2j5N+ei0i62leO3QRwVB+53L7bXXlvUTnkQQygUwszMDE6ePImbbroJN998M9LpNEZHRw/UDvv+VwO/NTro97hJMennypUr6HQ6OHHihHmt3W6bgyJ0HlNoR6NRE1ZoZ2cS3fqeOFYB3q3zVADYGpkKcMCb9aXX5nUAdOVT1Sz30xTt9g6yQ3fTmPye0w9+wt6PhrA1tm6OrW7X/8lPfoKPf/zj5vXXXnsN/+yf/TPk83n87u/+LqanpwEAX/rSl3D//ff3bXevZ7RfU+2DGgngzYiLxWIeTz7HXnlDO8Y4EomYanIjIyOYm5tDILATn8zjuLa3t9FoNBAKhZBIJBAIBFAqlRAOhzE2NmbqboyMjCCRSJiU6lwuh1KpZNq/sbGBra0txONxbG1tecad9Ivdfj4j/7djzG1qxB5jvmbTF8S1TILx24T5dzgcNhskS8byEIarFbrAwWqidIPKlW5rZ5B7McKEc5HKgfpKNMdALUC7/+y13+v+x0qh2ELa1oZtkwPYq4HrJO+2IdjfVZOGWh89x35g+1SLswfd1uJ7WQjdBqSbpm5r937f6Sc47e/dfPPNeP755wHsbIKLi4t48MEH8e///b/Hr/7qr+Lzn/+8bxv7oZeWpAK31WqZvudk53iw0D95QWopbKsKP0Y3xONxjI2NAdgp6JRKpfDud78byWTSLJZ8Po98Po9cLmeKEFUqFYTDYUxPTyOZTOK2227DyMgIxsbGDI87MjKCdruNWq2GYrGIN998E9VqFa+//jqKxSIuXLiAUChkIhBI8di8OBe4ms061+0+VAGvc6kb/32Ulpq9trR0qmre4XAYmUzGHJ4Qj8cxNTWFiYkJjI2NmTre+70354LeX/vWdmR3A8eF80pfA3YpNMoWZmD2u+7k5CTq9TpOnTplqBP2jSbuUKHopqh1e/ZuOBYBbmsRWrGL7wPec/M0BEwHkVwoF6ltaurnCdXqqdFxB7V3ZDWJ6Zwpl8tmQFTzVS5U26PCXTU1W6vWHXoQbcBP4z6IZvL9738fN954I06ePLnv73aDPcZ8TTPTusVkKwcdDof3lO/UvuQCI+caDocxMzODVCqFxcVFjI6OIhKJoNlsIpVKGfM+HA6jUqmYwwQmJyeRSCQwNzeHVCqFTCaDTqdjQt/Yjmw2a+gRAFheXjYOrEgkglqtZg7U1XHWyANgL7du99tB+vtaRaHwNXt9qdZNCioej5v1wzGic3NQ6HirksR5oUez9esDloNVh7g6pgOBncxLdTaq7OkGWmy06lKpFJLJpLHaeln49hrYz5o+Ng3cLj2pgpMNVsHOCUDngXY+d0o+OHdU1aptYcnXODjk5PQ18qXcgfka71Wv143GxQnBdmlSgp+2oG1iu/TzhJ823svk8psc3a5FPPXUU/jEJz5h/v/qV7+K3/u938Pdd9+NJ554wjdaQAv/9wI3SWqfdopxKBQyzi7VwIlYLGYiONrtNqrVqrkmiyONjo6ao7fC4TDOnDmD0dFRnDlzBiMjI0bjT6VSGBsbw/j4ONLpNLa3t7G2toZqtYqzZ89iYmLCFHziBjIyMmJqdNBJNTU1hUajgWg0isXFRSSTSWxtbZmzFDc3N7G+vo5Go2E2CZsD5RxSTZDwew3YnRc6n3RsDxLFsF+ooqKbE60VHUMdFyoIg3DfvIddjpYKG+9NQaw+DZUP7A+uZx5eXKvVsL29bZQ/yhHKGF2fbIeOnR8ymYwR+KxguLq6auYueW5ew1ZC9Pls9BrXY9PAu72uD6evKXRQVVByQtmaHl+zqRnVwilkYrGYR4ADu2FR3Ei4+DTDyuZ1+T/vp9q1n/Dt1Sf9zKxu1xnkPvV6HX/wB3+AL3/5ywCAX/qlX8IXv/hFBAIBfPGLX8TnPvc5fP3rX99zTy38309jtPuZfc1+JmfIz3IBabytOv6i0ajhucfHxzE9PY2JiQnccMMNCIfDmJ+fx9jYGObm5sw9Op2OMeszmQwSiYQ5rZwCfGZmBnNzcx4rihsIQS63Xq8bYRCJRFAqlcxZiuvr64hGo9ja2sLGxgZarZbR8vTZ2Hf2vKRgspUN2zpU9DO1DwI/bZCv628KR27OXCN0OrN07CBRJ0qXUFhT2ePGoclQdrYjr6ECkw5DtplzUZ+Hr9H/Ym+ILJ3Qba7zudPpNKLRKDKZDJLJpJnbKrBpEep8sBUzv/72w7EIcFvTpqBUAawmIQeNGrFSH8y+U62FQkG5bd1Bad7RkcWMqlAohGQyiWQyiXQ6jVqthlKphFqthmAwiPHxcZTLZeRyOcOJsphNpVJBrVYz94vH46jX66hWq55n4MTodDrms5wYFFK6GP0oFXuguy1c7WNbGBB/9Ed/hHe84x2YnZ0FAPMbAD7zmc/gIx/5yIHGuBt04+TG6Be2Z9cQ4XcmJycxMjKChYUFTE5OYmFhwVAls7OzCIVChipJpVKGXycdkk6nTSZgu93G4uIims2moU7oPFXN0m4/nZTz8/Oo1+tIJpOo1+tYW1tDsVhENpvF4uIiNjc38cILL6BUKplTclQR4PNyg9L57mdV+QltbddBqJde6KY4cP1RKEajUXMIMQBj6czOzmJ6eho33ngjFhYWepaJ1aPPuNZJU3ETpiCkoqTt5HdVyAeDQXO9arVqtGHdIHVz5niTmuHzNxoNM+bdMDIyYuZzs9nEyZMnTe3wQqGASqWCer1uNn62UWnXbmVkr4oDP6pohW6Uhk0/ECoAbXOG2rAKBnZmvV43r7MYDfmpYDCIyclJhMNhnDt3zjhiWHGsWq2aw00BYHp6GrVazQTrF4tFlMtlk+BRqVTMpGNthLW1NWMaUSugNsbnpaaifaNmo2r7urntZ7C7LcZvfetbHvpkZWXFnKv4+7//+7jjjjv6jmU3+G00FF7UrOiEtOcBJ3apVPI4OmdmZpDJZPC2t70NZ86cwalTp7C0tGTCBgfB2NiYmbfEfqrgRSIRT5jh0tISOp0OSqUSisWiKfy/vr6Oer2OCxcuYHNzEwCMENK5q5ShLmrbp9MLR6GBA/5OctWy+b/G6nMNchMdGRnxWFl+YIw/14aeY6n0m58Vy+dXQah1uHktKgVUnOyxoLMR2NW4AW/GZDfwM/F4HJ1Ox1iGGxsbZmPjvWq1mmeOa9amH3ptHH0F+FFEK6jAtcPEgL0cn1ISFMjUWjVChTs3B4+8WDKZNKd+jIyMmNM/EokEJicnkclkcPvtt3tMqFQqhUqlgnK5bAYzk8mg2WxidXUV9XodGxsbKJfLCAR2YpJLpRKazSYikQiKxSKuXLmC8+fPo9lsolKpYGtrC41GA8VicQ8Hx4HiYuBrNCPZD/34b7++7vZapVLB9773Pfz2b/+2ee/Xfu3X8PzzzyMQCODUqVOe9/rBbpv+9Gonv6Mmq+0joYbDSAaenUituZdw8IMfpXU1IFWQSqU8mZqzs7MolUpYXl72+E+4BtRPo1Zkv7bZWvlRaOD6bEr5UJjyftxcSTdlMhmjJJFy4mcGuRewa0WrhapCnK/bFClj7xnaScVJKVL6NPQZ1NJiqKlel3NMaSKVPfyf4zs6OmrO9czn8yiXy8hms0aJq9frRpBz3I9EA1ccdrRCt93UXvwaV6oUCk0gDcnqdDrGpGNM7+TkJJLJpIk0oABnbeJ0Oo3JyUlzbcYU6/WpKbZaLUxOTqLVaiEWi3koku3tbVSrVUSjUeRyOcTjcUOtZLNZE6UQCAQ8oUZ22riaiXwmv8nfTaseVDAlEglzBBfxzW9+c8DR2wttpy2k+JzktRmlQc2MmxnfJzUG7Jjk586dQygUMrU0zp07h7m5OYyNjSEej+9LeNkc6GGBwiGZTGJ8fNw4f2+77Tak02nk83kUCgUEg0FjSmvtDDWjVWiqkCKn67cpHrYT06ZNgN05SiFGTVbpMEZj6HodxE9C4aeaNu/Ntah+AFsBUk6cNAUAE/fPtqosITQ7slqtotPpGAtAfTb8HCNp7Mg43kdlF+lU0rZ02qsC2wuHJsAPO1qhl8nACUzBSbOVg8cOpJML2OXKUqkURkdHsbS0hGAwiIWFBWQyGczMzCASiRiBzWuk02lMTEwA8GobdJYpZdFutw2Xlk6nzSADMPRIp9PBxMSE4QC3trbwxhtvYHl5GZVKBcVi0WhlnFxqSqkHXTVwts/uO1sj9/tMvz4/DPTSFv14fUJT5LlAyK+m02mMjY3h3LlzJl57cnISk5OTSKfThn45Ku3zIKD1l0gkMD8/j2AwiL/21/4aNjc3zeHCm5ubxr9iW1nRaNSTfKTvAXv7U+97lLAVK64LhgdS8eEGrMK2H7i+NPKMygu5bbYhmUx6BL1SJ+x7OgopJCm01cGq/UVlSaPX6vU6Op2O2QwoxAeh22jNMzKGlhlPuW+328jlcgB2Lc9u49oLAwvwo4xWsDVG7k4U3FpcSB9Q4z9jsZjRepaWlrC0tITbbrsNsVgMExMTntRqXpPXY0f7CRkVDhoCx01EI09UA5idnUWz2TRxwZcvX8by8jIKhQJKpRJyuRxeffVVo4VVq1Wsra3tOarJpk20z/z4SXuRX0vY99NFp1qTprPTGazRBEpH3H333Zifn8ff/Jt/02h3+vt6RCgUMuGH8Xgc8/PzuPnmmxEMBvH888+bMxU7nQ7K5TJqtZoxqdX6YjSE/tgFvRSdTge1Wg1vf/vbzWuH5bMC9gpwDR9kKQLGffNHaYpeG4xabBqSyzXUaDRMan46nTbXsykICnUN/QXgEdi2BaFKGzVwWjkaYEGFUq2Qbs9BKpZ9wzwCXr/dbptUetI13bTxXsrJwCvgMKMVejVIJ4aaZuw8dZLw84wHHhsbMwejnjp1CrfeeiuSyaRZ7LazUKEZfoTNwduDzb/7aX+NRsNwYvl8HsViEZubm4bTX15eRj6fNzWFeW+9nx9FogJe31Pe+VoKcbsfdKHrs3B8OaZsNxcgtRY6lBnhwEWh3OP1DGqnKsAmJyexvb2NZDJpTjUnzQZ4N2ZqserssseXIC0Uj8fNmZiH6bOy76PWqWqaquEy1LPXuiMorEknUVttNBool8vY3t429/ArfWFHoAHeCCa2nZ9VX4POJa0eqP4JzlnV5LvNQVI1TFoil89xo9JJ+daPSjkUAX4Y0Qq200WhThnVwLmD6U7Ozyq9Mjs7ayISWLKSDi5ypP0mkQ6ueoY5UHZnU4skL0mBZN+HzreRkRE0Gg0UCgUUi0VkMhlUKhW88soryOVySCQS2NraQqFQQD6fN8kFnU4HlUplT9yrCkS+ZmvmGsVy1MLc5urZd9S0+TfHkpl69C2QTpqensYNN9yAeDyOEydOYHZ21qS2HwV3fZTgImX6+MmTJzEzM4NGo4HV1VW8/PLLGB0dxerqKi5fvozt7W2TnKbzXCMk/KCcLXG1PisdT96bgovOSj4jsxe3t7eNtsu1Rz9HPw1cI2/4P5OharUawuEwxsfHTf+oNawKTyQSQTKZBLDrkKTWS0VBrWZVyhqNBkqlkrGEuVlRFumzqP9KwTFLpVKYmJhANptFsVg0US+JRMKEJjabTUOh9mMo/DCQAD/saAXC5mpVY1NujeF/wK6TANgRoOl0GpFIBDMzM1hcXMSpU6cwNzdnOO5B6hjYsE1VTSRQzZbCm6aWhlbZz0nBTgHG0KparYZMJoNisYjFxUWsr69jc3MTr7/+OvL5PLa2tozVoVoB28IFpaakLjo7ROlaccVqams4llYBpABn+F+r1cLIyIipF51IJDA2NoaZmZmh0br7gVrr6dOnkUqljM+EdVnK5bIJf7WjsZQn9nNk2v1zmD4rVQo4pho6NzIyYu7P+jG6Zqmd9wLXia4zzUxUSoXX5TXZh7yObbXa817DePl5hgMz8kxpEw1sUOqlWz9x4+bm3Wg0EIvFsL29baJy4vG4kRt6Xma3a/q+17mGNrZOAp1s7EgKbhZLZzgSz6BTgc8djim7t912G97znvfg7W9/u0ng0HsOAjosOIgUlhSCGsvabreNEKag5M7fa+LwPvbfFMqVSgVXrlzBX/7lX+LixYt44403UCgUcPHiRWxvbyOfz5uwRDXryH+qA1QTGzTtGOidnrtf+D0vFzrNaWrgtEaAnfTjaDSKiYkJJBIJzMzMYGFhAQsLCzh37hyi0ShOnDiBZDJpnMzDDgrler2ORqOB559/HufPn8dLL72El156Cdvb2ygWi2g2m9ja2jJarTrXqB2qRRYI7Dg/77jjDjz33HOo1+tYWFjACy+8gNnZWayurmJqagqBwI7PamVlxddnpdCxpPCiUjU5OYkTJ04gEAiYY9Kmp6dRrVZx6tQpTE9P49y5c8bpTP7a3mQ477ne+DwU1JVKxZz4TlqNa5IbBWkq9ks0GjUWWyQSMdFhKtRVsaFsYfx+sVg0gQ7xeByjo6MeKkjlV7dNiYrY+vo6Ll68iHw+j42NDdRqNaytrQHY8U+USiVsbm6apEFa2qpwxWIx3H777YYaU1zzMzG77Re2c4Sf5cRV8AGr1aoJydG4zEHokm5g2BsXmIZ3qROEC0m1X40t9uOn9Vn9/mYEzfz8vEkLbzabGBkZMUlDzPr0s17UUuCGqBujajbXAnYb+Tf/V4cdTfLp6WlMT09jdnbWOKbJab4VwDmkpQBYtY/zmGFn1Eb1sOBufg17rh2Wz0r5e781Sqfi2NiY2az1lHYKvm6+Is1K1gqOpE5oldRqNZMQQ546lUqZvmBkiD3XbNpR72v3a6FQQC6XM+1h7SXlvPspZwQ3G8aPk4JRC4oWuzpFbf+GrezauKYC3NY2bahg5kPQkWGb3wAMRzoxMYFz585hamqqZ72CXuBA82BSUhfUaMmXMWyROzonEVOqVSMnBzgoOHjxeBx33nknzp07hxMnTiCbzWJubg7VahXnz5/HlStX9jheqKlpAoL9cy1gO3h1Uur/FFbRaNRED83MzGB2dhYzMzOYmJgwIWkHocBUMz3o923aSYUnrYurRSaTwdLSEvL5PFZXV1EoFMwYci7pcVy9IlD09cPMsNU+tDlxhkuyPEG5XDYWEy1kP+Gn1yaXzL/r9TrK5TJKpRKq1arRxDc2NtBut7G1tYVgMGg2ByZ3MXGIoZmBwI5vihmyjARhxqd+plqtmo0CgKkDr5UL1fneb07FYjFTuKvRaGBtbQ2XL182wQrlchmVSgUATJAFs7ntce3l+7ju4rCU39VQImDXK0+ahfHBLFxEk/wgXCkXBzVv1nbW+3NRc0LRbFPOnHwuhSjNu0EFKLUaOm55BFU8HjfeeDo/uLALhUJPJ+W1ikZRzUyFtDqBNbKIkUOzs7OYmprC/Py8iZ1Pp9OeKJX9QDMa9yvA2VdaI1otQi1ffFgCfHFxEdVqFblcDisrK4Y6YdvVqafP1g2H5bOy56xuwroRU9Hi/EwkEsY31S9Ki9fQipMMMyX3zDnDtVkul000ErA73lxvTAJjXROGaNKioVKo65vRL81m0ygN29vbAHYVJHV+9gPvRU08EAiYjUXb2y/AoN/9jk2A9xIonKBaU5nCm4uGCTk333wzJiYmsLS0hNHR0QOb2zqw5I45+OQbeW1OIk4sCu9QKGSO3aLQoCd7UE5eHUTM6uORVKy1QqcId/HNzU1sb297MvpsU8z+fdhQh5U6uuxYXPZZNBrF5OQkpqencfbsWSO85+bmMDo62jdqwQ8UdH6O20G0JmDXEtP68Bpypn8fBngQxfz8PM6ePYtYLIYLFy4YBQLYLY3a754cg8PKsFV6QTdkPXGIFisFKmmHdrvtCSJQK4Zato4T/2apVxaKI3/NvxuNBvL5PACYeVSpVJDJZEwBOibYUWByQ+BGw4M5SElyjVKQM7uaPi1NrOIzUSh3W89ai0nnZafTQSKRML4y+q34LMHgbtVN4rqhUPxg8z387bfoaFaSG52YmDC1Fqix7hfqVNLd0dZ6dOIBMBmAFLbUxMmL89qcQPQ227TCoH1Egb60tIRKpYJcLmcSfwjV1vR/7dujBDdY5d41UoDmNEM/JycnsbS0hLvvvtucosMTdAYZS5vvpKDTRCgKZPKz9nVtQU+zmZu2HapGc18txavpWya+kDrkAi8UCsZs1834oJra1ULboNYNx7nVapnDMvRIMdtxyf5Wxyyw63BXAU6lhMlw5Ne1KJXSmZ1Ox3DOpEuA3RKuFKitVsvE4HPd8/O1Ws1EnKhFwI1crQ72Qa8+o9XGecPNjxQQhTeVPL9xvC4FuJ8zQDU3YFdjU4fJ+Pg4UqkUTp8+jaWlJUxNTRkn0EEEOBc9s73K5bIpEasJBVxg+romFnFQWbYyGo16qqtRI+F7gwop1Wyj0SiWlpbMNX7yk5+g2Wzi0qVLZjGQeuJ3rhV9orDvZ/PfdOCl02lP0aP9JOmoRWHTHlobhwKDDis6u5TLpYYJ7AoSm/fkvZSv1e8dFCrkeP10Om3yAbipADAhh92Ex1GEWfIZObdIG2pUE6mFarWKUqmETCaD8fFxzwk19jWp7XKz5d9ch1tbW9jc3MTW1hZKpZK5t0bn0CrmvAqHw4beJAXCdUHhSCG9tbVlon0YV842MAqONCnXaiCwmyFMhYDzqVt0jTpnme8wOjpq2qdWAcfU7q/rngPX6A4/5xsnEUPQpqamPKVE/YrT7AfUrFlgiKYa4F0kdK5yEZOX4yLnxKOZBcDUAQa8Jvgg/KBfP01PTyOTyRiuOBQKmboaV65cQaVSMSYgcS0FuGqlKuCoqcXjcczOziKRSODkyZOYn5/H3NzcwGGCKpSB3XAzpqNTe2IUgzqlGXurkQWxWMz4VLiQuNBp/lJz4gLkBqzPxt+qjAwK9R0wWaXT6ZjQOgCmqBrnKLA3FPSwi1kpum2YGpZK2kfrbdtrkhuVLagoaLk5sMREsVg0Y0ehqtnK5MXtWjrU4nldto2v08LiawDM55h0pbVL2HZac+zrbn4QKgHcqOg0JRXjxzZ0s6KuSw6c0AdRxwjgLTvLHw3JYb3hq72/dpAKWQpmNZ8AmIXPDCpeh5q28oPcEOioUVpov3w9NQBycNvb21hZWcHLL79sjvViNIo+z1Gj26TT8aRw4rjxaLOpqal91eK2k1sowNU64lgw5JJxxGwD5w1rN2sEDzdp9SkEg0HPQQ9MjVb00owHAePhs9msSYxhsTXVdqk48H5KQR5lbRgVNFwTmlijVooKWK4HvY4qLhqwQA2c1+Ta2draMtYk57ZaTrru2DdKyXQ6OyGCoVDIw63bNWj04Gy2waaBAoGAScDTCBsde8oLDeflBsQ2cBzpOyP8lLpea/jYBTiw17tNTlCdJoxYWFpawsLCAm666SbMzMz0vC47kR3J3ZShSdxNuZMDu2Uo1dHC30x/5QBycDkQ6tnWEqd8BtYEpxDhAqWWNygFxPtmMhmcOnUK73rXu9BoNDA2NoaVlRW88cYbaDabKBQKnsMjiMMW6qqd0fFM01rDMKnBzs3NYXp6Grfffjump6f3FWrJ+5C7ZPgXfQ/sGy7eSqWCbDZrsuuAXcdhJpPBwsKCoS0AGG2JfDR/mIqtoaOHyTlTgJ86dQrnz59HNpv1FFUCdkuu2vMa6J1jcRiwtUSlDVl7m2uJAlUjZmwhDniVJSo73IgpyGlJKZ9szyvN11AFptVqGXqE84NOSv7UajVPEhGtN84dPhc3LdKmtM5UsQTgK9Apvzj/eYYA5RwFvq3ADoJjFeDKebNz9MFotqrWtLi4iJMnT5rYz17gJOJuXC6XUS6Xkc/nzeKgx1yjCzhpODG05CcHUx0j1Kw1MYAamx70qhxbu932lLAFumd1des7CiIeD3bixAmzoKhB8tn8fA6HDfu6tnVDq4STfJDIim6wY6I5f1QLJU+ZSCTMouPYqLNVNxwNMbPpIHKpvK793AehT7StzGicmpoyJzlxnLXYlQpvvedRauCqQWoRJgrHcrmMVCrlGXMNvbOhz6DOy2q1augrUimkBKltcz1zPCioebC0Zv9yDVDO8HqkNKhpc+0qN89Nn+ucFqTy26RQCb81oMqp34bGjcCPQukn0I/diakPaO/y7DRSBrOzs7jrrrsMd9oPtgNve3sbuVzOVP+j1kr+WrlNFrWxQ8o0MgCA0Rgo0FOplEl1587Lz21tbRkvN3ddAB4TbT+IRqOYnZ3FO97xDsNHklrhJqWOomsBexJqqj8XPitE2lzyIFB+lAKMGjytHqZ7AzAFwlSb1YiEUqmEkZERsygpmDT9n/OQJzuNjo6a8VM+VzV2ncODgGM3NjZmzmTl5q/PQ9+MOmuJo9LAuTmQShoZGTEWpGrC29vbiMfjJhoF2Hu+p7ZV+edqtYpKpbLHAmYmJtvBPmAQAZNgeGhKrVYzSTTclHmmbTweN8kytEx1s6lWqyZLkkqcCnA+N6Eafq8yAdzI6vU6isWiJySZPD1lil/QwXUnwG0zzDbNbNiprHzQQYrjALuaraau8nqqIZPbphmmQpb31mxQvq+DTP6ShyXzu/RYc7D1eXSg96PF8dQhTmguKvaNakrK4R3FQtd2a3+qNsQNzm7bfkCKg7QWr8PNVecQS4+q6a2aPzU/OqO5mJSi4PhyI9LysPZi4zX52kGiopQ2pH+nUCiY63GToSmv/X+UGjjBtrEvSJnwPRZv4vz3W9PabxwL/tZoEQ0ztLX6VqvlUQRUw1XrjDQM47x14+N3OKYakECZoxFHvdYnv+dnXbJNWutfI1QU/axYG8emgduxoeSRlA+KRqPGpFxcXMTNN9+MsbGxrvWFVQNSDzV3dYYD8v40p1Wz5uAD8HB6jNsMBAKewjZMfSVHWi6X0el0PNflBKMDkpXb6Jyhac9rswD8IMItFosZa+SnP/0p1tbWjBBnYoN6+FWAnzp1yhQZCofDeO6557C5uYmPf/zjuHDhAk6dOoVvf/vbvlXr/PpeLSguDi54CnAmevCgDYZsDQpNalErDYBnUwZg6lmMjIyYE1HIgZKeazab5uBqzjvlZrkoNYlD+V1bc7IFOoX4fqwMFlLKZDLGCaf9C3TXbA8b9sbMdcQ1QeUEgKE1Z2dnTVa03zy2NzpgV0ljX6q/qtFoeDR6RhOl02mzlrRdWl+dAnNsbMxY0pq7wWuy9ow+IykaDR3V9hM65+3n5fzXOcpTuTS0lZ/xG9erdmIe5kIHvIWX+L9qEPyfWggr1C0uLmJyctJk6vlBO5de7GKxaJwHTPfl4GnMKAeIzjEKctIT6gihQCJNwvtSS+KmQfOO6e/A3nPz2BZuCpzMg4LtHh0dNSnp5MFpLnbj6X7wgx9gamrK/P/444/jAx/4AB577DE8/vjjePzxx/GVr3xloDb4CXDbuolGo4YiOEjpA72m7UhULUitGm4Yah5T2+L5nBoZoNEDqoXxelrGQE1nP15aefRBEAqFTFy83beAN2FJhUg/rvQwQM2StBz7kPdleWCuC50DhCpQtFq5IVOoqnNaAw70bEkKSq5Rfj8Wi3nqfzNsT7MxbSuLY6bx9sCuskDFTzl7O//Dfkb2CS1QUk8jIyOe+HLtH/t/7fduGFgDP6yFTtiN1J1Hw4OSySRuvPFGnDlzBjfffPPAUQvsaC5WcmGMXuCkUG2YgwXAxBQz7I2LnRuHnSDARBtq6bQAstms0cI5gEwE0IWvdAw1x/1ibm4O4XDYhM6RK242m6ZUZT8K5emnn8YzzzwDAHjkkUfw/ve/f1/jCuwVOGr208rIZDKeSnXdYEda0CeitUo0xI4baLvdNpqY0mDcGKnZEhxLtllzC9i+YrFo+Hxej7/9eMuD0FXBYNAIcL0+tUylAWxt3zbHDwvUiFutnXrt9qbFfkokEpiYmMD4+LhJzOo2ttwISMPQUauaNNcUlSfOBcoHOpxpSXN9AcDGxgaCwaDxRXEta5y51lhiDRRuKCyexUxvPguFM+Py1U9nKxMcS67L1157zUSi2cEO3CT8tG/NtrZxYArlaha6vfsov6yvsZj/0tIS5ufnMTY2NrCWwY6mA4hcp31Wn2qOwK6DBNgN22JyCB0YtkZLBwedJ9TigsGgySLTBcdB50ZE7YEL/qAJGRMTE6bo1fr6OpaXl5HL5dBoNMyE1kkWCATwwQ9+EIFAAL/4i7+IRx99FKurq4aSmZ+fN7WLbfgV/tcJ2IsnpmNOTctu8AtFU3OW/cxNkFoZN2w9c5L9TCezJu5oPC7pMqVidEy5GZHnVc1O+/YgYN9QIGlInk0fsT/ZD70W+kHbYoNaLp2VzWbTrCdqz6QNul2Tz0gnIPtQozyUbuMcIeVARYvrhfOaJWy3t7c9viCOOa+h2ZF+0A3ZpsbUr8P5osqY+uj0epRt6o+zLbNultpVa+CHvdDJU3LgqNWkUikPbzY/P48TJ07g9OnTmJubw/j4eE9qQbVuhviomaQ7LgeJppU6ILhLcwIxe48naJBuocDg4IyMjJhaHpyEFBzA7qDxGrrgOOjkqtU8HhRM1T937hySySRee+01Uyze1hAA4Ec/+hEWFhawtraG++67D7fccsvA9+p2WDX/1j7Vic6xHjSEkItHQ0K5ADU5hwKEGiG/R+tDSyRoGJqa0Dr31ETne3wmfsfmbdleAMbBuh/6hN+nlaglTTlfNFTVxkE3jV5t0Y1CFQwtEsWUedZx1/rmftD+JOVC3wT7nnQMFSiuS1pbfJ3tIv3I97VmCrVbDX0Edjd/fofRKqz4qXOE61znBy1lrf2iz8gNZGRkBPV6HXNzc2i1WigUCggGgyY8Uq0HxqEPioEE+GEvdOWp1eE3OTlpHiKZTOL06dO44447cOutt3rMmG5Q5weFpC4IcmPk1WhG6bVHRkZM2ygQNeNKnQ3U8JXDsoUBNyotkMR78WBj3peT0HakDSoE+N10Om3qMXPC2mYvACwsLAAAZmZm8OCDD+LZZ5/F7OysqR29srLSN1lK+14FGRcVNzcuOB5YkclkzObab0w5hvQtUHuynYj0O3C8OAdYD5pp2KolqSBqt9vG2amfY7QKsFu7mXw+37f7gYL9oJEo7C+GRtqRMX7OrsMW4H7XZZ9S4IZCIYyOjiIUCpmKgGrhdrseNXSNqVcakeuYgphKn2rNNnXF9c+1phs05wu1ebXqqMXzM7Y/h23T6BTOwV59blMqOmd0zpNuoVVq+6t6rY+B+IheCx3AvhY6oU4mXYh80FgsZtKt0+n0QMWq/LzD7HRODvVu07y2D0nVUCge2cRFyzBBRotoWKEuLN6bWjljY/WkdY1mUI3ONtv2w6PyOfQMP8BbyjUYDJqC+QBQLpfxp3/6p7jjjjvwwAMP4MknnwQAPPnkk/joRz+6r3HVCa/9r9p4JpPB6OjonvHqdb1u2X02nQLAY4FpRTqGClLQAzBjqhw955ktKDmu1A51gWromvbBQThwzhGCc41t8BPgB90s+kE5Z7YP2HXwkecdHx83R6exP3sJNx7WwTXEQyFohQPwJMqoVq5jpuua6fFU4NgvtHo1QYubtF37iH1KKojrUi2CTqdj7sNaR378NbBLOWkBN0Yy8T5cK37j2u26RF8NvFwuo93eSTXmQv+n//SfmoX+2GOP7XuhM2yHTgryXdTYTp48ifHxcdxyyy2Yn5/3xIH3u67u3NzpqXWrBsYB57Xp7MhkMgiFQmYRdTq7qcI0q5gwwMG1w9j4HWDXKaXmIE/zpjZBxxD5O41/pYAalPvnd2jFcLOgFkKhsrq6igcffBDAziT65Cc/iQ9/+MO455578NBDD+FrX/sabrjhBnznO98Z+L4cA9VseSg128MqdRSA/bRGLjImc3AD4iJXbZdmLTlvxufTzFUHJ/tfeexSqWQOnaV5zjFOpVKmkNj4+Pge4a0WiF6fbdQIq14IBAKmnzhPeDYjq+hxbtvXOgjttl9QIGkWsh4fRkVhkM1ELXKuNa4rnqDD6+n4qXKi85prmxq7LRR17JVS5WdpCWgxOm7aKoPUmczyGt2gPhPKN9JLOobqoPVzZHZDXwF+FAtduWNGJMRiMXMs0vz8vCn2rzRAP3BjKJVKJuQvGo2aXbbT6RgzPpPJeML2+H3lbilIab4zpjqbzRoeVdNso9GoSb8lV8kqd+qs4yDpvcLhsOFoaYWoVbJfhMNhk9Vnh2wCwJkzZ/B//s//2fO9yclJfP/739/3/fTZqBWpw4j3bTabyGazCAQCmJ+f70uf6AlJAPYIfrWc+KNlD+x5xk1Si/2Tl2c5VPo8qIFxPGhB2Y5a7V/V8JRD70Up2M/M+UrulOGvGrpIGk/74SgTeXTOUiCp05fhjzxGrdua5XX0eSh02WekjlKplFk7FKh8n1SnWno87Z00G6kOKl8qgKm9U/hqPgGTzVhygXJCqRVeRzeGbs+slBjlT6vVMhYwLUK1xhW95k3fET+Khc6H5qTjqRfRaNScMD8/P4+TJ0/uEUD9rst471KpZML3GJGgWrROQppXbIOaZcViERsbGx7nyPr6uhlw7sDazkAggEQisaeanQb1BwK7CUGcdKwmaHNg++U2OWHS6TSmp6exsbFhJup+Tfr9gONKIa4mKheLeusHEWrKidIZFQqFjKDgmAG71gqFsTrYAHhC8FSTajQayOVyKJfLRrujMsFFydRqxvOzj4FdDU4Xsxb1ok+km/PRBtcFU7/Zdj4PfQFK0XDuHxVUcOnf+j7DH3slobGdLCtBRUsLSNHfQOoB2C2nyzXMjTwc3i3pzNBFrWCoFKLGnQMw648bE8NsKQNI7yjXrhsGv6d+kG7PHQqFjDVKa4ObEAW4rRjw76sS4EcBdSLRlAkEAp4BUo/vfsHvUGPmtflb+VRqYRTG9k5of0a1SftgBg1zooDgZ+hJV0cFBTuFNqNcbG/5QZxTqsXbGtNROrtUS+Emx7/5/CzANehzBAIBsxky7IyLT+/Nz7JPeU8uFNa2AXYdzlrojGNNa4vjxk2eKe3dNkE/a8kW2v2EOOcBQxhbrZahT1SQ2Ny03vswE+9srVP/1sgt9V310r65wdGy4nNplUDy0dycyTGrcsCYdFpYSu0AMBabhu1xPKmdc11wzVEga3tVBmhQgV0WY5B+TCQSGB0dxejoqKH3eF+7ZMCgODYBzge3ucRgMGgekgfbDgJ2Pp2PtVoN6+vrRmgorcGEAQoE3pvhTNT0qNmpA5DXoSZGbSGfz6PT6ZhqaOTSI5EI8vk8SqUSJiYmMDY2ZnhV5dm442tmGTeCg2jNNNE0o7Qbx3aYsCezcouMtaZjehA+mM7r0dFRD7epkUO6YQB7z8a0nVW0/trttkl04mbLa1YqFbNRUBPTVHyNTya4SWp2Lvva3jh6PTs5ZZraPLiDZ6DSIlGtTecycPiJd7ZCYQt0AIZ66vZs6qyj4LTjtO3QS76ntd5tDZU+JA3x41ri/TRAgcoj2825ouOiPjc7/p5BDkqP9aN5aSF0Oh0sLi6iXq/j0qVLZjz9krO0bd1wzQU4B8B2TvA9YPf08kG5b+7OmnWpg6ZhSfychhsFAgGTdg7sOhTsgjo08ehgUW+yms4AjKbIxaynenAS8fk4mdhWbi6cLAcRuOpoYUjVUQpujqvyovoaf0ejUXMI7iAaYK8QvW5gmrea61tbWyZigEK4Vqthc3PT49zlPbSMMekv8qHcSLqB76mAtfnqXqASkUwmPZajRm3tFwdJvLPbyf813M62GPrNMeWruT4IXce2A1q1VBWuFNBqZVOoc43bVrU6mklh0FrXEEh1kGoklW2J6GbWrz/pK2A/6H20fYPimgtw5Xlsk5NChp/ZD33CQdIjltjxfI+nUNtCLRgMGhOaRdypOQYCAVMCkg4lnUjkZDUqgrtpIBAwm0a7vVsbPBTaOZWDmaUaK83nV9PsoJSHagyqeRwV7PHUe3FDJl3ByJh+0L5TD77yz/ZCoADg5quhh1ywTO5hUSHWHtHjtOywLjrXlDvXzaTX8wy6MJUq4HmSdOaqctJLkAcCh5N4Z2uk3MQYcDA7O2v8BWNjY0ilUkaQ94L6DLjuaC3SAamWsJbSZWkLTQrTOc41HYvFkEwmUa/XsbW1ZbhtbhjsO/Wf0HFKXp2Wgp0NyoqYrVbL+L76ySpt49jYGEZHRzE+Po7NzU3js2P9ItW4rzsNHNjlkGiyqoOPWu1B6jrYGo8uGNWu/OItm82miQLh+xQ6eno128WOZbspkMjhU4PXo5voDOMk1KI8nAQai6y+gn5QAUHBxev1S4C6WugGo/exHZZ21MKgG5P2qV8iBKH+DfWvaC4AP6MZnQq1qnQuqdDmaxrhoG215539LHafKXRtqGCgEkIz229O8JqHnXin46dCXOuicA3Y8fF+oJO5VquZgyu4oepGy/6mMKfQ18gv9oMdush1DHizglXDp6KkXHoqlTIp+Tbfz+uGQrtZv3ye/UBlDykiKnp+fddr7R4LhUKHB5NiyHOySh3jcPcDdZCwc+xykapxa0YZHSDALpdYLpeNM1XDzgiNZuh0dkO/AHg4OE27Zxump6eNtkCtVJ9BqZVBzGXbscVTR3iQa7+Ffxiw6QEuCE3o4ATV4+T6PRcXjo4lr69atx/vTRqFRYx0Q+H8ojOV/C2PS9NQPZYkLRQK6HQ6mJqa8vhWdLNVBcHuj0H7kaGK0WjU0HraJ/1M7cPMsOU9VROn8KbixegTLfbW7xnVqUjlhs5MdVqyzCtzChjxpVmQuk5sik3pSJvfpkXHuvocO65Vjf2u1+vGl8XQZCqfgx4JSC2cYbGvvfYaLl26hFwu58kUttf8damBa+QGO5PC3I6H7Lejt9ttE8PLnVz5LxWMGhnCjqJGrIuCGkKxWNxTp4HX5e6sQp6DNDs7i2aziXK5bIQBJwsnAU+DoTDiBuNX2UzbZjtygN3EhFarhVwuh2w2i9XVVeRyOXM25kE1hv2C/aTFqkhbcQMdtA1cjNTANTqIIWG21k2qq1KpeJxiOqc0q488bjQaxfj4uIcC41iTLgF2HNZ0ZFGQq1Wh48OxHTQUlptLIBBAKpUylB7f6yW42e5SqXQoiXe6KWtUj44f16069QYR4olEApFIBHNzc6amCNcbHe+amMO+tNcu5706JdmucDhsNFudMxwT8usat97p7AYYMEJGLWGGh6qPaVCfhD0PNd1fNyB7bV9XGjihJi53YC2Dyok7qBOTFAiFlRbot8137rI6iCpA6QQBdk+AobZOPoxRKxxUTbLQaAhgd+fljk3BzntQAFDAqYbIfrIXrl+/UEhSeGkiDb/jN0EOC+rX0DAtNUMDgYBJUukFdTapZaWv6/xQDbxYLJqNPBQKmRKelUplz+n1tLJ4UAjrbTDxi9oSo5vYv4FAABMTEx7KxY7UsDdctrdf/1MQk2LUaA27rxW0LN/3vvcBuPrEO7aX/d1qtVCtVj0bWCKRwOnTpwdWtrTtFMiBQMD4FmiBE5rinsvlPGdSst+5RjnXOQdUmw6FQiiVSqbd6XTa8N30r9Dy14g1PcSDGwqTwvgMqrn3en7OT1JDwWDQowyqNanotUEciwCnwOVDM2aTqcIXLlzA9vY2xsfHkUqlei52dV4yTIzOEO6kNL04GamNaqIAY3wZzK+FdHieJf8m58kjr4Bd3pnCgQdHcPek2cWzOBOJBKrVKuLxOAqFgnFwAjCaA9sO7K3yRzDZaHt721Qf/Mu//EuUSiUsLy+b083prDtoWOIgUIeLLhx1OjIEU6mVfuA80YnMhQrsaihMtllbWzMmL8eMfgi2jRsM63eMj4+bcgvcoFXL0mqAjFgoFAomMYOf7aaF876DPLPy61oDSM1rP66dc/q5557bc82rzbBl+9VJzPFmXZBBU+ipfTYaDWSzWbTbbY/mDXgj0biJ8fkqlYrHR0UrmXKAwQiNRsNYBtSU+Z10Om3ay3ux5EMoFPJQq+SpualrvLtq7uyjbs/MNpCO5RrgxnMQC/nYKBQuapowdiQBDzTVrCsbqolxN2f8LQCPo4ECwI5MIG9Nc4YTk5OJ16bQZltV6+IOyhA5cujk0TRelG1oNBrGhK9UKp5qhHTOaG1kbQefj5OqUCigXC7j4sWLWFtbw2uvvWY0JbbjIE7h/Ywnf6uZrX+rJUS+dNBrUrO0KRQKMY5tpVLxxNWq9aTjRYGq9SnoM2ANGxW2FFzU6GkpcYOmNqVRQ9pW3ndQUCjwORjpoO2xNwP201FBNUU9cIF9qJbCoNCxU2c7I0K4MVLpUGqGlhzHjWNdKBRQqVSQzWaNVRsKhcw696tqOjIygrGxMUxPT5tnIK0WiURMCDIFeCKRMHkMg8SAExqcwb/tYApb2+7nBzsWAU5OkZ2odUqKxSJ++tOfYnNzE9FoFIuLi0bQc7B116PQZcwusCvY1Dzjd2iSURiSPwdgJk25XEahUDBaGDUMgoKD6fm2cLDbRUcmedRUKmVCrmjGs3gWhXg+n0cqlcLs7KzZmGjqcbJQy8xmsygWi3j11Vfx//7f/8MLL7zgKaFLYXOUmreOK4UNN0tO1M3NTZNtOiio4VWrVTP+FMicPwwV29zcNFp4p9MxwpzCgbU1+F0qBwCMttZoNFAulxEMelOzKSR4Snqn08Ebb7yBRCJhhCw3a6XhNFxy0Kw93XwAmHIA1BTZ57YGfpSbNO8BeDcY1Ug53r2KOxFcm6OjoybqgxYiNV+uT2BnbjFChAqRVhPlD3M1WOqWG5tSHBrSqv43PXVHqRUAHh+IhuXaiUf9wO/QWufzaP9263c/XFMBrg+pXmaG/NApRW//+vq6ObxUhTAXgmpiugsytI+V5jiQFKpKdXCAONBMw06lUp5aDapV2c+kA6sV2ZRSALzeZGr5pBo0+YibDgBkMhlPgoFmEnKiqhUB7Jra9g5/lPy3Xl+5QQ33arVaiMVi2NraQiQSweTkZF/OUGtP62vqlCb1xnBPFglSmo4RQxw/ms/c0JVmCwaDnsOE9fk6nQ42NjbMHGGiEBUQbvrUzDS6Ceh/Uj0FMQUGrTVgNw7dr7LiUWjgOjZqSbGdpJTob6CS0y+UkEIM2KEh2+02ksmk4Zg5Z4LBoMfSAmACBbhJsn/4owoDLTctRqXWPq1mrfvPZ9NSHrQw+J5aWkqj9AM/pxugypNu67PXtY9NA9czCZUm2N7exhtvvGEK/hcKBdRqNaOxknPkJFGBzMVK84fhRjS/WaWQmhn5dwpALrhOp2NqSVC70l2cglI1BZrTepahPie/xzYXCgWsrq4iGo1iamoKkUgExWLRQzNQM6FlQT6PBzVns1lcuXIF6+vr6HQ6eP3113H58mXDEWo9bBX8h62JqwauGhPgpUKoFW9ubgKAee5e6de8pvYpNXFaWqztXiwW0Wg0DA1CTYlRBly8GiLYbrdNzXZSI5FIxMQk8/oULrRk6CQPhUJGeFF7pDDgpsHQt0EWOQVjrVZDqVRCvV7H6uoqstms50Bu2/Q+KusK8AoqbjAMNAgEAlhfX0cymdxz1GAvqBOaY8L+4vjo54Dd2tl8fm6Sdlw/hb4tHCl4dU5pNjiVQh1DjcWnDLBr8AwKbkRsoyo7vTaB6y4KRb2xmj1Ffq1UKnkGq1qtGq/xzMwM0uk0Jicnza5I4a1mcTQaNVwdB1dP6tAMSR1oLlpyxwA8USq8H7CrSdDUo+lLNJtNI0zUcauhYZFIxDiA6LCKx+PGtARgFj/DrMrlMjY2NrC2tobl5WVcuXIFzWYTa2trxtmq91MN5igXOqGmNmN9gd2651wgGl3STfOgtqSWkG7c6qBSLYsLmJ8Fdh2f6sPQsy95TQCew2YpsHQzUMckr6VWFAUSlYNB6ROi3W6bzDzeh+3z40SPmgMHdp2PALC1tYV8Pm9qDsViMROd02089TrqCwKwJ3FGNwy9pr2R05LiZ3htCnMKYc2wVR+YvS4CgYCn3rkKVrZHx34/64n31pwRXofXt/utn9XcV4C/+eab+NSnPoUrV64gGAzi0UcfxS//8i/j13/91/G7v/u7mJ6eBgB86Utfwv333z/Qg1C4qMnNhcjdl/TF1tYWrly5glQqhcnJSZw+fdqkoI+Ojpravbqw/GqYaMlG7qSkMHS3tdOwdeLw2sqH2Z2tGWA0r/nM6lTlBKYVQB4uEAiYCBUKYxb1oqZQLBaxsrKClZUVXLp0Caurq6YkaqVS8ZQgVa3bpgSOAn6HDXBT7XR2Drr4v//3/2JpaQlTU1M9k7bUf0FLhvOGk52RPXRmATCUDb/DsaNmRk2RmybbTcc1LSv1j2hKNdvLtvkpAfxNXpdjbQtZdXZyo9rY2MClS5fw6quvotPZOdyAjmpq4PY42nTdYUAFsYa3aR9tb29jfX0d0WgUGxsbA80xFVjUelWA87dSGqqdc9Pk89KK02xGtpfzJZ1Oe4Q+hbEGNlCrZ+SJhh5rmQubugJ2C/H16ksyBRTguj51U9iPVt9XgIfDYTzxxBN4xzvegVKphHe+85247777AAC/+qu/is9//vMD38wPfrw4hS2LDQUCOzG3zI5k1Ean08HMzIwpdsMFQ02Y4TocTC5AdhhNbA6UOqBYdpIONO7odHBwElFgUkiq8KTGoo5QLj6dBBTgDEsMh8MmGUPNdU7ora0tFAoF5PN548RkDWRyxgA8G5YK8KN0ZmrUgGrHHJtKpYI333zThGb1awvbq8KXPgN19ildRKGqIWn8vIYecnypdXGT1CO9KOQ5f2iJ6YJVC5JCWHlSfka5X+0vDb1sNnfK1hYKBZRKJQSDQU/sui54myI4Sg7c9qOoEKbyRLqJwraXIKK/gG1WnprXVeGolo19XU0aowLIOjK0ankfBirQCuPcUguuWx/4WQI2jdXrmVUxpMOVc9bu125tsNFXgM/Pz5siOOl0GrfeeisuX77c72s9oRotO1wnN3crjQulZlqpVBCPx1Eul7G1tWUGmrsbHXu1Wg2FQmFPDK+axaoVkZ/m/dXUB7zV5DSmmYPGiBIOCicdzUQWqtdaK7qL63l7uujJzXICc5HUajWT2svi+HxO9WorP83XjhKqpfC3Cp9YLIaNjQ2MjIyYUC/NviVtQoFPB6Ga03ofdWLZC5KbLb/L/qegBmCEv4Y1cqw4flrTndfnJs/MXAAe4aLzm1qm9r3OD13YjN9fXV3FxsYGAKBYLHp8RqqpKvajuV0tuHkyBHNtbQ0rKysol8uYn59HKpXqeRjLfkplqMKlGjQFN8eOa5snA1GQVyoVU3toYmLC/K31W1TrtjlpW4grBqUmaYGXSiVcvHjRJJsB3sqdfve4KgGuuHDhAv73//7fePe7340f/ehH+OpXv4rf+73fw913340nnnjCtzxot+pmXAg2n6yLgJW5qIWqINAsQ40uYZiXXQQnHo8bAd9utz0x2BSC7XYbMzMzRgDSLKOQYFuKxaIRCGpaqtPSdljoaer8rH5PNXnGGlN4xeNxo40xw6zd3gnLu3LlCnK5nBFg5Prtwbc1haOgxvzuR0tF/REM1ctms6aMgWqu5Me5WanTkb+1TKhftA3/5vU4T3hQg5rFHBdq5Yx64XdVYNj8vr2hcPy0jICfdmULcI53LpfDxsYG8vk8crkcABhL0o/71n4/Cg5c224/K8E5VygUDCVI3nk/vH836OatWrDSHOwf9rsWmNOUeWrgLBdMf5mm0++HxtD29PoOrfFms4l8Po9CoWDWgkaj2GPYb1wHFuBbW1v42Mc+hn/5L/8lMpkMfumXfglf/OIXEQgE8MUvfhGf+9zn8PWvf33P97S6mXaQvcOxI1SLUq8xhSA5Ki58fp+akH1dXWhcWNSgKCjVkaJaNe+tGwbNeAoY5crtsCBb0NhasApwfoa7P7lTmvh0ELEtaknYppv+r/2t1sxRUGN6L9VmdHxVWL3++usm7Z9at/oCOOEpQIFdRyQnPwCP1sTP6oZga6sqiNhejpMqCrSqNMqJTma1nuiw1HYAuzHszWbTE4ZGRYJamcaxl0olbG5umr+VG1Wqz09gHJUTkxamCmP6i5Ri5FGDqkAc1v1DoZDxczDIQeczrSMKe1qlgUDAWFFKmVI75/m4XHN8zv1YM4N8lk7pjY0NU0KWsodrhYXL7Ov12rgHEuCNRgMf+9jH8Lf/9t/G3/pbfwsAMDs7a97/zGc+g4985CODXGoPp8kOB3ZrRnP3VDOLuyhN8unpaXPoKwUazWH+VsGo/Lit+fOeWndBFy7NblIXuVzOvM8FaHu0NWabg6DOKltbJM1ibz6MUCE/rtwnY5/VUrEdI5z8uqg6nc6RUGNsHyckF4r6GYLBINbX10146OjoKJaWloznn6Y3i4vxOlqTgguZ/cAwzlAohEQi4YlisOs/a4p2IBAwDk2OJS00glRevV43GXhTU1Nmg1Vzmz9UNqiNAjtCl+dZZrNZrK2tGUosn89jfX0da2traLVayOfz2NzcNGnm3FA0AYX317l0lL4NWxO3f9SfwaCAo9hQVHFSpUcpT9uqUl8DlR4Anrlpb+qHDVrs29vbnmQ81bD5DHZC1lVp4J1OB3/v7/093HrrrfjsZz9rXmdpSgD4/d//fdxxxx0DPYiflsYH4qnSHHwKcHYytbNUKmWuQ7qEoX+bm5vI5/MmAkQFLOO/NdifE4IdqpylbibUujlR1VrQ59J+68ZrKU+qv/l5XpcTEYAnE5R8OB1j/K69U6vmq4vbnqBXS4353ZPtVMchOUzGu1OABYNBE9uvzl5dnDz4WseRfWD/kIJhRIz6DdTBRg2Z85Eats5PXodzhEoDrQW+z88Du+Y8rxkMBs1BIJyjly9fRiAQMOF42WwW+XweAEwmMHl4rajH+ULHrPb5YdAVfuDc0nnKjYsbTDQaRaFQQCQSwfLystmwDrNNXIvMqA0Gg+b4RB3jZrOJkZERs3my+BT7jVaU5pMAXiWLz3kYoMJIai6Xy5kNmMl/gcDuOZv2fXttzH0F+I9+9CN885vfxJ133om3v/3tAHZ40W9961t4/vnnEQgEcOrUKfz2b//2QA/jFwnBxaF1Qwg6kMgnkuNmqjw7x57MGgXBztva2jKavMaghsNh5PN5RKNRzzma1NK5UFlrg4segBEavLc+ozoqVfO2B4T/89k1moOLlw4PFdTKeev9lJrx49D0/odBjflNOApv1oOhlqZUVKVSwcrKCpaXl025gLGxMUxNTWF6eto8eywWww033IBoNGoSvILBoCfdmhsxOVDek2Y+rzUyMoLR0VGMjIwYTY6btgoqCm1uJORL2eeabcsNiJFRfD7OGS30VCwWsb6+jp/+9Kcma5Q+GBYc40bDec2NSDcVPydmL1P7MGA7xfnDPuLaLBQKCIfDmJ+fH7hWdj+QxiEdYis6SmEqZUp5wO8w5FCLqekaVSv5MKCWPzcaOtPtKCy/KBu/9avoK8Df9773+e4A+3FsdQMHn9e3TQeCi6zT6ewxP1Rjp3OSmppmVgG7IU8MC2P0CXlWFZz8rDpJ2WbVhNjBtibut1Hp+wD2CFp7MCmAlGrSz1FI8lpq0uq97F2dfx8mNUaoM083UvoodIFpujQpIOXJWTwqnU6blGdSJKrBsMwr54f2D9tEB6jWAQdgIimUD1fhwA0/HA6b08SVmtPxUm2T16nVap54ZvYLLUOlvuwQSY4VNykdQ52H/NxRaOCqUNj3V+oyEokgl8uhXC7jxRdfxPz8PBYWFjw5FlcLTbpiiLH6vejfCod3ji1jFBsFOIMZeA1q7FpY7rBRr9eRy+Wwvr7uyZSmfAkEdjO7uwnqQ3FiHiZ0t6TgpamrWoQOPjW0dDqN0dFRADCaOWkRCgVqQur04eag3ml2ol1bgxqcRnSotquJICpcubDVJFd+0v7bFvrqnOImYh/EoAtBFzqvo2BbNKxON5fDpMZ0bNlXHA86frkZ0ZpiGCiFodZKZ5hXIpEwY59MJjE1NYVWq+U5bQgARkdHzb2ZPEUntY6tRrZEIhGTJMUxZXQC09gpGKm5k99mH3ITUupFw0xpAZA2yuVy5sCNWq2GbDZrYr5LpdKeGGoV+hRCGmGlm9VROA/5W3l+zXzk2GkQATVxapr7zULtBs4lVikEdte2hgUDMKcwsb2qNNDpzPe1PoqfwnM10DlXKBRMFBmwG/uuCqGfXLhuBLgKMC4q1dgImjuMI9WaFpoWzxBA5Sk5edRbbQsTdWYymkH5boaqqadYJ4mfM5LPxL9tjtwW4Prb/tvWevS69mfs1/V/P0uAvw+bGrMnv44nN2y1tHRBUzNm32tiBus2l0olBAIB42QkD84x51zodHYTnzi2HEOa+RQ6gcBONTylTygUNWKJ4PW4efC5uCFowggtC2rWbAcpFc495fztiCK1YFQhsP0rxFE4DXk/rhGNqyfYn3zOjY0NBINBrK6uIhgMYmxsDJlMxuPMPgj4fNSU6ejmmLGvgN3CcqS7WI6CVBfHh9SMns17GMKbtO3m5ibefPNNrKysGD8HKWN+hlSefaSaypFuODYNnI1SIaMRI5qxRgFMHptlP8knaTIQBbQKXnKlGk5EbYW8FIU7E2poktdqNSPIbS1bHZr6bBQGSm/4CVH7O7awt+kY/awf7+xH2dj34/UOmxrz26io/SvtoAKKzi8uQq09w/KsXFRaw5lWD7VCCklCnYcUqsrVMvNVqRjVnJVy4zziAQEa/UB+PxKJeMo6aOIY+2NzcxPVatWY0DxoI5/PmwM5qNlrtjA3PvYZrQX1+7BP9P/DgI4pI7F0fDketFSY67C+vo5AIIBsNmu0W3LhVyPA2QfUXBmZReqMaLfbxmqjD4Vtp/NSq5AmEgnjJFe5czWg1bW2toYLFy6Y4w159iXnHhUQOjj9spOvOwEOeM0DDrJqwNSCAXiEJustTE9Pm+OXeA0OpA40Dyylt5yaj1aLo9Dn4tva2jK1SFjljlo9qRgKI1so2QKXrxH6WRV0Nuz37N8aQqYbRa/+PirYbfN7XZ/Zjr6hkPcLy+PzUUvnwtPkGgCGeuH12BeaS8B7atIG702BrKY5NWj2rwpUtcToV7ErUVK483r6PHo/P9+F9gFf0/nmN8eOaoy7zXNqq1Ru6GCtVCooFAqm/ns6nTZ+psMAx5T9rQoX20mHM9tJC5rHwDHiDYCh6a7WQlBwc97Y2EA2m0WpVDKUkoau0ipTSsxWCK87Aa7CWzkoOiDZuQwj1MVH51Yul0O9XvfERwPwRA7YnDKwe9gDBQBTboHdcq3t9k6ssVInatr4ab/23ypou1Effv3Sra9U6yEdpH2j1gG/5ycEut3nsKECh/2n/gRqktwQucA57hT0nBNTU1MmY44LTXlfaq125IhScDSjKXD9FAUuYL+xV82fWj2hRc7sRBulHth+2wmp0LHU+USBz03FpjFoxfyNv/E3Dr34nK3dq6Czo8AYwvvmm28C2PFPTE9Pe7TkqwH7nyWmE4mEmUucT1x3FO4MViCFwsM6OCZs+2GhUqlgfX0dq6urRutmG2l5aukPPdzBHletcGrj2DRwwF8DVU2NZjEnECmU9fV1tNttTExMeOgTfpYLgIs2HA4bE5iad7lcNpOOndlqtcwp7nq6C8MWyV2qoOy2EFVD9nvmbv1ha+f6mvYRX1POjk4Zv3sqLdOrDYcB1Shp3dip4Hyd48T619S0WSlSU58pHIBdgcVzLFm8jIuFjm+NDrEtAf4mJaJ0hUaJ6FgDu8fqcQMg/1sul43AUJ8JBQkdtaT9VAiq5qW+D10j/F8jdhRs+1EWn+tG1fHe3DQZ+57P581BG4cVUkjYwpfzH4DZJJX2IRXHCpjqszgs4c0xoqZt+89UEaOF12sz79e2YxXg1BI5+BrOZ4eV6c4aDAaRz+fNJGG0inaE7sialaXaELk7hrcxLpcHrHK3BHYXlWq2fuF//N82ffV9YK9g7sZt299T6OSjdqjX6fZzlFBrRwWfCkfbStDEF2ZjkickDeFXVoF+EK1GqIKUAoXKgPpWIpGI4XXZJsb3ay1orXDITYTCm852ziVuHDrenGsUXhQcOseVFiG6Ccl+m28kEsE73vEOAIebYUuon0opCvahRmSx3s3ly5eRyWRMmw5rHnJsKZjVItM5Rl6Zn7OzLw8D7BM6qTc3N40cYT8xo5p+F/2uUnL7UfqOnQOnJtbp7B5JVSqVzGLhAuGOykWysbGBYrGIYrGIRCJhtHFgt0NYepYn83AAGRZGoa/Fiyi0eaK5VhDUUDFttz6LHy+p2l6vRdlLiPsJZ1IBNOeZydcLgwiBg8LWcP20fi5u3QBtXlmzZNnHtpOak52+E/owKFz5m+FkymWzxChfU0cqsBvBAOzy6TwNis+pVI9WkqSiQM2QceZK/VCoqJKi/QPsteD8LAe//lccVvE5W9FQpcq+J8PmgN0kN1pXLM7m19aDwlYCeV22TUMy1To4bEWGc5e1ffi8WoVUS9tSA9cNcT/UKnFsHHi3jlQzUbPq6HykkKfWvLq6ing8bpIJ2DkAPJpPMBg0Dk9qU53OTpSJZl3yftTUWJGQAoSCRRefLaB4P/1tm8c6WDa/qH3C9lJr0w1NBVWn43Xgarv8NpbDhk2b+FkCfoLK/q46NhuNhombpYCm9s0F6le7mxs5Q8P4njrMOZ5KTfAzTBqiUKdywdc0KqbZbJr3ODfVKctnoQLBDYwbk0bKaD+oVss+UovGjwrTcT3MDFt7zDTzmH1JJYJUFmmEra0tZLNZjI2NIZVKmQOjqXhcTeijWrk2tI6SFhI7KlDx0+AHjcvnvKJ/hPSf5jN0E+LXrQauf1MTC4VCngy8VCqF0dFRs5tR+8rn84ajjEQiKBQKprYB6xu0222jnbGcLM8upFZOnpsm+Pr6OnK5nNEWODCaHg146yaoGcz3ycdRuOpkU8EB7BVm+ltNfAoZbnCawUWNXM1Y3XD0+kepgWu8sgofPy2OfcoY42Bwt2BYqVTC9vY24vE4CoUCZmZmTG1nls9lNAEjUEi7pNNp0xeMTtJNmn3AKCVtE2OGualqWjb7khsoE4VY1oEOdtahZxhbNptFo9EwZj61MoYNasw5x00TS9jmbk507Vfg8DJs7ftoWzRaSK2dQCBgNjN+Tkvr0tHLTeha4ijvZ0eKaO2mdDptchXYF+pzAfbmbgyKYzuVXsHFTu2FD6EOIK1fEgwGTdEaCgEW/6GTgvdih1FDp2akOx8FRrPZNLsntW4NJwPgEdrdtFsV2L20JHvQbEGnGin/tweY1og66gD4fu+o4Ndu+37d+kH7UB2T+XzeHJTbbDaxublpPPaFQmFPDC2LmAUCO3G9TPrSVHjOMWqKWkQIgNGYAoGA0aYYsUShQ/6S6ddUKjgG5H2r1aop5pTL5YyWqsXVeC8KQG46yo1rBFa3uabX6XQOL8NWr60Cips028k6M9rHukmXSiVPTHir1cLk5CTGx8dNVMywgnOCCiUP3+C8CgQCnmQzUkqUJ1rnqVscfy8r5dg0cNUU1RxWukATJ+wykVxg1KgZAxwKhczC4aLodDrmFA6t/c2471wuZ0p50vnAhUuhYscE63P4wU/r9OOCbdjUg1IngNe7zoXFiA3daGzL4CjNR30e7R87ntuO5iAoFAOBgDlJiTRFp9NBKpXCxYsXAeycTsM5wU2L84LVJ6lZa0latkEjYqLRqEnbJ3/eaOycmqIla9m3nU4HpVLJUHHke2u1mknfbzab5rg7zmsu5o2NDVy4cAHr6+ueSBWdV7Ta1FHIe9lRKfaYdjodlMvlQ82w1Wvrff3mF/9WDpyUF/uNRxVyUx12Ac7wUVImWkKAyiHzSDQDl3HrgH/Oh7523QpwwMsZ2wuTuzWFNM0zLd9pm+MUBsqbk3dj+jXTtpPJpCn8zrbwu+qg4UajHnbAK6RtLcXWiFUwd+OkbS6YZpheTxczv2dTFsR++bSrgbbd7399Zj/uks+szwTApJvn83kjoNR5zKgCdYTSacmTfqgha312jgPT2BkjTqFM2gXYLTFMJYOgdQbszhHlMrmhajQLBTfnmU2/sR/Uyam0lC1Atf9CoRBSqZTvGF9N8Tk/BYDPZz9TILBb7oCnXLHQFQU5P9doNExlSLWahwnb29soFAr4yU9+YvhvACaabX19HZcvX/ZY+aTUeKhHP/67m5IIXKUA/+M//mP88i//MlqtFv7+3//7eOyxxwb6nu7YygcrDwnA4xwBvEKR7ykXpwtAw86q1Sqi0aipQZBMJpFMJk2dDcbn0gykM4pts7Vam9MmtK2281IFsz0g+h6vZ0dUqFNLuVJqh3ov+7o2f3rYQpx9wk2Y91EHnd0POv4qrHk99vnKygoAYGpqCgA82Zg2X6yFqxKJhDmxib+pOasZS46WYYVMsmAdHq1XQeql2WyaeHNqUxw7hhSyvgU3js3NTXN4A/0qduy5CmqtOUKlRhe5n0Blnx0W+s0Vvsd2UbGiVcEIHPYBs5ypTNFvwXEdNgHe6eyURN7a2sLq6qrnWMRSqYRCoWCyMBneqnNPSy1ccw681WrhH/yDf4Dvfe97WFpawj333IMHHngAt912W9+HBvbGVfM1OoK0KhxTT+nVBmBOolH+lxNfzVIK4Hw+j9XVVYyPj2Nubg6FQsHEZLIMJrBj9pGv0mw/3Sl5XX0eW7NWYeTHcetn/QaMgpCTn/dQKofXVQHup9nrPY5SA1foxPTb9PwEkM2dAjDCkSGSzJxVYafJPe122whsWl6Tk5OeOtDkbCnsmZXHNjJ5iM7RcrnsmZvsfxZS46Ll86lmzvlDwa0RHN38Bqrg8Br2/LLnH3B1dUa6wU9R6QZarbRoOXZc37SIotGoWcdMbWdkmH102/WIVqtlaJELFy6gVCphdXXVzJVAIGCc2aw8ScHNeWQ7Mbtp4MARRaE8++yzOHv2LM6cOQMAePjhh/H000/3FeAEF4/+rdl6FM7UsGiGMjqEHUF+mg+qGiqwm6FF4U6TmXUayGsyqYd1UEjbqCPJTmDgPf2El1JEfp/1M0t1ofJzNOf5LH7OXtu85mf1WrYQPWzoRNTXOp2OobpU6LKN9ne0GiA3K1IdwE6NCYZQ6rNrG2ieRiIR1Go1pNNpZDIZMxfIPzabTUxNTZnopUajYf6m5sjNgBFMzC4EdsxkUnS0HDXagvw4lQJbKNtRJUrD6PMQNi3VbQyOAn5KiH1fPg8FNwWWWiKMnae/guG9o6OjnuPwrmdtvNlsmmi1ixcvmnNMKbTpkysWi6ZYGfuA42srXX7yYBAcWIBfvnwZJ06cMP8vLS3hxz/+8Z7PaWKAmtSscaFUAeuaJJNJkz7d6XQMP8ZDQelsYp1dnVRc3DSPeQ86VsbGxjA5OYl4PG7OKOSkSqVSiEQiyGazyOVyngVFTZ58qt5TKRs741CFMj9rUy28vwo00kS8L81oCiEKNp0YvI8dU6w8cDKZNEdNHQX8NhGbF+fGPYi5qLQRn0O/58f7k76g95+8KzcGXkOrHbIWj12aQeum0NzXmif00bA9ag0CMI4rv7yBblaZ0m/8X3/r5mz31VHBb5z4GiO71GqiFqrZsu122xxJRzlQq9WQSqUwNzdnXidvruNwNW0+rH7hBlypVLC6uorNzU2srKygUqkYSq1YLAKAKWfNOieqePVL3NkPDizA/W7u11GaGJBKpXDLLbf0vTZDj3iMmIILhLwlhX4/qGZ06dKlnp+NRCKYmZkx/6+vrw+dt7xXmy9cuHBk99V50Y1e4uRVgcTPc5FoCKcKaQpxLblKaonfp2a3urqKbDaLSqWCVCqFpaUljI+Pe1KvVdvjNdTiYyhhtVrF2toarly5guXlZQC75RoYX87PabKRHpKstKFq4/oc+hk/ioWf89PYuoWhHSbU2tHxUo6eY6CJeLR2mQHN8gepVMocozc+Po5Wq4VMJmNyQK5GGz/MDY0WGx3qly9fxtraGi5fvmwoWkacMMSVlrxtdWmCj63g9Noo/XBgAb60tGSqjQHApUuXsLCw0PM7t9xyC5577rmD3vLYcPfddw9du6+nNtsao83x8jWdyKqtALvlWDW0VNOS1VcB7Fg21Iba7Z0U5/HxcWQyGcO1aglPOtaAnU0+HA6bePStrS3jqMrlctjc3PTQO2wzI2YowGkd8X21jrQf2EYV4H5Ugi7kbn162OhFu9m0nD6Lhj3qeDMSqN32HhCezWaNJk9HcSCwWy7Ctuq6gXND6x8xmqvfd21oO5vNptGqNzY2zLwgJatUru2bsuvj2H9fzSZ1YAF+zz334JVXXsHrr7+OxcVFPPXUU/hP/+k/HfRyDm8R2EKGr6mA0b9tesXvOyqslPe2KSqlklS7r9VqJnQ0nU4bAa7XsiNDGLVULBZN3Z1sNmsWrdI6qsFTCKug8vNB+GnRbA8/Y9Mx/fjSw6ZQuo1Xt/uo4G61WiajWk/SorXEshdauXBsbAwTExOIRqMYGxvDwsICFhYWcNNNN3kihkZGRnzvX6/XUSgUUCgUcPHiRRSLRSSTSUxMTODMmTOeqpa9wE2AtMjq6iry+TzW19dx5coVbG5uYnV11Ww81WoVxWLRo2DwedXXoXOW/+t86IZe7x1YgIfDYXz1q1/Fhz70IbRaLXz605/G7bffftDLObyFYWvc3QSCLmjC1kD9OGJeWzViDWmkRhePx83RXqq96qHG+n0AxnFeLBZNOQcKZ7bFDg2lAOvm8Nb8BZsTtwW9X1/6/X0U8HNc2huu/q3Zotp+Jt5RUNPvBOyGCpN64ObZbDY9llKr1fKMmz0vSK0xPPHy5cvY2tpCOp1GrVbDwsKC8Xv0OliCm3mn0zGHuGSzWWSzWVy5cgUXL140mwSFPOkgAJ6kP15LKRTdFO372nOQfXtkiTz333//vhIEyIUPG4ax3VfT5oPG9xN+wsc2s/marS0rdaCv2xPeT6uxI1zU+aeVJtXM1jA3wOuQtoW5Hx+v3+0leG30ojz8hFO/7+6XHhgUurnar9nRT9ou9jUdxLqp0XlpOz7D4TCuXLliqouy9v/29jZSqZQ5gSudTiMUChn6gk7PWq2GCxcu4NKlS3jppZfQau3UVhodHUWj0cDMzAzOnj2LsbExz0lO3OBJey0vL5vj7xqNBl555RUsLy+bA6hpmWneATV7at4aIad1lPh5mz7rNq5KxfjhmmZiDqMgBIaz3Qdt80Hi+3XS2VEhfiFxOnHVKccJrk5E/s/3yWvyOnbUj2rpDDWklnfx4kWjCTJhK5PJmNjdTmc35JExvXRE1Wo1T9EzXXR8TtVW/cI9tT/0NVtgq0NTo5W0D7rRL0eFXhRKt42IbdLa2/bGSyjXzIgp1r25dOkSkskkFhYWjAUUDAY9WdSxWAzVahWrq6tYX18359iynk6xWDRhxAzNDQQCJra/0WgYrfrNN9/E1tYW1tbWsL29jfX1deTzeeOgVmqEbbEPq/CzppQm7LV5+23O3XCsBzo4XH+42vh+P/PW/ruX4LHNTeWp7b9trc++LoUlwwnX1tZMBAs52FQqhUAg4DHpWcGyWq2aeF/GfCu3DewKZNVG9f9uz6195fc6r70f/vuwE2Bsa8YWRDp29nO0220TTsg+DwaDnrwGPRaQlSRTqRQSiQQKhQKuXLmCUCiE559/Hq1Wy3DkMzMz5rrKObMmjdbzD4VCSCaTKBaLiMViePHFFzE+Pm5CiDmuzLotlUpYX19HMBjE5uYmtra2UCgUjLath1yro1Q5cM2y5PMpnWJbcfYY7wdOgDt4MGh8v0LjdVmQjFAhoLywfpax+iogNAVfa4JwIfjF2/O61KIZeTA5OWnM8qmpKY8ji3WqY7EYKpWKcaC12ztlYIPBnfK029vbWF5eNkk8Sglp6Ve7fTa90037Uu1ME5+UntCIHA2xDAQCyGQyntDXo4atiXMcAHjGhb4IvqblMPhDAUxKhZQGaRb6GBgFonkRfJ08NDVsXoNlF9bX140GziQ+bhwcDyZlMWckn8+b0r8akcJNQyk15oZo6Wnd0P0spl6auPbrkXHg+8HV8qrXCqdOnTIcWzgcxnPPPYfNzU18/OMfx4ULF3Dq1Cl8+9vf9j3N5Frh05/+NP7wD/8QMzMzOH/+PAD0bOOXv/xlfO1rX0MoFMJv/uZv4kMf+lDXa3cTLjY0QSsWi2FycvK6j5VfXl42MdyDwo6nj8ViuOGGGw67aYeGo4rxV2rKpgJsTZOf52uA1ycB7BaqU26cvDYFtF12IJVKGSojGo0a2oIUl5bqpYYcCATMeQEAzIHp/G1nSGqbVfPmJsK2sS/YfpvntvtNN3HdzO2NsJt/oxuuiQA/aN2U48IPfvADUzwJAB5//HF84AMfwGOPPYbHH38cjz/+OL7yla8cW/t+4Rd+Af/wH/5DfOpTn+rbxhdffBFPPfUUXnjhBSwvL+Pnfu7n8PLLL3c1tweN79cELeD6ijs/TLxVn2s/6OYwVb7f/nw3Go3fp4Vl+xIIm+tXhydrIzHG3i6loVy6OqABeLKYWX9d6w1xY+l0Op54dPsZtQyI7efoFkliO/JtAd5NUB+7AL9aXvW48fTTT+OZZ54BADzyyCN4//vff6wC/Gd+5mf2aFrd2vj000/j4YcfRiwWw+nTp3H27Fk8++yzeM973uN7bRff7+AH1ahV4Njcfy/YsfPksdU5C3gdtrxPIBAwh6yUSiVDd5FuoibNcD6l6zqd3Vo63AQ0CsYuLKUbAbVvrYtEHhyAiWqyfTF+TmXbQmFf+vlN/PreD9dEgB+EVz0uBAIBfPCDH0QgEMAv/uIv4tFHH8Xq6qo5zWR+fh5ra2vH3Mq96NbGy5cv49577zWfW1pa6nlKuYvvd7Dh56DU120N0s9xTShNYReL4/u2ZkprkWF+6gTVs2o15tr2Qdht5TU1pl83I/2O+mOorWv4o1+Ynz5XNye1Hy/uJ6x7RRddEwE+KK96PeBHP/oRFhYWsLa2hvvuu2+g2i3XMw7S9/uN7weGM9RyELxVn+tqMGgIXK8IJDuiiNBTjOwUc9aXsWvW2Fy8TfHwuvZ9NDJE28ONxa6Wyk1CnZd+z9Drdbs/+jky+0WmXBMBfpC6KccFtmtmZgYPPvggnn32WczOzpozBVdWVq6pt39QdGvjter7t6qge6s+1yDoFuJoa7P6ngpoP+dcv6gLde4B3rNAqflqfRn7h5o1D1XmEYR6fxXW6sRUvl3j1/UwlW4/tkDez/Pqbz/00sC7x6ccIpRXrdfreOqpp/DAAw9ci1vvC+Vy2VRALJfL+NM//VPccccdeOCBB/Dkk08CAJ588kl89KMfPc5m+qJbGx944AE89dRT2N7exuuvv45XXnkF73rXu46zqQ7HiD/+4z/GzTffjLNnz+Lxxx8f+HvdOF5bWA3C4Q76v1Ig+qO8tf6t/2s8tv0e+XYtjOb343d9m1vv9yxHjs41wn/7b/+tc+7cuc6ZM2c6v/Ebv3GtbrsvvPrqq5277rqrc9ddd3Vuu+02086NjY3Oz/7sz3bOnj3b+dmf/dlONps91nY+/PDDnbm5uU44HO4sLi52/t2/+3c92/gbv/EbnTNnznRuuummzne/+91jbLnDcaLZbHbOnDnTefXVVzvb29udu+66q/PCCy90/TyAff0EAoF9f6ff9fx+gsFgJxgMdn1fPxMKhczffv/3u84gP4f5zH4/kUik8853vtN3jAL//0A5OBwIwxLfPwiGIQfgavDnf/7n+PVf/3X8yZ/8CYCd/AAA+Cf/5J/4fn6/fqpBElOuBt3aY9/zIP6161UMMo79zjvv9A1ndZmYDgfGsMX3D4LrPQfgajBINJh9gtYgwlD5Z5ta6HThyrv9r/y6cuH2a37JMvw+2+53XRv6mt9G0O07Gnmj/3e60Ej9oO3Xv9PpNObm5rp+zwlwhwNj2OP7B8H1lgNwNfATLLZw1QStqakpJJPJ6z7D9iAYtlO2umXYOgHucGAMU3z/IBjWHIBBsd+IpI2NjbdsJupb5bmcAHc4MAbR6IYJb7UcABsuy/atByfAHQ6MYYrvHwTDmgMwKFyW7VsP1yQO3OGtiWGJ7x8Ew5wDsB/cf//9ePnll/Hqq6/iC1/4Qt/Pv1UTmd4qz+XCCB2uCt/97nfxK7/yK0ajG0QoXI947bXX8OCDDwLYSbP+5Cc/iS984QvIZrN46KGHcPHiRdxwww34zne+g4mJiWNurYPDDpwAd3BwcBhSOArFwcHBYUjhBLiDg8MeHLRmyvWIU6dO4c4778Tb3/523H333QB2TrC67777cO7cOdx3333I5XLH3MqDwQlwBwcHD5hh+0d/9Ed48cUX8a1vfQsvvvjicTfrqvCDH/wAzz//vIn9ZobtK6+8gg984ANDu0k5Ae7g4OCBZthGo1GTYftWwtNPP41HHnkEwE6G7X/5L//leBt0QDgB7uDg4IFfhm2vU5yudzDD9p3vfKep8/JWybB1iTwODg4euAzb4YHTwB0cHDz4q5RhC2CoM2ydAHdwcPDAZdgODxyF4uDg4MFbqWbK6urqngzbD3/4w7jnnnvw0EMP4Wtf+5rJsB1GuExMBwcHhyGFo1AcHBwchhROgDs4ODgMKZwAd3BwcBhSOAHu4ODgMKRwAtzBwcFhSOEEuIODg8OQwglwBwcHhyHF/we06qy29JFAQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_plot('try0.nii.gz')\n",
    "get_plot('try1.nii.gz')\n",
    "get_plot('try2.nii.gz')\n",
    "get_plot('try3.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "600bd1dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_plot('newmodeltry0.nii.gz')\n",
    "get_plot('newmodeltry0_1000epochs.nii.gz')\n",
    "get_plot('newmodeltry0_1500epochs.nii.gz')\n",
    "get_plot('newmodeltry0_2000epochs.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "220c516f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAADjCAYAAACcnE9mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABrcUlEQVR4nO19eYxkV3n9qaW7qqur92UWz5hhMMaAMRbY4BALEcAYGWTkIHkhCY6c2AhFCquII8sRihCMlVhKEFmAmGBIYgf/QYYQViEcgkMwluJIxiJ2jA32TE/P9PRSe3dXdf3+mN+5dd7X91VV9/QyNdwjlaq76tV7993l3O8733fvSzSbzSYCAgICAnoOyd0uQEBAQEDA5hAIPCAgIKBHEQg8ICAgoEcRCDwgICCgRxEIPCAgIKBHEQg8ICAgoEcRCDzgvMEXv/hFXH311e7/fD6Pn//857tYooCA7UUg8ICeww9/+EO84Q1vwMjICMbHx/Hrv/7r+MlPfrLuuFKphMOHD+9CCQMCdgbp3S5AQMBGUCgU8M53vhN/8zd/gxtvvBErKyv4j//4D2Qymd0uWkDAjiNY4AE9haeeegoAcMsttyCVSmFgYABve9vbcNlll607NpFI4P/+7/8AANVqFR/5yEfwohe9CCMjI7j66qtRrVYBAP/1X/+FN7zhDRgdHcWrX/1qPPzww+4cX/ziF3H48GEMDQ3hxS9+Mf7xH/9x+28yIKBLBAs8oKdw8cUXI5VK4dZbb8XNN9+Mq666CmNjYx1/99GPfhQ//elP8Z//+Z/Yu3cvfvzjHyOZTOLYsWN4xzvegS9/+ct4+9vfju9973t497vfjZ/97GfI5XL4wz/8Q/zkJz/By172MszMzGB+fn4H7jIgoDsECzygpzA8PIwf/vCHSCQSuP322zE1NYXrr78es7Ozsb9ZW1vDF77wBfzlX/4lLrjgAqRSKbzhDW9AJpPBP/zDP+C6667Dddddh2QyiWuuuQZXXHEFvvGNbwAAkskknnjiCVSrVezbtw+vfOUrd+pWAwI6IhB4QM/h5S9/Ob74xS/ihRdewBNPPIHjx4/jgx/8YOzxc3NzqNVqeMlLXrLuu1/84hd46KGHMDo66l4//OEPMTMzg8HBQfzzP/8z/vZv/xb79u3DO97xDvzsZz/bxjsLCNgYAoEH9DQuueQS/O7v/i6eeOKJ2GMmJyeRzWbxzDPPrPvu4MGD+J3f+R0sLi66V7lcxp133gkAuPbaa/Hd734XMzMzuOSSS3D77bdv270EBGwUgcADego/+9nPcO+99+KFF14AADz//PN44IEHcNVVV8X+JplM4rbbbsOHP/xhHD9+HI1GAz/60Y+wvLyM3/7t38a//uu/4tvf/jYajQZqtRoefvhhvPDCC5idncXXvvY1lMtlZDIZ5PN5pFKpnbrVgICOCAQe0FMYGhrCj3/8Y7z+9a/H4OAgrrrqKlx66aW499572/7uz//8z/GqV70KV155JcbHx/FHf/RHWFtbw8GDB3H06FF88pOfxNTUFA4ePIg/+7M/w9raGtbW1nDvvfdi//79GB8fx7//+7/jr//6r3foTgMCOiMRHugQEBAQ0JsIFnhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyIQeEBAQECPIhB4QEBAQI8iEHhAQEBAjyK92wUICAg4N5FMJtFsNrs+PpFIRI5PJBJd/7bZbHqP52c8bzfH+L5rVx7+rtt77XTcRuqsExKJBNLpNIaHhzE3N7fu+0DgAQEBWwJLXBslMnu8nRC6OcZOIO2I3xL3Rsq7lSTdzXUOHTrk/T4QeEBAgBc7RVLtrt/O4ubfaoHHHZ9MJiO/Jbk3m02sra2tmwT0PLtdD41GI/a7QOABAQEbRjvZotNv7GdxsgvJWa+VTCaRSqWctJBOp9HX14dms4lGo4G1tTUkk0kkEgnU63Wsra2hr68P/f397rd8bzabWFlZwcrKCur1OlZXV905SPr83xI8X7462GrCbydFBQIPCAjoCLVc7efdEpaSsc8SVsLmi2QLwP3d19eHgYEB5PN55HI59PX1ufORtOv1OhqNBhqNBlKpFLLZLAYGBjAwMIC+vj4AwNraGpaXl1EsFlGtVlEsFrGysoK1tTWsra0BAFZXV925lLBJ6jzOkno32AqiDwQeEBDgRZyGbOUFS+o+nZrv/JuSRjKZjFi4StyJRAKpVCpiNafTaWQyGYyMjGBsbAwDAwPIZrMRi3p1dRW1Wg31et2VIZPJYGhoCIODg+jv7wdwRppYXl5GOp1Gf38/ms0marVahMRJ0IlEwv1NC53/856V0DvVh9bb2RB5IPCAgB7Ebbfdhq9//euYnp7GE088AQCYn5/HTTfdhOeeew6HDh3CV77yFYyNjQEAPvWpT+G+++5DKpXCpz/9aVx77bUbvmY7Eo87zn5uydxnlZOsSdh9fX1IJpPo6+tDNptFPp/H2NgYxsbGMDg4iIGBAaTTaWdRr6ysoFKpOEmEcks+n8fg4KCz2BuNhiPzVCqFtbU19Pf3OxInkVu5RAmYE5Eeo1Z5u/rQetksiSeau63QBwQEbBg/+MEPkM/n8d73vtcR+Mc+9jGMj4/jzjvvxJEjR7CwsIB77rkHTz75JG655RY8+uijOH78ON761rfiqaeectJEHDSN0B6r1mM3FEKy02AiX5boKJP09/ejv78f2Ww2IpuMjIxgfHwco6OjGBoaQi6XcwS8srKC5eVllMtlR8IkW5K9Evjy8jJKpRIqlQoKhQKq1SoqlQoqlQpKpRJqtRqWl5fdZEArW61zrQdfUJTf2//jsmcsUqkULr/8cjz22GPrvgsWeEBAD+KNb3wjnnvuuchnR48excMPPwwAuPXWW/GmN70J99xzD44ePYqbb74ZmUwGL37xi3HRRRfh0Ucfxa/92q91dS2f7t0trMWt0gnf1ZpNJBLo6+tDLpdz8sjw8DByuRxGR0eRz+cxNDSEqakpJ4mohLK6uoqVlRVH4OVy2Wnig4ODyGQySKfP0B4Jf2BgAJVKBZlMBrVaDcViEZlMJlJOotFoRMpM4uVnDKK2m9h0wvL93an+FYHAAwLOE8zOzmLfvn0AgH379uHkyZMAgGPHjuGqq65yxx04cADHjh3znuNzn/scPve5zwGIz4+OsxytHu4jfpI0SZSfUy5Jp9PI5XIYHx/H2NhYRC6hZELrm9Z0f3+/O1+9XndW9erqKkqlkvuMlrySbb1eR7lcRrVaRS6Xw8rKChYXFx2B89w8hhY9dXANvtp71r99soqdBOLQ7vtA4AFnjY1YZAFbD63/ycnJdSv2Oq1QVNxxxx244447AEQllLgc67hzxqUDUkZhJgj/z2QyyGQyGBgYwNjYGPbu3Ys9e/ZgeHgYExMTGB0dxcjICHK5HIaHhzE0NIS+vj6k02kX5ATOWMhqgVMLp4WdTqedfNJoNLC6uor+/n5kMhkkk0ksLy+7NEUALv1Qreq1tTWXm91p0ur0+dkq2IHAAwLOI1xwwQUAgJmZGUxPTwM4Y3E///zz7pgXXngB+/fv39B52xGNkpgvSAm0ApO0wCmVDAwMuAyRsbExHDhwAHv37sX09DSmpqac5U2yzmazyGQyzjImufLctKrz+bzTuJmVoimLSuDLy8uoVqsYHBzE2toaCoUCJiYmMD8/j1KphKWlJSwsLGB+fh5LS0uoVqsolUou40W1cA1q2rrrZsXoRhEIPCDgPEEikcDp06cBAPfffz/e9a53AQCuv/56vOc978GHP/xhHD9+HE8//TRe97rXbfj8cVkVvmwSu/pRF+CQdJPJJEZGRpDP53HgwAHs378fBw8exN69ezExMYGxsTGnf6dSqUjWCC1pvjTVUMk8m82iXq+jVqsBgMssqdfrqNfrkRzzdDqNZrOJwcFBlMtl5PN5lEollEolFzAliSeTSUfkao2rZm6J2+aO++p1I94SEAg84FccWyH/nCuJXIlEAoVCAS996Utx4YUX4qGHHgIAvPKVr8SNN96IV7ziFUin0/irv/qrjhkoQPx9+VZOqhZMK5vvepz+LpPJIJvNYv/+/Th8+DAOHjyIQ4cOYe/evU7/prWti3N8C2q4YEcnDf1tOp1Go9FAOp2O6NjMK+eEwOOHh4edFV8ul1EoFDA2NoZTp05hfn4eqVQKc3NzLtMFOEPQrFdbxo1m7XTTDkAg8IAegVp4QHekuRHNtpvjLWwQqpOrvN1IJpO4+OKLvelmd911F+66664Nna+Ta+/Tfn353b7fcfFMLpdz8sno6KgLXo6MjLj8bpah0WigXq+jWq16V0My/ZDnp0VMzXp1ddVlqVSrVSwvL0fOTfLv7+9HvV7HyMgIGo2GI/h6ve6W3Q8NDaFcLrsFQCTpdla0r16CBh5wzmIrAjeqoWrnj9MYuyEP3/96HT0X/7bygR2ouuDDl+Pru+c4S/ZcQbsME99nWjcqJahsQpLNZDIYHh7GgQMHcODAAezbt89Z3kNDQ8hms24vEu5Hsra2hmq1ioWFBaytrWF1dTVCnlzow/P39/e79EKeh3ueMF+c52UWDKUUBjUTiQRWVlacNzAwMIDh4WF3XL1ed/nj7fqAr163oq0DgQdsKbqxYjdq6ao77iMRface2slljQu62bxfoEXOtiwKJZk47dOWxVcPnepmNwk+TgOPgw1gkiT7+/tddsnevXuxb98+TE5OujRBZn4ArVztZrPpFtkUi0VnUWvudSaTcWTNa+u+JyR9vur1eiQHnWXMZrNuBSgzVvgZvQbeV7FYdGXUjbC0zuJkJFtXIQ884LyHzwXVQUIdlK4zEM3BbZcO5yP7OIu6naXvO1a3BD2XrOzNwHonFjZtUJfE9/X1IZ/PY2JiAtPT0zhw4AD27NnjNG+VTWgtV6tVJ51Qj6acQSSTSUeiAwMDSCQSTjpJJBLuXJbAWV6u5NQFRsxLBxDZzZA54olEAktLSwDgUhaXl5ddmXRC9xkePWeBb0XAKGBrsJUk4tOB+Xnc8b4y+KwQX3aDlVJUi6YVZa/frYzhs6As8esx1nLXfGGWSZdgx5Wn3djotq12cmKImwh95K2ySTabdRtRTUxMuJxu6sy0khm01GXxq6urKJfLWF5eXkfgmuHC/1nnzGBhEFTbwWcQ6AZWKv2wzThZ1Go1TE1NoVwuY2lpyRE2z6fXjJP/zhbBAg84a/hI12IrPvNlNlh9WqUWXS1nSVfL3k4zjyuXzzW2GQh2QqAl2O5ezwa7aSDZiVX/1y1gR0ZGMDk56XK9h4aG3IpKatzcSZD52+VyOfI/tet6vR65ZxImrXVa4ErgSuJaRk60vA7QiocwQyWZTCKbzQIARkZGsLy8jMnJSRSLRSel2H1SlPR1YujkCXaLQOABZ412+h2/13fCZ4HaY3yuuCVuC9+EYoNq9ngdyHYvCz2epKy/1YFpy6xarg7kTmXfjLW2kxa4rRP929Yv5QiuspyamsK+ffswPT2NfD6PbDaLRCLhAovUt2u1Gmq1GiqVSiSThASuJKjL46mNk5T5HX/PPcPtgxsoc+lKS2ah6ApS9pPV1VVMTk6iUChgcXHR7YTISYCSjMoz2rdsW9vl9r76tegJAt+IZdFpdrOweuVm0YnEflXgG9jtLF/+ze8tCQKIDB5aubp4gu/W9VUZQ6/vs+L1bx1oem7dB1rf4+7T3petm3bB1c1gpyzwuDb2kTfbgqsoh4aGMDo6itHRUQwPD2NgYACpVMp5JrqfNzVlknoikXD6NUHphISowUqej32GxGylDZ6Hlrk+hYfv1O91Msrn8y71cW5uDuVyGeVy2XkQnKzZnygRxWWq8HjbN3qWwOM6hT3Gd6wlcdUldcbTQerTcTtdM+463ZxHz3e+kr+vDX3Eal1udV/Z+TlY1VJRi1Z/qzp4XAqiEj0HMIDIMm2fS8z8YWrbPn2b5dRXN3LNVhkUOwHf2PNZ4/ZJOtxBkGl/rL9Go4FareaCldyXu1arRep7dXU18lg1PlJNrW6CfYR/60pM3xL41dVV9PX1ue1jSfSUTnRvcqYUcrl/qVRycg/Jmn2P5WJQ1PYffuZDu36wqwTezk20ncI3s+s7Z7k491oHkLXOdNZrV5m6yovv1v32zawbqQsftmMg/+///i9uuukm9//Pf/5z/Omf/ikWFxfx+c9/HlNTUwCAT37yk7juuuvanqudJakEqsfaic/KJLogwwaS1NXlO+s8lUo5Arbkz+vq8RxoPA+XX+fz+XWDDTiz212pVMLa2hpqtVok6NZOGrHpibrtqK3HzcooO2GB+zwkJWptR5I2XwxiDgwMoL+/3xEl748ZJyRv1cJt3je1bQBOAlEvydYrUwaZqUJi1vtiG/GaPq+ORgL/z+VybjHS8PAwFhYWIhtsWSvccoRNUd0odpXAu5E64o5hBerAsLOdbURr+el59Bi7V4Fvsoj7rt19bARn06jd4GUvexkef/xxAGfkiAsuuAA33HAD/v7v/x4f+tCH8NGPfnTLrmXv3afxcWAw15a6I4Nb1tLWAaX7YqTTaZdLnM/nXV4vv6cOyl3reF668dyLenR0FJlMZp3noBv/k2y4nJrEoO63JXa7zFyNCLX0eb2N9Jvttth91jUQNZ74P8lbn6AzNTXl9jkZHh52lnO1WnUbT+kDFFT3ZtaJGl+rq6su5ZAvShcAIp6cbl5FC1w5QmMVnHCYBcPykNi5sdba2hoymQympqbcXiuUfSgD8RraP9kPrcHYzgCIwzkhoXSSEKyrDbQaR60klUbizqnHWSvCWuO+cvrkAH5nj1W3u9092t92M7FtJb73ve/hJS95CV70ohdt+bn13u07EJ2IlbxJngwA2Y4OtIJjJH4+4HZ4eBiDg4MYHR11JMLfcmAxYMaBOTAwgNXVVfeQgOHhYbeaTyd4PqmFqWOFQsHtFa2P4uqUbaLkolalDu6433bbj7YScdKItp+1wPv7+zEwMIChoSFMTk5icnLSLZXnAxt438Vi0bUH608tZVrQvH8dq9S7Scwqo7GsnLT1pROmSmh6DH/H+6ZxSIJnfx0bG0O1WnWPbVtaWlrnbVqD02bREBsh8V0n8HZkFWcx62zG1VdqxejvFXFuH60F1cbarajSiLe1klRKsS6yr0w+dJrQthoPPvggbrnlFvf/Zz7zGXzpS1/CFVdcgXvvvdc9V1GhG//7oG3XbjJUd5uDgZv0E0rgHJysX66M45NaMpmM2zlufHx8XfCI+2DQRa/VahGranR01LnFcU9woU67uLiIxcVFLCwsoFAooFgsolQqIZk8s680J3ElBmsd8l44mFUe2qhFtpN9htAxpV4uJ99sNovR0dFI6uDw8HDEK1peXkalUnGBS668JFlq6p/Wp45BTn66DkBJnJO3WvZaVq3DRCIR0bo5odACV2mOfJPNZjExMYEDBw64h0Toczk1FdHeTxyXdINzQgPXvztZn74Zv5vz62dKHtRbaaXZRRdKQja30zaCtRDsS2Hv09eJupFR2jV4NxPBysoKvva1r+FTn/oUAOD9738/7r77biQSCdx99934yEc+gi984Qvrfqcb/29kAubxOtioV1MX5f+2gwOtrAMAjrz5jMTx8XG3xwY3SNJ6oEtcqVScBEICTyaTGBwcdM9YHBwcdCsDfcuxV1ZWUCwWsbCwgLm5OUfkJHNdcKJ7RqvsowSu/VL7y0Ym8622wDtdO2588V3Hl92/m2OHD1/gZKrySbu86kQi4axyu3ReN7JSi5felmrfuisj+xljLWx39TbW1tZc/6ThR9kmlUohn8+71Eg+ok0nbgARD8Dyhg/t2mDXLXALH6npu11xZTVEn/vJBgFaszI7E62+0dHRSLK9EoW6t7TQme6knUKtASUfteoB/yTls2QUG3Wbu53Nv/nNb+I1r3kN9uzZAwDuHQBuv/12vPOd7+x4jk6wspUGgzi4lbgpm9Aj0vuwD7sleU9OTmJqagqZTMZZ0Pl8PuL2ModYl3VzAKZSqcjv+GLAzcZJuMc0U+IWFxcxNzeHfD6Pubk5LCwsuIfiqptv4yeaBUNYq2ynPbJ20IlGJ2GNP2n7ZjIZjIyMuEegkQTpDVUqFZd+R/lJ9ykBWn2fY1atbR37/I2WhySsuwnayZLQ8atP6yGo03NiosTG9Fbq/fQ2VldXkUgkUKlUnHelkwfvrZ2h1wkdCXwrsxUUnawFn9zBAa9ut5KuaqaEJt8DiFh6JAI+Z4/n104IRMmQpM2Aiy4sYIoSXUASu2qtNg/U3p/V93h9djbb6bohdp8FTDzwwAMR+WRmZsY9V/GrX/0qLr300rbttFH4LDM+lFbvl3XIDs/vSbC0skdGRjAxMYGpqSlMTExEBhXdVlrNJADWtZIygMhjvXK5nLOidL9o7ZeNRsPlAReLRbcNKol/fn4exWLR6aHa9qwHtrN6hb60R+uR+dp9O0k+TnpkO2pdsn1JcqxPxjXW1tZcsE9zp+nZ2KfcWK5gWzDOoEFOSlHqpSUSZ/b55iSqBK/XsOOd8o3KXCTwvr6+yBJ5ThT0CkdHR1EulyM565ofnkqlIt9ttu06EvhOZisQ2km0c5N8Sd6WyGxKjga3+PtsNovBwUHnGlPnHBkZcbMogHVWFzMhSMrcT5gdh9oog2O00Ghl6BJhWvS+FCV2cJ9+rt+zXLZzd2O587NKpYLvfve7+OxnP+u++9jHPobHH38ciUQChw4diny3UcQNelq+upm+WlPqYpLQSPK05qampjAyMuIWhnA/absfNIAIGbA8Kt1YPdQSqiVXauLanzT4yUDW4OAgTpw4gUQigYWFBUcCdpL21ZOmoGlbb1TS2CpYIwJApJyqN6vRxPbWyZMZO9VqFcVi0RG4LmVXyQloSR0c+2w/tdI1UEnQu9JApO9ds5x0bGrKorXstYwsDycsGhnMsOEko9zVSQLuBhuSULYzW8FC3TNWHLVq3riVLgh2JMojIyMjbvXU4OCgc5Wz2ayzDPQJ1xyk1orVAaUWNRuIgRiSOh/HVCwWXZCrWq06K97uyaB6aJzMAqx/2Kw9Rj/zDXqeJ5fLuUdwEV/+8pc31VbtPAFLiBqozGQybnDqXhXs7Gy3iYkJZLNZXHDBBe55iVwQwgk5n89HXHklS5aF7cpz0xKq1+vO2uYrm806gwFo5W4DcJOQ7o3B7VH37t2L/fv3Y2ZmBs8++yxOnDiBdDodWW6tnoC685YU2cbngpxipTDbripXsG7V01HpkfuVVCoVZ437gpNANPVS8/wtyXPckcQTiYTL59aJkb/huXhPHIvsNzrxU6ZRLZzn4HM9lbM0O4pkb/PDdQVqp3qPw4YIfDuyFSy0Q3BGo5XGAQ8gEk3WWZrETesol8thcnLSDXASN7+nm8wBqZOGDlw7Y6srrO4VNTYS+vz8PAqFAubn53H69GksLi46d5HWhup9apVrneiM3SnbRrFbrjb/toNcc73ZDmp9c5LksQwoTkxMYHBwEHv27MHU1BTGx8cjky/bMpFopeFZvdFOzBqwajab60jHSixsBx5Dr6C/v99N4tTMWR4Gw9bWziwQKhQKLtNC6wiIxl34HbVd/t1OPttOC7wddLwyrsTxRTmLRgcJVSVItpMSPdAicisf2cwx9drIB8zLtsaMDZprGjItdc0yI8HqQjC+0+BYWVlx57Gemlrt1luJk8s2gq4JfDuzFQg7u3NRxeDgYKTCaaEBrRvWlV+0yEZGRhyBM6tAXXYNnAHRPX/tQNdKBqLZBBxc7EC06lZWVjAyMoJSqeQe0jo3N+eCW/V63Vkgmqaks79a4z6pRTv7uQifLKBECcBZ3TZLgO3PoNDo6CgOHjyIkZERl4rGOAbbkucjQVoLXAewEiSNBQ2ssu71GYqMseRyOaerq56v8RX2tZGREWQyGczPz2N2dhanTp1CqVRye0erJwcgUk9sX3Xr49BsNlGr1XD55Ze7z7YqZsX687WtGjw0oLiQik/XIblr+2gwknVNYtTxpvnxVnrUyYOrPTX+ZL0tle9YZp0YOdY0XsHxT+OC1+eYZTuqd6fX4b3RsGC/1wA60H79Shy6JvCtzlaw7pidKXWmVZdbo83qVmpAkgOcljeJnJvF24mC0I6h1iA7gg062VlUg6y5XA6NRgP5fB7VatXtfzw1NeXSzCqVCpaWlrC0tOSI3Lr/1iJTmYX12E1Qcyfgs76B9S4w0NpThHWqXgiP0yyTPXv2uPxum59tLVTdvN/GRWywzQarNc+c59YNkHhN4EzQ02r1nAi0LzGwNT8/j4mJCczMzGB2dtY9XaZcLkc8OrsPixoS1gq3yGaz7pmY2xGzsoYEiVPznNXDokeiBG7JUpMRlCyVTLlylvWpKxltv9FtFGgI8jOVd3gNLY9NW7T3q6TOPqELcjgZqZHgG8NAS2qyqzJ9dR6Hrgl8O7MVfAX3WbzMJuCg0plXN4mfnp52A586pQY9tMI0ZYszKY9l2VSbszqXBhzZcNqI3PSG+coTExNuCfbCwoJ7LxQK7p1aurXGNRCjgyFOD98t2HbzuY2qKSop0fXmTm8HDx7EgQMHcOGFFzoZjJqjBrLZN2wf8emYOhAtOXMy0VxkletSqZSLYzSbTQwODqLRaESC3uybjNdks1mMj4+jUqlgenoae/fuxfPPP4/Tp0+7p5wvLS05t9/KA2qVtoNt/62MWVk5zHoznJitxMG/VQtnW6hspn1YJ3yCBJ5KpVy2F8eZxpJszj7QyjzTeAOP4bn1Nz7pzUpq7Ds0FNRb0vpSD42SmhoCuqS/k4flQ1cEvt3ZCgqfXko3h4NLiYwVMzo6ir179+KCCy7A3r173YIM7VxAa6DbZbU6q3JAqwunnU1dLyUEvY66k4lEAplMxmUrcMXZ8PAwarUa5ufnsbi46IKri4uLzsW2M721WgjtdJ2CmNsJ691Y+UQHq3o6/F93eWOK3vj4OEZGRtal9fFeleQsgds6sNasjSfouZTgOViVXBgzodtOElcdmFJKPp9HrVZzgfS+vj4MDQ1F+gzQIgsNcjMA5rM6fXVPnG3Mqp1hpcaTejHqdelvbNDWp9/biV/HpE5ijCuovmyDnuodaeaati/BfqLrOqzcpxMKjQ6tI19aqvIY+wvJm+fghl4qG225hLJV2QoWViIg7AysWz8yks0bzmazbrDTutm7d68bJGwInoMNp51KZ1J1iTW4poNcG0M1aUaVSdociCyH6u90MblYgAG5kZERLCws4PTp0yiVSu4xUqrpqnunpBgX2NxpEtfrWhcUaFk+ahHRWsnn804yGR8fx9TUlJNOSIaWhDXfnv1DLSklF3WFNYOJx9OS0v6m6W3JZDKiVfM46q/qnqs2TvIYGhpyS/6ZHUUS53VUTrF7qliiU2gbb1fMyvYjlXg0sGfjCkqGuhkUvRUlfG1bJVV6yQxWa4DU9jWVZfR7tp+v7Zn+S2JVrwKAawv2OZ6T/U/TA+lFMs+dz/Wkt8ZJh31E43obwTmzlF4/s+TNJ3Koy0PC1JV33NKR6WUaQNBBoefXWY8BK3ZKEowOZqA1cFX75Oc2tVAXDimRMv2IHYWb/lAXn5qacmmICwsLTientKIuJK9r93GJk1Y6ueJnC5/1reSpeqEObG74Pz4+junpaWd55/P5yGBlW1gt0m47agPdel0AkaenqNXFND87geuLW5/ynJlMxk3WupcL743ZU2wztuP8/LxLbaUlz/3G1evqtt6J7VxhaycRlo8kxmPY93V733aTgtX3SbK66jmZTLrVrerlcBxxIZ41HHTC1nNynOo7J1tdSs9yqteo/VzTXulxU/+3hotKiqoybGZcnrPbydrZTV1loLXnAXOEDx48iD179kR2kWNjckAC6we+utvMHFEStOWz7hHdIb76+vqwvLzsysdy0EJj5+DvuG0q3WwOYO6lUSgUcOrUKRw/fhxzc3MoFApugRCX6qrVoYOe96odUN+3E7budJLkAKD1kc/nnaQwOTmJPXv2YO/evS6LwQ4Ca+lpSia1UrYnr6euPQefBigtOXMCV11UDQiuouN98fokYdX6OVFp4Gx0dBSNRsPtbMinrWtmiubF+ybhdu241TEr7fdWAiPU+2C7asDZeow6yapn3Gy2do1kxgrzxDl2+FAHpmtynPF7loPGHsus6wx0Z0ptV10+T+MIiD79h//zdyphUkqr1+sYGRlxm5xZz07HpZ28usU5aYGzs+v36vIoeTJAqMEtTREDotFlX/DSvmuKk1rXbDydgZXoqVNSOkmn005SUS2TZSKJM7jBoJdqr9VqFdPT0y4NcmZmxnUIDXTacrH8es/t6n27oV6QEjjjFwxaTk1NYXp62i3c4WRsPSG1nkngKqFoepcN/qp7rV4L/2faoBKNTgSa1sZyqQdES1BJXy2uZrOJkZERNJtNl31UKBTcY7n4oAjts1p+JU0F23UnVtjawDQAJ29R5x8YGIiMSStfqL7N86psom1rYxJKurxvNfRUZ2afUaONW2BwmwuC/QeAS3pQ4rWBWEKlUR6na0vUcLDxqk7k3e67c2ozK+uWqWumkWZdSDE6OoqRkRGXKw60GoGDUDdx5wBVa81aOyQGrXhrvWrZCLXGNUWIL6ZDqjWpD0wFWiv86L6trKxgbGwMw8PDLpB36tQptyiIW1bqpEfrkuS+07DtaIOJJAAGLNmGY2NjmJycxNjYmJNNNCMIiD7XUC1vX5v6CJwTLetbz6UTnmqz1lLS8ynB8HM9v+q9ACITVz6fd/fNSWxpackbDLPGhJXJFNsRs/JJYtomvE9a3kwi0Bx9jktdfam6L+tIPSO7PwrvmymF/B1XzirB8m9dncnr697taoTV63Und5HANVXScoJyBvsOSZuy7ujoqPO0rFfsq09f3cfhnCJwAOsGi/2OgRFuZMTd4JjjTcmE5wCwrjOw8ZS4lQB0ENNaJGyZVPNjYwNRl5Bgg9O94oIfu7CFREwipyVDb+P48eNuplfLz+ae2pjBbpC5Wq427pBOpyPplXy2ILfj5GSnuiPbRN+1fdXCshtisQysc1pDPA9X1HGy18GqpKp9k8TP72hgANG+onu0UL/lGoXx8XFMTExgdHTUpZLauvN5g9sN22csibOM+r1m3tB78mWBcfxpHamVbMcm4wL0cFQmVGK3W8ASvB4nD926VqEEqxJanJWsBgQTE4BWKig/U2OEHMA+rWmWG23Xc4bANSAAtBpaBxArhUur+WKWAmdg7Riql1LzYuNZi1tdO+sB8Fy2w/I6qvEym8H3uW0wztYkX3ZQuyqUEoo+aQZAxAplp6MFyMnMNxluN9p1RLVCudKWC3S0s6uGqZ2eBGtTQbU9dWJTK08tens+wC/1aDCM98YXvTz+VvutSjZsr1wuF7GsufhsdHQUe/bsQaFQcDnhzHtWqU6tbx/Bbhd83rGOVytPaXDO1quOM8LKWDbFV+uXky3HtsbF2E/sGgGdGNSqVwmS98Hxb2MoPIcaFZxMaM1rvA2AMzgZ1FQdXyW7duR9TlrgNsClg8RqWkD06eTctJ8ZJ6p7W0IGsG4mVwtcpZU4TZywDW2tInucdiBLCKrjk9D7+/udFc/VfLokW11E5pKXSiW3KRChXoB6EzsJ30THsmjGCSdg7lOjaWGqddZqtYhlq+1st+rl9ZXMlfCUgHisusMkH01D5X3o/uX8Xl1/Bt7URVetnOdXj42pk1ylu7Cw4J76o/2I5aAVZ9t0Jz0sncRoOFgvC4g+nszq2+rtKjlru+qaDdXAdfm6LtCj56RjV7VvW1/8ne0POqHoi+dj+ykJK4EzoKrtpytB6ZHRqLSTobblOa2B245gB5pauEp63K6TA1/1b51hgZaVys2m6DqpJaAEYN+1LDagxIbXY2kB8Dglch3wdDk5+VDn04FA6426HPcur9VqKJVKbll+Mpl0ZMdzUIOPs9i2qz1tu9r/mffOSZieBWUT9V6AlhelbaGrI62sYttJy8F21Aca83PWjwbG2I7sd2p5cqLhBKwkw4mKpM8J1qaocbLO5/OYmJjA0tISxsbG3DJ79hlrEPgsNjUitgK2r8RZgrxntgmhxhGPs4TsG19KjDqWlaTZp1ku/s38c7WENV5i+5GStWrbGhdh2+px1sjkZKx76KgXqP3Gjm/1mntSQrHul7WM+Hcy2XoGIvdeHhwcdG4MiVSlEjaGfdqHdgLNFtBZ3s6CtHislkvwd9aFU+2Ov1eC4LXpMjN9UDMkNKrNp36Uy2W3VS3LpultLAfLtFPwdUK1QrjHCffz5mRM+cS6sbRo1XXlQLaDG0Ck/XgcoZKcauN0vTlggehybABu8OtAZf0SOqDZJ7mOgVYzU984UWlQk2sZlpaWkM1mXd9VK5ftaet5qydnnfB9ZKn1ogRqJ0ttH0vyPJ+VKdVq5iIYawXbPGr+nu1EPtExr563lXI4zrTPsN454do6UGNPl9XzOyVvvT8l9E654OekhAKsD4r4CJzHcTEEN8ix+Z+6TNZa1TaXVGdhteJ05rfuKX+nVrd1edQC5+f6tx0MBPPVdWCyU/H+6GUweDkyMoKpqSkUi0UsLi5ibW0NMzMzkfPr/W10Zt8KWIuDA4HBWG4wpoRJ/ZhtqA/NsG6rDkzftbVP8Pw68ABE2lL7Is/hOyYO/B3PzT5krXheh+2iRGIzZuLGgy1Lp7KdDSx528wJ7WO2rkioVt/W++HfOmFbvVq9yzgDSr0nndCtrq4xEhtI5TUJDWT6Jkj9LSccyiPqHSrPqGSj9RlngZ+zBE6oG0pYN5gETgtF80s1aOerrLicUp7bunk6kGw51Zq2FppCg29sKLWO9X6VYNV1SyaTqFQqLiWJ90o5ZWxszAW+qInTWuV1NL1wJ6GDWC0NykFMsSKRM51S64DWJwmcINFrqmQi0UoPZNtYV1e1TB2M6g3Rc7IyjnpJ1KB1Zajv/AxmUzLR/plIJCKPkmNwnvu/0ALXtvSRpxoP2wXfuX0ThsqHmh+u3pBa4GrU8DglWEuA2m460ZKg1ePli16tWsl28teJnTygkgrvhTEqex2dMHheErjG4+xLiVtTT7upf2LXCVw7gp2R+b1abnzYrGpGSrqWvDsNWm1sYL3brGWxv7d6n5bZEoCm91lLxQZvfRPQ8vKyS8tip2IgkHvBnDp1Ctls1j3GCUCkA+qg325Yy8IOBpWENIuDLx0MHPTWtVZ9kdlHOhDUA1LLXd1avrNMeh0lGJaLkwNX7PmC10rgnMSVEBgI1UVd9nO+dPLVc/sscDWAtht2MiFp2z32tU74O/7WypZsW19Kr3o1KrswaMnzaF2xn2jb8jeWGwh6DJRl+a4yK7D+GQE+b4Dveh3lCS72sh6FndjaYVezUHwvbSzOelzswY39mTM8MDAQcbftzErrTZfksgJ1MNhBB0Q7itWtAER+C6wneTvIdACrVczzK3kAcAFIeg6VSsUNFmZrMJ2wv7/fpVEeP34czz77LObn5532ysFlO8ahQ4fcjnjpdBqPPfYY5ufncdNNN+G5557DoUOH8JWvfMW7a10c4iZhvvMe1Vr2TcRqeVP+Yj2SQDVO4RtIPK+dIFm2uEGifVD/57lUQ/VJCeq68zPKA4xxcFGXSnLMi2c8gPdjpRnb94g4b3Cz8Ekctk44qSUSCbf3h5K4NRbUQuc6CN6/EpyOZZ/kAkQzXRQaD7OyJM+nEzcnA72v5eVl9/naWivtjy+V/SzpKk+oda2TE79nP+f9bIuEsh0DXQusM7nePFPr1PLmEl3mCpN0rX5mZz2bUqeWnlpLahVoWdpBj9NG0NlVg5pxM6xPAqL1zUGhj+kiCXAwMPK+uLi4bqGEj8QA4Pvf/z4mJyfd/0eOHMFb3vIW3HnnnThy5AiOHDmCe+65Z0Pt2sml5/eaQmmtX3Wn1RLTNovr7GqhAf7J1jfYtF2YccLfq0fDvTh0LxogumJY65x/05pTiUC/Y5syxjMwMIBqtRqxSFmedve0VbDjMW4ssE50R0Z9Io09J++X5dYYRpxe7fOK7Yt1z3rnbzWXntfQsvA3qq8DraB1MpmMtAG9JV+dWKvc1iXPm0wmnWEJIDIOfO0Qh64t8K0c6D7LOy5YQ9Ji1gmfrsMBRl2ZVpldequDymdFsOFtEIbXV7eU5VT325ZXK9vn/mqnaTQakY7Md9XX6QJyFh8YGHDn0cd4cb8VAJibm8Py8rJ78IDVg9t1iKNHj+Lhhx8GANx6661405vetGECt7Byip20tS20HX1aqK1vwpKCSi02cKWeDo9lm/jccaBlaVnytARiSVT7nNVAVX9VEmSAnmmVcVKh71pbiTjvUuvNTnqU9nQxFutPf2PHOv/WetDl6zwHxx+hBpha9UArBZd9CUBk7LE/adzDetn8jc+D5T2rla9t2Ww2I4vVBgYGnCfN82odKB92i01LKGc70FlQdae0gkhQDFxyuTH3UuZg0iW2qplat0sHgM7WSvRAi3SVaOxqOJ/1ppKJWl08pw2oEeoNqLsMrF9BRilB92/QfRpIfCdOnECpVHIPUNbULtsGb3vb25BIJPC+970Pd9xxB2ZnZ92udfv27cPJkye97ddp4387WbIedUGWndw06MONqZSAlSxZfp5b653EqMFr3+IRrWfrtflkrnbBYCu7+CZuNSjY7rrwhOPBt/eGLeNWk7UPdjxaL9PnYaiOr4F6Sk/23LYNdYwSrHdartYg8dU7602NIZ0s1TDTNRtqpbOfqnWsf2sczlr1PKdORNqmVBd4z5xAfLJTO3RF4Fs10O05LYkriVEP5J4n3BuasgEQ3QfEF7W21yL0d5qjqjOjlXfYEWzer0KDEfrOcwDRx3qxo5KIVWbRYB/1M7WqdcJh55icnES9Xse+ffswNzeHY8eOOQvEh0ceeQT79+/HyZMncc011+CSSy7xHueDbvxvrSJfveiL8BG4DVypBMZj9fcaRPMFInViUKuXYNtr22j2iCUutb61n8URq5XObNmsFW/vEYjuxOk73tbnVsOOA52MaWip56AauGaAaBmViGnFcik8+2x/f38k3kDwgQ5Kfta44hgn8TNzy3oLLBezhFg2HZMqA2lf0O0vAKx7oAfHBRetjY6Our2YGo1GJNlAs+l8skwcuiLwrRrocVYgCVxdZGrfw8PDGB8fd6mDnLU0R1QtNxKiDla1uH3ZKQrr2rOMShLW7bdEba10a8XT+tK9LjiB6ASlnQBo5aIzsMeOyYGUy+UwOjqKCy+8EKdPn8Yvf/lLt+2slpfYv38/AGB6eho33HADHn30UezZs8ftHT0zM4Pp6emu27odLIGrN6KBY2ulkrx9ud4kB50Etd0sUbaT0bTvsc7V2lKysOcHEGlXn7TBtrfSED0LLSdJjXXis4LjvJ7tgFqVfKm1yL0+9HF3uo2qr97VeLHnVstdDT2f96bJBlo3rFudNO09+SQynweuExcQNdS0b/iMk0QiEXmsXrlcdt/R++f9WBL3eeUWXS3PazfQAWxqoGulWzdMNTCuUNPAJQeYusncoEqXyfs0PNUrrTVmJxhL5LYTxhGOyinaObVjk7Q1Mq3l1s7EazUaDZfvrdtx8prpdNpl7IyPj7stWX0di6s4+fd3vvMdXHrppbj++utx//33AwDuv/9+vOtd79pUu9rJzFqsPJautU7ISmpxA1+9LmtpKznajCQNdFtJQid1nytv78v3stD+pv1F75Vl0lWl7H8q4yhhxBlDW424samWuD6FSJ/AYw0e1of2bR0jJDL1ilgG/Z4eqW+iVHmKpKokrGWx3wHRPcX1/il3WalVFQOVCFXLT6VSbnJjcJr3QO5SLrLt2G5i7miBcw/boaEhN9D/5E/+xA30O++8c1MD3bo8BG84n8+7Z1xOTk66zZ3UpdaBqd9pdF+tWLXUVQ/VnFXrEaj1q7ol70HdN3ZeWtC2Qdno/f39jrjZyXhdXktdeSUOEl6lUnEdRJ8VySfbcGtWbrOrHTGRSGB2dhY33HADgDPu5nve8x68/e1vx5VXXokbb7wR9913Hy688EI89NBDG2pXAOsGLF/U/XSDLg34qHxksxc0XqEWjwaa9MEZvsFtF25on7DWoA2m8aWWoBoTet9K/rQQAbh2Z2Be+6p6EmqN2TGi/dPnlWwl9Np2Yta4FevBbp+qQTo7EbebbOxxmsJnv/eVFWht6EZphNKnNQaA9Q8ptnKqGkC2rX3l0HoCWhkmWl8qAdl4na2LsyLw7RroPp2HHYObVHGv71wut25/aHseJXbb6TS4yQCZpgQBLbdIo9O2UTlglMBV01a9TrVPnaHjLHqddFhWGxvg/VEL7+vrQ61Wcw+z0NgBnyU5ODgYGUy8/uHDh/E///M/69plYmIC3/ve9zbUlrYdrdunA5h1qtYv69NKWzzeZpOoVafejbWGeR6bncT6VEue7jbJiFsWaL/U+7P3TCK3cohKbOxvek3NfuLLlzVhSc03Dtq52mcDa0DYGFEikYhkztinYvkywXRscXzxM1r1vniIBuStN6LtyvFg9zCxBM3z8LrU5TmWdBLXOJZez078Kr/QYKPlbXXyOGOnW3Qk8O0a6EB0MOhsyFWXY2NjGB8fx/j4eGQGtS4m/1dy4LG0VJQIdBUfO4lafXamVy2W59CBxUmDFiKtXoIWJlcKqs6mnZSDmWSjHYD3qo+SAuDSlDTdSolc3VnV1XYS2mbcz0VTtjQwrINZBz4QXSWrXgvvj4Oeg5zPJ1Uy5/k4UTKeoNozLWUlZUuWOnBVNtNBaH9rDQPer9YRy61ZErZf+rwMJcXtgCVv/Zx1RkNLPQer+2vSgd6LLy+eHpWSttYL/+dnWh/qRenYsl4T0GpPXT1tZSEGZq3BqNdTb1v7B+N5TCXU1FDCJzF2g11fiak3y0pQ65s71gFYpxHqzMdBCax/Sg2JwOpMJFCbVcIZXEH9mdKHNqROFjw3H26r16UHwUZmQ1rLk8SibpjVsZPJJKrVKkqlktO6KTMpSSlh+iy37YJvotCBy/rSjq+SANtF5S5tJw2k6f3xOzvQCR6je63o/hX69HhOLNqfbFtZq1JfWg+JRCIyWat8wnJxLNgFPXYC5vG+SQXY+oV37fqNlU7sZGPjEdqW9Fpp/Pj0fx0DKkfZ1D5LhjyHNfg0LqPGANtJ0/00k0ZfOmnHSUN6TRphrCN9tKIe75P+9Jw+7AqBK4mp1cLZKpfLuQ3u+bg0kqS+s2IoYahGzewUWldsHA5UnYU5mNgwDIYS/JyWMa06hXYyXoPWMtOiMplMJAiiRM7rWsLQTkY0GmdSCtPptCNxpmDV63X3UFx6MplMJkJoG3HRNgPt1DrJ6qKsgYGBWBJSacPGNXSQ8cWBRb2TMksmk1kX81CtW4OJ/Hx5edm1tw4olofHa/qYEgCtfd24jMaJTgA2zsH20ZgGA/c6+fKcth0tkWz1ClsfiaiVax8+bSdgtcI1lsF6YEDekrydHFnffPcZJuo5q3SoVq8aGCwz6599yhI5f28nbU5A7OPsf6p709AgifPcKsuoN6Y4Kw18uxA3eHmj+kRy3igbhC8djLZxdNDTGvVZP2opqQVgZ1V+77PYOTjX1tacda6SgOZtczADrad4sxy0VjhB6XV0gPI+mFNarVbdEnslLFq5JHBLADsFtpEOBjsobH+wMpZ6Tmq560TYzhLmubT943LCbXaLJRCW02qdvrajYaJErt6DBivVjdf6suWP00nbtetWrbC1FrK2ga1HtayVnG1aqLaH9bg2Gqj1SSNaXiuzEBrI5G8tpyhh88VxxglJ+6OtmzgZyieJxd2Txa7vRqiWmgYN6I7x5n3aIm9cSVxdHR2EQGtvA/2MjaiunLqqWj5rFaj3QK1dZ2CeT4NVai3q9YHWsly9X60nWy7KOtVq1bn+jUYD5XIZlUrFeQA7JZv4ym0tcHoFdi93bUM9F+EjQevFqdVnn7yk57L6upKEZiRZcreyCcEJnFa51WWV6Dmw1YPgQG80GpHPfDp/O61b++pWrrBlP+Z1dVLRzCLViTWuwbrmSzeVY1xHpSytXzuhsf45htQwU6iHbfuWlaL0eEpYPoPCNx5ZJk0Z1ElaDVDV1H3jMU5GOeckFCA60HVmYsMoORLaEFbL0iCIuuJqafNcVjPjb+xxPp2Mv6dVr+e3A0snBJI6P2fjtut4drAqEagmury8jHK57AbC3NycezCuunN25t9uKOGqq839v60FbnV+Lau9d+uiAlGi8KWLKuwA1uv44g7sS9aytxa4nVjYT3xGhs0dBhDRYAmtP04+vnbkZ1u18M4mCPB+7P3zHqzHzHNo21gvVttL8/M1A0vLojJYXFBRx4/1lqwXqpMq69VKu76Yh947ydkGPjnGyWeaYkxv2ZZ7o57VrhO4dVV8wSutMHs8OzQtLlpTeqx2NiUKO+NZnVkr17rTWlagNSmodqvfkcxZDjY0LTTVRfW+uKSYVod10xKJM1p7qVRyFvni4iKWlpZQqVQi19wNaJ2r5anEq/fOPpFKpVy2j3opasmre2otcKaLsk/wvHZSUK+Hn+kkwQlV214Htt4j21mXZZP49GU9CztpqP5qrXb2QzsZ699btcLWentaZmbq6DG6LoHGTb3e2q9bjSxmDPFzvQftG3YNA+uAx3P8WOgEyPpTb1nHkTXeNIVTrXYfGMPg79Xg43U42TB4qckMPm9e0Wnc7jqBq36kQUirl1kLiAOVbrJ1wzQ45WtglkEzSFipNlvAd7zOzOyo2hm04mk58TfpdBq1Ws01KsvHCYuNrlakPgCA9dVsNt19JxIJJ6csLCy4F5fS6yy/G1a41ou15PR/6/ay82uKpMoOdhJggJcrVnWzM5VfaOVxMPEcOsHoxMrv9WUte5ZR69rek57Hd68sk324gy5SYpDc50U2Gg0Ui8UtWXhnJxufrKiWrEqgOp7VM7J9wurc6q3YutQ6tFKohZJhnOxlJwcbV/PJZjr2bSxMYeVcvabWr94Lfxd3Hz7sahDTWhE23UjfVS7RjqzLkGltaRDPN6OxceJmVw4kBk3ZEdmIlEW0g6qlTwsJiDYIz8ngI89Nt4qTEoDIfsOq9wEtEta9YJrNJmq1GsrlMk6ePImlpSUsLCygUCh4PZOdgq1/7fxqTRJx0pY9F9vWBos5iXNi4+SmpKsel8Yi1HXXsvJ3yWQykoVCA6Kd5EfotZUMVBaiFcs2IonbvOG4NuSkf/XVVwM4+4V3PuJQAtOMEn5HTZz1wewr9j/1CGmR6+RlrXAlek1i4LjXtmSZaEzRGEqnW0+p12vQovelCLLNWFa11rWs1oNX71DTEFk3rB/2N/XS1Vuz7RqHcyILhQVnBTEVjnt+0JW0M666uCRkDSixwbmqzlrL1nVhgwCtzgggYoGRIFSDJqw+qFogz0nSoedAa5KdnrIBEM1uUcuLbqRanPV6HaVSCYVCASdPnsTc3Bzm5+edDs4c9k4z+lbAeii6iIOyhlplrGclM5vZwLr2WS2sU+u1qcuubci65XmBlpeky+V1gmCQularubanVLC8vBwhfUvi2u42UMd70j6uK4/V+9DMGXsd1kcmk8Fjjz22rk22YuGdvSe2LceEfs970lQ869Fa7VmJUWMDJGMaTlaW0vqlQaOxAj2nnWxZhmQy6bK1yA26ZsGSqHrUjUbDtZfuB6PtZ+U2K3+R0H3kfU4TuNWg2CEoB3DJuFaqjWrb4BUbmNaM6p++ylMi1u+tlGI7BmHJilalde/4Pf+mJcEcbloaem0NkrAsJD126nK5jNXVVSwuLqJQKGBubs7JJ7SQ1E3dCQtc60wlLW46VqlU1tU528EGITnQ2rnMOqHxvDoYfZadkjp1axKGphISlGM46PP5vLPCVatXr4DnsPEPegd2UmW/ZSott07l+fSciu1qU5VN7PWtDGQtVvZfEjgNKkI9YOUB9ndaqjy36tjJ5Jkn2mi5WBadwDnxsm14Dl6X12buvcZnrHTLfmRXZSrH2CA17wFAJFfeZtjZWF+3OCfSCG3lV6tVR0TNZtM9Q1CJkg82qNVq63aY4wBSvUmvp9/p5KFBDOrGbBQOQjaqZpVYzVPPZ/U9diy1uFQiUf0vkUg4qUWtbvVUKpUKisUiVlZWnGxy8uRJFAoFVKtVdw9M3dI62GpoO+q7EjknaKC1naZawdwDmrCTrtYpyV0nVk78vnxjWl+8plrAOujoaak1x7LQyqtUKlhaWnKelAYjNZ6idQMgMkHTIyEZ6FOVNCOC/UR3rbSarrVutxrWq1ISt/3KBoLbEZKSHv/X1EQlcLX8OW5snrhOdnbsqSFnNWrfoh1dGKaeu0ojlj94LfU+bN/QGAvlHZbdV1ftxuuurcT0WcOspFqthlKphPn5eQBnbsBui8pju+241opWgtEyKJHreTXlx868PI8vqKGupkLzjTkjq0bGCYuERwJmJ+KEUqlUnM69uLiIYrHoApc2A6WTO7bV0MlZJzT1lNSysb/R+lHrWgeLxiHYd3QzKGB9YEjbzRco9ckTCl6fFiCASPvZvUz09z5Ss7IQJwXWEXfcVFnIErjvPrca2pbsl+l02sliujOo1gVftIQ1mElSI9Gp5apEp3EmNaL4Uis+lUo5A4z9wAYQ1TPQ4CsDx5pjrvE424Y6yeo5baBdXzoJ8Dys1059z+KcssCbzaZLiVPramVlZd2iByC6oZEFO4lmlfhcPiVdJQd2EKtB+6werXRtWJaDJKSamk8PpCuXy+UAwFloqn2r+7+8vIxisYhCoRBJHywWi+syWnxW23bAutz6ucpbvAedFLUObdoc0CJeJXMlNNYXiVWtY5I0LWwtlw4kILqXisYeCLalBobVulpbW4vkQ6vOynft8yTDRqOBSqXivAg+Fq9YLLqMGisTKbbbAreGDglcvR6Snd5vJpNx/U89EPYDkhzHtl3kRM+V/ZjX5xYV6gEpF2h9qJemCQIEPSAuItTHNvIaqtcD0RRQTZG1kw/Pr3yhnj3rt13bxmFXCVw1NS009VLgjO5YKpXcvhDUkTRoYEncanU+S1D1T3WnCdWbfda1JWCdiRWW4K3EYvU4jeozOMKZXQNF9Xod5XIZS0tLLlipD3rQiUP1ye1GnHdlj/F1Uq0LDhbNQALW7zip1rxKDDog1M31BXJVm7VtxXbW73VCJ7lo1pF1t4GW1Wb3byE4+VSrVZTLZZTLZZw+fRqFQiGSCmrLbut1u2DPrV7PysoKqtWqi29wUrVasGrROn6s5EACVBLUBVlqwVupqh3i6k2lNE44DGg2Go2IJa07kdqAq046/I2StPZX1pHlmI0mGnQk8Oeffx7vfe97ceLECSSTSdxxxx34wAc+gI9//OP4/Oc/j6mpKQDAJz/5SVx33XVdXZSVZrVjJUrO5KVSCcVi0T2VJ5fLuY2QqB0yTYcNaaPH9qXX0wb0BYbUfWfn0U6jv+U7OymJhVDSttoqy68WJcmcHUStQpWZFhYWInnwOgkogWtH3UpYS9tKNlaXrFQqKJVKKJVKrmy0SkmMbEMu+NDyk6DtRkk2MEi3V8lEy8a65Lk0pqHkQouNYNl8xK0DkOfRga253bwuj1tZWUGxWMSJEydw/Phxlw7KDKKd8KDioGPJgtJVoVDA0NCQW7fAurASg37mS5ukkaapswAienEcyVmZglDvhy813JR7+HvGJHSs6/l4rTj5x64UZf/w7Q3Dfu9r43Zt3pHA0+k07r33XrzmNa9BsVjEa1/7WlxzzTUAgA996EP46Ec/2ukU62AtYiVftYwpn1Bb0x0CuUG6dgAlf3W3rV6qllgcmdkOAMB1HpZP/9YOpfeg37GBVDoAost6lQR0stB3Zj6QBJmFYi1Mvb9O1ttWQz0SHTAAnJVZqVTWeS5qiVipQ/uLzfW2WrHeM8FJUa9n5RuFTvB2IuK5tO14vGqk2r+1Hti39X44KZ84cQKnTp1CsVjsyvJmfW93fMNOzDpmmXxQqVQcMalmrf2d9akeCMebEjjHtvWo7MRtpU6glRbKMqplrH3RGnP2fjUtWSd5lUFsYFMDuEA0fZnrVXSVsK0fi3bec0cC37dvn9sEZ2hoCC9/+ctx7NixTj/rCCUYnzWun+t33OSKnd+m89BitRom/47bUZCwlaVEbgOZtsxqtQHr0xA1U0Y7jTY0P+MCFLXceAw7QbVajXQCnxehdb2T8JFJs9l02TJ9fX0YHByMDDhtc5/0YQeweh3qmvI3LIc9J9tK3du4QWJJnOA5SOKqdbKc9Kg0DxhobQfMz2ic8Dmls7OzOHHiBObn51GpVGL7qq+c2wnraXESSqfTbg3C0NAQisVipK44gTEwn0wmXUomx6lq33YBDNtMrVYbMFWDB8C6fmXPa/un/q/WtBpOHL96bCqVQi6Xc7n7TEXkb9lfGSfgpKxaPq/fbjKJw4Y08Oeeew7//d//jde//vV45JFH8JnPfAZf+tKXcMUVV+Dee+/1bhCvu5sp1Eq0n/FvvRnNl1biU7eMC2zsLAv4N7vh+e1koY2mx+lMyutbK1utNetO2wwCS+Aa3FJNvFqtuk5KAq/Vau6lrlccWdvPt0saiwPJrlKpuHbjPu+UGezxPE6DlGp96dYJNo7is7DVOFDrWIOU7axx6xGxjOpx8Thaa6qFq9vO8mu9MCB9+vRpzM/Po1QqRax0K//4yrjdUEtfvWSSEq1wXZGocifPoZMX0CJwJVlLzBxr1gBjmXxphewHcda3Sje6AMfurMjxrN68avf5fN7JvNlsNmKpc+JhnIAyn1rqynu+Oo9D1wReKpXw7ne/G3/xF3+B4eFhvP/978fdd9+NRCKBu+++Gx/5yEfwhS98Yd3vdHczLYgOBH3pzehvlARp1dhZ0zaOWm2sSJ/mpdIFBxzPw06hE4qSuRK4Wlkkdzs56PWtdacEYzUz6qAA3GBRK4TnUnJSzV//BrZHGvNB24NWR6lUcumSDHzZXezoXWnQi+1j8/6t1cIJ3VpjWicKWoUsr607bXOeH0CkbNrG7ENqnVN+00Aez085jBlEzDxR7Vut/E7ewnaC98lxqm1SLBaRy+VQLBbR19cXCQaqpKB1pRq3ptxxPNNKJ5GyT6h1rHXOoCP7EpMALDGTPDVQqufzlYnjTycdX3qiHs81A4VCAYVCAfPz81hcXHQemHKeLwmC9RWHrgh8dXUV7373u/Fbv/Vb+M3f/E0AwJ49e9z3t99+O975znd2cyoH21D8zEdEeiOsaJVIdDZnZ2IKmdXg9MVr6fdWH+VnLItdCq0egZ149H/fdZW4Lcnyb87Y3JiL7rfqf9b69E2AFtsljfkI0pK4WjG5XA6Li4vOBaVUBMAFsTgotL00K0HrUwkmnW49gYnHqOWm9a6Lb9RA0EGtZMHgqg58hbYJ+61em4O30Tizf/vCwgIWFxdRKpXcdsCVSsX9Hmjtaumr452Ava7+z4mIkh6lDSVM7e+6HoDn0vEdN8laeUO9J5v5wjJo8FIzRtTS1/ZT403JmdKtjWdoAF6NOXrLlUrFyWPlcjmSpcPzKPdspG07Eniz2cTv/d7v4eUvfzk+/OEPu8+5NSUAfPWrX8Wll17a9UWtxa0Sgh7DG+MsrJUP+FP01CqzQUOboqNuGaFlUH1KPQa14DUY2g2Bx7lK1q1T95SDX93zOM3bVxY9v69zbKU0Zq+pk5KSFsvKyalarUYetcZ+wQVc3HqXg5bn18nVTvY+YrdWtVrqPCfrVomdbajn9w16fmflM5I9/6c3wnUPJPBqtYqlpSVUq1UnHWjbWU/K18ZbDeuB8DOgJftpJgofFkFPKJ/PR8Ysz0MPQz1f9WrVWNPxz2vaDCMbQPTJOEw/1uCjbu+qC3F89amBV37P37Fv85GLy8vLbmX08ePH8Ytf/AKzs7Mol8uubXWLDE6CFmcloTzyyCP48pe/jFe96lW4/PLLAZzRRR944AE8/vjjSCQSOHToED772c92OpWDlRDUAtdBowPHZ63rbK2WiWplVu7g9bUzKtnrCknVNilXsNNop/ZZ1HZA23v21YmWR3+n7qROHL4JIe78ek4t21ZIY2pd6UvvzScfcTIioa2srKBcLq9bIceHNauW7CMTnWj5P+/dN7myzFp+K1mo5a6WcKdVd7yu9i1a94xdWPmEKYMkNnoC6h20Q5y0slXQtlNLU4N8KysrKBQK6O/vx8jIiNvPhQtjNL9f+7SObTvh+ow9tXKt8UPYfkhyt6sj1TrXdlXDkta8tgHv2wZd6Rlzncbc3JzzqsrlslunwTLqwsSNoiOBX3311V5S2IrAFmGtDK1QBfdG0HQdQolaB7QOUj1WG5bulm18DaBZEuB54u5Hv48j77iZVScYJRT1NCwh2jLFTSKK7ZDG1AJSj8V3z2x3WtT0OPSJJgx0ErTq1DvSwJVOdhaWcBS6F421fLU/cRJRK4/9UgNSakSQxFOplHvUXbFYXOdiV6tVF5jtNOH7+s5WW+C+fmVjNECr3XQMZzIZLC0tRTaJ0vFL6YOTGj0SHa9K2Pw9iVQzzayHopq5Gol2PYVdKWmtfJbDZj+xbXXbWC3T2tqZdRpK3nNzc25/olqtFrlH9VDj+m0cdm0lpnYEta5UN2NDcLDqqku1kFQ60cAkdUq6r2qRA1FLTDsDXWglbx9p+txwHez8v10DWJfYkr4ep2UmrFSgn/ve9futlMbUmtF3ew8kSPVwWJ56vR5Jl8xms6jX6+6hF2wj3U9Z00K1nTRTiINKPSp+rntfqPTBtlYri+dj3+Jj4fR+LejqV6tVAEC5XHaL00jWmlGkspy2L8dHJ2LfSvj6JbB+vyCOGdYN8/x5T4zZqGVrrWkSuI5TQqXTRqMRWZqvaaM8joFMJXKfYeEzHJWIebz2Cd2SIplMuj5g87654ybTK4vFoiNvbjXAY7Xf9QSBW1dMLUytSA4Oq2+qZWN1TcC/nFsHsG8QqJWeTLayBvhbn4bdSbbYTL3Y8+lk43tv99u4cjWbzW2RxoD1Fjjh87BUv6aWaeMaHEhq7aiUouenFchzq2zGsrEcVo5T6OSr2xdQF9WgqvZZK+FxUmo0zuySCMBZ2hzIulDNN/nbdosb5N1ILBtFnMfJMqgMwMkUOFNPxWIR8/PzEZ3Z1j+zQhqNRmSnSDXI2Jf6+vpcfITWO4OKuhiGfYReHH/LPuSTYfmdrpK1/de2Ncl7YGDAnZ8Ezy2eFxcXcfr0aRfb0E2xWA/sA/w8zgqPw64+Us0GtdRlYcWqxcwBQauL78B6q8ASrLWefQEvfubTwn1yjZ1YtqpebLks4txndrC4gafHbqU0Zi016w7qZGqPs8fYlDHqpzq41CJmn1FLXFMsOVisVsq60oVSdIuZ8eMjf0uUVkvlserBsRzNZuupSVyxqLntcRlNdgzEyXnbrYHba/Fdx6yuKOVuoiRHnXgZ1LQpukDryVSaBaKpf/SY2Q8Y+OO4pXVs0wZtqqp6UgMDAy7tUX+jvMHrkfAHBgbc71gvvH+mDM7Pz6NQKLjn0+paEACRCbynCJxQ3dpal2wkNjJn3lQqhWKx6Coy7ly6h7Z1WXwBTTvr8nh7/na6uNWd7XfWytJj7eD1ySU+K8tH3nEWuq9cWwmdmFkOn6WrBG7vXQcWg2BWVgPWe2J6fbUSrfUHxFvgVrvWsukET8NDvQm2gZVyVNP2SS3t2qrd59vZjt1A2069XVqVxWIR6XTa7V/EnTY1kyybzQJYv38+/9bxTOlM5Ve73QXQepKVylzNZvRxdUBr/3ndY4mETElN65iTAHcs5KKd/v5+R75cUcugtN1JUqGplL504G6wq0/kITRgx3cbwOR3TKvj0lTNGyaBccBwADJowhlVB6bKMGrZcfBbwlaNslNl+6xhve84V9kHJTjfOfQYn8yi1/BZ8GcDPTfr1kpYGlhUy0ZdUnVlOTh090ldOEPoxMzgI3VQJXog+rRzGyjXiV0Hmt4DwXPrNWgwqIREIgHgHcC+CdtXp/b/uL7XbJ5Jy/yN3/iNbVthaz0ogvVO65hyArfFLZVKyOVyyOfzbkK2qXqUwHTy5UtXaQIti5wpe5z0dUMz9hltd6DlTakVTULO5XLOW9CJl/eYSqWcYcHf0gOx5F0qlVxgmhwTl6nkMxji+oJiVyUUBQcDrZRUKhUR+2mBs0F1fxKgFeiw7qteyxKfteTYQXTFpt17wVZ0HEHHfR9XB3Gusz0vP7dyhIXe305Zamp9q6xgLTUto5ZVpRPNRNGgkk689tq+a/C7uPKyP5GQ6b7bewGifU0ncfZZq+2TJLS/2hiBrS+tG3sv3RgMiURiy1bY+gwQXx2qlqz3CLS2UNANnEj0QGu3STXeNGioXi7POTAwEMkk4aTBXRDpAXBJu8q0qqnrY+uUyNlWuo0Bf6sETkud98mcfkomjHPw97qKlH2Iv40j707Y9Wdi+ixxn1Wsuvfa2tq6yuOg18CnXstab6qvs6MyKKIrH/mAAN1jux05d0sePFYHqW8yiLOWlcR9g91n/W63dNJukrHHat1bvdwnM1jLT2UKXQzSblMvQiUz1SJ14tfyWc2bKYHU1/UerKSgkw8llzjCbtee3aKvrw+vec1rAGztClsL1hWNHJ2gdOJdW1tDuVzG/Pw8UqmUe1iJL7VQF8BYLmB7aMaHToY0vBjMpKfOetTxrpa3yibMVU8kEpGVvpRvKJ1QbmFZSPTU/blcnrq3GpLsT7qfkTUMN4Jd18DjLE52DP2fDUM5hAs/ms2mq1zN4VSXhWTPxgZaKzHVAlDdkhvQ2EdcKRnaAaf6r/UA2tVBtwSrBNfNBLIb6ERC1opWDTlOtlJpQydSO8A1RVGP0ZQ1vTbrUAePyj1KtNbY6DTg1Ashwfjku269tU5ta39/titsO5VH70MTEexCGI4j5rsnk0kMDw87aURJmPdJAtXJUWU2K7U0m2fkI7VseX313gC4oCWJmNY0j9XMFsq55Jd8Pu9kFqoEAFxqqC7I0iQLjlnrmbSTTrrBrhO4Qi0TdlYNZKolxDQ/DgIOEJuvS3eJnSWuktgQtL5J2toQvoG2UcJsZ3G3qxf921pv9pw+wtlJ+NrRB0tkNiuDedKMeZAYOhGeteht2p8SOsuhkz7P0c770etoDrBel8dYKUYnqrh7sBN0N22px2zlCts4KHmTiHQ9hWZycMfFQqGARCKBxcVFAIgEp4GWPMI+oPVADtB1ICRXlcH0OLatenWqYefzeecxEGxL3g8t9nw+j6GhIQwODroHVjBlkAFMPkGJWyHw9+zfrBf1+DrF1dq1wTmhgVvCUde/HVGRbIEzRE8XjC9GoPWpLmwcu31pMplct+kVCUVds3YDzt6bPdZa7XH14Tuv1ola4Bux3nca2hZKnOy4/JxBIA4+DoxarebIm4t8mLVAYtA0LPu3Xamn9WTrX8vGtuZ1VGrhZzrxsNx2EiWh8Fj1IjXtsZMVFteP4rAdK2yJdhOJehy6joPf6ZilLMk61KwUSqZMD6T0xPpjjMLuSMl20PrXCdyWj5OH5nFr/2D5NRuK1jr7Fi3vxcVFt8ug7nXOyYFBTr5rksbZjOFdJXBrWdp3a6nSzVJNS11mPTaRSLjN5rPZrCNkjWQDUclECcG3252Wr5Nl0s1n7RBndasrpiQSd47dInefxgtEyU3LqPWsVhT1Rc3o0BW6GmDmS+uJsC4/y6DuuR1IPitay6xl5TUYsGQZbR+ymSjt6s2OkU7tyHvY6s3nFHFSztramnt6kI4v9U6q1aojOY0z5XI5DA8PRwiVMgfHpH1IsLYhJwuVVKw+rtIODQHq8DxOvf5MJoPBwUFH9CwPA6PNZjPy8GnuNEjdWx9vqJY6932nkbGZ1EHFrkoo1rX0DXi1hPRhsBx8mh1Cy0mtH11Gzc5FK46gXNJoRDen7zbfW9GttcSB4BsQvknMBysDsL703DtJ4HqtuL/1WLYRwU5NcJDr8nqVOmiF8Xe6WT7QynAgdKD6JBVrLZMcWF79nMZAtVp1FiTPowE2ehJLS0uRQUxrTD0+n1FjvYd2bcqA4XassG0HlocrMVVv1swwlpFtnEgkkMvlHIknk0ln3Wr2CA0xErMmHjSbTbeXkRp6lF90szGSN1NTdWGPjiVNVeRLF5M1m02n5xcKBad7M/uEE5Rm3zBDxW5WtpnApeKcscAJdnzdiwCI5vtquh/QCkbazs1Bp6u2gNam+lZnVEJop1H6rCI9V5yE0q4ufOe259V6i6s/PW6nrW+fxdvuGJ+MRI+oVqu5IBHjF4RaYTyej5jTzBAObM3LtgFKDbSRGCwRs2zqLQCtOAytPspCJH7NNigUCi6YpzEV9a7iJETbx9q1aT6f936/lZvPaX0QNpDJ7+3qWdYBLdtCoYBms4n+/n4MDw9HMkzsknh6V6q32xxv3rvN+WcQkgTOvy2BK+FrarIGQrmD5Pz8PE6dOoWZmZnIZlUkbGaw6dOKVGLp5I0R7cb5WRH4t771LXzgAx9Ao9HA7//+7+POO+88m9NFXBhN9NeZld8zCszPNQPBustq3eisCrQCI9Zd9gUV2lUkcTZSif3fWuDtrLC4CWQnidxex0oftqxqCZOIgVYuMK1iZgANDg66Qczf0JLRlCwAbtCz/mhFsa9QNrETJLB+lzq+629JIpzwqddb673RaLil5QAi8pxvolf5R9tfM7K0vhVxUtp2w0pTHFOa762586yTWq3mHrGn8SqbM08e4CSuda/7imgZtG6Y2qjkbeUTErTuicLPWK8MVHL737m5ORw7dgzHjh3DqVOnHIFzf3tNSbaP/1P4+qCt3zhsmsAbjQb+4A/+AN/97ndx4MABXHnllbj++uvxile8ouNv1ZoBEGl4nptWFWGDiu0kDdXC1GXTQAXLwEHPd31QMK+rbk47iWSjlrfWRRxsuhs/s+RjJRSth+1GnDzi04yJuMlJJ04SogYjdZGI5n7rAFHSSyQSToLR+rDl06CS/q9yHK10apy8hq4gtMYCLTZq+b5UVFsnbFttY81p1uM3amBsFWzbqUypfZDSh5aXcgvbhsFMTRNVKVT3H6ElzuPIE+pR6+MH1ShkDjdXgmpwU6197nqqRh7lL1reJ06cwIkTJ/D888/jxIkTmJubixgRlE9ofdvFgJ1IO66uLTZN4I8++iguuugiHD58GABw88034+jRo10ROLCe+HQW9bkYmqFAy0zdZCt7WC2Y3zHwQGuMZK0auE2+b0eEmyHtuN/6GtXWkxJ13DnPdlI5W1j93dahzwKndKEkBrRW2GqWimaG8DMOECVPXfUGtFxpa7H5NGjte+w/qrVrFommuLIv0gJVS5HEpYFNK5vwelaesHXo65Nb3cadzmfLrHnbLLNa4UBr8yqOu2QyGVnwohKXBuxpgWsbamIC/yd3aDYKZVfN/+ZveC0u1CGB8/54Derdc3NzmJ2dxfPPP4/Z2VlH3gsLC5EHVbOvkMR9suxGSDwOmybwY8eO4eDBg+7/AwcO4Mc//vG643RhgLqFNrUHaGWZcOMbVjKDkRzI/FuXpKoVrxYBB7PuTMaG4vGsSM1w0awWH1mycTsNqjjYgeuzrJTgfJZ6nBWr5dWBb4PG2wnbtgrfBMO/lUBZx9ayU4lDvTINomn7c1UeCUDLyHrhhGCDnBxwtPToHlPasYaCWpK04DS9rduglbaXrVP9e6ctcF+7av+3sg8/U41Zg8aaE63fWwmT7c6X1i1JnzsYAnBtwz4EtKxwpirqd7qSG2hNmsztnp+fx8mTJzEzM+NeJG4+y1Sz2ZTEdbLXsbwVxtWmCdx3UV8H0oUB+Xwel1xyyWYv2RYjIyOb+p3NSPHh1KlTbhOgXkG7Mj/33HPbdl0d4L7JRf9vN4mplaIpk7S4NI2UA57EqTqjppdZKYbX4YCjAaGfK3lzfwsSuKYPqlWvpG8n1TiJS+tMiVAnhnZS3m5BDQQtO9DyYjixsu00NsLH5RHpdNrVMQN+9IzZbjTCND+f59aFQyROPhjEBkbVW9AgJS1nPh5ubm4OMzMzOHbsGF544QWcOHHCbVpVLBYjfUK3IuYkr/LbZus4Dpsm8AMHDuD55593/7/wwgvYv39/299ccskleOyxxzZ7yV3DFVdc0XPl3sky+yzBdpaa/bvd91ZK4GAgMZPU4ghOg9Qc9LS+fGWzxA60HhlGF5mkojILz8dysLydZA7f/6p3q5XoI0vFTmrgPtj24/+ceNWbBOAmRv5PnVv3R+ckyfxrAI70dXJWi5dPBOJTgLgORHVxxlLopQFwMTemevKBDLOzs3jhhRdw/PhxnDx5EqdPn46swGTqqk0L9SVC+OpsI567xaYJ/Morr8TTTz+NZ599FhdccAEefPBB/NM//dNmTxdwHsC6+Aqf5diJxAlfINd+p8SpeiwtYZ/WbTORSCA8n/0tXWRa+GpRWbnLZ5G2k6989eIj6nPJAm9HPuqJUC7RWIFuUEdps1AoYHFx0e05MjY2htHRUZRKJWQyGZRKJYyNjaHZbGJwcBDZbBaNRsPp0yRcpvbp3ijMM+fagMHBwcgSdwYd6/U6SqWSe47lyZMnMTs7i5mZGZw8eRLz8/MolUpOslGytuStpO7zQH11tlFsmsDT6TQ+85nP4Nprr0Wj0cBtt92GV77ylZs9XcB5hHbk3ckaiTuHQrVhnwVrZRHVk3WZt831ZaCLg1OtM8140YEJIGIxa5l0QlHtPe6eeQ7+znc+qwvHnWs74fOOfMeoh8S/4zycRCLhJkfKKJRCgDNWt3pf1WrVEXilUsHS0hJOnTqF2dlZF1zk9rLJ5JnNs5hGSGmDeelM8+MDp+fm5nDq1CmcOnUKc3NzblLgU+VrtZqb3IH1QWbbB7u1sOOCmu3q+azywK+77roNLRCgFt5r6MVyn02Zzza/30dmnY5XdJIC1JK1g8aeVy1zQkncPumcGq2Sv1pTalFt5J7i5AXfse3OdbYu92ZhPYq4SVm/04nWBtCZjECSZkJBvV6PWMaFQgGFQgGpVAqjo6MYHR3F6dOnXUCSRKqPMDt58iQWFhZQq9VcmuLq6qqzpkdHR93SfeZrM8ZRKBTc8ywpozBfXR/QrJ6Zrac463ujfaYbJJrngi8WcM6g0Wjg4osvjuT3P/DAA23TQ5VwfYtQgPXEoyTf7px81x3obBASWG+dsixcJDIwMICRkRH3RBhuCZrP5wG0lt0zK2J1ddW55XyiDB9GrFkTtqxxw0nJz1pmcROYWvY+Hdz+rq+vD5dddtmWxT58WU/6t9W04zJmfP+rZMX71EU0utiGedvZbBbDw8PI5/ORgDMAt9q1VCphcXER5XLZ5erncjmMjY1hYmICY2Njrg/wd4xtcLl7o9FwC6+4bwlzvPWJPz4C16C6tvNmJn0ilUrh8ssv97brObWdbMDuY6vy+30EbV3/TmSnsAMlLlVSf6cWIANYy8vLyOVykYyIZrPpXGu68qurq+55hiRuaqRxK+naEbith3ayUhxp23u1iJNptgq2TeOkFOt9KdFre/AzpoVS2uDvuOUFt1UYHBxEuVxGJpNxerrGQJg5omSr2xksLS0hl8u5lGJKZc1m0y0mYiCT/YM6N3VyTR2Nk0w6TdAbxbZJKAHnH7rN71fY9Dy1wNVS9mnD7TJYCFprek5dOKMDSM/LsukOdNwDg+fhYOMCDK70I5nrPhrMZNE9UKyco+96T4TPY/D9zqYRah1ay473OT4+3radthrdSF1APPHr/Wi6pGrLmi/OlFKutNT7Z8qmbr1AK5iLher1OiqVSmSJvE4WlGPYBzjBA60YiI+cLXlvlrC7NQAUO0bgW71vynbh0KFDGBoacgP/sccew/z8PG666SY899xzOHToEL7yla94n2ayU7jtttvw9a9/HdPT03jiiScAoG0ZP/WpT+G+++5DKpXCpz/9aVx77bWx5/Z1IN9A1QVa2WwWExMTPZMr32w2Ua1WUa1WUSgU2h47Pz+PqakpDA4O7lDpzg7bmeNv0UkGi9P5fRM535vNZoS0NVOIx2Wz2YilTOImkWoePtCaDPlwEN2OgfegvyGRajzEZptYg8SXxrpRC/yc1cA3o6vuFg4dOoTHHnsMk5OT7rOPfexjGB8fx5133okjR45gYWEB99xzz66V8Qc/+AHy+Tze+973OgKPK+OTTz6JW265BY8++iiOHz+Ot771rXjqqacie38ofvSjH+HjH/84vv3tbwM4Q/4A8Md//Mdty9SLufLd4Hy9r27gI+e4lMiNBK31NxrT0GXyTPNUuYNkPjAw4Lygds8y1Q3ANK6gawJ4rKaKEuqh+axsS+A+b6qdVGYR9306ncarX/1qbz/cXtHs/0N11f7+fqer9gqOHj2KW2+9FQBw66234l/+5V92tTxvfOMb17nLcWU8evQobr75ZmQyGbz4xS/GRRddhEcffTT23Jrfv7KyggcffBDXX3/9tt1LwPmBjRC3z0q1ufYMLFLP5mpH/s1Vmrpc3S4A0r1q9Jx2h0Dfb/U3/N9a4z6r3MJOeFu92GpHJJTN6Kq7hUQigbe97W1IJBJ43/vehzvuuAOzs7PuaSb79u3DyZMnd7mU6xFXxmPHjuGqq65yxx04cKDtU8pDfn9AHLaKfNqlIyoJcp8TWsV2DxxKH75MD1rV9rwkUU0T1W2AfefQMmtZfUHmuLrqZoI7ZzXwbnXVcwGPPPII9u/fj5MnT+Kaa67Ztr1bdgqbqfuN5vcDvZkr3w3O1/vaDsSRWDtSstq4/k+SVZmDEohmoSihKvnqhnQ2QOzT731kr5lMmk0TN/G0S73sVBebwY4Q+Gb2TdktsFzT09O44YYb8Oijj2LPnj3umYIzMzOYnp7e5VKuR1wZd6ruz1eiO1/va7vh08eBzlsp6OeamUJrudlsLWH37VhoM1p812lH4Hz3ZUfZY+3Lnte+b0e4cUc08F7RVcvlMorFovv7O9/5Di699FJcf/31uP/++wEA999/P971rnftZjG9iCvj9ddfjwcffBDLy8t49tln8fTTT+N1r3vdbhY1YBfxrW99Cy972ctw0UUX4ciRIxv6rSUgG6jrdGzc8b6MDdXFGaRUTdtubaBpg7oK0u5RYrVru2oy7nj7uU/yaScHbVuuSHOH8G//9m/Nl770pc3Dhw83P/GJT+zUZTeEZ555pnnZZZc1L7vssuYrXvEKV865ubnmm9/85uZFF13UfPOb39w8ffr0rpbz5ptvbu7du7eZTqebF1xwQfPv/u7v2pbxE5/4RPPw4cPNiy++uPmNb3xjF0sesJuo1+vNw4cPN5955pnm8vJy87LLLmv+9Kc/jT0ewDn3SiQS6167WZadKGcqlWq+9rWv9bZRWEofcFbolfz+btALawDOBhtNET1X41S/aghL6QO2BY3G5p+Leq7i+9//fmQNwJEjR/CWt7zF5dcfOXJkV9cAnA26yQbzPUHLh27IPe4YazOeizak1e3b1YOWfyu0f/2+0wrbQOABm8bZ7pvSCzh69CgefvhhAGfy69/0pjf1LIH7iMMSkz5Ba3JyEoODgz2zwnYj6LWnbMWtsA0EHrBp9FJ+fzfo1TUA3WKjGUlzc3Pn7UrU8+W+AoEHbBrdWHS9hPNtDYBFeIrW+YdA4AGbRi/l93eDXl0D0C3CKtvzDzuSBx5wfqJX8vu7QS+vAdgIrrvuOjz11FN45plncNddd3U8/nxdyHS+3FdIIww4K3zjG9/ABz/4QWfRdUMK5yJ+/vOf44YbbgBwZp+N97znPbjrrrtw+vRp3HjjjfjlL3+JCy+8EA899NCO77sdEBCHQOABAQEBPYogoQQEBAT0KAKBBwQErMPZ7JlyruHQoUN41atehcsvvxxXXHEFgDNPWrrmmmvw0pe+FNdccw0WFhZ2uZSbQyDwgICACLjC9pvf/CaefPJJPPDAA3jyySd3u1hnhe9///t4/PHHXe43V9g+/fTTeMtb3tKzk1Qg8ICAgAh6/Qla3eBce8rWZhEIPCAgIALfCtt2T3E618EVtq997WvdPi/nywrbsJAnICAggrDCtncQLPCAgIAIfpVW2ALo6RW2gcADAgIiCCtsewdBQgkICIjgfNozZXZ2dt0K27e//e248sorceONN+K+++5zK2x7EWElZkBAQECPIkgoAQEBAT2KQOABAQEBPYpA4AEBAQE9ikDgAQEBAT2KQOABAQEBPYpA4AEBAQE9ikDgAQEBAT2K/wcyodQSQvlUEAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_plot('newmodeltry0_2000epochs.nii.gz')\n",
    "get_plot('newmodeltry1_2000epochs.nii.gz')\n",
    "get_plot('newmodeltry2_2000epochs.nii.gz')\n",
    "get_plot('newmodeltry3_2000epochs.nii.gz')\n",
    "get_plot('newmodeltry3_2500epochs.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c56af506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_plot('newmodel_img0_2000epochs.nii.gz', plot_name = 'Generated')\n",
    "get_plot('I269254_I989324imagedata.nii.gz', './Dataset', 'Original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2aed5255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_plot('model_temporal_img0_2000epochs.nii.gz', plot_name = 'Generated')\n",
    "get_plot('I269254_I989324imagedata.nii.gz', './Dataset', 'Original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2e7025f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_plot('newmodel_temporal_img0_2000epochs.nii.gz', plot_name = 'Generated')\n",
    "get_plot('I269254_I989324imagedata.nii.gz', './Dataset', 'Original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e65e04f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAADjCAYAAACcnE9mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABfbElEQVR4nO1daYxlR3k9r7fX3W/pdZYej8c9jgGDjUBeAEsIERkDsYiJgXgBgiUCRmQRASzHkeXYIYo9VmIlYkmEkQmGBFsmCkxAgUAsHBRDMrIUIxEneAIMeBnPTPf0vm/5MTrV53791Xuv3/T2TB3p6W331q1by6nvO/VV3dzKysoKEhISEhIaDk3bnYGEhISEhPqQCDwhISGhQZEIPCEhIaFBkQg8ISEhoUGRCDwhISGhQZEIPCEhIaFBkQg84UWFu+++Gx/4wAc2/NhqyOVy+L//+78NSSshoVbkUhx4wk7GF77wBdx33334yU9+gnK5jGuvvRb33HMPuru7tztrGeRyORw9ehQXXHDBdmcl4ZcIyQJP2LG477778Id/+If48z//c4yNjeE//uM/8POf/xxXXXUV5ufn1xy/uLi4DblMSNg+JAJP2JEYHx/HnXfeiU996lN461vfitbWVgwODuKRRx7Bz3/+c/zd3/0d7rrrLrzrXe/Ce9/7XpTLZXzhC1/AXXfdhfe+970hnS9+8Ys477zz0NfXhz/90z/F4OAg/vVf/xUAMsceO3YMuVwODz74IA4cOID+/n782Z/9WUjnyJEjuOKKK9Dd3Y2BgQH83u/9njuIJCRsJRKBJ+xIfP/738fs7Cze8Y53ZH4vFov4tV/7NXznO98BABw+fBjvete7MDo6ive85z2ZY5966in8zu/8Dv7+7/8ex48fx9jYGJ577rmK1/33f/93/PjHP8ajjz6KT3ziE/if//kfAEBzczP+8i//EkNDQ/jBD36ARx99FH/913+9gXeckLB+JAJP2JEYGhpCf38/Wlpa1vw3MDCAoaEhAMAVV1yB3/iN30BTUxM6Ojoyx/3DP/wDfv3Xfx2vf/3r0dbWhk984hPI5XIVr3vnnXeio6MDr3rVq/CqV70KP/zhDwEAl156KV73utehpaUFg4OD+NCHPoR/+7d/26C7TUioD2t7R0LCDkB/fz+GhoawuLi4hsSPHz+O/v5+AMC5554bTeP555/P/N/Z2Ym+vr6K1927d2/m+MnJSQDA008/jY997GN44oknMD09jcXFRVx66aXrvq+EhI1EssATdiSuuOIK5PN5/OM//mPm96mpKXzzm9/ElVdeCQAVLeqBgQE8++yz4fvMzAyGh4frys+HP/xhXHjhhTh69CjGx8dx9913IwVwJWw3EoEn7Eh0dXXhzjvvxO///u/jW9/6FhYWFnDs2DH85m/+Jvbv34/f+q3fqprGu971Lnz961/H97//fczPz+POO++sm3QnJiZQLpdRLBbxv//7v/ibv/mbutJJSNhIJAJP2LG49dZbcffdd+OWW25BuVzGa1/7Wpx77rl49NFHkc/nq55/0UUX4VOf+hRuuOEGDAwMoFQqYffu3TWda/EXf/EX+PKXv4xSqYQPfvCDuP766+u5pYSEDUVayJPwS4PJyUl0d3fj6NGjOHjw4HZnJyHhrJEs8IQXNb7+9a9jenoaU1NTuOWWW/DKV74Sg4OD252thIQNQSLwhBc1Dh8+jH379mHfvn04evQoHn744aqhhAkJjYIkoSQkJCQ0KJIFnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgSASekJCQ0KBIBJ6QkJDQoEgEnpCQkNCgaNnuDCQkJOxM5HK5F+W1vbRXVlbcY2O/bzX6+vowNDS05vdE4AkJCVFUI9JK/9dCwjwml8tlXvqb/VzpOisrK2tePLepqSm8bFrLy8tYWVkJ7/rS33gNfa+GsxkEeO7g4KD7fyLwhISEmnA2ZB47TslV3z3y1u/2fIKEu7S0FIiX6TY3N6OlpSWQuD2H51nS5n/8XAkeWdtyWQ+h53K5iscnAk9ISKiKesi7FutciZska61xS/Ie4QOrRLy4uBhey8vLyOVyaGlpQWtrK1pbWwOR8xyS/eLiYvhsSZu/53K58LuC3+1gUktZnY2Fngg8ISHhrFAPeVtibm5uzrxbom5ubg4vHsfPxMrKSiDu+fl5zM/PY2lpCQDQ1taGtrY2tLe3I5/Ph4FiaWkpnLOwsICFhYVA/CRutfxpgaslTivfEnE167lSWdVK6onAExIStgSeJKKatGrUapHTYm5paUE+n0draytaWloCCRO0oufn5zE7O4vm5mYsLi4il8uhra0NhUIBnZ2daG9vR2trayDwhYUFzM3NYXZ2FrOzs4HEeS4HAQuri58NideLROAJCQ2I97///fjGN76B3bt340c/+hEA4PTp07j++utx7NgxDA4O4pFHHkFPTw8A4J577sEDDzyA5uZmfPKTn8Rb3vKWbcl3NUnE6tWUPtrb29He3o6Ojg7k8/kMgVvpZH5+Hm1tbWhpacHCwgJyuRza29tRLBZRKpXQ2dmJtrY2AGeIl+Q9PT2NqampQORNTU1YWFiI3ovVw7eDxHMrOyVOJiEhoWZ873vfQ7FYxPve975A4Lfeeit6e3tx22234dChQxgZGcG9996Lp556CjfeeCOOHDmC559/Hm9605vw9NNPo7m5ueI17ORhpeOq/ebJJh5pk7hbW1uRz+czljPfSeAECXxhYQFLS0uYm5vDzMwMpqenMT8/j1wuh46ODpRKJRSLRRSLRbS1taG5uTmcNzMzg6mpKUxOTmJycjIQOaUYWuWeRl4pUkWxXqrVtC699FI88cQTa45JFnhCQgPiDW94A44dO5b57fDhw3jssccAADfddBPe+MY34t5778Xhw4dxww03IJ/P4+DBg7jgggtw5MgRXHHFFVuSVyudWPK2xN7S0hKsbVrNpVIJhUIBhUIhTEQCCIS6tLQU9OyFhYVgsZPA29vb16TR2toKAFhYWMD8/HwYIDo6OtDe3h6IXHV2YNXSbmpqCla4fo5Z4mdbfh4SgSckvEhw4sQJDAwMAAAGBgZw8uRJAMBzzz2H173udeG4/fv347nnnnPTuP/++3H//fdveN5isdhK7k1NTUEuKRaLKJfL6OrqQldXF8rlctCvKZ3oRKMS+eLiIjo6OoIFDQDt7e3o7OwMadDKb2pqCpEotM4LhUIg8dbWVkxOToY8AquW8fLycjifESqER+KbIaUkAk84a2znir2EbPn39/evWbFXS2wycfPNN+Pmm2+ueEy1dLw47UqLaRhl0tbWho6ODpTLZXR3d6Onpwc9PT3o6upCsVgM0onq3ipp6G+0wBcXFwGsRqFwEpRWPF8A0NnZmSHxQqEQolZaWlowMTERylNJXO8zVt6bhUTgCRuGrSLy9Vynllhk+107YD2dcSs6sJKI5vucc84BABw/fhy7d+8GcMbifuaZZ8K5zz77LPbt27fpeSRiESb8j+Td2dmJcrmM3t5e9PX1obe3F93d3YG8NeqEpE3pQvVihhN2dHRgcXERKysrmUnRmPbOtGita9QKNXMOHisrK2GCFEC4H7XEY7HiG4lE4Akbgs0m73qtwWr/WWtRQTeYn2vFVlhisWsMDw8DAB588EG8/e1vBwBcc801ePe7342PfexjeP7553H06FG85jWv2bS8aR5jRMn8K3l3dXWht7cX/f39a8ibBOlNGKolz/eWlpbMsVauUWudgwj199bW1mCt86UEznxwoGBaqpXrdTWfSUJJ+KVALYS7nnNjRK0kbTu4d4xNs5K1vplEbheSEOPj43jJS16CAwcO4Ctf+QoA4KKLLsJ1112HV7ziFWhpacFnPvOZqhEoGwVPOtG4b5J3d3c3+vr6MpZ3oVBAPp8P5ag6t92zJBbh4u2NonKLtd6VyDnoqNyiAwnBQcBOYm52DDiQCDxhh6FW4lZyjP3nfa9E3t551ZZDe8dsRRywXYHI95e+9KVuuNntt9+O22+/fUPz4MHTv+2iHH5vbW1FoVDIkDc1787OzhAlQqLVJfJKxLwWSZvneRYzJzz1PI1gUV2c5F0oFDJlbAcPhi/aNuQN7hvdFhKBJ2wZ6pVZrIWsv8fIupoVHrOqvWOtG2x/j3VKG4EQ+69RUa38Yta3EmNPT0+wunt7e1EsFkOkCSUKjS6xJK7X0wlJzYPKJfpi3cWIWy3vjo6ONefTE5ienl5jgVMLrzSBvBFtIBF4woZjo/Rwz3KuhaArHVPNElqvB2BJvNrSai+9RiTzauVfC3l3d3ejv78fu3btCno3tWZL3pbArRXNcD5gNSZbJzl1EFDy1fxSNmlpaQnHcMl9LndmOX6xWMwstedAMjs7Gz6vrKyE/JzthHg1JAJP2FBsNHl7EodnedtzY+Rdy6SSN3DE9O1K6cSwlVr5ZqBe8tY471KphP7+fuzevRu9vb0oFApoa2sL0pC1crlnif5ur2u1bmBVOvEscCXx5uZmLC0thdWZqpOrFMPBRze+stej9a3tsNqAXi+2lMBr7dyV9EVFozT4nYidWnaehgqsbjtaSxuq9Tgro3iLS3icnmN/8/JtrS89JyanbMWk19liPeSt3ylx0Irt6elBf38/+vr6UC6Xw94kMaK1Ky497dsrV5VQqIHH4sc9q3lxcTHEnhNK4taiZ9q0/PnuyW3V6ruW9rBjLHCt5NiG6/ysqLXBV7Jyakmz3usm1I6Y1V2tk/Jdz9M4Y3u8R8Kexeid523uHyNm/hbrsN7vjUDi1WAtcPZrLpEvlUpB8+7q6kJHRwcABMlkYWHBXWGpJMnreO820sTWt+5l4v1P0PrWCUq+8vk8SqWSu484P3NAsJb4RmLHEDjgV4T37p1TS7r8XC1KoNJI7qHRO9xOgEfaMWvO05ktaaghYPVmG0mgBMPl1bpZEs+ze0frvtGWgGMyjNeO1muZ7STYevNkE9ZHPp8P8d49PT1heXwut7osXi1ayhOe1QzAHWSVmNUStptP8Ximr/ejUS66H7hG0bS0tKCzsxMAMnuwqEZv2zGvuZF1vaMInKiHuGsl8nq0KG9AqcWySqgNlSxuj4SBbBlbK4/Lprnyzq7g08ksurfUZrnVKCMQWLfUYLnV6MzMDGZnZzE3N5dJyxtUbJ514Ki1TVUrv+1scx6J6wDKesnn8ygWi+jq6gobS3HC0D5QwVrf1mpWaYJYWTkzech9vIHVCU1bR55eTmmGk6iahsorbFeccAWQeYgEHyThWfe1GJDrQUMQuP3sdfhKaVWzwCv9Xgm1jKqJzOOw9RKz4jwSBNYSNy08LoPu6OgIm//TyltcXMTc3Bzm5+cD+TY1NYXtRsvlctgDgxEIKysrgVxmZmYwOTmJ8fFxjI2NYXJyEtPT05ibmwuTbJZYeH+eYVLpyS7bTczMQy3/WbLWJ+eQ7Do7O0MZl0qlEK9tLVidrFSrmt91AASyAyLJF1hd1s4B2OreVhLjsRwgbHtkurT8uWIzl8uFwZ0Du42UYboxI7Leut6RBA5UnyzyXG0vjZgcUuv17bUrne8R+k4l8x//+Me4/vrrw/ef/vSn+MQnPoHR0VF87nOfw65duwAAd999N66++uoNv/56yNtr7Nay49ajXV1d6O7uDiv5NKZY94qemprC1NQU5ubmkMvl0NnZGRaTdHV1hS1H2XFJLDMzM5iYmMDIyAiGh4cxPDyMsbExjI+PY2ZmJmORKxHbe7aufGwr0vVa4psJL32tO91vxK5k7OjoQKFQCATe0dGBlpaWzEMYlMStpuzJXhyUtb3YHQKB1fL1BgS18gFk9kSxHgWAjCVOzy2Xy6FcLmN6ejoM6Lwfz9LfyIF5RxJ4NZnDEnfMxWYa1QhUXSU9zjte0+N3T4vTY3eixPKyl70MTz75JIAzFtA555yDa6+9Fn/7t3+Lj370o7jllls25bqWuPmu9WglFIJlTVLI5/Nob29HoVBAuVwOi0L6+/vR09OT2UeDVvTs7CwmJiYwPj6O8fFxTE1NYWVlBYVCAX19fdizZ09YUKL7TpPE5+fnMTU1hdHRUQwNDeHkyZM4deoUTp8+jdHRUUxOTmJmZiajkfN820ZJEjxmI0h8sxCTLC15U36i16MEzr29dWBVsuZnlVCslaweC4A1A75ay2pQecvnlbytlKXtT3/jecoDOkDR85uensbCwsKafu/Nl5wNdgyBx+QI75hqafCzJVJ7rGf1WbfMI2xrDdhr23O9wYTH7QQ8+uij+JVf+RWcd955Z5VOre42v+vAWcn6JhhHTKmEFje3HyV59/f3o7u7OyzHJkmqBT06OoqOjg6Mj49jYWEh7MfBFwncGgdLS0uYn58PW53ydfLkSQwNDeH06dOYmJjA9PR02I9a441tGcQIvpoRs53wBl4dVDlo8n8+nIHSFvfjtsaVJVVrHXv9kRY3gIzuzm1kCSuZ2CgV7cdK2vxdJ0StJMPJTRoU3IJWCVylmUrqwnrre0cQ+HpcP08ysY2/mvWuep1GH+hOaWw0OuvNa9nYVE/LUret0sQJ09xOPPzww7jxxhvD909/+tP44he/iMsuuwz33XdfeK6iYj0b/9dD3lY+YT1pFAND0bq7uzPSid3FjkTJLUa5vzOvMzc3F/aiZudTK9IO9CQjfegArzs8PJyxxicnJwOR27bifbZtTctsu9uJhVrfnATu6OgIMd0Enwav27Jaa1j7hCXwWN9R0lU5BchGqGiZehq4ggSrAzfTZ2RJU9OZZ2WSL5gWDQzuZDg3N5cZfLzQxbPVwrf0mZiVtONqerYe5xWunqckqhYwO6A+c0+3i9SVYAAyYUw6Ei8vL2c0O10VxjzQFeTz9GwI1HaTOa8zPz+Pffv24b//+7+xZ88enDhxAv39/cjlcrjjjjtw/PhxfP7zn6+YVqV6i0kmfLehZl4cNrXGjo4O9PX1Yffu3di7dy92794dFoIUi8UwgUmStvs3ayTJ+Pg4RkZGMDIygqmpKbS0tAQJplwuo729PZMHHeyZLjX1yclJjI6O4vTp00EX52clc3ZoW/c60Fu9V+vKfvbq85JLLnE3s6oHKmHZeuR/lE04qHV2dqKlpSUzMUh5au/evdi7dy96enrQ1tYWvJmZmZnwJHl9BqVH4Hqvnlfjyak8x0pVei8WrOOYsUeiZrTT8vKZPVEoqw0PD2NiYiITqTQ3N+dGLek9aX71c6xed4QFXgusFVSLnKJEoRvU0NXxHp+kz9pTl05dp8XFxVAxrBAg68ItLS2FB6vSneZxMddJPYmtIvJvfvObuOSSS7Bnzx4ACO8A8MEPfhBve9vb6k67Gnl7Epa1vll3JO+BgQEcOHAA+/fvD2TA0D+NfvD0So0f5sTn3NxcsJ70aS8MRdN6YRuizssXI1h6e3uxe/fujD5+/PhxvPDCCxgaGsL4+DhmZ2fXSCrqtrPtxCTF7bLErZfLvKj1TQubEgblBfVwAWQW6jC6x05eVvJY1XuzFq3GdDPf3ndrdHCgYr3osnpvwNXP+oR7NRA5mWtlFG9AqtcSr0rgWxWtUM2Ksx3eSiiEhpbpyMnRksSt4Ux80jUbmBe2pL/Nzc2FeODZ2dnwZA5eg5MzU1NTGBkZwejoKJqamsKxnhXG+/QqbTM77EMPPZSRT44fPx6eq/jVr34VF1988YZdK+Y1eSSu2mo+n0d3dzf27NmDAwcO4Pzzz8e5556LXbt2oVQqBTlE3W7vCeLAqgQGIESwkDBJ4Eqguu8GyYr1R88tn8+HzZl27doVJjlfeOGFYM3rLnmzs7NrSMmWC/MaK8ftllO0frRvKWEByBhOXBylk5V8oLCGDlrZUmGlNdaNEn/sXJuG8gOw1rKn7q31EbOaNQJGrXWWB9OIafD1oiqBb0e0gueuqTXGQlXrSF06lUR0YQfdvFKpFB6Wyu0r6QaxYXl6JS0Gunt8MZZYrZClpSVMTU0FTZDP1JuZmVlDLlbq8UZk5mEjMT09je985zv47Gc/G3679dZb8eSTTyKXy2FwcDDzXyVU84j0uFpedun13r17ce6552JwcBDnnXce9u3bh+7ubuTzeQCrCylYpl5IGrBahrSw2V6AVbdZB2+74EcHYNvmOOm5uLgYJlIZjsi0PCvTKyPrjdVroW0U7OCrYZy8f3pAGn2jT7jRARJAxvK2oZexPKjURotXy0nJ1RpHsbwrudrrk5RZ/wDWTJACCNY6gJA+r0cDg/nSgYp5i/X5aliXhLJR0QoKK3XErDQ72WVBS61QKIToBEYhqJXAcB9GL3Cxx/LycljgoZVuJ8GAbFA+ib+5uTlM4pAQuMOaNpbW1taMJW5Xm1UjcV5/I9DZ2RkewUV86Utf2pC0Kw3CNlzQk0xaW1tDZMiuXbuwf/9+HDx4EAcOHMDAwAB6e3szoWhANt7XW9nnDcgqnTBiRf/noE2rnORjPQWSGCNk+DQZWp1qabKe5+bmotbiTrCyY/AMJrWwCbsylgMcsHbTKjs46R4kaqTxP1rN9hFtNnTT5lvj1JXAOcBWCvvk79rOOGCr5W/bNpANUda5s023wBWbGa3gdXhPE9WC4LEk73K5jP7+fuzZswd9fX1hqa4eR72O2reupAIQCldntIHsajF1qZlvDhK6ETwbdVNTU2jEo6OjmZV7uvrMk1esp7HVEks9iFnjlrC9CBRdUdnb24u9e/finHPOweDgIPbv34+BgQH09PQE8rar+Ei4nntuOxjLzZs8VRfXxiR7hoQSDOU4Tt5ZuS0WfWElFC3Halb4ZhK+Z1SpFavzAjR09OEIdmsC3o9HXvYavG+1gNmX+buWEUGvWMlfjSj2T5VN1wO1zJm3SnWpXgN5QfNo818raibw+fl5/NM//RPuueceAMCHP/xh3HHHHcjlzkQrfPzjH3ejFW6++WbcfPPN4aaqwWsk1kKzGls+n0dXVxf27NmD/fv3Y//+/di1a1dmlzOOkkrkOtHFzq5WuE6GKSl4oYPMFwmFZFQoFIJ1x+XElFJI2rq/hsYNexMoFp6lsJ2EbvNirW9vYFYy4ODKqIXBwUEcPHgw1Gm5XA4eDsuPJK1krZtO6b7NzJNq0qpbMj+WwHVVHc9XY0JlG06IA2cWefT29oZ8KFHoAGAHByWF9WB2dhavfvWrw/fNnLOyk7okJf5Pw0VXZur9emnaY7QMNI5a+6e2HwX7kNavJW+WtZ4TI2LPi2Q+bdtQCUXn5TjpySi2amGF1VAzgW9FtIK1wrVytLC0INva2lAqlTJu9v79+9Hb24vW1tZMxIhauAwFZAUqeZPAeQ1rfVvdnRNoBBsE5RPuuEaZpVAoBLLhhChjhnUZrrqY1gKsZIlvF6F75K2fYwOzknexWAzRJoODg0Gy27VrF4rFYpCsLHnzM+tJ69KGetJyt56dWkwsN5281sk1HqfEzbrkHAxJoqOjIxCnneTK5XKYmJgAsKqtso7VMLDvWq76vb29PYSbbdSclTUSYhY4J+xIpCwD/mcHPM8Dic37aP2x7rQemB9NTyVJGlTMs/W0bF+znBTzGD2FQCVT1eo5oC0uLqKtrS0YFmdjhddM4JsdrRDr/LZTERxFi8Ui+vv7MTAwgHPOOQcDAwOhs+dyuRAmph1RIwvYaDScyVpKqoNqx2fBq2VgIx6oq1LTZYdmerpB0sTERFj4wQFHBxZdFGBRyRK3pLQViNWfZ4mTvMvlciDv8847D+effz4OHjyIffv2oVwuB71RY2pJ0PqIK7XI1dKxndDLk5JLrM0o+ZC82R5nZ2dDqGpHR0eYE+nq6soQmRokxOzsbPhciaRrxWbMWTE/JCqNMOHgyjrVUEv1WGJhgh552/pQcraDAutQvStCQ0xV7rAeOA0n3p/erzUq+VI5TbcToJfPtsdj7U6ZMSu8lnqvicA3MlpBESNnC72JlZWV0HAKhQJ6e3uxZ88eDAwMYPfu3ejq6srsf2HTUfdaLR0lAKtFex2YVrdqWyRuJXEl96ampjDhowQxPz+PUqmEUqkUNkQiiXPnO8aSM01t8LHK9oi7XjLYCGh987OS965du7Bv3z4cOHAABw8exODgIM455xz09fUFb0rrSHcUVOmExK0DH8vfEqN12ZU0NDqCn4HVNQIkEXbk+fn5YHHqPAlDDamNK2moxKP1qm2zWseO1elmzFnp4GtJWsmNREU5yW5LYMmZZcL/bKidTvYvLS1lLFxvPsIjdl5b+zHTZHuKSVwx6U/vmcdpJJyGEZLE2R7UwLB1XStqIvDNjFaIQSsYWG2kLKz29nZ0dXWhv78fe/fuxa5duwJ506LWCS0bI6wTlSqT6DnWEvM6EvOqkyw8bnZ2ds1kqKfV6cIiJaKZmRmMjY2F1X3c8U7D5bS8PKuOqPTfVsAbqHWeoLe3N1jeBw8exHnnnYdzzjkH/f39KBQKALLurvWYrOat1rl6RLGysR1ISVtfzLeSvFpjGkqmHXRlZSVMtHP14eTkJKampkKdajut1biJYTPnrDwJRS1RSp4kKpK49gOPoPm75/3Y0F4td7YjHSC8/NsBUgdpNeqUkFVztwOBvQ6vr9IRJ3YBZPJqPRM1bDZFA99MxBqKJXCOYNwYnrvH8cGonZ2dwc1WAteID29xB7BWC9PVYsyL5lctOitfWMJQ9wnILkbgiN3S0oKOjo7MqD43N4fR0dFMOCKvw6gZLSN+r0bkm03i1sq2DV8tOO5twjmM8847LxMqyFBQ1Ye1Uytxq2VOC1xD1HRS2nPb1aOx6WooIgf85eXVlYa0tKivLywsoLW1NUxSs/6ampoy4ZFjY2OZza/0WlqGmr9a6nGz5qy0H6oMoJYmv1NC0lXOWh/WArfkqnVNsrW6tp2c1LBArV+dO1KPWg0mpq3kzTxoWKNGoLAtaNvWkNL29vaQRlNTUxjI2C504Nk0C3w7wQpVza1QKKCnpwe7du3KPNWa0gStGbXI6ErbCA8bDqTumkYeqPXNymIHtoWu7hvdPZVSrKbGjs3BiQ1+eXk5s3ub15Fj1mU1It9KeBo4XWwNF9y3bx/27duH/v7+oBmrd6SLYLQTetq3Tlrzulb3tMuc+a6RJ7FYcrXC9TeN7aVWr14X50Ta29vR3d2N/v7+jERmY4R1cF4PNnPOSo0MG0utdcuFcxqVYw0oS+K2/9k61jrQwWJ5eTlcw1r6Cmt124dHWMmK1yBJ0/K3CoHmhzIJt2rgdVR+0XkDLxqmVuxIAo8RD/VSWi979uxBf39/CC1jAdvoBHWPLInbyUdLiKp3KumqDm6habKyWbGWwOl2a1gjsKqd6qIFtR6UJAA/jlUboleemwm1SPhuvQ6ty4GBAezduzdsJsXBmJPQJGhL4t5ArQO4ErjVLi2hA1hDHpqOdnIlHdajDgJLS2c2arIDPiU+WuI0RKanpzPzHzqZFpN+YtjoOSuvDjWqQw0WtnVd0EQDxGrPSuKexayeEOvfeuVedJCNWtO8kbB18tsOCjogc6GY3rf1AHgsz+NAwm2KNYSVx3rhlxwkGk5CqQUa703pZNeuXeju7kZ7e3sgb30unad/WwtONXDrohLehKT+ro2Endvq5pqWbQwtLS1h72AlGhI446IpB/Fl86zWjObfa5ybBU+DtOTNwYn7h/T394edBfmoreXl5czGTzZSQAnW6uHWaia8cC/rIuvAYNuKEjZJSwdhlrV2RK2T5eXlYIktL5+JB+biM934TNcIWH2+ljrcjDkrW242+kLLifqu7o2t+bYE7mnTViqz6yM0T1Yi42fWB99Zp9a404HZS0/T1fhu643RMFGLml6YrkXQAdBOxnpzaZWw7QQe078VHLE06oQdvlAoZHRvjf+1sdRqbelLSRdApmOrFQ5gTQFbq0HDlHh/tsMTOplpQ5/YmLm7G6UGPg6M5GblFDYC7exbbYF7kom13JTA+/r60Nvbm1mk48kJ2pmt7h2b1NSy0MlBdjaWM4+1Frc3X6KDuLq+OjHmeUskD43YoBWuoaQTExOYmprKDCosy62Ww7z605d6LyRZ1cX5mWkRStrA2n1MrBfkTWRqmip1eJFFrBs7V2KtY4/A7USjnXTkuz1+cXExRJ2p/q7HaxSTlmmt2HYCrwTepO6LQWutv78/PDWFHUUnr3RywlpB6pZpYbFwWZB049kwCEva/E2/M/+x41jBhOdK8qWWeE9PTyBwzb8dRKzbba3yzYZKB/yuuik3qeru7kZPT0/Yl0YnAe1T39WltuQd87o8z0plMMpcHMStLuoRhrrxVg9VyY3X8tqeLuvmQNbb2xv2KOfWs5ys9sh7KwndkjdhSTE2L2SNHQ9aTipl2UFVYeevvAGXaVuPKnasGl9sjzTqWJ9KtLEBhZY1BzA1IllOOshpTLhNN4ZtI3Dt4HaU12PobheLxbDfsqd7Wy3Us6JU59SOSmgDU+1aGxYQnzjUfFsS1o7N9IHV1XckdG08ClZ2qVRCX19fZiMkHsvO7jXIrSRvha1nTkSTwLu6ulAqldyHD3OLAY3lVpJVV9iTTqznZfOjFjjLqRKBq+ylnU1JAchKNTp/oZagDuQkcT7dR3fItK51rTLKRtehftYBkBIByw44swLTlgvr1RsQtU48D9nOQWh9aHnyetaS1T6oebKSDNNUTy02kBO2XtUDsNfXgcguElIrfz2e87Zb4DFiURejo6MjrNLjJBfJ20odXqVrRfE4DSv0Csh2XJtPtertvXjHAlhzLRIBgKj7xLS4OKm9vR09PT2ZEZxpTE9Ph3PsoKP52A4ooZGwisUiisVi2BESWN1MjDq/Z4WrlKKWt0feljCVMNQ7Y6dR8raDrlqe2sG5ElOtM2t983xrtPA8epl8shC3JaZUtt0kbsFy1UfFMULDs549Avf6iyVjG31kjRO1wCnfKNGrZ6pSmVrkNk1Nm+GgsYEHWBu5xN+tlc+2bQ1F9mFryO5oCxzw44St1pbP58PTTvj8w46OjtAB7QiplWPDkHQkto3LNiodWa2lbcnRWrjqIjE/VtbwJI5YQ2K+qJ0Wi8VQNgy10wbGRqQew06wwjVUkmTFbVcZ0UMpTJfK892LQLG7OlqSsB1Z65m/s/N4VrKmZS01EnhTU1OIkGBaOiB7rjiJn9DQM+6U6e1aZ62yrSBye10tSyVqNZD0My1zbxJSQ/QIK3MoQVoSVYLk9ZiGlpP1gm3f8NLVtqbn6ECsabOeCDU6lMBVB2dbsUZirXW6rRJKpf+oIekeGZROdNtKHVFjJG5HcFoLamlZaGdmA7Sk7ulxvC92VJ1UtOlrOaysrKzRxbUsCH0Wnz6IQssCQCbPtbhimwFPJqMFzhhhtb5pnegqSjuvYSe0VH/VDm9f1QZHIL6wxA60Xt0Dq+sCNFJD5QZtmzY0DVjdR4OLQEjgqrPzOtYK3w6L3N4P27D9z5IlYY+151kpJdbftJ/GvF+bH61bry6ZvhI407HEq+lrvdrr8H+2DwYx6NxCLP8xbAuBW9fJamx850o9Tl7qakvdB8Mjb9sA1ELQSrGWmjeS247sjfLqPnud3oY32YpXkHy1c6tVp5vkk/Q1fljz7Mk/WwnrVdHS5MMvdJESB1Y7caWaN/VwOwDb+tQyiFlZ2nF5/di8CbDaodVFJpaXlzOTVhyoOGgxLZ0Qt3XDNq+LPDTcUSU2j7C3s54BfzKPv3t9nJ+tHKFWvNdPeSzrwpK+lx+brlrk9lhg9ZmYnoFojS9bj8yXZzQwHZU+AdRthe84DVxJR/Xv7u5uFIvFsFGQhgwS6q5oJ7dSCxCPRVWdVYncawxeBfF3djbmRXdOUzeLsJWuDZJyEUlQdzgsl8tYXFzM7CduPY7tsM6s9a2/6W59NsyKVoo3ENsQQUvcniVXTQfX/FrX3XZYa30Bq22O5M4B1soE1GbVw9IBQduSHbi9l1efW1HHOhDbcD0Amf9sO7eDuUe2arHb3zzy1P+tYaVp8HPMK7D92a4LsHo2yVcXDVUqL2ut2/Ly+kst2BESih2h+X9zc3PY5ImTXc3NzaET081WC1itbLWodFLK69w6ocX/NB3mU6ENDFhdSq1WOMl7ZWV1NSaPrZQeiZtp8rNKJTqpqc/ppNyg5eF1+sHBQZRKpUAwTzzxBE6fPo3rr78ex44dw+DgIB555BF317r11rNKS9wrgos8OLhZC9qWvQ7M3qDM4+xAEHO/LcGwvqxXZ89RctZJNNW9WebsoFzAo+1c9V/rIXjlWKljbxV5q3TAe7OejBKWJ6PZfMcGI8+TjcGTOOw5tl3YtqVgvVgC97webyAjf1k5jf/ZMtVy0983RELZyI5eaaSxFg5dbU7qkAi1E5Oo1AWOySme1GJHVxs3bsnbcwVtdIKSEK1n687b/SNsGfBebIXSkuMA09LSgmKxGCb/+FAIjYX3PAXiu9/9Lvr7+8P3Q4cO4corr8Rtt92GQ4cO4dChQ7j33nur1qutY++73oNdSqwEri/PffU8J60nW7+2w6q1r6sovTbhSRZa7yqpWCKw1qqVw/RYa3jYOttqDyqGahY4yUojK+z5lYjce3nnA/6eJ54xaInaGmbesdZa1jZl81MpDY31Zpl4A54NI6wVa335CL773e/iySefDE/7YEc/evQorrzyShw6dKjmizLj9rO+Uyvlfgq0voGs/GFjgm0kgrWmrVukGrm1vDzrS0dUu+JMA/Jtp7aSTsyCtGSu+bMeBX8jiXOLgZ6enrCDoWcJVcLhw4dx0003AQBuuukmfO1rX1tXfVqyttdUHd/upcF7rtaJ9Th+1t+0nm0kki13q7nbOHCrsVvLzRtk2I40PZuuZ1na+/W+byesjEMwX2p0qWfFY9QzYXpAfBFb7Pq8lm1bdsC29W1lUU3XkzWr5cfz4JS8yRXcWsA+mUit7k0ncIt6O7rCs8Z5U1zswcUNDDdTd1M7q+qjMQuan22H9iIYNC9K0loZGhGiT+HQvX6VzO3AYxcoqGtOKDEo6eik3vLycni0XG9vb9hThBOEMZ0ul8vhzW9+My699NKwif+JEyfCrnUDAwM4efKkW3f3338/LrvsMlx22WWZ9LReYy97jC2bSha2vZZ3XVt2dk5A655labV1JV5rwXt50PbIa8cGbzUqvDqJpb+TwPvlfeRyuTVhkCRxz5qNpVeJLD3yjpGd54Vb8rbtxhJoJSKthWTVUOGgpg+28Dy09dZ5TRIKO3oul8OHPvQh3Hzzzevq6PYJHzEXW0cjjf/u6+tDV1dXWDbvjbxWpmC61hK2soolfD2PLzs61jpCeu6wDkAa6gdk90aJ5Z1hSHNzcxmvgINJuVxGT08Penp6MDExESJTrMsOAI8//jj27duHkydP4qqrrsKFF15Y030B1Tf+t2VoG629N7sk3XY8Dx5x81h1qfndtgstD+tm2+PVevTuwb5796KDiB5HWC8vptNvJWKSBMmQEpT1mDs7OzNPILJlEPM2LNQ6roVYbR4ryYcAMvlTYyJmkdvPMQ+J+SZ5M+pqZmYmXEuXz3v7xXjXtaiJwDeyo8cqQcmSq9K4dJ67Di4vn9mhThs6EbNqvMbn6eHqBmpe1MX33DZ7Hc8qs5EsWukaleANELwm01ECseXQ3NwcNkfq6enByMgIpqamQuidTXvfvn0AgN27d+Paa6/FkSNHsGfPnrB39PHjx7F79273fmPw3Fx9t+WoncfKXdaTsu637dycQNRJXmv9eR3athlLLnYewnpWniRi828n0K13odISPTr7HEfmoVLZbzS8/uNZtJxQ7+zszOwBbif2Abj3H/NwtI6rkfHZ3Jvu32Lbqe03ti2x/tiGdeD3wmY18MLrH4pqA3ZNEkqljg5g3R3duonqSujiHd1mlCGEdqWbumhWrww36VhL9lgWqHYiXUjhPbLJWlp6Pb0frpjUh5mqNmjDHZkf69Kxw6iLr4tcKKUUi0V0d3cHGUV3v2N6U1NT4WnoU1NT+Pa3v42LL74Y11xzDR588EEAwIMPPoi3v/3tZ12vnmXpdQqWnQfPyuHxVmekq2r1detWM5JJl+zbfVD0HG2nqmlako3BygTW0vc6u33sVqXXZsLWFQdbIKt986WT0/ZcLV/tqzpI2fkm+3k9+SVi58XkjFg7tfXncYm2Z3Ka1qmXl3rqsKoFPjU1heXlZZRKpdDR//iP/zh09Ntuu21dHd1ze7Xw8vl82HWP1ndXVxfa29sxPz+fqUh9WYsOiD/kwHPl7CDiNRTP/fXOt+TrkYh1p22HoGulA5Teg5J4c3NzKBtaQtwcqVAoYHJyMsx+My8nTpzAtddeC+CMF/Dud78bb33rW3H55ZfjuuuuwwMPPIADBw7gK1/5Sk31auvYI3CWrZ3QiRG9LTuPvNVroxvKwYz7aeu5MW16ZWXFJQibBx3kta51T5BK5aL3om01l1t9FFehUAiv9vb2zL7onvSw2YhZ4eo96CpSJe9Y/rQPWOnBWqasY9Wv1Sr3yFAHXEU1a155SduDZ0jR4uY70+c6APICy0YHN69cYhLSWUkom9HRY5Yab5Ta965du9DT0xOeFcnjSQRtbW2BxLhnhHVZrc5pGz8bhcZna+NRGcTTZqsNAKpxKcGzQaoFwsbI33Wllge1IjUypqmpKbNUPZ/PBy2cZX/++efjhz/84Zo0+/r68Oijj9Zcl7ZO7QBtLRs7CWw7uy0/pmctG01bLW7+p4+wsvWvUQqcwGQbYDqavvWudKWl3jP3Q/FgdVzNE3+npVYsFtHV1YVyuRy2ldXJbiV/O0BtFayVycVZrFf1MK3cYH+3c0TWwwZWH2Ksc2AcPHge3/l7JcmlmhWvA4e2I+a5qWl12wv+7w2wTIflw5flk5i8Vw1VCXwzOrp1+7TSOBFCHVef0AKsjnAshNjSa0u6nv5WaTKJv6uLpBIHid+TX5aWljIWmhKLruBS3UzTW1xcDKO4arl2YOGx7PwkQV5TR/xqK8bOFiwDa01Z69tKSiRxlcFis/K8DuuIxMG0OFjS0/DO1QHZEri2B62rSvdlBwjdoKqSpuv1AQ5GhUIBpVIpLF7jroS8P3ueV/YbBa+/al9ivrUe1LOyBlNMQooN0LwmCZLvtm49o5B5rOeeAUTboc4nablb8racpRFr3A5aZRgvjWp1uuUrMT0tT/9TCaBUKmU2+QcQRjNa32qBKyl6jaTS6G8Lk78zRNHGX+s5hEZSaPrWWtN8NjevPnNP0+K1rPum5aXaqY175u8c8T09ciugFrgXaqlhjrScNJ9eYyZp2k6Ry+VCxI2FEnfMo2J+mbaNePIkHHZOzaeNM7aopLWqzKTeyXrqbjNW2Fpji/cBILowi9D+YD/bfmcHSl4nJi/Ycq3FI/EGAAuV8Gyf03amg7g3yGl6NF7UQ2GftcRdK+qOA68HMQtEGyg7JWez7cSlNmw7UWLJLiZ7KLQzW6tM95q225rah+d6izb0pY8BI1lobLk2ejuQ6OClo761XFQWUPcuNqG00Yg1Pit30AL3pBJv0PUsMyU5O9GnhK8DtB0IbIexxOIda8tbByi7kMt2ZuuRWI2V+fSMiZilGivzjVp4F/OYCZWTqIF7sd+xvNv/vDq3+fF0batV28FRP8dCNJmWHmfr0bYRrdsYmD/lLu3DykHrsb6BHWCBa0HrKMUZW43csNpkzL3Wz17DUWgBEjpBUWk1nrUW1LWqZF3oPVuPQS1Fusza0KzbpvfLazGqguFKANYQxWbCkw503sJ6BTzHW/0Ym9lXScYuxffeNV+23pQUWO86/+DVjeaJ97eyshKs8Urlota2dffp8XE/dLvAyBJcrZbb4cOH8dhjjwE4s/DujW98Y81bJNj+akmd96IWOOekvDLwXrE+5uUFWBvaqb+xLHXAUeOIv62srGTCBzV9+5l5V55Qic0zIG352Wi2lZXVCDRbj7Va4tu2nayOjDqCM5a0o6MjQ+LqGqu1bPVvWxh2hPdg9SdrCaumrmlakHB5PCuJmrgtAwAZacUSuc23bSDW0qTUwwcfc08UpuM1rs2Adw21mlUrBbLlrB6EjVLQjqIWryVAbRtKDJboVAKhXMW0vc5r01eL27Znrw16ZWStb5I3NybTR8rFrHCvrDdq4Z2WQ0zOY//V8FvKHkq0Wodarra9xwwklquVQGLEqcaP9SK0TtjvbJr6X2ywjJG3vV9vvsTKntXq1cOWE7i9Sc/ytuTd1taG+fn5YB3ZpeTa4ZVoYwXiVbaep5OBvB6wVvO27pOez8bDd82H9SDs/2yg9nqV8q3lMTU1hbGxMYyPj2N6erqmELeNgEfc1nJmJ7eSkUfenjWmbcbKJlYCs5arHQwJJe+Yd2QlttbW1hAZoS63RzAeQVkNmB6crUvdn6ea9c33jV5h68koej3Wid3HnG2fA6LO92ja1mOyWrith5j0occQHqHqPdn6sjIMEbtujLw9b4Xps+xq8TiqYUc8Uk1JXN0wIGuhaAeii6lPb9FnXMbcLx0NNTbaTm5pp1PXyUtbR29Wil6HbrXXsHTA0MrUhq3X087Kz7qfy9TUFCYnJzE2NoaxsTFMTExgZmYmo71vBmIWlj3Gki+QtZwryScsZ9splFz1OZmxDc5IJtoO+HtMZrJeH+PvSU5sr5q3mCeoVrwOFJ4HYfdh8axA/k5s5ApbS3z2s16b5cqybWlpyZSTatVANmRX26Za77GB0F5X68keb8lTj7VErfmq1B48gtY+r/Xq5U955WxIfMsJXAvVVoCOTiRp7h1gJxMteWtsr7pG3miq+VBNSzsYj7GyiWfN23T1P12QY6MTmCYJRF19yi6WvO31lexmZ2cxMTGBiYkJjI6Ohq1lrRu+WfBcSOa7GvT+vUHUS0v/s23Dtgtta9aVtv/p7/rSQcILMbNyAWHnQjgAqFtNEvPu21rYev9ee5iYmNiwhXfe/djr23zowMgQYN0GmcaaEqh6MEzLaswx71o9V+ZL+zHT0rmNasRt792bf/M8aTunZxeGabmxLXjSb63YcgK3VqitgMXFxUBEIyMjYTWh6oH2SeTaUTVt7VBeeBmvy9hdatfMi46SsRFSK08bEAcpjelWHVwr0XP/NDRNO7TeF89fWFjAzMwMJicnMT4+jvHxcUxMTGBqaipMZqo1txWIufq2gcYIU91qICtfKRlqGahu7O2Hzk5nOxXzo+XK9C2hanSP5w3ESNaz4JXASTB6jk3HpulhcXERr3/968PnjVh4p9ezdaN9Q6UPEiXJiRvRxSYr+dnOP2gZWQ9GB0adp9JyVtDjtov2bLvw6jJG3kxHZSK7tbQep2WoeVcS12tWI/MtJfCYS8lKoQU1OTmJkZERnDp1KuyLsry8HB4ZZt1iC0vcQPbpJzqqWtj0eA07Stp0PNmFefCglWcnOJi2bdze4Le8fCbqhLo3Le/JyckMeVvvYrNhy8KzmmIEaO/ZasB6/wBC25mZmQmDvMppPF47lKerqgwWk0Co4eqj/AguSLJE7JG4F0Jo244ty1qss3w+H0IHFfUuvIv1V82nR0IkOLW8PRJn31HpEFgbqaOGFOtA46cBrAlq0GACQuUyT9Kz8yraBz15RM9T/d+SuOUVOxh5RmItlvi2SCh6M1b3m52dxdTUFEZHR3Hy5Em0trZidnYWLS0twbKyk1wrK9nd3NgQtGI9N9yOvmol6znW0lCojs5j2diAM4NJLJyKHdnTZHkeLXimoXMDlJbU+p6YmAjkbSfB1uuerReVrGjPurTkbdOwhMEyYN2zDLxNqVQ60XK1kSs2P3znwKptiS/usaKaJ9NiGnawoiQW67TaoWvpxLVYZxuBSvXhWcE6aNpyZxnYQcx6V1quNLLYX0jKtl60rSmBx8pO9Xf2N64nsHHamr+Y1q0DAaNwvDUBvBfeq42iW2+dbguBA6uWLjuiRpZMT09jdHQUp06dQnNzM2ZnZ9He3h6scHWP2WlYUNqotAKtFaQDCSdNtDF5M85KQp7Oae+Rv9MqY37YGHltVrDGNfM/lgvP4yCTy+WCdMKdBScnJzE1NbXm4caWWDebxLWxWyKzx3p5iZEGiZAkCqx2AtXArfXlWVreNQlLIJpPuvk6Ua2DgVqMWg4815aHvU9L7h7s/NFWDMrajmk5xzwlLXddsGQtXY/A7XEcRHVAUy9S+7QOftpebB3qfbEMObDrg1hsvdiBySNvb/dSW/92EIxN2NeCbY1CsW4lJy4pobS1tQXS7ujoALC6tJ1uspItC0y1bL0GO3vMIvUsQoVaZEB2uS2QXe3n3SeAqGVA74GrtbQTMA0SBBsTn4GpmrfKTPW4ZPUiRs62oXoWnJ0LsWnawdcO0mrJWOKzFpOtW60be30r8dj8kCA4mJAs9Do6iGm6mj7T1H5gO7Q9nt+30gr3iLqalerFyXsWrD1GNyejwaMDCY0uLR+PpO3nGDlqfu32DqwXbVdW9/YGKG0HXj9gmlY+WU+dbhuB2xG1qakpTERNTk5mrMzJycmwJ4rVrbXgtfN6BaaWtzY45qNawSlZexpdU1OTqzF7Vr2SBs+lBZDP59dU7OLiYoifpqfByd6xsTFMTk5mLO96G8RGwVqX6t5qnDMjOmqJkNEOZfen0br2BmVL3jZfsfoB1sbp839tYwCCRKCyim4tau9Fsby8uoiHXiYjh7aj/ghvEPUsUzsIsgxoiCipqaxgZSi1hFln9HasVKnXA9ZGndkJx5iBZs9RC5pGE/s286CSrfUq7P411lDVgIKzIW9gGwncWpa8ubm5ufDf4uIiZmZmwr7IfK4c35XQtDDsRInt5Dqa2zxoA9X/bN4tKei7tXj5ro1dOwTBhmsrmeRN3VDLhmGDjDg5m1VdmwG9T0oc09PTYaUod0u0Uo+1orUD2g6hxB0jSxK0V+f8bgdfpmMHFpKSzad6SSq/KKx8o+2OZcOXDsbbXZdaN5bMvQk4tZJtZAn/V9JkP1YZQ2PrAYS4csKziBWsb0u2mqbNkxK4cpTmVw0uJW0St/U6WFY2as4j7w0n8GeeeQbve9/78MILL6CpqQk333wzPvKRj+Cuu+7C5z73OezatQsAcPfdd+Pqq6+uekEdAa0LyZvkd1rk3J2ws7MT5XIZuVwu89QP615r2Jy3ss82OsJa5rbB6sQkSSEmucQs0Epal05cat7n5+fDgMUGuLCwEBbt2HDBSm7idlnjJFsOOlxo1NLSEuQxO5FjpQNrQXmSCq/l3afKNeqpecfaPHi/xwic8MrbtgXeM+tzYmIC4+PjwaOam5uLDshbUZdaB/qbWuBsr3bA4XmVXmp9K8lqW1fSjLUNteo1zpz1o3KHNQiYjtWzeX2tY513Yxoekev5yh/6BK2Y9b2eeq1K4C0tLbjvvvtwySWXYGJiApdeeimuuuoqAMBHP/pR3HLLLTVfTOFVppK4dbf1kWG6VWZzc3NwNVUHtRYsO4nV8ZSs7e9awOriWwtdG7enE1rvwKswloFG5JCQuZWAJXBaajbixGsI3iCzmbCeCeuVE9TDw8MoFApoajrzNBsu+Iht3gT4Cy2USGOuKMvXkm0tHcdz1T3ytvfODm69LXZk6wnOz88H8uacxvT0tOtR1dvZNwp6P+o5646dlJMsrJWr0pVGcZAI7QBi+y1Jn/limjzW6wOWuBWWxPmbvW8lb0v69nytd2/tileftdZrVQIfGBgIm+CUSiW8/OUvx3PPPVdT4rXCs6xsxwTOxNlyXw+Cx3mWK8lfNVOr28VkFC8POunGfDMmld/1eEss1guIlYWVfngfJPCVlZWgH1P3tuluV8f2OgmQJfDx8fEQ4w8A3d3dyOfzQWfUXfjsfVny5rslZK/je+dUImJ7XK0kznv3CFvPVW13dnY2eCW0wOlVWXlpO+qW8Mib1jelMdafbtQWi9hgGVmLVh+IojKXLXO14HmMzWtswOM19TvfVUZhG7QTsZ5XaCdnCTVI7dqMs6nPdWngx44dw3/913/hta99LR5//HF8+tOfxhe/+EVcdtlluO+++9wN4u3uZrVCK45Wmt1j29O51dr29utWYuV1rHWkx1nt3JuktKO5bTjWCq9G4F4n4f2oBq4bHmknj5Wnkt9GS2M2fd6LlgkllLGxscyzAefm5lAoFDIx+FrHdjD07rGS11EraXtpVOpces+athKzNUx4b6rHUirU/WuqaeDbaYXb9qnkzbkNLr6z6xvsCmPWqSeFqIbO8tP6V83bq2vP0LPtQ0lc01VNfWUlu6si692Tg6w0xDRZ97EtHry+W0u91kzgk5OTeOc734m/+qu/Qrlcxoc//GHccccdyOVyuOOOO/Dxj38cn//859ecZ3c3s5mqpstaC5iLfebm5sKe0vxfLXDVj+1EphaWEnmMLKw8E7PyYjKFTS9mSWnj8Mifi0uA7KqzauSt+SA2SxrziJy/07pmlBGjT2ZnZ1EsFtHW1paJMrANvBKBWy8oRqz6m56n+Yx1dv1siZmdlZ91LmJpaSmjvWokBoAQz0/5JGZ9bzdsm+QgxX6mIa3d3d0hcowyh53f0Lat8CxbPQdYJVkeb9P0+lasf1ryB7Kb6FWzvK3VbSNQaLxo5NVGBRrUROALCwt45zvfife85z14xzveAQDYs2dP+P+DH/wg3va2t9WVASVS2/mAtZ2FRMyC4LlsHNZipeTidV5L3lr5nuwRGwQ0rzbfep8eOSiR2Hu3+aFFp+VQbyPYCmmMsNbQ1NQUAATJhATe3t6+5ik96opqWXhWEDVn7Ti8vhelYuUeb7BgPvV6hHZwe7/acfkb363VR6OEu0hyH3drpW0nYoYFDSdG0GhUVKFQyMh+1oPgubpoJmbkKXl6ocR2kFZOsKTqeYrMk+ZTSdjmRcMivYgU/q4DHPfp8eL8Y4ZDtbqvSuArKyv47d/+bbz85S/Hxz72sfA7t6YEgK9+9au4+OKLqyVVMVOVrFpObHCrWQBrLFIgu+Wn1bwVrBSdZNI8WPK2kQ7VSNPTgWNWt96/NhZLKNRUrcRitd2YlWHTVGyGNKZ51Gury6wEPjExESJt+NxAu3e4puFZR4QOFnZw9zqotd41DZ7HY2JhaDH5zEpwnldBz4TbSGg8/04gb8L2Ey0byijcj6enpwflchkdHR1hgZqWGclbLXNrvABrrXEbnqflrv2Bcy4AMoSqlrXCWuEe0assYqNOvBfzw7LZjHmNqgT++OOP40tf+hJe+cpX4tWvfjWAM7roQw89hCeffBK5XA6Dg4P47Gc/W9MFbaEDaxu/JUC6YPr0cRJapULwCsVrJPxsdVFtFLVIILF7jBFnpUqz53n3ZsnbXrsWbJQ0ZvNm86d1aslxdnY2kDfj/flAD8b8c0JJ3WaNPtCl84TOO3AQVljX2CNvHYAoGahFWUkuYudVj9DGretx9lFqeu1q2Cqi9wZmtcJ1L/ru7u41z7VlGtZAqnSvqp9zMNC61AG9ubl5zUpnK3MwTe+YWFnrBCuvYx/kbB9EzUFEn5Cle/OfDXETVQn89a9/vXuRWia2PFSzECu5OlpoOhqyQahl5c0EA6uNx7OgWOh6nJL4ekbMmBu0ngqrRPoxYudvtXT6zZDGrHUJrPVErMs7NzcXBmc+lUkfzExo2Bnf9aUDsFq8HklY6UXLi+nZmHpr6fNYrwx4b5T79DmlOollJ+DtvIYnxXnlvdmIeYba/zgZOz4+jpGREXR1daFQKIQ6bWpqCuGiniTpeZlaRyRxRn5pf9c82UlIT5u2nheQ3fPblq21/jmYsN3ypU+cV2mMstJ6pbGY8Uds2zMx+a6fVUuybq3VtLwnO2uauiENoRWuJG7f9bNtpLGC90jT8zBsOXjWc62DQy0WfKX/N1oaqyWvtvPr5LPuE8POpqvbtFMSltyUDJm2DUVU7dIaC9bNZpqEWt8qp1iJR/Oi5Bx7ULE30c7re8aM3v9Wk7h+t/Woe/kXi8WMBU7Di9FHnncbk1HYFtQCV2NLz2X6hN3fxL4AuNyj0DrmPXDbC76UwHVjPm7vrHMb6zEGK2FbN7MivFGWJKvumbdVKDuKFoJOLOjIrhYa02ckBN+t1R4r6ErWcYzIK50bS6sS1tN5bZ42WhrT63gdBFg7gawWthK2jS9mx/W8JDu4qmxilytrOXj58+5D8w4g09Ziaag0oveibdbm14aYWutvO+ERq2fg0NuYmprCyMgIOjs7A4HbfbKXl5cDEWs9edcGkJFQ1AuzbUAHez3WDsyWkNWqtiRuj9U9i1T206g4Hcx0YZa3yZyW43qxLQTuWYdqObOyVSfUPTQ6OzvDwg/dY9k2eiVjr0N4JO5ZBp6EUu3+aoVnhdt82uP1uJh77XkTio2WxngdT2+MDX6e96QaN+ue1ox1dZWsPb3b1p3m0Su3anWrg0+MuAGssbp10YaWlfUEvXzEBumttLwV6iXGvCmN9VfLtLm5GcViMTNQs79bi9rTqClZAMiUqTUMVlZWwp5Ctsz1s41soXWvIYoEv5O8KeXqxHtbWxtyuVxmPqBaaGjDWeAx8tbPqm8Dq1YPV/Hl8/lAuvl8PuO62nQ1BIy/eVIGBwy13jz3O3ZPHmq1nGrpjJ7cU6uVvhWwEoQ3aRXzRKzspRsJkcBJ2Aw7sxKH1ltsgqgS6dq25g2QlvRJviqxMD92PYLVetkulcTXU1fbQd4eLInTCueWCdZCbW1tRXt7e6afa71rWpq+Wr6eAWDLw04Se9a9Smm5XG7NJKQaEzYPVvvmIMW6J1dxQpfWtxc+6JVnrdgREopXYdqgV1aysbJczbWwsJBx0ex5tkPYiS77m8ZseiumKskmsftSrKeDesRhrcdqko49f7M7vbXAraVWKQ96b3YjIEZvzM/PB/1RrSTVnNUajKWveaGLzXS8PHrtxt4P5SBO5vHFvUG4HiE2kRprG+ttc/Pz8/jVX/3VDVthWwnWilQrnBZoPp8PUgoji/ShCdb69uRODnq6zwkNOHpchFrOKsFqH9by1rkVlVDsxDbbtUabqCVO7ZvbW4yPj2N0dDQ84rDSwqyzscJ3BIETMZmD1jH30gCyS8m5CEQLvFKnU/AYbzLNs8DPhgQ978NDJfK2DUsbZbWGsNludzVSso22Wllw4NaIDLvhkVrjXuewskxMy9XfrAWucgeQXaVn3Xe1vDUk0Ea0rLfTxgZBz0jYyhW2mocYiY+OjgYSt1Ep3n4ousmXR7o8hp6X3TRL82nnQvQY6wHoZlReH1Ty1olLTs7S2OA9nz59Oljfurf7RsknwA4hcK/DeRYWrZvp6Wl3kooVaXXPaoWko3SMvDeisJmX9UgrfK/kWdRKzJtN3t5nrw7sgKMatkoQwKpXpBOeLS0tmf3gaf145O29W33TWnseeWu92brgPah0QvKOPWBZ77uSh2fLrtJvwJkIjEsuuQTA5q+wJbx2qX11YmICo6OjKBaLKJVKwWum9arEzX5n4+V5zxpK6BE988E6JsHrAGo9RSVuO3ejx3oEzonLpqamoBCMjo5iZGQkWN/cErha/6unf277I9WALAHRVWKn1MIkyS4sLGQqjtZ5e3t7hvRZaDoAKPR3+77RxF0NHinz3bNqa7EkPb18szTxmEsYsziUxHXicWFhIbNhl5IiXWXtwG1tbVhZWcno5gAy7UC/a3loh7Z59aJWbBnqbzrw6LaquhWyrSc7CVtrW4sZJvb7Vm4+B2TbqvZL6sEjIyMolUqBxKmJ82ElPN5uuWq9TBty6fVV5sNOahM2qsSzvJmOSix28pKeIO9zbGwMp0+fxsjICCYmJoJ0Yut3o3hlWx+pZjuOjop0S7QS9N1uFsQHP+h59nqx91pfW1kmgC+bxDRw/me/63m1SEpng/WWnw6WJGm1nNhhGYXC8lGXWa0tu37A/sZr2vxWGtRi5W3T9KKWbBSK1+a1TdfS1mpthxu9wrbawB+zwu0+L5bEOzs7M7tQcvDjAGglNJaBJW9L5PZYlS+sPGvJm2Wsoa2WwDWyhpE3o6OjGBoawvDwMMbGxtasutwMbLuEYgsbWBvHrRML1qqmZT47O5sZETVu2OvgFp5uuxWkbVELietxMRL3OpS9v82A9XRqISR2dLWwlKS1A2r0htf5tKxi91uJxLUNKjTcL5ZejMg9C1sJ3LbnSmVVCzZz8znNix2MbD5Zl5zY0wU+XV1dKJfLaGpqCrsW8lj2Y++5oNpGrIFgpVM91s5B8H+VTjTv/E/5xL4o+0xPT+P06dM4ceIETp48idOnTwfpxFt16bW/erGtceAeCWljBuKkw/91hZs3k6yjp90pjPCkCc8l2ypC965jSdxaG5pPPSdmrW9WvrUzqVShebJekBJ+LFRQiTWXy4V6Z4ghf29ubs5ElvCasfx6Xov+zu/epLgl7NgkVYyoPUKvhFra4MrK5qywtYRtZSh96X8sF07ujY2NoVAooFQqhZWajEyxAy8J1JsA9gY877dYeKkaATZsGUBYOapzLbqSc2VlJTwGb3h4GMePH8fx48dx4sSJTNigV9+27mPlXQu2LQ7cdh6tbHbOmNtvG5OGHWmFaKiSTj7Q9fHIzSNu75r2vM2CWn4qF+h/McK3BL6Z1rftXPbalsT1d23k7GyVBlldKMPQQkosuvTeWuO1wvNe7L1qXu27ddk9C9ES0nryVun4qampTVlhG0OMlOxArSsTh4eHMysXAaBcLocJTf5GElWJyZNMvPwQKmVZqcoO/AQl3Pb29qDV69N5uDXs5OQkhoaG8Pzzz+OZZ57BCy+8kNG+bZCFV/9eG4h99rAtBB77XScx5ubmwn92oUTsfHXFdRQngefz+czx7OTaIGznA/yois2GrURalWx4lTwTb4D0rOGtyLeXN+84JTlPNrKfSeBK3pRebEiYTljZMvPKJKbn2nZiDQ7vcVl2cKrWeW0e1otiseimd7Yx3xZe+7Tt0urQCwsLmJyczNQNgOBBl8vlYI1TZ9Yyt5FKarjZfLF8dbWntyJWDT7mnVFOxWIRhUIhI/HwQdO0vJW8h4eHw54nlrztfdRL2BZnReDf+ta38JGPfARLS0v4wAc+gNtuu62m87xMctKK5K0N2OpI1VwPjSDwVsApSGyxZc8x8q6XCCt1TE9OArISg3oOsYkvj8S3YuCJEVE1UtfPSgAkBdW9Waf2uMXFxcx+G+p92c2MvDhuz0OwUoBdI2CtO4/INQKiVvLWsvAQkx83A7YNxYwG+/K8a1rh+rtulbCwsICenh4Ui8XgKWs0kSVyNbbsvAv/U2OA8pr2bf6nr9bWVnR2dqJYLGbWmMzOzoYtYU+fPo0XXngBzz//PF544QWcOnUqs2DHDuKVrO+zQd0EvrS0hN/93d/Fd77zHezfvx+XX345rrnmGrziFa+oeF7MKqOuqaM4/1teXl6zDBXIdjS1TrWAaK01NTWFEDXrYqslZWNPiY0mwkrWs72ObZwx8rHlYv/bzNlwr9N6BFNtUPEIwMpI6pFp3WkntBNOtOi8J6V75K0DvRKNRkfYmGVrkXtWWL0dt5b2t5kSGVGpntUS9/JpSVzLjJ9V+9ZwYp6vRg3rxFq3lNY0ekWX6RMegbOtdHR0oFAoBI+AaU5MTGBoaChD3sPDw5iYmAhRJ3ZgqUc6qRV1E/iRI0dwwQUX4PzzzwcA3HDDDTh8+HBVAlfYDLPwCU5mWbdVz9dK8Dq/HsuOSGuNjUVHbGstARvfMTwX3RKedgoOYpU6fyUy1M9bYYnHUIsH5R2rA61n2XGA1s6ok9j5fD6Tjp2MUmgawCppkLy5Jah9qLZnIep7rOxraVtbQcxnA89w8rxCa4l7xoXWEeetqE/zqT66mZmtB+5Jrg9Bz+Vya1bu6svOmVH/5tO/aHmTuJ977jk888wzOH78OE6dOoXx8fHMU5SsbLLRVreibgJ/7rnncO6554bv+/fvx3/+53+uOa6WhQEx4mXH1GNYAPydlaAasS0oz2JlwdKai42UdjKm0j1YVJNL+B6zVr206RbWAm+k32oCj91XLRallgvrVwdw1pdKY9ai4uZHtMLYKe2gb40B67pzrcHs7GzG0vLaTGyiLWZYVCs/LcetkE3qhWeFeyTuhQJbw0w/d3R0hHqjPq5bTqt0pXVDUrW7B9q5EZXjNIqNA83U1FQIE3z22Wfxi1/8AsePH8fJkycxNjaGqampNTHr3v1tRr3VTeC1EpYuDCgWi7jwwgvrveSmIp/Po1Qquf+dOnUqbALUKKiU52PHjm369bUtWPLx/ovBc9GtZ6IdUCUQJXyulmtra8Py8nKw5qzergSu7rpdIl9pW9CYxWXJ2N5nrAw9Eo+ds9GwfTr23d67N6mp9WE9FoJWNSM9pqenUSqV0NHRETbGUnmFkhotb0oqjBLRBX4tLS0hnViYKstYN6UaHh7GyZMnQ6jgCy+8gJMnT2J0dDTzlB1vYIq9Ngp1E/j+/fvxzDPPhO/PPvss9u3bV/GcCy+8EE888US9l9w2XHbZZQ2X752Q5xiJx37z5AyF54l45M3POvk5NzeHfD6f0c/VFSfsE8VpfdMl5/L4ahPr3r1Y2cYOajESr1QOW4laPD9L4PZ3O2hq2es8A/dQ6e3tRblcDnHj3d3dGa/LymG68nN0dBQzMzNh5WdTU1MYBHT1p85baMz66OgoTp06hRMnToTX8PAwTp8+HR7QYAdz2yYqkbb1NOpB3QR++eWX4+jRo/jZz36Gc845Bw8//DC+/OUv15tcQoMj1rljsoH3e60EYdOJETj/X1hYCBNROllmV3Dy3cbas3N7DyWudC/6n5V5SFzWQo915EoEv13yWAzqubCMrGQBrJaDTv5qjPXIyAi6u7tRKpXQ1dWFrq4u9Pf3Y3p6GrOzsyiVSiE0mHuRjIyMYGhoCKdOncLQ0BBmZ2extLSEUqkU9HBq3axHymJ85xOFSN4nT57E0NBQiPFmJIq2h0qTlJtZP3UTeEtLCz796U/jLW95C5aWlvD+978fF1100UbmLaHBYS3ketxHS8oWlUiNeqsu/PEimdS112uodBILCYwNIDZv1pKuR8u2Fu1WIja42gHMekn0gjiHoW1C568sidMCLhQK6OzsRKlUQnd3N3bt2oWRkRH09/ejXC6HJ3NxSfvo6GiQO4aGhkKd0Xrv6OgAcMb74kpKPvaMDx8eHx/H6dOnMTQ0FIh7fHwcU1NTmQnSSjuWxt5jZVgvzioO/Oqrr17XAgFq4Y2GRsz32eS53vh+JTL7ImINupL1vZ6BwCNRDVWzUQKaniVvPScWxqqauSeReBZ2LO87xYL2EPM09H9g7cCkUpdH4jyHWrbq2XwgBGWPUqkU9Oju7u6wtzjP5UrP0dHRENoHnIkiodQyOzuLcrmM5ubmEBbIBy/w2ZV8ks7o6GiwuC1xxxZr2fKwn+1vZ0vsuZWd3GoSthxLS0t46Utfmonvf+ihhyqGh6ocoaRmZQqSqTcj703weelVmiD0FujkcrkweVUsFlEul1Eul8NiEYapMi29ztzcXKZD6/agTFsXDtnY8dhkViziySvX2ODlDUAAcMkll2zY3Ic3AGtd2ePs50r34p1jB0SWKaNICoUCCoVCWCGpS9yXl5cz8gsnF3O5HMrlMnbv3o09e/agr68PxWIxWODcn4X1y+fuzszMBFmF4YhqAMQmLBW1EHetBB6r123fjTBhZ2Ej4vuJGAFUOt6+U7O0G3l5MkbM2tdzrJTC/2w+VOeOrQ0gedsYY0vSXlxwrWWyE6FlUOm7PSdG9JU8N26VwCiimZkZTE5OBlLXcE/KLyRyznUw/HNsbAzlcjksjafsoqQ9OzsbJiY1xjxWnzHpZKuQCDwhg1rj+7cK1hpTIvC0bLXSPavVrpDT//nZDhT8TPB/JfC2trYMgav2ThnBkrf1Nrw8xFCPhl4vYvmxZax5i6XDcyzpVyJzzlEoSTNaSAdMb2VsLpcLhM54bi4OsqtrPW27Uoy/3pMtg2rluZ7jK2HLCLxeXXWrMTg4iFKpFPbPeOKJJ3D69Glcf/31OHbsGAYHB/HII4+4TzPZKrz//e/HN77xDezevRs/+tGPAKBiHu+55x488MADaG5uxic/+Um85S1viaYdc+UtdIFWoVBAX19fw8TK0yKrBYyn7+vrQ19f3ybn7OyxFTH+Co/cqxG+pw9bAudnTzYjWXvSnJ1oBlZDBRmNZAdaDSG01rbmsZLFXQsZb8aAuyUaeD266nZhcHAQTzzxBPr7+8Nvt956K3p7e3Hbbbfh0KFDGBkZwb333rttefze976HYrGI973vfYHAY3l86qmncOONN+LIkSN4/vnn8aY3vQlPP/10ZvtMxQ9+8APcdddd+Jd/+RcAZ8gfAP7oj/6oYp52Qtz5ZuDFel+1wJM5Kh1b7ZxKVGOJW9/tUnfvCTokb29SkVa83QURWLugqNI8jTfnUO2+7Pn1En5MA29a88smQHXVtra2oKs2Cg4fPoybbroJAHDTTTfha1/72rbm5w1veAN6e3szv8XyePjwYdxwww3I5/M4ePAgLrjgAhw5ciSatsb3z8/P4+GHH8Y111yzafeS0BhYj51Xj0TgTfZawlQL224aFrOg7fG6GEsfOq3n16J511Mu9aLSNbaEwD1ddbOflF0vcrkc3vzmN+PSSy8NEsGJEyfC00wGBgZw8uTJ7cyii1ge11v2Gt//8pe/HNddd12K70+oipikUA/BeZaukmc1ko19t8Rv5ylik5PeoLLee1vvebWmvSUaeK266k7A448/jn379uHkyZO46qqrduzeLbWinrJfb3w/0Jix8rXgxXpf9aDW6BnVujdislVJnLHkuvDKm2S0ZKvn2cltew3v/I3K/0ZjSyzwevZN2S4wX7t378a1116LI0eOYM+ePTh+/DiAM88W3L1793Zm0UUsj1tV9i9Wonux3le9qERCnpW5kaSlJB2ztvW4s32tF17I7EZ4JJWwJQTeKLoql9Xy87e//W1cfPHFuOaaa/Dggw8CAB588EG8/e1v385suojl8ZprrsHDDz+Mubk5/OxnP8PRo0fxmte8ZjuzmrCN+Na3voWXvexluOCCC3Do0KG60qhEcDES3wgr3L5icoe9tk2jmlRSD2JRNzYPtd5rrdgSCaVR9k05ceIErr32WgBnQo/e/e53461vfSsuv/xyXHfddXjggQdw4MABfOUrX9nWfN5444147LHHMDQ0hP379+NP/uRPcNttt7l5vOiii3DdddfhFa94BVpaWvCZz3wmGoGS8OLG0lJ9T9GKQYnGLqDyFvno52rpeGno7xpiyN8rkbg9z167EvGvJ79nMxdQl9W/shnCTMIvDRolvr8WNMIagLPBekNEaw0h9M6J/Vbvqlzv9xgRx6zpWsIbayHwSvnVPNh0YtesBB536aWXbl8YYcKLE7TovvnNb+Kpp57CQw89hKeeemq7s3VW+O53v4snn3wydJZDhw7hyiuvxNGjR3HllVfWLTvsBNQSkXT//ffjsssuw2WXXRZ+W4/UUE1XriXKQ4/zIk1seKB9fF0splvPjaXr5bGWPFdaYl9L2dUr56Sl9Al1YyP3TdmpOHz4MB577DEAZ+Lr3/jGN27rIq6zgUcI1rrUJ2j19/ejUCg0zArb9aDRnrIVW2GbCDyhbuy0fVPOFlwDkMvl8KEPfQg333xzQ6wBqBXrjUgaGhp60a5EfbHcVyLwhLpRi0XXSHixrQGwSE/RevEhEXhC3Wik+P5aUGkNwMDAwI5dA1ArGiUaLKF2pEnMhLrRKPH9taCR1wCsB1dffTWefvpp/OQnP8Htt99e9fgX60KmF8t9pTDChLPCP//zP+MP/uAPgkVXCynsRPz0pz9dswbg9ttvx/DwMK677jr84he/CPH1diOxhITtQiLwhISEhAZFklASEhISGhSJwBMSEtZgI/ZM2SkYHBzEK1/5Srz61a8OC5ROnz6Nq666Ci95yUtw1VVXYWRkZJtzWR8SgSckJGSQVtg2DhKBJyQkZNDoT9CqBTvtKVv1IhF4QkJCBo30BK1a0KhP2aoFaSFPQkJCBmmFbeMgWeAJCQkZ/DKtsAV27lO2akEi8ISEhAzSCtvGQZJQEhISMngx7ZnSKE/ZqhdpJWZCQkJCgyJJKAkJCQkNikTgCQkJCQ2KROAJCQkJDYpE4AkJCQkNikTgCQkJCQ2KROAJCQkJDYpE4AkJCQkNiv8HL6rmKGb/yrYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_plot('newmodel_temporal_img0_1000epochs.nii.gz', plot_name = 'Generated_1000epochs')\n",
    "get_plot('newmodel_temporal_img0_2000epochs.nii.gz', plot_name = 'Generated_2000epochs')\n",
    "get_plot('newmodel_temporal_img0_3000epochs.nii.gz', plot_name = 'Generated_3000epochs')\n",
    "get_plot('newmodel_temporal_img0_5000epochs.nii.gz', plot_name = 'Generated_5000epochs')\n",
    "get_plot('newmodel_temporal_lam10_img0_5000epochs.nii.gz', plot_name = 'Generated_5000epochs')\n",
    "get_plot('I269254_I989324imagedata.nii.gz', './Dataset', 'Original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbeec0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I269254_I989324imagedata.nii.gz', 'I269254_I1304069imagedata.nii.gz', 'I269254_I1501115imagedata.nii.gz', 'I269254_I1241097imagedataLMCI.nii.gz', 'I269254_I235238imagedataLMCI.nii.gz', 'I269254_I1132801imagedata.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d311d831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(2161., dtype=torch.float64),\n",
       " tensor(1921., dtype=torch.float64),\n",
       " tensor(1862., dtype=torch.float64),\n",
       " tensor(1572., dtype=torch.float64)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9311b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97be1e78",
   "metadata": {},
   "source": [
    "The following can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5edaa596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2202, 0.6590, 0.7569, 0.2700, 0.8852, 0.1977, 0.5833, 0.0263, 0.1568,\n",
       "         0.2667]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_new = torch.rand(1, 10)\n",
    "h_new = to_device(h_new, device)\n",
    "h_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e096d3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1949, -0.6692,  0.2503,  0.0041, -2.3825,  0.6658, -0.0488, -0.7783,\n",
       "          0.4877,  0.7172]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_new = torch.tensor([[1.1949, -0.6692,  0.2503,  0.0041, -2.3825,  0.6658, -0.0488, -0.7783,\n",
    "          0.4877,  0.7172]])\n",
    "h_new = to_device(h_new, device)\n",
    "h_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f47758e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 91, 109, 91])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_h_new = model.decode(h_new)\n",
    "dec_h_new = dec_h_new.squeeze(0)\n",
    "dec_h_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4b0a7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 91, 109, 91)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_h_new = torch.Tensor.cpu(dec_h_new)\n",
    "dec_h_new = dec_h_new.detach().numpy()\n",
    "dec_h_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14e684ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_h_new = dec_h_new * 1000 + 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "810b99f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_new = nib.Nifti1Image(dec_h_new, np.eye(4))\n",
    "nib.save(img_new, os.path.join('Generated', 'firstone.nii.gz'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a7742",
   "metadata": {},
   "source": [
    "Run one by one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1186d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=0\n",
    "lr=1e-5\n",
    "opt_func=torch.optim.SGD\n",
    "train_loss_history = []\n",
    "optimizer = opt_func(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6568953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 91, 109, 91, 177])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    debug_batch = batch\n",
    "    break\n",
    "debug_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d556de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_batch = debug_batch.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71ed4e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = debug_batch\n",
    "tol_time = x.size(-1) # x is of size batch_size*channel*x1*x2*x3*tol_time\n",
    "x_list = [x[:,:,:,:,:,t] for t in range(tol_time)]\n",
    "h_history = []\n",
    "gh_history = []\n",
    "mu_history = []\n",
    "t=0\n",
    "h = h0.expand(batch.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4de546bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7748, 0.9898, 0.8597, 0.2896, 0.6602, 0.5488, 0.3357, 0.4594, 0.9147,\n",
       "         0.3443],\n",
       "        [0.7748, 0.9898, 0.8597, 0.2896, 0.6602, 0.5488, 0.3357, 0.4594, 0.9147,\n",
       "         0.3443]], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "815fee43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
       "        grad_fn=<NativeBatchNormBackward0>),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
       "        grad_fn=<NativeBatchNormBackward0>),\n",
       " tensor([[-0.2693, -0.2893,  0.0075,  0.7103, -0.5138,  0.1778, -0.0302, -0.2398,\n",
       "          -0.1408,  0.2811],\n",
       "         [-0.2693, -0.2893,  0.0075,  0.7103, -0.5138,  0.1778, -0.0302, -0.2398,\n",
       "          -0.1408,  0.2811]], device='cuda:0', grad_fn=<AddmmBackward0>)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh_history.append(model.g_transform(h))\n",
    "gh_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501a2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb6111b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
